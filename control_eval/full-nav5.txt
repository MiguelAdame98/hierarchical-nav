Project Path: hierarchical-nav

Source Tree:

```txt
hierarchical-nav
├── Dockerfile
├── LICENCE
├── Oracle_Astar_algo.py
├── README.md
├── control_eval
│   ├── HierarchicalHMMBOCPD.py
│   ├── NavigationSystem.py
│   ├── __init__.py
│   ├── arguments.py
│   ├── automatic_env_run.py
│   ├── input_output.py
│   ├── keyboard_control_navigation.py
│   └── meta_controller_system.py
├── dbg
├── dommel_library
│   ├── datasets
│   │   ├── __init__.py
│   │   ├── dataset.py
│   │   ├── dataset_factory.py
│   │   ├── file_pool.py
│   │   ├── memory_pool.py
│   │   ├── transforms.py
│   │   └── utils.py
│   ├── datastructs
│   │   ├── __init__.py
│   │   ├── dict.py
│   │   └── tensor_dict.py
│   ├── distributions
│   │   ├── __init__.py
│   │   └── multivariate_normal.py
│   ├── modules
│   │   ├── __init__.py
│   │   ├── dommel_modules.py
│   │   └── visualize.py
│   ├── nn
│   │   ├── __init__.py
│   │   ├── activation.py
│   │   ├── composable_module.py
│   │   ├── convolutions.py
│   │   ├── film.py
│   │   ├── module_factory.py
│   │   ├── modules.py
│   │   ├── summary.py
│   │   └── variational.py
│   ├── setup.py
│   └── train
│       ├── __init__.py
│       ├── loss_factory.py
│       ├── losses
│       │   ├── README.md
│       │   ├── __init__.py
│       │   ├── constraint.py
│       │   ├── elbo.py
│       │   ├── log.py
│       │   ├── loss.py
│       │   └── loss_aggregate.py
│       ├── optimizer_factory.py
│       └── trainer.py
├── dreamer_mg
│   ├── LICENSE
│   ├── README.md
│   ├── bs.py
│   ├── bs2.py
│   ├── dbg
│   │   ├── cogmap
│   │   │   ├── t0015
│   │   │   │   └── snapshot_t0015.json
│   │   │   ├── t0030
│   │   │   │   └── snapshot_t0030.json
│   │   │   ├── t0045
│   │   │   │   └── snapshot_t0045.json
│   │   │   ├── t0060
│   │   │   │   └── snapshot_t0060.json
│   │   │   ├── t0075
│   │   │   │   └── snapshot_t0075.json
│   │   │   ├── t0090
│   │   │   │   └── snapshot_t0090.json
│   │   │   ├── t0105
│   │   │   │   └── snapshot_t0105.json
│   │   │   ├── t0120
│   │   │   │   └── snapshot_t0120.json
│   │   │   ├── t0135
│   │   │   │   └── snapshot_t0135.json
│   │   │   ├── t0150
│   │   │   │   └── snapshot_t0150.json
│   │   │   ├── t0165
│   │   │   │   └── snapshot_t0165.json
│   │   │   ├── t0180
│   │   │   │   └── snapshot_t0180.json
│   │   │   ├── t0195
│   │   │   │   └── snapshot_t0195.json
│   │   │   ├── t0210
│   │   │   │   └── snapshot_t0210.json
│   │   │   ├── t0225
│   │   │   │   └── snapshot_t0225.json
│   │   │   ├── t0240
│   │   │   │   └── snapshot_t0240.json
│   │   │   ├── t0255
│   │   │   │   └── snapshot_t0255.json
│   │   │   ├── t0270
│   │   │   │   └── snapshot_t0270.json
│   │   │   ├── t0285
│   │   │   │   └── snapshot_t0285.json
│   │   │   └── t0300
│   │   │       └── snapshot_t0300.json
│   │   ├── collages
│   │   └── history
│   ├── dreamer
│   │   ├── __init__.py
│   │   ├── algorithms
│   │   │   ├── dreamer.py
│   │   │   └── plan2explore.py
│   │   ├── configs
│   │   │   ├── atari-boxing-v4.yml
│   │   │   ├── dmc-acrobot-swingup.yml
│   │   │   ├── dmc-ball-in-cup-catch.yml
│   │   │   ├── dmc-cartpole-balance-sparse.yml
│   │   │   ├── dmc-cartpole-balance.yml
│   │   │   ├── dmc-cartpole-swingup-sparse.yml
│   │   │   ├── dmc-cartpole-swingup.yml
│   │   │   ├── dmc-cheetah-run.yml
│   │   │   ├── dmc-finger-spin.yml
│   │   │   ├── dmc-finger-turn-easy.yml
│   │   │   ├── dmc-finger-turn-hard.yml
│   │   │   ├── dmc-hopper-hop.yml
│   │   │   ├── dmc-hopper-stand.yml
│   │   │   ├── dmc-pendulum-swingup.yml
│   │   │   ├── dmc-quadruped-run.yml
│   │   │   ├── dmc-quadruped-walk.yml
│   │   │   ├── dmc-reacher-easy.yml
│   │   │   ├── dmc-reacher-hard.yml
│   │   │   ├── dmc-walker-run.yml
│   │   │   ├── dmc-walker-stand.yml
│   │   │   ├── dmc-walker-walk.yml
│   │   │   ├── minigrid-default-temp.yml
│   │   │   ├── minigrid-default.yml
│   │   │   └── p2e-dmc-walker-walk.yml
│   │   ├── envs
│   │   │   ├── envs.py
│   │   │   └── wrappers.py
│   │   ├── export_world_model.py
│   │   ├── modules
│   │   │   ├── actor.py
│   │   │   ├── critic.py
│   │   │   ├── decoder.py
│   │   │   ├── encoder.py
│   │   │   ├── model.py
│   │   │   └── one_step_model.py
│   │   └── utils
│   │       ├── buffer.py
│   │       └── utils.py
│   ├── dreamer_mg.egg-info
│   │   └── PKG-INFO
│   ├── main.py
│   ├── overlay_cog_env.py
│   ├── quick_watch.py
│   ├── recon_demo
│   ├── setup.py
│   ├── test1.py
│   ├── unified_test_runner.py
│   ├── viz_help.py
│   └── world_model_utils.py
├── env_specifics
│   ├── __init__.py
│   ├── env_calls.py
│   └── minigrid_maze_wt_aisles_doors
│       ├── __init__.py
│       ├── door_view.h5
│       └── minigrid_maze_modules.py
├── experiments
│   ├── GQN_v2
│   │   ├── GQN.py
│   │   ├── GQN_v2.yml
│   │   ├── models
│   │   │   ├── GQN_model.py
│   │   │   ├── __init__.py
│   │   │   ├── encoders.py
│   │   │   ├── modules.py
│   │   │   └── scene_decoder.py
│   │   └── train
│   │       ├── __init__.py
│   │       └── model_trainer.py
│   └── OZ
│       ├── OZ.py
│       ├── OZ.yml
│       ├── models
│       │   ├── __init__.py
│       │   ├── modules.py
│       │   ├── oz_base_model.py
│       │   └── oz_conv_model.py
│       └── train
│           ├── __init__.py
│           └── model_trainer.py
├── gym-minigrid_minimal-1
│   ├── LICENSE
│   ├── README.md
│   ├── build
│   │   └── lib
│   │       └── gym_minigrid
│   │           ├── __init__.py
│   │           ├── envs
│   │           │   ├── __init__.py
│   │           │   └── aisle_door_rooms.py
│   │           ├── minigrid.py
│   │           ├── register.py
│   │           ├── rendering.py
│   │           ├── roomgrid.py
│   │           ├── window.py
│   │           └── wrappers.py
│   ├── gym_minigrid
│   │   ├── __init__.py
│   │   ├── envs
│   │   │   ├── __init__.py
│   │   │   └── aisle_door_rooms.py
│   │   ├── minigrid.py
│   │   ├── register.py
│   │   ├── rendering.py
│   │   ├── roomgrid.py
│   │   ├── window.py
│   │   └── wrappers.py
│   ├── manual_control.py
│   └── setup.py
├── main.py
├── navigation_model
│   ├── HierarchicalHMMBayesianObserver.py
│   ├── Processes
│   │   ├── AIF_modules.py
│   │   ├── __init__.py
│   │   ├── exploitative_behaviour.py
│   │   ├── explorative_behaviour.py
│   │   ├── manager.py
│   │   └── motion_path_modules.py
│   ├── Services
│   │   ├── __init__.py
│   │   ├── allocentric_model.py
│   │   ├── base_perception_model.py
│   │   ├── egocentric_model.py
│   │   ├── memory_service
│   │   │   ├── __init__.py
│   │   │   ├── experience_map.py
│   │   │   ├── memory_graph.py
│   │   │   ├── memory_graph_config.yml
│   │   │   ├── modules.py
│   │   │   ├── odometry.py
│   │   │   ├── pose_cells.py
│   │   │   └── view_cells.py
│   │   └── model_modules.py
│   ├── __init__.py
│   ├── pcfg_spatial_planner.py
│   ├── pcfgtry.py
│   └── visualisation_tools.py
├── run.py
├── run_docker.sh
├── run_tests.bash
├── runs
│   ├── GQN_V2_AD
│   │   └── v2_GQN_AD_conv7x7_bi
│   │       ├── GQN.yml
│   │       └── models
│   └── OZ
│       └── OZ_AD_Col16_8_id
│           ├── OZ.yml
│           └── models
└── setup.py

```

`hierarchical-nav/Dockerfile`:

```
# Use the Huawei Ascend PyTorch image as the base image
FROM ubuntu:22.04
# Create a new user
RUN apt update && apt install -y \
    python3.10 \
    python3.10-venv \
    python3.10-distutils \
    python3-pip \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3.10 /usr/bin/python \
    && rm -rf /var/lib/apt/lists/*
RUN useradd -m 2025_06

# Set working directory
WORKDIR /app

# Copy project files
COPY . /app

# Set proper permissions so the non-root user can access files
RUN chown -R 2025_06:2025_06 /app

# Switch to root user to install dependencies
USER root

# Verify copied files
RUN ls -l /app

# Install local package
RUN pip install  /app/gym-minigrid_minimal-1

# Install additional Python dependencies
RUN pip install --no-cache-dir \
    matplotlib==3.5.3 \
    h5py==3.8.0 \
    torch==2.3.1 \
    PyYAML==6.0.1 \
    tqdm==4.66.4 \
    imageio==2.34.1 \
    pyquaternion==0.9.9 \
    dill==0.3.7 \
    gym==0.17.0 \
    imageio-ffmpeg==0.5.1 \
  && pip install --upgrade imageio imageio-ffmpeg

# Switch back to the non-root user for safety
USER 2025_06

# Default command (replace with actual entry script if needed)
CMD ["/bin/bash"]


```

`hierarchical-nav/LICENCE`:

```
MIT License

=============================================================================
Ghent University 
IDLAB of IMEC
Daria de Tinguy - daria.detinguy at ugent.be
=============================================================================
Copyright (c) 2023 Darie de Tinguy

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`hierarchical-nav/Oracle_Astar_algo.py`:

```py
import numpy as np
import gym
from gym_minigrid.wrappers import *

class Oracle_path(gym.core.ObservationWrapper):
    """
    Extract the Goal in Env and store them to count the shortest number of steps to reach objective
    """
    def __init__(self, env):
        super().__init__(env)
        self.goal_position = []
        self.door_poses_in_env = {}
        n_rooms = self.rooms_size + self.corridor_length
        room_threshold = self.rooms_size/n_rooms
        #print('rooms in row and col?', self.rooms_in_row, self.rooms_in_col)
        self.maze = [[0 for _ in range(self.height)] for _ in range(self.width)]
        if not self.goal_position:
            for pose,tile in enumerate(self.grid.grid):
                xi, yi= pose % self.width, pose // self.width
                self.maze[xi][yi] = tile
                if isinstance(tile,(Goal)):
                    self.goal_position.append((xi, yi))
                if tile.type == 'door':
                    room = [int(np.floor(xi/n_rooms)), int(np.floor(yi/n_rooms))]
                    room2 = room.copy()
                    if (xi/n_rooms) % 1 >= room_threshold and room[0] < self.rooms_in_col-1:
                        room2[0] +=1
                    if (yi/n_rooms) % 1 >= room_threshold and room[1] < self.rooms_in_row-1:
                        room2[1] +=1
                    
                    room = tuple(room)
                    room2 = tuple(room2)

                    #print('xi, yi', xi, yi, room, room2)
                    
                    if room not in self.door_poses_in_env:
                        self.door_poses_in_env[room] = []

                    if room2 not in self.door_poses_in_env:
                        self.door_poses_in_env[room2] = []

                    #print(self.door_poses_in_env)
                    self.door_poses_in_env[room].append((xi,yi))
                    self.door_poses_in_env[room2].append((xi,yi))

                    
        for room in self.door_poses_in_env.keys():
            doors = np.array(self.door_poses_in_env[room])
            #doors: R,D,L,UP
            self.door_poses_in_env[room] = [max(doors, key=lambda x: x[0]), max(doors, key=lambda x: x[1]),
                                            min(doors, key=lambda x: x[0]), min(doors, key=lambda x: x[1]), 
                                             ]

        # print('Oracle Goals ', self.goal_position)
        # print('in oracle init door_poses_in_env',self.door_poses_in_env)
        # self.steps_to_explore_env()
        # m = self.steps_to_closest_goal()
    def get_doors_poses_in_env(self):
        return self.door_poses_in_env
    
    def steps_to_explore_env(self):
        
        agent_pos = np.array([self.agent_pos[0], self.agent_pos[1], self.agent_dir])

        full_path_length = 0
        full_path = []
        visited_rooms = 1
        self.door_poses_in_env_exploration = self.door_poses_in_env.copy()
        while len(self.door_poses_in_env_exploration.keys()) > 1 and visited_rooms < self.rooms_in_row* self.rooms_in_col +1 :
            #print(visited_rooms)
            path, path_length, agent_pos = self.go_to_next_room(agent_pos)
            full_path_length += path_length
            visited_rooms += 1
            full_path.extend(path)
        path_length+= self.corridor_length + 5 #goal is door step, so we enter room + turn around

        return full_path, full_path_length
    
    def go_to_next_room(self, agent_pos):
        n_rooms = self.rooms_size + self.corridor_length
        dir_room_vector = [
            # Pointing right 
            np.array([1,0]),
            # Pointing down  
            np.array([0, 1]),
            # POinting left
            np.array([-1, 0]),
            # Pointing up
            np.array([0, -1]),
            ] 
        start_room = [int(np.floor(agent_pos[0]/n_rooms)), int(np.floor(agent_pos[1]/n_rooms))]

        #if orientation 0 or 1 then we go +1 in rooms
        if agent_pos[2] < 2 :
            #print(start_room)
            start_room = start_room + dir_room_vector[agent_pos[2]]

        adjacent_rooms = []
        for vector in dir_room_vector:
            room = start_room + vector
            #print(room, 0 <= room[0] < self.rooms_in_row, 0<= room[1] < self.rooms_in_col)
            if  0 <= room[0] < self.rooms_in_col and 0<= room[1] < self.rooms_in_row:
                adjacent_rooms.append(room)
        
        #order the adjacent room from smallest room value with borders coming first
        adjacent_rooms.sort(key=lambda x: (x[0]+ x[1]))
        adjacent_rooms =  np.array(adjacent_rooms)
        min_val_idx = np.where(adjacent_rooms == np.min([adjacent_rooms[:,0], adjacent_rooms[:,1]]))[0]
        #print('adjacent_rooms', adjacent_rooms ,'and min value index in it', min_val_idx)
        
        if len(min_val_idx) ==1 and min_val_idx[0] == 1:
            tmp = adjacent_rooms[0].copy()
            adjacent_rooms[0] = adjacent_rooms[1]
            adjacent_rooms[1] = tmp
        else :
            for i in range(1,len(min_val_idx)):
                #if NOT following or same numbers
                if not abs(min_val_idx[i-1] - min_val_idx[i]) <= 1 :
                    idx = min_val_idx[i]
                    tmp = adjacent_rooms[idx-1].copy()
                    adjacent_rooms[idx-1] = adjacent_rooms[idx]
                    adjacent_rooms[idx] = tmp
        
        #print('current room', start_room, 'adjacent_rooms',adjacent_rooms)
        
        for room in adjacent_rooms:
            #print(np.array(room- start_room), dir_room_vector)
            dir_vector_idx =  np.where((dir_room_vector == np.array(room- start_room)).all(axis=1))[0][0]
            room = tuple(room)
            if room not in self.door_poses_in_env_exploration:
                #print('room', room, 'not reachable')
                continue
            start_room = tuple(start_room)
            door_goal = self.door_poses_in_env_exploration[start_room][dir_vector_idx]
            #print('GOAL room', room, 'goal door', door_goal, 'direction', dir_vector_idx)
            break

        path = astar(self.maze, tuple(agent_pos), tuple(door_goal))
        # Calculate number of turns in path
        path_length, num_turns = include_turns_in_path_length(path)
        #print('The oracle said')
        # print('path len',len(path)-1)
        # print('n_turns', num_turns)
        # for node in path:
        #     print(node.position, node.direction)
        
        if path_length == 0: #if we are at door 
            path_length+= self.rooms_size*2
        else:
            path_length+=4
        #print('num steps:', path_length)
        agent_pos = np.array([door_goal[0], door_goal[1],dir_vector_idx])
        
        count_adjacent_rooms = 0
        for vector in dir_room_vector:
            next_room = room + vector
            #print(next_room, 0 <= next_room[0] < self.rooms_in_row, 0<= next_room[1] < self.rooms_in_col)
            if tuple(next_room) in self.door_poses_in_env_exploration:
                count_adjacent_rooms+= 1 
        #print('count_adjacent_rooms of room', room, count_adjacent_rooms)
        if count_adjacent_rooms > 1:
            #print('deleting', start_room)
            del self.door_poses_in_env_exploration[start_room]
        # print('===========')
        # print()
        return path, path_length, agent_pos
    

    def steps_to_closest_goal(self):
        agent_pos = np.array([self.agent_pos[0], self.agent_pos[1], self.agent_dir])
        
        #TODO: to review when multiple goal #ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() (shouldn't exist)
        self.goal_position.sort(key=lambda i: sort_by_distance(i, agent_pos[:2]))
        
        # print(self.goal_position)
        path = astar(self.maze, tuple(agent_pos), tuple(self.goal_position[0]))
        # Calculate number of turns in path
        path_length, num_turns = include_turns_in_path_length(path)
        print('The oracle said')
        print('path len',len(path)-1)
        print('n_turns', num_turns)
        # for node in path:
        #     print(node.position, node.direction)
        print('num steps:', path_length)
        
        return path, path_length -1

def include_turns_in_path_length(path):
    num_turns = 0
    for i in range(1, len(path)):
        if path[i].direction != path[i-1].direction:
            # print('turn',path[i-1].position,path[i].position)
            # print('dir', path[i-1].direction, path[i].direction)
            num_turns += 1
            #If we do a 180'
            if np.all(abs(np.array(path[i-1].direction)) - abs(np.array(path[i].direction)) == [0,0]):
                num_turns += 1
    
    return len(path)-1 + num_turns, num_turns
    
def sort_by_distance(goal, start):
    goal_dist = np.linalg.norm([goal  - start])
    return goal_dist

class Node():
    """A node class for A* Pathfinding"""
    def __init__(self, parent=None, position=None, dir= None):

        if dir is not None:
            dir_vector = [
            # Pointing right (positive X)
            (1, 0),
            # Down (positive Y)
            (0, 1),
            # Pointing left (negative X)
            (-1, 0),
            # Up (negative Y)
            (0, -1),
            ] 
            dir = dir_vector[dir]
        self.parent = parent
        self.position = position
        self.direction = dir  # added to store direction from parent

        self.g = 0
        self.h = 0
        self.f = 0

    def __eq__(self, other):
        return self.position == other.position


def astar(maze, start, end):
    """Returns a list of tuples as a path from the given start to the given end in the given maze"""       
    # Create start and end node
    
    start_node = Node(None, start[:2], dir=start[2])
    start_node.g = start_node.h = start_node.f = 0
    end_node = Node(None, end)
    end_node.g = end_node.h = end_node.f = 0

    # Initialize both open and closed list
    open_list = []
    closed_list = []

    # Add the start node
    open_list.append(start_node)

    # Loop until you find the end
    while len(open_list) > 0:
        
        # Get the current node
        # if end_node in open_list:
        #     current_index = open_list.index(end_node)
        #     current_node = open_list[current_index]
        # else:
        
        current_node = open_list[0]
        current_index = 0
        for index, item in enumerate(open_list):
            if item.f < current_node.f:
                current_node = item
                current_index = index

        # del current off open list, add to closed list
        del open_list[current_index]
        closed_list.append(current_node)
        #print('current node',current_node.position)
        # Found the goal
        if current_node == end_node:
            #print('found goal')
            path = []
            current = current_node

            while current is not None:
                path.append(current)
                current = current.parent

            
            return path[::-1]  # Return reversed path 

        #This part actually ensure we first try to go straight before turning
        directions_next_node = [(0, -1), (-1, 0), (0,1) ,(1, 0)]
        # if current_node.direction != None:
        #     directions_next_node.remove(current_node.direction)
        #     directions_next_node.insert(0,current_node.direction )
            #print(current_node.position, current_node.direction, directions_next_node)
        for new_position in directions_next_node:  # Only allow orthogonal movement

            # Get node position
            node_position = (current_node.position[0] + new_position[0], current_node.position[1] + new_position[1])

            # Make sure within range (if no walls to delimit plan)
            # if node_position[0] > (len(maze) - 1) or node_position[0] < 0 or node_position[1] > (len(maze[0]) -1) or node_position[1] < 0:
            #     continue

            # Make sure walkable terrain
            if maze[node_position[0]][node_position[1]].type == 'wall':
                #print('wall at position', node_position[0], node_position[1])
                continue

            # Create new node
            new_node = Node(current_node, node_position)
            
            # Store direction from parent
            #if current_node.parent:
            x_diff = new_node.position[0] - current_node.position[0]
            y_diff = new_node.position[1] - current_node.position[1]
            # print('xdiff and y diff of new node', current_node.position, new_node.position, x_diff, y_diff)
            if x_diff > 0:
                new_node.direction = (0,1)
            elif x_diff < 0:
                new_node.direction = (0,-1)
            elif y_diff > 0:
                new_node.direction = (1,0)
            elif y_diff < 0:
                new_node.direction = (-1,0)

            # Calculate f, g, and h values
            t = 0
            #if we need to turn, we add a weigth to it
            if current_node.direction != new_node.direction: 
                t+=1
            new_node.g = current_node.g + 1 + t
            new_node.h = abs(new_node.position[0] - end_node.position[0]) + abs(new_node.position[1] - end_node.position[1])
            #print('new_node.h', new_node.position, new_node.h )
            new_node.f = new_node.g + new_node.h

            # Check if node is already in closed list
            for closed_node in closed_list:
                if new_node == closed_node:
                    continue

            # Check if node is already in open list
            for open_node in open_list:
                if new_node == open_node and new_node.g > open_node.g:
                    continue

            # Add the new node to the open list
            #print('show me the appended node', new_node.position)
            open_list.append(new_node)

def main():
    #test maze must be minigrid env now
    # maze = [[0, 0, 0, 0, 1, 1, 0, 0, 0, 0,1],
    #         [0, 0, 0, 0, 1, 1, 0, 0, 0, 0,1],
    #         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,1],
    #         [1, 1, 0, 1, 1, 1, 1, 0, 1, 1,1],
    #         [0, 1, 0, 1, 1, 0, 1, 0, 1, 0,1],
    #         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,1],
    #         [0, 0, 0, 0, 1, 1, 0, 0, 0, 0,1],
    #         [0, 0, 0, 0, 1, 0, 0., 0, 0, 0,1],
    #         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0,1],
    #         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,1]]

    start = (0, 0)
    end = (7, 6)

    path, n_turns = astar(maze, start, end)
    print('path len',len(path))
    for node in path:
        #print('node',node)
        print(node.position)


if __name__ == '__main__':
    main()
```

`hierarchical-nav/README.md`:

```md
## La forma en la que lo hice funcionar
Primero clone el github del proyecto y me puse adentro de la carpeta copiada

Despues hice un virtual environment
```
python3.11 -m venv 11_venv         
source 11_venv/bin/activate
```

Despues me cambie al folder de gym_minigrid
```
cd gym-minigrid_minimal-1
```
Y despues instale todo lo que estaba dentro
```
pip install .
```

Despues me salgo de esa carpeta y descargo otras librerias importantes
```
cd ..
pip install matplotlib==3.9.0 
pip install h5py==3.11.0
pip install torch==2.3.1
pip install PyYAML==6.0.1
pip install tqdm==4.66.4
pip install imageio==2.34.1 
pip install pyquaternion==0.9.9
pip install dill==0.3.8
pip install gym==0.17.0
pip install imageio-ffmpeg==0.5.1
pip install --upgrade imageio imageio-ffmpeg
```
Despues solo tienes que cambiar el documento "input_output.py" en la parte de 
```

#TODO:MAKE THIS MORE MODULAR
def create_saving_directory(directory:str)-> str:
    """NOTE TIS IS PARTICULAR TO ONE INDIVIDUAL"""
    if os.path.exists('/Users/lab25'):
        dir = '/Users/lab25/Documents/hierarchical_st_nav_aif/' 
    else:
        dir = '/Users/lab25/hierarchical_st_nav_aif/' 

    home_dir = dir + directory 
           
    create_directory(home_dir)

    return home_dir

def create_directory(directory:str)->bool:
    try:
        os.makedirs(directory)
        return True
    except FileExistsError:
        return False
```
A la direccion donde quieres que se guarden las cosas en tu equipo.
Y el unico script que necesitamos ahora para probar funcionalidad es este 
```
python run.py --env 4-tiles-ad-rooms --seed 218 --rooms_in_row 3 --rooms_in_col 4 --test exploration --video
```
## Description

This repository is the source code of the 3 layered hierarchical model.

Information can be found on our Smart Robot blog [Hierarchy to navigate](https://thesmartrobot.github.io/2023/09/25/Spatial-Tempo-Hactinf.html)
or in our [article]( 	
https://doi.org/10.48550/arXiv.2312.05058)


Basic usage shows how to run the code for a demonstration. 
This condensed version has not been tested for a full training. 

## Basic Usage

To run a simulation in minigrid maze environments:
```
python run.py --env 4-tiles-ad-rooms --seed 218 --rooms_in_row 3 --rooms_in_col 4 --test key --video
```
<p align="center">
  <img src="img/3x4_s218.png" width="300" title="3x4 env seed 218">
  
</p>
#==== ENV related arguments ====# 

`--env`(str): Select the desired environment among
```
4-tiles-ad-rooms
5-tiles-ad-rooms
6-tiles-ad-rooms
7-tiles-ad-rooms
8-tiles-ad-rooms
```
The environments are generated randomly (rooms colours, configuration and white tile(s) positions)\
`--seed`(int): Between -1 and max system's value to select the environment generated. -1 will generate a new random configuration at each new call.\
`--rooms_in_row`(int): How many rooms we want to generate on a row\
`--rooms_in_col`(int): How many rooms we want to generate on a col

#==== Model arguments ====#\
`--allo_config`(str): path to the allocentric model to load as yaml\
`--memory_config`(str): path to the memory_graph_config as yaml\
`--memory_load`(str):path to a .map memory, None by default, the agent starts without prior on teh environment\
`--lookahead`(int): how many discrete steps ahead does the agent projects its imagination 

#==== TESTS arguments ====#\
`--video`: if we have this argument, we record what the agent is doing at each step as well as what is imagined.\
`--save_dir`(str): where we want to save dir. NOTE: currently adapted for a personal usage (not generalised), can be modified in input_output.py\
`--test`(str): Select the desired test run among :
```
key : Manual keyboard control of the agent navigation (we have a direct visual of the env) 
exploration: The agent autonomously aims to explore all the rooms of the environment. 
goal: The agent autonomously aims to reach a white tile 
```
The autonomous tasks can be agenced together such as 'exploration_then_goal'
NOTE: For the Goal, Another colour can be set, currently in code in keyboard_control_navigation as `preferred_colour_range`, However the Oracle currently search for a 'Goal' like tile to identify the goal. It can be adapted in Oracle or in Minigrid Goal setup colour.


The main.py is used to train and evaluate the models. 
However it might not be usable as it is because of migration issue. The usual usage is meant to be:

```
python3 main.py evaluate experiments/GQN_v2/GQN.py config=xxxx/GQN.yml learn_steps=20 n_sample=3 lookahead=20 lookahead_ratio=False unique_csv=Truev show_img=False save_img_seq=True scenario=xxx
```
# Requirements
Please run the requirements file 

To install locally:

git clone https://github.com/my-name-is-D/gym_minigrid_minimal.git
cd /your_repo/gym_minigrid_minimal && \
pip install -e . 

To install on cluster:
git clone -b master https://$DML_DEPLOY_TOKEN:$DML_DEPLOY_KEY@gitlab.ilabt.imec.be/ddtinguy/hierarchical_st_nav_aif.git && \
cd hierarchical_st_nav_aif && \
pip install -e .

# Environments

Currently strict minimum in repo, would be great to have a submodule instead (once disponible)

# Minimalistic Gridworld Environment (MiniGrid)

[![Build Status](https://travis-ci.org/maximecb/gym-minigrid.svg?branch=master)](https://travis-ci.org/maximecb/gym-minigrid)

There are other gridworld Gym environments out there, but this one is
designed to be particularly simple, lightweight and fast. The code has very few
dependencies, making it less likely to break or fail to install. It loads no
external sprites/textures, and it can run at up to 5000 FPS on a Core i7
laptop, which means you can run your experiments faster. A known-working RL
implementation can be found [in this repository](https://github.com/lcswillems/torch-rl).

Requirements:
- Python 3.5+
- OpenAI Gym
- NumPy
- Matplotlib (optional, only needed for display)

Please use this bibtex if you want to cite this repository in your publications:

```
@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}
```

List of publications & submissions using MiniGrid or BabyAI (please open a pull request to add missing entries):
- [In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications](https://openreview.net/pdf?id=rUwm9wCjURV) (Imperial College London, ICLR 2022)
- [Interesting Object, Curious Agent: Learning Task-Agnostic Exploration](https://arxiv.org/abs/2111.13119) (Meta AI Research, NeurIPS 2021)
- [Safe Policy Optimization with Local Generalized Linear Function Approximations](https://arxiv.org/abs/2111.04894) (IBM Research, Tsinghua University, NeurIPS 2021)
- [A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning](https://arxiv.org/abs/2106.02097) (Mila, McGill University, 2021)
- [SPOTTER: Extending Symbolic Planning Operators through Targeted Reinforcement Learning](http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1118.pdf) (Tufts University, SIFT, AAMAS 2021)
- [Grid-to-Graph: Flexible Spatial Relational Inductive Biases for Reinforcement Learning](https://arxiv.org/abs/2102.04220) (UCL, AAMAS 2021)
- [Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments](https://openreview.net/forum?id=MtEE0CktZht) (Texas A&M University, Kuai Inc., ICLR 2021)
- [Adversarially Guided Actor-Critic](https://openreview.net/forum?id=_mQp5cr_iNy) (INRIA, Google Brain, ICLR 2021)
- [Information-theoretic Task Selection for Meta-Reinforcement Learning](https://papers.nips.cc/paper/2020/file/ec3183a7f107d1b8dbb90cb3c01ea7d5-Paper.pdf) (University of Leeds, NeurIPS 2020)
- [BeBold: Exploration Beyond the Boundary of Explored Regions](https://arxiv.org/pdf/2012.08621.pdf) (UCB, December 2020)
- [Approximate Information State for Approximate Planning and Reinforcement Learning in Partially Observed Systems](https://arxiv.org/abs/2010.08843) (McGill, October 2020)
- [Prioritized Level Replay](https://arxiv.org/pdf/2010.03934.pdf) (FAIR, October 2020)
- [AllenAct: A Framework for Embodied AI Research](https://arxiv.org/pdf/2008.12760.pdf) (Allen Institute for AI, August 2020)
- [Learning with AMIGO: Adversarially Motivated Intrinsic Goals](https://arxiv.org/pdf/2006.12122.pdf) (MIT, FAIR, ICLR 2021)
- [RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments](https://openreview.net/forum?id=rkg-TJBFPB) (FAIR, ICLR 2020)
- [Learning to Request Guidance in Emergent Communication](https://arxiv.org/pdf/1912.05525.pdf) (University of Amsterdam, Dec 2019)
- [Working Memory Graphs](https://arxiv.org/abs/1911.07141) (MSR, Nov 2019)
- [Fast Task-Adaptation for Tasks Labeled Using
Natural Language in Reinforcement Learning](https://arxiv.org/pdf/1910.04040.pdf) (Oct 2019, University of Antwerp)
- [Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
](https://arxiv.org/abs/1910.12911) (MSR, NeurIPS, Oct 2019)
- [Recurrent Independent Mechanisms](https://arxiv.org/pdf/1909.10893.pdf) (Mila, Sept 2019) 
- [Learning Effective Subgoals with Multi-Task Hierarchical Reinforcement Learning](http://surl.tirl.info/proceedings/SURL-2019_paper_10.pdf) (Tsinghua University, August 2019)
- [Mastering emergent language: learning to guide in simulated navigation](https://arxiv.org/abs/1908.05135) (University of Amsterdam, Aug 2019)
- [Transfer Learning by Modeling a Distribution over Policies](https://arxiv.org/abs/1906.03574) (Mila, June 2019)
- [Reinforcement Learning with Competitive Ensembles
of Information-Constrained Primitives](https://arxiv.org/abs/1906.10667) (Mila, June 2019)
- [Learning distant cause and effect using only local and immediate credit assignment](https://arxiv.org/abs/1905.11589) (Incubator 491, May 2019)
- [Practical Open-Loop Optimistic Planning](https://arxiv.org/abs/1904.04700) (INRIA, April 2019)
- [Learning World Graphs to Accelerate Hierarchical Reinforcement Learning](https://arxiv.org/abs/1907.00664) (Salesforce Research, 2019)
- [Variational State Encoding as Intrinsic Motivation in Reinforcement Learning](https://mila.quebec/wp-content/uploads/2019/05/WebPage.pdf) (Mila, TARL 2019)
- [Unsupervised Discovery of Decision States Through Intrinsic Control](https://tarl2019.github.io/assets/papers/modhe2019unsupervised.pdf) (Georgia Tech, TARL 2019)
- [Modeling the Long Term Future in Model-Based Reinforcement Learning](https://openreview.net/forum?id=SkgQBn0cF7) (Mila, ICLR 2019)
- [Unifying Ensemble Methods for Q-learning via Social Choice Theory](https://arxiv.org/pdf/1902.10646.pdf) (Max Planck Institute, Feb 2019)
- [Planning Beyond The Sensing Horizon Using a Learned Context](https://personalrobotics.cs.washington.edu/workshops/mlmp2018/assets/docs/18_CameraReadySubmission.pdf) (MLMP@IROS, 2018)
- [Guiding Policies with Language via Meta-Learning](https://arxiv.org/abs/1811.07882) (UC Berkeley, Nov 2018)
- [On the Complexity of Exploration in Goal-Driven Navigation](https://arxiv.org/abs/1811.06889) (CMU, NeurIPS, Nov 2018)
- [Transfer and Exploration via the Information Bottleneck](https://openreview.net/forum?id=rJg8yhAqKm) (Mila, Nov 2018)
- [Creating safer reward functions for reinforcement learning agents in the gridworld](https://gupea.ub.gu.se/bitstream/2077/62445/1/gupea_2077_62445_1.pdf) (University of Gothenburg, 2018)
- [BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop](https://arxiv.org/abs/1810.08272) (Mila, ICLR, Oct 2018)

This environment has been built as part of work done at [Mila](https://mila.quebec). The Dynamic obstacles environment has been added as part of work done at [IAS in TU Darmstadt](https://www.ias.informatik.tu-darmstadt.de/) and the University of Genoa for mobile robot navigation with dynamic obstacles.


## Wrappers

MiniGrid is built to support tasks involving natural language and sparse rewards.
The observations are dictionaries, with an 'image' field, partially observable
view of the environment, a 'mission' field which is a textual string
describing the objective the agent should reach to get a reward, and a 'direction'
field which can be used as an optional compass. Using dictionaries makes it
easy for you to add additional information to observations
if you need to, without having to encode everything into a single tensor.

There are a variery of wrappers to change the observation format available in [gym_minigrid/wrappers.py](/gym_minigrid/wrappers.py). If your RL code expects one single tensor for observations, take a look at
`FlatObsWrapper`. There is also an `ImgObsWrapper` that gets rid of the 'mission' field in observations,
leaving only the image field tensor.

Please note that the default observation format is a partially observable view of the environment using a
compact and efficient encoding, with 3 input values per visible grid cell, 7x7x3 values total.
These values are **not pixels**. If you want to obtain an array of RGB pixels as observations instead,
use the `RGBImgPartialObsWrapper`. You can use it as follows:

```
from gym_minigrid.wrappers import *
env = gym.make('MiniGrid-Empty-8x8-v0')
env = RGBImgPartialObsWrapper(env) # Get pixel observations
env = ImgObsWrapper(env) # Get rid of the 'mission' field
obs = env.reset() # This now produces an RGB tensor only
```

## Design

Structure of the world:
- The world is an NxM grid of tiles
- Each tile in the grid world contains zero or one object
  - Cells that do not contain an object have the value `None`
- Each object has an associated discrete color (string)
- Each object has an associated type (string)
  - Provided object types are: wall, floor, lava, door, key, ball, box and goal
- The agent can pick up and carry exactly one object (eg: ball or key)
- To open a locked door, the agent has to be carrying a key matching the door's color

Actions in the basic environment:
- Turn left
- Turn right
- Move forward
- Pick up an object
- Drop the object being carried
- Toggle (open doors, interact with objects)
- Done (task completed, optional)

Default tile/observation encoding:
- Each tile is encoded as a 3 dimensional tuple: (OBJECT_IDX, COLOR_IDX, STATE) 
- OBJECT_TO_IDX and COLOR_TO_IDX mapping can be found in [gym_minigrid/minigrid.py](gym_minigrid/minigrid.py)
- e.g. door STATE -> 0: open, 1: closed, 2: locked

By default, sparse rewards are given for reaching a green goal tile. A
reward of 1 is given for success, and zero for failure. There is also an
environment-specific time step limit for completing the task.
You can define your own reward function by creating a class derived
from `MiniGridEnv`. Extending the environment with new object types or new actions
should be very easy. If you wish to do this, you should take a look at the
[gym_minigrid/minigrid.py](gym_minigrid/minigrid.py) source file.

## Included Environments

The environments listed below are implemented in the [gym_minigrid/envs](/gym_minigrid/envs) directory.
Each environment provides one or more configurations registered with OpenAI gym. Each environment
is also programmatically tunable in terms of size/complexity, which is useful for curriculum learning
or to fine-tune difficulty.

### Four rooms environment

Registered configurations:
- `MiniGrid-FourRooms-v0`

<p align="center">
<img src="/figures/four-rooms-env.png" width=380>
</p>

Classic four room reinforcement learning environment. The agent must navigate
in a maze composed of four rooms interconnected by 4 gaps in the walls. To
obtain a reward, the agent must reach the green goal square. Both the agent
and the goal square are randomly placed in any of the four rooms.

```

`hierarchical-nav/control_eval/HierarchicalHMMBOCPD.py`:

```py
import numpy as np
from collections import defaultdict, deque
from typing import Dict, List, Tuple, Optional
import time
import torch
from scipy import stats

class TrueHierarchicalHMMWithBOCPD:
    """
    True Hierarchical HMM where hidden states are (mode, submode) pairs,
    enhanced with Bayesian Online Changepoint Detection (BOCPD).
    This creates natural hierarchical structure with proper transition dynamics
    and adaptive changepoint detection.
    """
    def __init__(self, use_submodes: bool = False, use_bocpd: bool = True):
        """
        Simplified 4-mode configuration (EXPLORE, NAVIGATE, RECOVER, TASK_SOLVING).
        By default submodes are disabled to make the model tractable and debuggable.
        Set `use_submodes=True` if you want the original hierarchical (mode, submode) space.
        """
        self.use_submodes = use_submodes
        self.use_bocpd = use_bocpd

        self.current_mode='EXPLORE'
        # ---- Minimal 4-mode state space (call in __init__) -----------------
        self.states = ['EXPLORE', 'NAVIGATE', 'RECOVER', 'TASK_SOLVING']  # <- no submodes
        self.n_states = len(self.states)
        self.state_index = {m: i for i, m in enumerate(self.states)}
        self.state_beliefs = np.ones(self.n_states, dtype=float) / self.n_states

        # Optional gates + smoothing defaults (safe if previously absent)
        self.task_enter_gate = 1.0     # multiplier into TASK_SOLVING
        self.task_exit_gate  = 1.0     # multiplier out of TASK_SOLVING
        self.task_gate_fn    = None    # optional callable(self, evidence, tp_smooth) -> (enter, exit)
        self.task_progress_smooth = 0.0

        # Stickiness tuning knobs (used below)
        self.stickiness_decay_non_task = 0.40
        self.stickiness_task_bonus     = 0.03

        # Build your initial transition
        self.transition_matrix = self._build_transition_matrix()
        self.emission_params = self._build_emission_params()

        # === Beliefs ==========================================================
        self.state_beliefs = np.ones(self.n_states, dtype=float) / self.n_states
        self.state_history = deque(maxlen=500)

        # === BOCPD ============================================================
        self.max_run_length = 50
        self.run_length_dist = np.zeros(self.max_run_length + 1, dtype=float)
        self.run_length_dist[0] = 1.0
        self.hazard_rate = 1.0 / 10.0
        self.changepoint_threshold = 0.1

        # Evidence buffers
        self.evidence_buffer = deque(maxlen=100)
        self.evidence_history = deque(maxlen=self.max_run_length)
        self.mode_history = deque(maxlen=100)

        # Counters / diagnostics
        self.step_counter = 0
        self.stagnation_counter = 0
        self.exploration_attempts = 0
        self.navigation_attempts = 0
        self.recovery_attempts = 0
        self._seen_node_ids: set[str] = set()   # places we've seen historically
        self._last_node_id: str | None = None   # for most-recent switch detection
        # If no node IDs are in the replay buffer, we quantize (x,y) to a grid:
        self._node_quantize_m: float = 0.75     # grid size (meters) for fallback place IDs

        self.debug_evidence=True
        self.debug_emissions=True

        self.prev_evidence = None
        self.params = {
            'loop_threshold': 20,
            'stagnation_time_threshold': 30.0,
            'stagnation_step_threshold': 50,
            'max_exploration_cycles': 3,
            'max_navigation_cycles': 2,
            'max_recovery_cycles': 2
        }

    def _build_transition_matrix(self) -> np.ndarray:
        """
        4x4 transition matrix with high diagonals (stickiness) and gate-controlled
        off-diagonals. Rows are normalized; safe fallback to uniform if needed.
        """
        n = self.n_states
        T = np.zeros((n, n), dtype=float)
        for i, fm in enumerate(self.states):
            row = np.zeros(n, dtype=float)
            # stay
            row[i] = self._mode_stickiness(fm)
            # move to others
            for j, tm in enumerate(self.states):
                if j == i: 
                    continue
                row[j] = self._get_inter_mode_transition(fm, tm)

            # normalize safely
            s = float(row.sum())
            if not np.isfinite(s) or s <= 0.0:
                row[:] = 1.0 / n
            else:
                row /= s
            T[i, :] = row

        # final sanity
        if not np.all(np.isfinite(T)):
            print("[WARN] Non-finite entries in T; forcing uniform.")
            T[:] = 1.0 / n
        return T    
    def _logpdf_normal_np(self, x: float, mu: float, sigma: float) -> float:
        """NumPy-only normal logpdf (fallback when SciPy is unavailable)."""
        sigma = max(float(sigma), 1e-6)
        z = (float(x) - float(mu)) / sigma
        return float(-0.5*np.log(2.0*np.pi) - np.log(sigma) - 0.5*z*z)

    def _gauss_logpdf(self, x: float, mean: float, std: float) -> float:
        """Try SciPy; otherwise use the NumPy fallback."""
        try:
            from scipy.stats import norm
            return float(norm.logpdf(x, loc=mean, scale=max(std, 1e-6)))
        except Exception:
            return self._logpdf_normal_np(x, mean, std)
    def _build_hierarchical_transition_matrix(self) -> np.ndarray:
        """
        Build transition matrix with hierarchical structure. When submodes are disabled
        this reduces to a 4x4 mode-level HMM with strong diagonals ("stickiness").
        """
        T = np.zeros((self.n_states, self.n_states), dtype=float)

        for i, (from_mode, from_submode) in enumerate(self.hierarchical_states):
            row = np.zeros(self.n_states, dtype=float)

            for j, (to_mode, to_submode) in enumerate(self.hierarchical_states):
                if from_mode == to_mode:
                    if from_submode == to_submode:
                        row[j] += self._get_submode_persistence(from_mode, from_submode)
                    else:
                        row[j] += self._get_intra_mode_transition(from_mode, from_submode, to_submode)
                else:
                    row[j] += self._get_inter_mode_transition(from_mode, from_submode, to_mode, to_submode)

            # Ensure positivity and normalise
            row = np.maximum(row, 1e-12)
            row_sum = row.sum()
            if row_sum <= 0.0:
                # Fallback to uniform
                row[:] = 1.0 / self.n_states
            else:
                row /= row_sum
            T[i, :] = row

        return T
    
    
    def _get_intra_mode_transition(self, mode: str, from_sub: str = None, to_sub: str = None) -> float:
        """
        Submode transitions are disabled (no submodes anymore).
        Keep signature for compatibility; always returns 0.
        """
        return 0.0
    
    def _get_inter_mode_transition(self, from_mode: str, to_mode: str) -> float:
        """
        Unnormalized inter-mode transition weight with TASK_SOLVING gates applied.
        """
        base = {
            'EXPLORE':      {'NAVIGATE': 0.45, 'RECOVER': 0.09, 'TASK_SOLVING': 0.01},
            'NAVIGATE':     {'EXPLORE': 0.79, 'RECOVER': 0.08, 'TASK_SOLVING': 0.01},
            'RECOVER':      {'EXPLORE': 0.80, 'NAVIGATE': 0.20, 'TASK_SOLVING': 0.01},
            'TASK_SOLVING': {'EXPLORE': 0.05, 'NAVIGATE': 0.01, 'RECOVER': 0.09},
        }
        w = float(base.get(from_mode, {}).get(to_mode, 0.0))
        if w <= 0.0:
            return 0.0

        if to_mode == 'TASK_SOLVING' and from_mode != 'TASK_SOLVING':
            w *= float(max(self.task_enter_gate, 1e-6))
        if from_mode == 'TASK_SOLVING' and to_mode != 'TASK_SOLVING':
            w *= float(max(self.task_exit_gate, 1e-6))

        return float(max(w, 0.0))
    
    def set_task_solving_gate(self, enter: float = None, exit: float = None, fn=None) -> None:
        """
        Configure how strongly the model moves into/out of TASK_SOLVING.
        - enter: scalar multiplier for transitions INTO TASK_SOLVING  (default 1.0)
        - exit : scalar multiplier for transitions OUT OF TASK_SOLVING (default 1.0)
        - fn   : optional callable (self, evidence, tp_smooth) -> (enter, exit)
                If provided, it overrides fixed gates and will be called each update().

        Example:
            hhmm.set_task_solving_gate(enter=0.02, exit=0.6)  # very reluctant to enter
            hhmm.set_task_solving_gate(fn=my_gate)            # use a custom schedule
        """
        if enter is not None:
            self.task_enter_gate = float(np.clip(enter, 1e-6, 1e6))
        if exit is not None:
            self.task_exit_gate  = float(np.clip(exit,  1e-6, 1e6))
        if fn is not None and callable(fn):
            self.task_gate_fn = fn
    def _build_emission_params(self) -> Dict[str, Dict]:
        """
        Mode-keyed emission parameters (no submodes).
        Means/σ assume evidence features are roughly in [0,1].
        Tune from logs once the pipeline runs.
        """
        params = {
            'EXPLORE': {
                'lost_prob': 0.20,
                'loop_prob': 0.20,
                'stagnation':               {'mean': 0.25, 'std': 0.15},
                'info_gain':                {'mean': 0.45, 'std': 0.22},
                'exploration_productivity': {'mean': 0.40, 'std': 0.25},
                'known_switch_prob': 0.15,                        # EXPLORE expects NOT switching into known places
                'known_revisit_rate': {'mean': 0.05, 'std': 0.12} # revisit rate should be low in exploration

            },
            'NAVIGATE': {
                'lost_prob': 0.15,
                'loop_prob': 0.12,
                'stagnation':          {'mean': 0.25, 'std': 0.18},
                'navigation_progress': {'mean': 0.70, 'std': 0.25},
                'known_switch_prob': 0.70,                        # NAVIGATE often switches between known nodes
                'known_revisit_rate': {'mean': 0.60, 'std': 0.25} # higher revisit ratio during nav is fine

            },
            'RECOVER': {
                'lost_prob': 0.85,
                'loop_prob': 0.60,
                'stagnation':             {'mean': 0.65, 'std': 0.18},
                'recovery_effectiveness': {'mean': 0.60, 'std': 0.18},
            },
            'TASK_SOLVING': {
                'lost_prob': 0.12,
                'loop_prob': 0.12,
                'stagnation':    {'mean': 0.12, 'std': 0.08},
                'task_progress': {'mean': 0.65, 'std': 0.18},   # swap to 'plan_progress' if that’s your signal
            },
        }
        return params

    
    def compute_emission_likelihood(self, evidence: Dict, state: str) -> float:
        """
        Emission likelihood p(evidence | state=mode).
        - 'state' is a MODE string: one of {'EXPLORE','NAVIGATE','RECOVER','TASK_SOLVING'}.
        - Uses Bernoulli terms for booleans (agent_lost, loop_detected) when configured.
        - Uses Gaussian log-likelihoods (SciPy) for continuous features when both
        the evidence key and the param key exist.
        - Missing features contribute neutrally (no penalty/bonus).
        - Returns a probability in (0, 1], floored to avoid exact zeros.
        """

        # Guard: must have parameters for this mode
        if not hasattr(self, 'emission_params') or state not in self.emission_params:
            return 1e-10

        params = self.emission_params[state]
        log_ll = 0.0
        contrib_dbg = []  # optional debug contributions

        # --- Universal Bernoulli features (if present) --------------------
        if 'agent_lost' in evidence and 'lost_prob' in params:
            p = float(np.clip(params['lost_prob'], 1e-10, 1 - 1e-10))
            term = np.log(p if evidence['agent_lost'] else (1 - p))
            log_ll += term
            if getattr(self, 'debug_emissions', False):
                contrib_dbg.append(('agent_lost', evidence['agent_lost'], p, term))

        if 'loop_detected' in evidence and 'loop_prob' in params:
            p = float(np.clip(params['loop_prob'], 1e-10, 1 - 1e-10))
            term = np.log(p if evidence['loop_detected'] else (1 - p))
            log_ll += term
            if getattr(self, 'debug_emissions', False):
                contrib_dbg.append(('loop_detected', evidence['loop_detected'], p, term))

        # Optional: stagnation as Gaussian if provided
        if 'stagnation' in evidence and 'stagnation' in params:
            x   = float(evidence['stagnation'])
            mu  = float(params['stagnation']['mean'])
            sig = max(float(params['stagnation']['std']), 1e-3)
            term = stats.norm.logpdf(x, mu, sig)
            log_ll += term
            if getattr(self, 'debug_emissions', False):
                contrib_dbg.append(('stagnation', (x, mu, sig), None, term))

        if 'recent_known_switch' in evidence and 'known_switch_prob' in params:
            p = float(np.clip(params['known_switch_prob'], 1e-10, 1 - 1e-10))
            term = np.log(p if evidence['recent_known_switch'] else (1 - p))
            log_ll += term
            if getattr(self, 'debug_emissions', False):
                contrib_dbg.append(('recent_known_switch', evidence['recent_known_switch'], p, term))


        if 'known_revisit_rate' in evidence and 'known_revisit_rate' in params:
            x   = float(evidence['known_revisit_rate'])
            mu  = float(params['known_revisit_rate']['mean'])
            sig = max(float(params['known_revisit_rate']['std']), 1e-3)
            term = stats.norm.logpdf(x, mu, sig)
            log_ll += term
            if getattr(self, 'debug_emissions', False):
                contrib_dbg.append(('known_revisit_rate', (x, mu, sig), None, term))
        # --- Mode-specific continuous features ----------------------------
        if state == 'EXPLORE':
            if 'info_gain' in evidence and 'info_gain' in params:
                x   = float(evidence['info_gain'])
                mu  = float(params['info_gain']['mean'])
                sig = max(float(params['info_gain']['std']), 1e-3)
                term = stats.norm.logpdf(x, mu, sig)
                log_ll += term
                if getattr(self, 'debug_emissions', False):
                    contrib_dbg.append(('info_gain', (x, mu, sig), None, term))
            if 'exploration_productivity' in evidence and 'exploration_productivity' in params:
                x   = float(evidence['exploration_productivity'])
                mu  = float(params['exploration_productivity']['mean'])
                sig = max(float(params['exploration_productivity']['std']), 1e-3)
                term = stats.norm.logpdf(x, mu, sig)
                log_ll += term
                if getattr(self, 'debug_emissions', False):
                    contrib_dbg.append(('exploration_productivity', (x, mu, sig), None, term))

        elif state == 'NAVIGATE':
            if 'navigation_progress' in evidence and 'navigation_progress' in params:
                x   = float(evidence['navigation_progress'])
                mu  = float(params['navigation_progress']['mean'])
                sig = max(float(params['navigation_progress']['std']), 1e-3)
                term = stats.norm.logpdf(x, mu, sig)
                log_ll += term
                if getattr(self, 'debug_emissions', False):
                    contrib_dbg.append(('navigation_progress', (x, mu, sig), None, term))

        elif state == 'RECOVER':
            if 'recovery_effectiveness' in evidence and 'recovery_effectiveness' in params:
                x   = float(evidence['recovery_effectiveness'])
                mu  = float(params['recovery_effectiveness']['mean'])
                sig = max(float(params['recovery_effectiveness']['std']), 1e-3)
                term = stats.norm.logpdf(x, mu, sig)
                log_ll += term
                if not evidence.get('agent_lost', False):
                    # multiply likelihood by a small factor (e.g., 1e-3)
                    penalty = float(getattr(self, 'recover_notlost_penalty', 1e-3))
                    log_ll += np.log(np.clip(penalty, 1e-12, 1.0))
                if getattr(self, 'debug_emissions', False):
                    contrib_dbg.append(('recovery_effectiveness', (x, mu, sig), None, term))

        elif state == 'TASK_SOLVING':
            # If your pipeline produces 'plan_progress' instead of 'task_progress',
            # mirror it into evidence earlier OR change the key here.
            key = 'task_progress' if 'task_progress' in params else None
            if key and key in evidence:
                x   = float(evidence[key])
                mu  = float(params[key]['mean'])
                sig = max(float(params[key]['std']), 1e-3)
                term = stats.norm.logpdf(x, mu, sig)
                log_ll += term
                if getattr(self, 'debug_emissions', False):
                    contrib_dbg.append((key, (x, mu, sig), None, term))

        # --- Finalize ------------------------------------------------------
        # Numerical safety (avoids underflow to exactly 0)
        log_ll = float(np.clip(log_ll, -700.0, 50.0))  # -700 ~ exp underflow boundary for float64
        lik = float(np.exp(log_ll))
        lik = max(lik, 1e-12)

        if getattr(self, 'debug_emissions', False):
            print(f"[EMIT] state={state}  logL={log_ll:.3f}  lik={lik:.3e}  contribs={contrib_dbg}")

        return lik
    
    def _extract_place_ids_from_entries(self, recent_entries, positions=None,
                                    window: int = 64,
                                    key_candidates = ('node_id', 'place_id', 'exp_id'),
                                    dedup_consecutive: bool = True):
        """
        Pull a clean, chronological list of place-node IDs from the newest `window`
        entries. Robust to None, numpy scalars, nested meta, etc.
        NOTE: This is *not* used for the revisit gap logic anymore (we need step indices),
        but we keep it for consistency and optional debugging.
        """
        entries = list(recent_entries)[-int(window):]

        place_ids = []

        def coerce_to_int(v):
            try:
                if hasattr(v, "item"):
                    v = v.item()  # numpy / torch scalar
                return int(v)
            except Exception:
                try:
                    import numpy as _np
                    return int(_np.asarray(v).item())
                except Exception:
                    return None

        for e in entries:
            pid = None

            # direct keys
            for k in key_candidates:
                if k in e and e[k] is not None:
                    pid = e[k]
                    break

            # nested meta/info if not found
            if pid is None:
                meta = e.get('meta') or e.get('info') or {}
                if isinstance(meta, dict):
                    for k in key_candidates:
                        if meta.get(k) is not None:
                            pid = meta[k]
                            break

            if pid is None:
                continue

            pid = coerce_to_int(pid)
            if pid is None:
                continue

            if dedup_consecutive and len(place_ids) > 0 and pid == place_ids[-1]:
                # same as last → skip dwell
                continue

            place_ids.append(pid)

        if getattr(self, 'debug_evidence', False):
            tail_preview = place_ids[-10:] if len(place_ids) > 10 else place_ids
            print(f"[EVD] extracted place_ids (len={len(place_ids)}): tail10={tail_preview}")

        return place_ids

    def extract_evidence_from_replay_buffer(self, replay_buffer, external_info_gain=None, external_plan_progress=None):
        """
        Centralized evidence extraction that calculates all metrics once and reuses them.
        This replaces multiple separate calculations with a single comprehensive analysis.
        """
        dbg = bool(getattr(self, "debug_evidence", True))
        if dbg:
            print(f"[EVD] extract_evidence: len(buffer)={len(replay_buffer)}  "
                f"external_info_gain={external_info_gain}  external_plan_progress={external_plan_progress}")

        evidence = {}

        # ---------- WARM-UP SHORT PATH ----------
        if len(replay_buffer) < 10:
            warmup = {
                'agent_lost': False,
                'loop_detected': False,
                # keep movement/stagnation middling but not extreme
                'movement_score': 0.7,
                'stagnation': 0.4,                      # slightly below 0.5 so EXPLORE isn't penalized hard
                # let HMM compute internal info gain later; do not over-penalize EXPLORE
                'info_gain': 0.7,
                'exploration_productivity': 0.5,        # modest; depends on info_gain and (1 - stagnation)
                # DO NOT seed NAVIGATE/TASK with 0.5 at warm-up; make them low until we know better
                'navigation_progress': 0.1,
                'plan_progress': 0.1,
                'task_progress': 0.1,
                # RECOVER shouldn't look good at warm-up
                'recovery_effectiveness': 0.1,
                'new_node_created': False,
                'recent_known_switch': False,
                'known_revisit_rate': 0.0,
                '_warmup': True                         # <-- tag for update()
            }
            if dbg:
                print("[EVD] warm-up: using neutral defaults (no metrics computed).")
                print("[EVD] warm-up evidence:", warmup)
            return warmup

        # ---------- FULL PATH ----------
        # Convert replay buffer to list for easier manipulation
        buffer_list = list(replay_buffer)
        recent_entries = buffer_list[-10:]  # Last 10 entries for most calculations
        if dbg:
            print(f"[EVD] using last N={len(recent_entries)} entries for metrics.")

        # ===== CENTRALIZED METRIC CALCULATIONS =====

        # 1) Extract poses and positions
        poses = []
        positions = []
        def _coerce_xytheta(p):
            """Return (x,y,theta) as floats or None if not parseable."""
            try:
                # dict
                if isinstance(p, dict) and ('x' in p and 'y' in p):
                    return (float(p['x']), float(p['y']), float(p.get('theta', 0.0)))
                # list/tuple
                if isinstance(p, (list, tuple)) and len(p) >= 2:
                    x = float(p[0]); y = float(p[1]); th = float(p[2]) if len(p) >= 3 else 0.0
                    return (x, y, th)
                # numpy
                if isinstance(p, np.ndarray) and p.size >= 2:
                    p = p.astype(float).copy()  # copy to break aliasing views
                    x = float(p[0]); y = float(p[1]); th = float(p[2]) if p.size >= 3 else 0.0
                    return (x, y, th)
                # torch
                if 'torch' in globals():
                    torch = globals()['torch']
                    if hasattr(torch, 'is_tensor') and torch.is_tensor(p) and p.numel() >= 2:
                        q = p.detach().cpu().float().clone().numpy()  # clone → new storage
                        x = float(q[0]); y = float(q[1]); th = float(q[2]) if q.size >= 3 else 0.0
                        return (x, y, th)
            except Exception:
                return None
            return None
        for entry in recent_entries:
            if 'real_pose' in entry:
                p = entry['real_pose']

                # Accept dict
                if isinstance(p, dict) and {'x', 'y'}.issubset(p):
                    poses.append((p['x'], p['y'], p.get('theta', 0.0)))
                    positions.append((float(p['x']), float(p['y'])))

                # Accept list / tuple
                elif isinstance(p, (list, tuple)) and len(p) >= 2:
                    poses.append(p)
                    positions.append((float(p[0]), float(p[1])))

                # Accept numpy / tensor
                elif isinstance(p, np.ndarray) and p.size >= 2:
                    p = p.astype(float)
                    poses.append(p.tolist())
                    positions.append((p[0], p[1]))

                # Accept torch tensor (only if torch is available in caller)
                elif 'torch' in globals() and hasattr(globals()['torch'], 'is_tensor') and globals()['torch'].is_tensor(p) and p.numel() >= 2:
                    p = p.detach().cpu().float().tolist()
                    poses.append(p)
                    positions.append((p[0], p[1]))
        if dbg:
            sample_pos = positions[-1] if positions else None
            print(f"[EVD] extracted poses={len(poses)}  positions={len(positions)}  last_position={sample_pos}",poses,positions)

        # 2) Extract doubt counts (primary recovery/lost indicator)
        doubt_counts = []
        current_doubt_count = 0
        for entry in recent_entries:
            if 'place_doubt_step_count' in entry:
                doubt_count = entry['place_doubt_step_count']
                doubt_counts.append(doubt_count)
                current_doubt_count = doubt_count  # Keep updating to get latest
        if dbg:
            print(f"[EVD] doubt_counts(len)={len(doubt_counts)}  current_doubt={current_doubt_count}  "
                f"recent5={doubt_counts[-5:] if len(doubt_counts)>=5 else doubt_counts}")

        # 3) Calculate movement metrics
        movement_metrics = self._calculate_movement_metrics(poses, positions)
        if dbg:
            mm = movement_metrics
            print("[EVD] movement_metrics:",
                {k: round(float(mm[k]), 3) if isinstance(mm.get(k, None), (int, float, np.floating)) else mm.get(k)
                for k in mm.keys() if k in ('movement_score', 'exploration_factor', 'speed', 'turn_rate')})

        # 4) Calculate position analysis
        position_metrics = self._calculate_position_metrics(positions, recent_entries)
        if dbg:
            pm = position_metrics
            # Print a few salient fields if present
            to_show = {k: pm.get(k) for k in ('position_stagnation', 'unique_cells_last_k', 'loop_candidates', 'pose_variance')}
            print("[EVD] position_metrics:", to_show)

        # 5) Calculate doubt trend analysis
        doubt_metrics = self._calculate_doubt_metrics(doubt_counts)
        if dbg:
            dm = doubt_metrics
            to_show_dm = {k: dm.get(k) for k in ('current_doubt', 'trend', 'spikes', 'mean')}
            print("[EVD] doubt_metrics:", to_show_dm)

        # ===== BUILD EVIDENCE USING CENTRALIZED METRICS =====
        revisit = self._calculate_revisit_metrics(recent_entries, positions)
        evidence['recent_known_switch'] = revisit['recent_known_switch']
        evidence['known_revisit_rate']  = revisit['known_revisit_rate']
        if dbg:
            print(f"[EVD] revisit_metrics: recent_known_switch={evidence['recent_known_switch']}  "
                f"known_revisit_rate={evidence['known_revisit_rate']:.2f}")
        # Universal evidence
        evidence['agent_lost'] = self._determine_agent_lost(
            current_doubt_count, movement_metrics, position_metrics
        )
        evidence['loop_detected'] = self._determine_loop_detected(
            position_metrics, buffer_list
        )
        evidence['movement_score'] = movement_metrics['movement_score']
        evidence['stagnation'] = 1.0 - movement_metrics['movement_score']

        if dbg:
            print(f"[EVD] universal: agent_lost={evidence['agent_lost']}  loop_detected={evidence['loop_detected']}  "
                f"movement_score={round(float(evidence['movement_score']),3)}  "
                f"stagnation={round(float(evidence['stagnation']),3)}")

        # Mode-specific evidence with external inputs (info_gain / exploration_productivity)
        if external_info_gain is not None:
            evidence['info_gain'] = float(external_info_gain)
            evidence['exploration_productivity'] = (
                evidence['info_gain'] * (1.0 - evidence['stagnation']) *
                movement_metrics['exploration_factor']
            )
            if dbg:
                print("[EVD] info_gain source: EXTERNAL  "
                    f"info_gain={round(float(evidence['info_gain']),3)}  "
                    f"exploration_factor={round(float(movement_metrics['exploration_factor']),3)}  "
                    f"exploration_productivity={round(float(evidence['exploration_productivity']),3)}")
        else:
            evidence['info_gain'] = movement_metrics['exploration_factor'] * 0.6
            evidence['exploration_productivity'] = evidence['info_gain'] * (1.0 - evidence['stagnation'])
            if dbg:
                print("[EVD] info_gain source: INTERNAL_MOVEMENT  "
                    f"exploration_factor={round(float(movement_metrics['exploration_factor']),3)}  "
                    f"info_gain={round(float(evidence['info_gain']),3)}  "
                    f"exploration_productivity={round(float(evidence['exploration_productivity']),3)}")

        # Plan / task progress (external vs internal estimate)
        if external_plan_progress is not None:
            progress_value = float(external_plan_progress)
            # down-weight external progress when we’re clearly still exploring
            alpha = 1.0
            if evidence.get('stagnation', 1.0) < 0.35 and movement_metrics.get('exploration_factor',0.0) > 0.5:
                alpha = 0.5  # halve the effect while exploration is productive
            evidence['navigation_progress'] = alpha * progress_value
            evidence['plan_progress']       = alpha * progress_value
            evidence['task_progress']       = alpha * progress_value
            if dbg:
                print(f"[EVD] progress source: EXTERNAL  value={round(progress_value,3)}")
        else:
            estimated_progress = self._estimate_progress_from_movement(
                movement_metrics, doubt_metrics, position_metrics
            )
            evidence['navigation_progress'] = estimated_progress
            evidence['plan_progress'] = estimated_progress
            evidence['task_progress'] = estimated_progress
            if dbg:
                print(f"[EVD] progress source: INTERNAL_ESTIMATE  value={round(float(estimated_progress),3)}")
                # --- One-shot NAV quickstart on revisit while we are in EXPLORE ---
        # Fire once per revisit event; we disarm if no revisit this step or we leave EXPLORE.
                # --- Growing NAV quickstart while in EXPLORE and revisit evidence persists ---
        # Accumulates across steps (capped). Resets when we leave EXPLORE.
        try:
            mode = getattr(self, 'current_mode', 'EXPLORE')
        except Exception:
            mode = 'EXPLORE'
        in_explore = (mode == 'EXPLORE')

        kr  = float(evidence.get('known_revisit_rate', 0.0))            # now "intensity"
        rks = bool(evidence.get('recent_known_switch', False))

        # Config knobs
        step_bump = float(getattr(self, 'revisit_quickstart_step', 0.05))   # base growth per step in EXPLORE w/ revisit
        kr_gain   = float(getattr(self, 'revisit_quickstart_gain', 0.40))   # extra growth scaled by intensity
        cap       = float(getattr(self, 'revisit_quickstart_cap',  0.55))   # ceiling

        accum = float(getattr(self, '_rev_quick_accum', 0.0))

        if in_explore:
            if rks or kr > 0.0:
                # grow while we keep seeing revisit evidence
                before_accum = accum
                accum = min(cap, accum + step_bump + kr_gain * kr)
                before = float(evidence.get('navigation_progress', 0.0))
                evidence['navigation_progress'] = max(before, accum)
                if dbg:
                    print(f"[EVD] quickstart NAV(grow): mode=EXPLORE  accum {before_accum:.3f}→{accum:.3f}  "
                          f"kr={kr:.3f} step_bump={step_bump:.2f} kr_gain={kr_gain:.2f} cap={cap:.2f}  "
                          f"nav_prog {before:.3f}→{evidence['navigation_progress']:.3f}")
            # else: no new revisit signal → keep accum as-is (no growth)
        else:
            # left EXPLORE → reset accumulator
            accum = 0.0

        self._rev_quick_accum = accum
        # --- RECOVER bail-out with EXPLORE pivot ---
        try:
            mode = getattr(self, 'current_mode', 'EXPLORE')
        except Exception:
            mode = 'EXPLORE'
        in_recover = (mode == 'RECOVER')

        # Config knobs (override on the instance if desired)
        bail_after    = int(getattr(self, 'recover_bail_after', 1))         # grace steps in RECOVER
        nav_phase     = int(getattr(self, 'recover_nav_phase_steps', 1))     # how many RECOVER steps we try NAV kick
        drop_loop_at  = int(getattr(self, 'recover_drop_loop_at', 3))        # at/after this age: force loop=False

        # NAV-leaning phase (early)
        nav_bump      = float(getattr(self, 'recover_nav_bump', 0.22))       # per-step push toward NAV
        nav_cap       = float(getattr(self, 'recover_nav_cap', 0.55))        # cap for nav_progress during bail
        stag_cap_nav  = float(getattr(self, 'recover_stag_cap_nav', 0.60))   # stagnation clamp while nudging NAV
        ig_floor_nav  = float(getattr(self, 'recover_min_info_gain_nav', 0.20))

        # EXPLORE-leaning phase (late)
        stag_cap_exp  = float(getattr(self, 'recover_stag_cap_explore', 0.45))  # stronger clamp to favor EXPLORE
        ig_floor_exp  = float(getattr(self, 'recover_min_info_gain_explore', 0.35))

        if in_recover:
            age = int(getattr(self, '_recover_age', 0)) + 1
            self._recover_age = age

            # After a short grace, try a brief NAV kick (1 step by default)
            if age >= bail_after and age < bail_after + nav_phase:
                before_nav = float(evidence.get('navigation_progress', 0.0))
                evidence['navigation_progress'] = min(1.0, max(before_nav, min(nav_cap, before_nav + nav_bump)))

                if 'stagnation' in evidence:
                    evidence['stagnation'] = min(float(evidence['stagnation']), stag_cap_nav)
                if 'info_gain' in evidence:
                    evidence['info_gain'] = max(float(evidence['info_gain']), ig_floor_nav)

                if dbg:
                    print(f"[EVD] RECOVER-bail NAV phase: age={age} "
                          f"nav→{evidence['navigation_progress']:.2f}  "
                          f"stag≤{evidence['stagnation']:.2f}  ig≥{evidence['info_gain']:.2f}")

            # If still stuck in RECOVER, pivot to EXPLORE (make EXPLORE win on emissions)
            elif age >= bail_after + nav_phase:
                # Optionally drop the loop penalty after a few steps so RECOVER loses its edge
                if age >= drop_loop_at:
                    evidence['loop_detected'] = False

                # Lower stagnation more aggressively and raise info_gain
                if 'stagnation' in evidence:
                    evidence['stagnation'] = min(float(evidence['stagnation']), stag_cap_exp)
                if 'info_gain' in evidence:
                    evidence['info_gain'] = max(float(evidence['info_gain']), ig_floor_exp)

                # Recompute exploration_productivity to reflect boosted IG and reduced stagnation
                try:
                    expl_factor = float(movement_metrics.get('exploration_factor', 0.5))
                except Exception:
                    expl_factor = 0.5
                evidence['exploration_productivity'] = (
                    float(evidence['info_gain']) * (1.0 - float(evidence['stagnation'])) * expl_factor
                )

                if dbg:
                    print(f"[EVD] RECOVER-bail EXPLORE pivot: age={age} "
                          f"stag≤{evidence['stagnation']:.2f}  ig≥{evidence['info_gain']:.2f}  "
                          f"expl_prod→{evidence['exploration_productivity']:.2f}  "
                          f"loop={evidence.get('loop_detected', None)}")

        else:
            # reset counter when we leave RECOVER
            self._recover_age = 0
                # ---------- TASK gate (external flag only) ----------
        # Until an external controller sets `self.task_external_ready = True`,
        # keep TASK completely decoupled (task_progress = 0.0).
        # Optional: `task_require_external_flag` (defaults True) lets you bypass the gate if needed.

        require_flag = bool(getattr(self, 'task_require_external_flag', True))
        flag_ok      = bool(getattr(self, 'task_external_ready', False))

        if require_flag:
            if flag_ok:
                # Arm TASK: mirror plan_progress (or nav if plan missing)
                evidence['task_progress'] = float(
                    evidence.get('plan_progress', evidence.get('navigation_progress', 0.0))
                )
                self._task_armed = True
            else:
                evidence['task_progress'] = 0.0
                self._task_armed = False
        else:
            # Gate disabled: allow TASK to track plan_progress
            evidence['task_progress'] = float(
                evidence.get('plan_progress', evidence.get('navigation_progress', 0.0))
            )
            self._task_armed = True

        if dbg:
            print(f"[EVD] task gate (flag): require={require_flag} ready={flag_ok} "
                  f"task_progress={evidence['task_progress']:.2f} armed={getattr(self, '_task_armed', False)}")

        # Recovery effectiveness using centralized metrics (agent_lost passed for gating inside the function)
        evidence['recovery_effectiveness'] = self._calculate_recovery_effectiveness_centralized(
            doubt_metrics, movement_metrics, position_metrics, evidence['agent_lost']
        )
        if dbg:
            print(f"[EVD] recovery_effectiveness={round(float(evidence['recovery_effectiveness']),3)}  "
                f"(agent_lost={evidence['agent_lost']})")

        # New node detection
        evidence['new_node_created'] = self._detect_new_node_created_centralized(recent_entries)
        if dbg:
            print(f"[EVD] new_node_created={bool(evidence['new_node_created'])}")

        if dbg:
            # Final evidence snapshot (ordered print)
            keys_order = ['agent_lost', 'loop_detected',
                        'movement_score', 'stagnation',
                        'info_gain', 'exploration_productivity',
                        'navigation_progress', 'plan_progress', 'task_progress',
                        'recovery_effectiveness', 'new_node_created']
            snapshot = {k: evidence.get(k) for k in keys_order}
            print("[EVD] evidence(final):", snapshot)

        return evidence


    def _calculate_movement_metrics(self, poses, positions):
        """Calculate all movement-related metrics in one place"""
        metrics = {
            'movement_score': 0.5,
            'total_distance': 0.0,
            'movement_variance': 0.0,
            'exploration_factor': 0.5
        }
        
        if len(poses) < 2:
            return metrics
        print("POSESSS",poses,positions)
        # Calculate total distance moved
        total_distance = 0.0
        distances = []
        for i in range(1, len(positions)):
            dx = positions[i][0] - positions[i-1][0]
            dy = positions[i][1] - positions[i-1][1]
            dist = (dx**2 + dy**2)**0.5
            total_distance += dist
            distances.append(dist)
        
        metrics['total_distance'] = total_distance
        
        # Movement score (normalized)
        metrics['movement_score'] = min(total_distance / 10.0, 1.0)
        
        # Movement variance (how varied are the movements)
        if len(distances) > 1:
            mean_dist = np.mean(distances)
            variance = np.var(distances)
            metrics['movement_variance'] = min(variance / (mean_dist + 1e-6), 1.0)
        
        # Exploration factor (combination of movement and variance)
        metrics['exploration_factor'] = (
            0.7 * metrics['movement_score'] + 
            0.3 * metrics['movement_variance']
        )
        
        return metrics
    
    def _calculate_revisit_metrics(self, recent_entries, positions):
        """
        Revisit detection with global node memory (v5-intensity):
        - A transition happens only when node_id changes.
        - A *revisit* happens when we enter a node seen before AND the step-gap >= min_gap.
        - known_revisit_rate is now a decayed *intensity* (not a normalized ratio):
            intensity = 1 - exp(-beta * sum(decay^age * revisit_weight))
        so single/recent revisits give a meaningful signal, repeated revisits saturate.

        Persistent state (created lazily):
        _rev_step:                 int, global step counter (ticks per evidence call)
        _rev_last_id:              last node id seen
        _rev_last_seen_step:       {node_id -> last step index visited}
        _rev_visit_count:          {node_id -> number of *entries* into that node} (dwell deduped)
        _rev_recent_transitions:   deque[(is_revisit:bool, node_id:int, visits_before:int)]
        """
        import collections, math
        # ---- knobs you can tune on the instance ----
        min_gap   = int(getattr(self, 'revisit_min_gap', 10))       # steps between visits to count as revisit
        win_trans = int(getattr(self, 'revisit_window',  60))       # how many recent transitions we remember
        alpha     = float(getattr(self, 'revisit_weight_alpha', 0.25))  # weight bump per prior visit (>1 revisits)
        gamma     = float(getattr(self, 'revisit_decay_gamma', 0.90))   # per-transition temporal decay (0.8..0.95)
        beta      = float(getattr(self, 'revisit_intensity_beta', 0.60))# saturation strength (0.3..0.8)
        # --------------------------------------------

        # ---------- lazy init of persistent state ----------
        if not hasattr(self, '_rev_step'):               self._rev_step = 0
        if not hasattr(self, '_rev_last_id'):            self._rev_last_id = None
        if not hasattr(self, '_rev_last_seen_step'):     self._rev_last_seen_step = {}
        if not hasattr(self, '_rev_visit_count'):        self._rev_visit_count = {}
        if not hasattr(self, '_rev_recent_transitions'): self._rev_recent_transitions = collections.deque(maxlen=max(8, win_trans))
        # ---------------------------------------------------

        # ---- pull the current node_id from the newest entry ----
        def _get_id(e):
            for k in ('node_id', 'place_id', 'exp_id'):
                if k in e and e[k] is not None: return e[k]
            meta = e.get('meta') or e.get('info') or {}
            if isinstance(meta, dict):
                for k in ('node_id', 'place_id', 'exp_id'):
                    if meta.get(k) is not None: return meta[k]
            return None

        def _as_int(v):
            try:
                if hasattr(v, "item"): v = v.item()
                return int(v)
            except Exception:
                try:
                    import numpy as _np
                    return int(_np.asarray(v).item())
                except Exception:
                    return None

        curr_id = None
        if recent_entries:
            curr_id = _as_int(_get_id(recent_entries[-1]))

        # advance global step counter (one tick per evidence extraction)
        step = self._rev_step
        self._rev_step = step + 1

        last_id = self._rev_last_id
        recent_known_switch = False

        if curr_id is None:
            # no node this step: compute intensity from what we have
            tail = list(self._rev_recent_transitions)
        else:
            # decide on transition/revisit first, then update memory
            if last_id is None:
                # first ever node
                self._rev_visit_count[curr_id] = self._rev_visit_count.get(curr_id, 0) + 1
                self._rev_last_seen_step[curr_id] = step
                self._rev_last_id = curr_id
            elif curr_id != last_id:
                # we have a transition last_id -> curr_id
                visits_prior = self._rev_visit_count.get(curr_id, 0)          # before this entry
                last_seen    = self._rev_last_seen_step.get(curr_id, None)     # historical last time at curr_id
                is_revisit   = (last_seen is not None) and ((step - last_seen) >= min_gap)
                # record (for rolling intensity)
                self._rev_recent_transitions.append((bool(is_revisit), int(curr_id), int(visits_prior)))
                # now update counters/last-seen
                self._rev_visit_count[curr_id] = visits_prior + 1
                self._rev_last_seen_step[curr_id] = step
                self._rev_last_seen_step[last_id] = step - 1
                self._rev_last_id = curr_id
                recent_known_switch = bool(is_revisit)
            else:
                # dwell on the same node → keep last-seen fresh
                self._rev_last_seen_step[curr_id] = step
            tail = list(self._rev_recent_transitions)

        # ----- decayed, non-normalized intensity mapped to [0,1] -----
        if not tail:
            known_revisit_rate = 0.0
        else:
            L = len(tail)
            sum_w = 0.0
            for i, (flag, node, visits_before) in enumerate(tail):
                if not flag:
                    continue
                age = (L - 1 - i)                       # newer transitions have smaller age
                prior_boost = (1.0 + alpha * max(0, visits_before - 1))
                sum_w += (gamma ** age) * prior_boost
            # saturation: big sum_w → approach 1; one good revisit already ~0.4 with beta≈0.6
            known_revisit_rate = float(1.0 - math.exp(-beta * sum_w))

        if getattr(self, 'debug_evidence', False):
            dbg_tail = tail[-min(10, len(tail)):]
            dbg_flags  = [f for (f, n, vb) in dbg_tail]
            dbg_nodes  = [n for (f, n, vb) in dbg_tail]
            dbg_visits = [vb for (f, n, vb) in dbg_tail]
            print(f"[EVD] revisit[v5-intensity]: last_id={last_id} -> curr_id={curr_id}  "
                f"recent_switch={recent_known_switch}  intensity={known_revisit_rate:.2f}  "
                f"min_gap={min_gap}  tail_flags={dbg_flags} nodes={dbg_nodes} visits_before={dbg_visits}")

        return {
            'recent_known_switch': bool(recent_known_switch),
            'known_revisit_rate':  known_revisit_rate,   # remains in [0,1], now a saturating intensity
        }



    def _calculate_position_metrics(self, positions, recent_entries):
        """Calculate all position-related metrics in one place"""
        metrics = {
            'position_repetition': 0.0,
            'position_diversity': 0.5,
            'position_stagnation': False,
            'unique_positions': 0
        }
        
        if len(positions) < 3:
            return metrics
        
        # Round positions to avoid floating point issues
        rounded_positions = [(round(pos[0], 1), round(pos[1], 1)) for pos in positions]
        
        # Calculate position repetition
        position_counts = {}
        for pos in rounded_positions:
            position_counts[pos] = position_counts.get(pos, 0) + 1
        
        if position_counts:
            max_count = max(position_counts.values())
            metrics['position_repetition'] = max_count / len(positions)
            metrics['unique_positions'] = len(position_counts)
            
            # Position diversity (inverse of repetition, adjusted)
            metrics['position_diversity'] = 1.0 - (metrics['position_repetition'] - 1.0/len(positions))
            metrics['position_diversity'] = max(0.0, min(1.0, metrics['position_diversity']))
        
        # Position stagnation check
        if len(recent_entries) >= 8:
            last_4_positions = rounded_positions[-4:]
            if len(set(last_4_positions)) <= 2:  # Only 1-2 unique positions in last 4 steps
                metrics['position_stagnation'] = True
        
        return metrics

    def _calculate_doubt_metrics(self, doubt_counts):
        """Calculate all doubt-related metrics in one place"""
        metrics = {
            'current_doubt': 0,
            'doubt_trend': 0.0,
            'max_doubt': 0,
            'doubt_stability': 0.5
        }
        
        if not doubt_counts:
            return metrics
        
        metrics['current_doubt'] = doubt_counts[-1]
        metrics['max_doubt'] = max(doubt_counts)
        
        # Calculate trend (simple linear regression slope)
        if len(doubt_counts) >= 3:
            x = np.arange(len(doubt_counts))
            y = np.array(doubt_counts)
            if len(x) > 1 and np.std(y) > 1e-6:
                slope, _ = np.polyfit(x, y, 1)
                metrics['doubt_trend'] = slope
        
        # Doubt stability (how consistent are the doubt levels)
        if len(doubt_counts) > 1:
            metrics['doubt_stability'] = 1.0 / (1.0 + np.std(doubt_counts))
        
        return metrics

    def _determine_agent_lost(self, current_doubt_count, movement_metrics, position_metrics):
        """Determine if agent is lost using centralized metrics"""
        base_threshold = 6
        
        if current_doubt_count <= base_threshold:
            return False
        
        # High repetition indicates being lost
        if position_metrics['position_repetition'] > 0.6:
            return True
        
        # Low movement + very high doubt
        if (movement_metrics['movement_variance'] < 0.2 and 
            current_doubt_count > base_threshold + 2):
            return True
        
        # Extremely high doubt regardless
        if current_doubt_count > base_threshold + 4:
            return True
        
        # Position stagnation + moderate doubt
        if (position_metrics['position_stagnation'] and 
            current_doubt_count > 3):
            return True
        
        return False

    def _determine_loop_detected(self, position_metrics, buffer_list):
        """Determine if loop is detected using centralized metrics"""
        if len(buffer_list) < self.params['loop_threshold']:
            return False
        
        # Use position repetition as primary indicator
        if position_metrics['position_repetition'] > 0.3:
            return True
        
        # Additional check: recent position clustering
        recent_positions = []
        for entry in buffer_list[-self.params['loop_threshold']:]:
            if 'real_pose' in entry:
                pos = entry['real_pose']
                if isinstance(pos, (list, tuple)) and len(pos) >= 2:
                    recent_positions.append((round(pos[0], 1), round(pos[1], 1)))
        
        if len(recent_positions) >= self.params['loop_threshold']:
            unique_recent = len(set(recent_positions))
            repetition_ratio = 1.0 - (unique_recent / len(recent_positions))
            return repetition_ratio > 0.4
        
        return False

    def _calculate_recovery_effectiveness_centralized(self, doubt_metrics, movement_metrics, position_metrics,agent_lost):
        """Calculate recovery effectiveness using centralized metrics"""
        if doubt_metrics['current_doubt'] == 0:
            # No recovery needed; keep this low so RECOVER isn't rewarded
            return 0.1
         # When not lost, do NOT reward recovery; this prevents RECOVER from winning spuriously.
        if not agent_lost:
            return 0.10  # baseline low; tune if needed
        recovery_score = 0.5  # Base score
        
        # Doubt trend analysis
        if doubt_metrics['doubt_trend'] < -0.5:  # Strong decreasing trend
            recovery_score += 0.3
        elif doubt_metrics['doubt_trend'] < 0:  # Mild decreasing trend
            recovery_score += 0.1
        elif doubt_metrics['doubt_trend'] > 0.5:  # Increasing trend (getting worse)
            recovery_score -= 0.3
        
        # Movement analysis
        if movement_metrics['movement_score'] > 0.3:  # Good movement
            recovery_score += 0.1
        elif movement_metrics['movement_score'] < 0.1:  # Too little movement
            recovery_score -= 0.1
        
        # Position diversity helps recovery
        if position_metrics['position_diversity'] > 0.4:
            recovery_score += 0.1
        
        # Avoid repetitive behavior during recovery
        if position_metrics['position_repetition'] > 0.7:
            recovery_score -= 0.2
        
        return max(0.0, min(1.0, recovery_score))

    def _estimate_progress_from_movement(self, movement_metrics, doubt_metrics, position_metrics):
        """Estimate progress when external progress info is not available"""
        base_progress = 0.3
        
        # Good movement suggests progress
        if movement_metrics['movement_score'] > 0.4:
            base_progress += 0.2
        
        # Low doubt suggests good progress
        if doubt_metrics['current_doubt'] < 3:
            base_progress += 0.2
        elif doubt_metrics['current_doubt'] > 8:
            base_progress -= 0.2
        
        # Position diversity suggests exploration progress
        if position_metrics['position_diversity'] > 0.5:
            base_progress += 0.1
        
        # Decreasing doubt trend suggests recovery progress
        if doubt_metrics['doubt_trend'] < -0.3:
            base_progress += 0.2
        
        return max(0.0, min(1.0, base_progress))

    def _detect_new_node_created_centralized(self, recent_entries):
        """Detect new node creation using centralized approach"""
        if len(recent_entries) < 2:
            return False
        
        # Check for node_id changes in recent entries
        node_ids = []
        for entry in recent_entries[-5:]:  # Check last 5 steps
            if 'node_id' in entry:
                node_ids.append(entry['node_id'])
        
        if len(node_ids) >= 2:
            # If node_id changed recently, likely a new node was created
            return len(set(node_ids)) > 1
        
        return False
    
        # ───────────────────────────────────────────────────────────────────
    #  0.  Small helper for consistent, compact arrays in prints
    # ───────────────────────────────────────────────────────────────────
    def _vec2str(self, v):
        """Return first k elements of a vector as a string."""
        v = np.asarray(v)
        k=len(v)
        head = ", ".join(f"{x:.3f}" for x in v[:k])
        suffix = "…" if v.size > k else ""
        return f"[{head}{suffix}]"

    # ───────────────────────────────────────────────────────────────────
    #  1.  Prior likelihood  (uniform over joint states)
    # ───────────────────────────────────────────────────────────────────
    def _compute_prior_likelihood(self, evidence: Dict) -> float:
        likes = [
            self.compute_emission_likelihood(evidence, state)
            for state in self.hierarchical_states
        ]
        prior_like = float(np.mean(likes))
        print(f"[DBG-PRIOR]   evidence={evidence}  "
            f"mean emisLike={prior_like:.3e}")
        return prior_like

    # ───────────────────────────────────────────────────────────────────
    #  2.  Joint weighted likelihood (soft mixture)
    # ───────────────────────────────────────────────────────────────────
    def _compute_joint_weighted_likelihood(self, evidence: Dict, var: str) -> float:
        total = 0.0
        for idx, state in enumerate(self.hierarchical_states):
            p_state = self.state_beliefs[idx]
            prm = self.emission_params[state].get(var)
            if isinstance(prm, dict):
                pdf = stats.norm.pdf(evidence[var], prm['mean'], prm['std'])
                total += p_state * pdf
        print(f"[DBG-MIX]     var={var:<23} mixedPDF={total:.3e}")
        return total

    # ───────────────────────────────────────────────────────────────────
    #  3.  Run-length-specific likelihood for BOCPD
    # ───────────────────────────────────────────────────────────────────
    def compute_run_length_specific_likelihood(self, evidence: Dict,
                                            run_length: int) -> float:
        if run_length == 0:
            return self._compute_prior_likelihood(evidence)

        hist = list(self.evidence_history)
        relevant = hist[-run_length:] if len(hist) >= run_length else hist
        print(f"[DBG-RL-{run_length:02d}] ---- scoring run length {run_length} "
        f"using {len(relevant)} past samples ----")

        if not relevant:
            return self._compute_prior_likelihood(evidence)

        logl = 0.0
        variables_to_check = ['Δ_info_gain', 'Δ_stagnation', 'Δ_movement_score',
                    'Δ_exploration_productivity', 'Δ_navigation_progress',
                    'Δ_recovery_effectiveness', 'Δ_plan_progress', 'Δ_task_progress'
                ]
        for var in variables_to_check:

            if var in evidence:
                vals = [e[var] for e in relevant if var in e]
                if len(vals) >= 2:
                    μ = np.mean(vals)
                    σ = max(np.std(vals), 0.05) * (1 + 1.0/run_length)
                    lp = stats.norm.logpdf(evidence[var], μ, σ)
                    logl += lp
                    print(f"[DBG-RL-{run_length:02d}] var={var:<23} μ={μ:.3f} "
                        f"σ={σ:.3f}  logp={lp:.3f}")
                else:
                    mix = self._compute_joint_weighted_likelihood(evidence, var)
                    lp  = np.log(mix + 1e-10)
                    logl += lp
                    print(f"[DBG-RL-{run_length:02d}] var={var:<23} "
                        f"fallback logp={lp:.3f}")

        # static emission mixture
        emis = np.array([
            self.compute_emission_likelihood(evidence, s)
            for s in self.hierarchical_states
        ])
        static_mix = float(np.dot(self.state_beliefs, emis))
        comb = 0.7 * np.exp(logl) + 0.3 * static_mix
        print(f"[DBG-RL-{run_length:02d}] combined like={comb:.3e}",static_mix,comb)
        return max(comb, 1e-10)

    # ───────────────────────────────────────────────────────────────────
    #  4.  BOCPD update
    def bocpd_update(self, evidence: Dict) -> Tuple[bool, float]:
        """
        Update run-length distribution p(r_t | e_1:t) using Adams & MacKay’s
        BOCPD, but:
            • work in log-space first        (avoids under/overflow)
            • derive a **proper probability** of changepoint (0–1)
        Returns
        -------
        changepoint_detected : bool      True if cp_prob crosses threshold
        cp_prob              : float     p(r_t = 0 | evidence)
        """

        # 1) log-likelihoods  ℓ_r = log p(e_t | r_{t-1}=r)
        log_like = np.full(self.max_run_length + 1, -np.inf)
        for r in range(len(self.run_length_dist)):               # only where mass
            if self.run_length_dist[r] > 1e-10:
                l = self.compute_run_length_specific_likelihood(evidence, r)
                log_like[r] = np.log(max(l, 1e-10))              # clamp at 1e-10
                print(f"[DBG-BAYES-LL] r={r:02d} raw_like={l:.3e} "
                        f"log_like={log_like[r]:.3f}")

        # 2) stabilise & exponentiate back to ordinary scale
        max_log = np.max(log_like)
        if max_log == -np.inf:                 # <-- add this block
            like = np.ones_like(log_like)      # uniform – no information
        else:
            like = np.exp(log_like - max_log)
        print(f"[DBG-MAX] max_log={max_log:.3f}")                       # safe, all ≤ 1
        print(f"[DBG-LIKE] like[:10]={like[:10]}")
        # 3) “growth” term  p(r_t=r+1 , no-cp)
        growth = np.zeros_like(self.run_length_dist)
        if len(self.run_length_dist) > 1:
            growth[1:] = (self.run_length_dist[:-1] *
                        (1.0 - self.hazard_rate) *
                        like[:-1])

        # 4) “changepoint” numerator  p(r_t=0 , cp)
        cp_num = np.sum(self.run_length_dist * self.hazard_rate * like)
        print(f"[DBG-GROW] growth[:5]={growth[:5]}  cp_num={cp_num:.3e}")
        

        # 5) evidence normaliser  p(e_t | e_1:t-1)
        evidence_prob = cp_num + np.sum(growth[1:])
        print(f"[DBG-NORM] evidence_prob={evidence_prob:.3e}")
        # 6) proper posterior run-length distribution
        new_dist = np.zeros_like(self.run_length_dist)
        new_dist[0] = cp_num
        new_dist[1:] = growth[1:]
        if evidence_prob > 0:
            den = max(evidence_prob, 1e-12)
            new_dist /= den

        else:                                   # pathological case
            new_dist[:] = 1.0 / len(new_dist)

        self.run_length_dist = new_dist
        cp_prob = new_dist[0]                  # now guaranteed ∈ [0,1]

        changep = cp_prob > self.changepoint_threshold
        print(f"[DBG-BOCPD] cp_prob={cp_prob:.3f}  changepoint={changep}  "
            f"runLenDist(0..4)={new_dist}")

        return changep, cp_prob
    def get_mode_probabilities(self) -> Dict[str, float]:
        """Belief mass per mode (since states == modes)."""
        return {m: float(self.state_beliefs[self.state_index[m]]) for m in self.states}

    def get_most_likely_state(self) -> str:
        """Return the most probable mode."""
        self.current_mode=self.states[int(np.argmax(self.state_beliefs))]
        print("CURRENT MODE",self.current_mode)
        return self.states[int(np.argmax(self.state_beliefs))]

    def compute_state_entropy(self) -> float:
        """Shannon entropy of the state belief."""
        p = np.clip(self.state_beliefs, 1e-12, 1.0)
        return float(-(p * np.log(p)).sum())

    def compute_mode_entropy(self) -> float:
        """Alias to state entropy (states==modes)."""
        return self.compute_state_entropy()
    # ───────────────────────────────────────────────────────────────────
    #  5.  HMM forward step
    # ───────────────────────────────────────────────────────────────────
    def hmm_forward_step(self, evidence: Dict) -> np.ndarray:
        e_likes = np.array([self.compute_emission_likelihood(evidence, m) for m in self.states], dtype=float)
        if not np.all(np.isfinite(e_likes)) or e_likes.sum() <= 0:
            e_likes = np.full(self.n_states, 1e-9, dtype=float)

        pred = self.transition_matrix.T @ self.state_beliefs
        if not np.all(np.isfinite(pred)) or pred.sum() <= 0:
            pred = np.ones(self.n_states, dtype=float) / self.n_states

        post = e_likes * pred
        s = float(post.sum())
        self.state_beliefs = post / s if s > 0 else np.ones(self.n_states, dtype=float) / self.n_states

        print(f"[DBG-HMM]    eLikes={self._vec2str(e_likes)}  pred={self._vec2str(pred)}  newBelief={self._vec2str(self.state_beliefs)}")
        return self.state_beliefs
    # ───────────────────────────────────────────────────────────────────
    #  6.  GLOBAL UPDATE ENTRY POINT
    # ───────────────────────────────────────────────────────────────────
    def update(self, replay_buffer, external_info_gain=None,
           external_plan_progress=None) -> Tuple[np.ndarray, Dict]:
        """Evidence → (optional deltas) → gates → T → (optional) BOCPD → HMM forward → diagnostics."""

        t0 = time.perf_counter()
        self.step_counter = getattr(self, 'step_counter', 0) + 1
        print(f"\n================ STEP {self.step_counter} ================")

        # 1) Evidence
        evidence = self.extract_evidence_from_replay_buffer(
            replay_buffer, external_info_gain, external_plan_progress
        )
        print("[UPD:1] Evidence:", evidence)

        evidence2 = self._add_deltas(evidence)
        if evidence2 is evidence:
            print("[UPD:1] Δ-features: primed next step (first frame, no deltas yet).")
        else:
            dkeys = [k for k in evidence2.keys() if k.startswith('Δ_')]
            print(f"[UPD:1] Δ-features added: {dkeys}")

        self.evidence_buffer.append(evidence)
        self.evidence_history.append(evidence.copy())
        self.state_history.append(self.state_beliefs.copy())

        # 2) Task progress smoothing + optional gate fn
        tp_now = float(evidence.get('task_progress', evidence.get('plan_progress', 0.0)))
        self.task_progress_smooth = 0.9 * self.task_progress_smooth + 0.1 * tp_now
        print(f"[UPD:2] task_progress now={tp_now:.3f}  smooth={self.task_progress_smooth:.3f}")

        used_gate_fn = False
        if callable(getattr(self, 'task_gate_fn', None)):
            try:
                eg, xg = self.task_gate_fn(self, evidence2, self.task_progress_smooth)
                self.task_enter_gate = float(np.clip(eg, 1e-6, 1e6))
                self.task_exit_gate  = float(np.clip(xg, 1e-6, 1e6))
                used_gate_fn = True
            except Exception as e:
                print(f"[UPD:2][WARN] task_gate_fn threw: {e} — keeping prior gates.")

        print(f"[UPD:2] gates: enter={self.task_enter_gate:.4g}  exit={self.task_exit_gate:.4g}  "
            f"source={'task_gate_fn' if used_gate_fn else 'fixed/neutral'}")

        # 3) Transition rebuild
        tT0 = time.perf_counter()
        T = self._build_transition_matrix()
        if not np.all(np.isfinite(T)):
            print("[UPD:3][ERR] Non-finite T; forcing uniform.")
            T = np.ones_like(T) / T.shape[1]
        self.transition_matrix = T
        rows = T.sum(axis=1)
        print(f"[UPD:3] T row sums min/max: {rows.min():.6f}/{rows.max():.6f}  shape={T.shape}  "
            f"time={(time.perf_counter()-tT0)*1e3:.1f} ms")
        print("[UPD:3] T (rows→cols):")
        for i, fm in enumerate(self.states):
            top = sorted([(tm, T[i, self.state_index[tm]]) for tm in self.states],
                        key=lambda x: x[1], reverse=True)
            print(f"         {fm:<12} → " + ", ".join(f"{tm}:{p:0.3f}" for tm,p in top))

        # 4) BOCPD (optional)
        changep, cp_prob = False, 0.0
        if getattr(self, 'use_bocpd', True):
            try:
                # canonical flow: pass dict evidence; expect (bool, float)
                cp_flag, cp_mass = self.bocpd_update(evidence2)
                changep, cp_prob = bool(cp_flag), float(cp_mass)
                print(f"[UPD:4] BOCPD: cp_prob={cp_prob:.3f}  "
                    f"{'TRIGGERED' if changep else '—'}  "
                    f"(hazard={getattr(self, 'hazard_rate', None)})")
            except Exception as e:
                print(f"[UPD:4][ERR] bocpd_update failed: {e} — continuing without CPD.")

        # 5) HMM forward step
        try:
            # Emissions per mode
            e_likes = np.array([self.compute_emission_likelihood(evidence2, m)
                                for m in self.states], dtype=float)
            if not np.all(np.isfinite(e_likes)) or e_likes.sum() <= 0:
                print("[UPD:5][WARN] Bad emission vector; using epsilons.")
                e_likes = np.full(self.n_states, 1e-9, dtype=float)

            pred = self.transition_matrix.T @ self.state_beliefs
            if not np.all(np.isfinite(pred)) or pred.sum() <= 0:
                print("[UPD:5][ERR] Bad prediction; resetting to uniform.")
                pred = np.ones(self.n_states, dtype=float) / self.n_states

            post = e_likes * pred
            s = float(post.sum())
            if s > 0:
                self.state_beliefs = post / s
            else:
                print("[UPD:5][ERR] Posterior sum=0; using uniform belief.")
                self.state_beliefs[:] = 1.0 / self.n_states

            print("[UPD:5] eLikes:", {m: float(e_likes[self.state_index[m]]) for m in self.states})
            print("[UPD:5] pred:  ", {m: float(pred[self.state_index[m]]) for m in self.states})
            print("[UPD:5] post:  ", {m: float(self.state_beliefs[self.state_index[m]]) for m in self.states})

        except Exception as e:
            print(f"[UPD:5][ERR] HMM forward step failed: {e}")
            self.state_beliefs[:] = 1.0 / self.n_states

        # 6) Attempt counters
        if getattr(self, 'prev_evidence', None) is not None:
            lost = bool(evidence.get('agent_lost', False))
            navp = float(evidence.get('navigation_progress', 0.0))
            thr = 0.4
            if lost:
                self.recovery_attempts = getattr(self, 'recovery_attempts', 0) + 1
                print("[UPD:6] counter: RECOVER++ (agent_lost=True)")
            elif navp > thr:
                self.navigation_attempts = getattr(self, 'navigation_attempts', 0) + 1
                print(f"[UPD:6] counter: NAVIGATE++ (navigation_progress={navp:.2f} > {thr})")
            else:
                self.exploration_attempts = getattr(self, 'exploration_attempts', 0) + 1
                print("[UPD:6] counter: EXPLORE++ (default path)")
        self.prev_evidence = evidence

        # 7) Diagnostics payload
        diag = {
            'mode_probabilities': self.get_mode_probabilities(),
            'most_likely_state':  self.get_most_likely_state(),
            'state_entropy':      self.compute_state_entropy(),
            'mode_entropy':       self.compute_mode_entropy(),
            'changepoint_detected': changep,
            'changepoint_probability': cp_prob,
            'run_length_dist':    self.run_length_dist.copy() if hasattr(self, 'run_length_dist') else None,
            'evidence_buffer_size': len(self.evidence_buffer),
            'evidence_history_size': len(self.evidence_history),
            'evidence_used':      evidence2,
            'task_progress':      {'now': tp_now, 'smooth': self.task_progress_smooth},
            'task_gates':         {'enter': self.task_enter_gate, 'exit': self.task_exit_gate},
        }

        print(f"[UPD:∑] wall-time={(time.perf_counter()-t0)*1e3:.1f} ms  "
            f"| belief.shape={self.state_beliefs.shape}  T.shape={self.transition_matrix.shape}")

        return self.state_beliefs, diag

    
    def _add_deltas(self, evidence: dict) -> dict:
        """
        Return a *new* dict that contains the original evidence **plus**
        first-difference features Δ_x = x_t – x_{t-1} for every scalar
        we care about.  Uses self.prev_evidence for the t-1 values.
        """
        keys = [
            'info_gain', 'stagnation', 'movement_score',
            'exploration_productivity', 'navigation_progress',
            'recovery_effectiveness', 'plan_progress', 'task_progress',
        ]

        if self.prev_evidence is None:                # first frame → no deltas yet
            self.prev_evidence = evidence.copy()
            return evidence

        # build a shallow copy so we don’t mutate the caller’s dict
        aug = evidence.copy()

        for k in keys:
            if k in evidence and k in self.prev_evidence:
                aug[f'Δ_{k}'] = evidence[k] - self.prev_evidence[k]

        self.prev_evidence = evidence.copy()          # store for next call
        return aug
    def _mode_stickiness(self, mode: str) -> float:
        """
        Probability of staying in the same mode. Decays for non-task modes as
        task progress rises; slight bonus for TASK_SOLVING.
        """
        base = {
            'EXPLORE':      0.92,
            'NAVIGATE':     0.93,
            'RECOVER':      0.10,
            'TASK_SOLVING': 0.95,
        }.get(mode, 0.90)

        tp = float(getattr(self, 'task_progress_smooth', 0.0))
        if mode == 'TASK_SOLVING':
            val = min(base + self.stickiness_task_bonus * tp, 0.995)
        else:
            val = max(base * (1.0 - self.stickiness_decay_non_task * tp), 0.50)

        return float(np.clip(val, 0.05, 0.995))
    
    
    def compute_state_entropy(self) -> float:
        """Compute entropy over the full state space"""
        probs = self.state_beliefs + 1e-10  # Avoid log(0)
        return -np.sum(probs * np.log(probs))
    
    def compute_mode_entropy(self) -> float:
        """Compute entropy over just the mode space"""
        mode_probs = list(self.get_mode_probabilities().values())
        mode_probs = np.array(mode_probs) + 1e-10
        return -np.sum(mode_probs * np.log(mode_probs))
    
    def get_strategy_recommendation(self) -> Dict:
        """Concrete strategy recommendation without submodes."""
        most_likely_mode = self.get_most_likely_state()
        mode_probs = self.get_mode_probabilities()
        return {
            'recommended_mode': most_likely_mode,
            'mode_confidence': mode_probs[most_likely_mode],
            'mode_probabilities': mode_probs,
            'uncertainty': self.compute_state_entropy(),
            'changepoint_mass': self.run_length_dist[0],
            'most_likely_run_length': int(np.argmax(self.run_length_dist)),
        }

    

    # Complete the get_diagnostics method (the end was cut off)
    def get_diagnostics(self) -> Dict:
        """
        Return comprehensive diagnostic information about the HHMM+BOCPD state.
        """
        current_mode_probs = self.get_mode_probabilities()
        dominant_mode = max(current_mode_probs.items(), key=lambda x: x[1])
        
        most_likely_state = self.get_most_likely_state()
        
        return {
            'state_beliefs': self.state_beliefs.copy(),
            'mode_probabilities': current_mode_probs,
            'dominant_mode': dominant_mode[0],
            'dominant_mode_probability': dominant_mode[1],
            'most_likely_state': most_likely_state,
            'state_entropy': self.compute_state_entropy(),
            'mode_entropy': self.compute_mode_entropy(),
            'run_length_distribution': self.run_length_dist.copy(),
            'changepoint_mass': self.run_length_dist[0],
            'most_likely_run_length': np.argmax(self.run_length_dist),
            'evidence_buffer_length': len(self.evidence_buffer),
            'state_history_length': len(self.state_history),
            'stagnation_counter': self.stagnation_counter,
            'exploration_attempts': self.exploration_attempts,
            'navigation_attempts': self.navigation_attempts,
            'recovery_attempts': self.recovery_attempts
        }

# Add the HierarchicalBayesianController class
class HierarchicalBayesianController:
    """
    Controller that integrates the TrueHierarchicalHMMWithBOCPD for 
    adaptive agent behavior management with changepoint detection.
    """
    
    def __init__(self, key=None):
        self.hhmm = TrueHierarchicalHMMWithBOCPD(use_submodes=False, use_bocpd=False)
        self.strategy_history = deque(maxlen=100)
        self.last_update_time = time.time()
        
        # Remove reward-based tracking
        self.step_count = 0
        self.successful_transitions = 0
        self.total_transitions = 0

    def update(self, replay_buffer, external_info_gain=None, external_plan_progress=None):
        """
        Updated controller update method
        
        Args:
            replay_buffer: External replay buffer with agent state history
            info_gain_func: Optional function that returns info_gain value
            plan_progress_func: Optional function that returns plan progress value
        """
        # Update hierarchical HMM with BOCPD
        beliefs, diagnostics = self.hhmm.update(
            replay_buffer, external_info_gain, external_plan_progress
        )

        # Get strategy recommendation
        strategy = self.hhmm.get_strategy_recommendation()

        # Store strategy history
        self.strategy_history.append({
            'timestamp': time.time(),
            'strategy': strategy,
            'evidence': diagnostics.get('evidence_used', {}),
            'replay_buffer_size': len(replay_buffer)
        })

        self.step_count += 1
        self.last_update_time = time.time()

        return strategy, diagnostics

    # Remove reward-related methods and update remaining ones accordingly
    def get_summary_stats(self) -> Dict:
        """Updated summary stats without reward tracking"""
        if not self.strategy_history:
            return {'status': 'no_data'}

        mode_counts = {}
        for entry in self.strategy_history:
            mode = entry['strategy']['recommended_mode']
            mode_counts[mode] = mode_counts.get(mode, 0) + 1

        return {
            'total_steps': self.step_count,
            'mode_distribution': mode_counts,
            'most_used_mode': max(mode_counts.items(), key=lambda x: x[1])[0] if mode_counts else None,
            'strategy_changes': len([i for i in range(1, len(self.strategy_history)) 
                                   if self.strategy_history[i]['strategy']['recommended_mode'] != 
                                      self.strategy_history[i-1]['strategy']['recommended_mode']]),
            'uptime': time.time() - (self.strategy_history[0]['timestamp'] if self.strategy_history else time.time())
        }
    
    
    def _compute_trend(self, values) -> str:
        """Compute simple trend (increasing, decreasing, stable) for a list of values"""
        if len(values) < 3:
            return 'insufficient_data'
            
        # Simple linear trend
        x = np.arange(len(values))
        slope = np.corrcoef(x, values)[0, 1] if len(values) > 1 else 0
        
        if slope > 0.1:
            return 'increasing'
        elif slope < -0.1:
            return 'decreasing'
        else:
            return 'stable'
    
    def get_current_strategy(self) -> Dict:
        """Get the current strategy recommendation without updating"""
        return self.hhmm.get_strategy_recommendation()
    
    def reset(self):
        """Reset the controller state"""
        self.hhmm = TrueHierarchicalHMMWithBOCPD(use_submodes=False, use_bocpd=True)
        self.performance_buffer.clear()
        self.strategy_history.clear()
        self.cumulative_reward = 0.0
        self.step_count = 0
        self.successful_transitions = 0
        self.total_transitions = 0
        self.last_update_time = time.time()

# Usage example
if __name__ == "__main__":
    # Minimal fake "replay_buffer": list of dicts like your code expects
    rb = []
    hhmm = TrueHierarchicalHMMWithBOCPD(use_submodes=False, use_bocpd=False)

    # Warm-up with exploration-ish evidence
    for step in range(10):
        rb.append({'position': [step, 0], 'place_doubt_step_count': 0})
        beliefs, diag = hhmm.update(rb, external_info_gain=0.6, external_plan_progress=0.1)
        if step % 3 == 0:
            strat = hhmm.get_strategy_recommendation()
            print("[EARLY] mode=", strat['recommended_mode'],
                  "p(TASK)=", diag['mode_probabilities'].get('TASK_SOLVING', 0.0))

    # Ramp plan/task progress to force TASK_SOLVING
    for step in range(10, 25):
        rb.append({'position': [step, 0], 'place_doubt_step_count': 0})
        prog = min(1.0, 0.05*(step-10)+0.1)
        beliefs, diag = hhmm.update(rb, external_info_gain=0.3, external_plan_progress=prog)
        if step % 3 == 0:
            strat = hhmm.get_strategy_recommendation()
            print("[LATE ] prog=", f"{prog:.2f}", "mode=", strat['recommended_mode'],
                  "p(TASK)=", f"{diag['mode_probabilities'].get('TASK_SOLVING', 0.0):.3f}")



if __name__ == "__main__":
        # test_hhmm_4mode.py
    import math
    import types
    import numpy as np
    import importlib
    import sys
    from pathlib import Path

    # Make sure the module is importable
    MOD_NAME = "HierarchicalHMMBOCPD"
    if MOD_NAME not in sys.modules:
        spec = importlib.util.spec_from_file_location(MOD_NAME, str(Path(__file__).parent / f"{MOD_NAME}.py"))
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
    else:
        mod = sys.modules[MOD_NAME]

    TrueH = mod.TrueHierarchicalHMMWithBOCPD

    def make_buffer(n=20, lost=False, loop=False, doubt=0, start=(0.0,0.0), step=(1.0,0.0)):
        """Build a simple replay_buffer the extractor can read."""
        rb = []
        x, y = start
        for t in range(n):
            pos = [x + step[0]*t, y + step[1]*t]
            rb.append({
                "position": pos,
                "place_doubt_step_count": int(doubt),
                # optional extras could go here
            })
        # optionally force "loop" by repeating last few positions
        if loop and n >= 12:
            for k in range(6):
                rb.append({"position": rb[-6]["position"], "place_doubt_step_count": int(doubt)})
        if lost:
            for i in range(5):
                rb[-1 - i]["place_doubt_step_count"] = max(5, int(doubt) + 5)
        return rb

    def test_init_and_shapes():
        h = TrueH(use_submodes=False, use_bocpd=False)
        assert h.states == ['EXPLORE','NAVIGATE','RECOVER','TASK_SOLVING']
        assert h.transition_matrix.shape == (4,4)
        # rows normalized
        rs = h.transition_matrix.sum(axis=1)
        assert np.allclose(rs, 1.0, atol=1e-6)
        # beliefs normalized
        assert math.isclose(float(h.state_beliefs.sum()), 1.0, rel_tol=1e-9)

    def test_emission_params_and_likelihoods_exist():
        h = TrueH(use_submodes=False, use_bocpd=False)
        # emission params present and keyed by modes
        for m in h.states:
            assert m in h.emission_params
            assert isinstance(h.emission_params[m], dict)
        # basic evidence
        ev = {
            "agent_lost": False, "loop_detected": False,
            "stagnation": 0.2, "info_gain": 0.6,
            "exploration_productivity": 0.55, "navigation_progress": 0.1,
            "recovery_effectiveness": 0.2, "task_progress": 0.1
        }
        for m in h.states:
            lik = h.compute_emission_likelihood(ev, m)
            assert lik > 0.0 and np.isfinite(lik)

    def test_update_without_bocpd_exploration_then_task(capsys):
        h = TrueH(use_submodes=False, use_bocpd=False)
        rb = make_buffer(n=12, doubt=0)
        # early: low plan progress -> expect EXPLORE/NAVIGATE mass
        for t in range(6):
            _, diag = h.update(rb[:t+1], external_info_gain=0.6, external_plan_progress=0.1)
        early_task_p = diag['mode_probabilities']['TASK_SOLVING']

        # later: ramp progress -> expect TASK_SOLVING mass goes up
        last_p = 0.0
        for t in range(6, 20):
            prog = min(1.0, 0.06*(t-6)+0.2)
            _, diag = h.update(rb[:t+1], external_info_gain=0.3, external_plan_progress=prog)
            last_p = diag['mode_probabilities']['TASK_SOLVING']

        assert last_p > early_task_p, "TASK_SOLVING should increase as progress rises"

        # sanity on logs sections
        out = capsys.readouterr().out
        assert "[UPD:1] Evidence:" in out
        assert "[UPD:2] task_progress" in out
        assert "[UPD:3] T row sums" in out
        assert "[UPD:5] eLikes:" in out

    def test_gate_function_speeds_task_entry():
        h = TrueH(use_submodes=False, use_bocpd=False)

        # a gate that ramps enter fast and exit low as progress increases
        def gate(self, evidence, tp_smooth):
            enter = 0.02 + 4.0 * (tp_smooth ** 2)   # small -> large
            exit_ = max(0.6 * (1 - tp_smooth), 0.05)
            return enter, exit_
        h.set_task_solving_gate(fn=gate)

        rb = make_buffer(n=25, doubt=0)
        p_task = []
        for t in range(1, 25):
            prog = min(1.0, 0.05*t)
            _, diag = h.update(rb[:t+1], external_info_gain=0.3, external_plan_progress=prog)
            p_task.append(diag['mode_probabilities']['TASK_SOLVING'])

        # should be monotonically trending up overall (allow a tiny wiggle)
        assert p_task[-1] > p_task[3]
        assert max(np.diff(p_task[-8:])) > -0.02  # no strong down-spikes near the end

    def test_strategy_recommendation_round_trip():
        h = TrueH(use_submodes=False, use_bocpd=False)
        rb = make_buffer(n=8, doubt=0)
        for t in range(1, 8):
            _, diag = h.update(rb[:t+1], external_info_gain=0.5, external_plan_progress=0.2)
        strat = h.get_strategy_recommendation()
        assert 'recommended_mode' in strat
        assert strat['recommended_mode'] in h.states
        assert 'mode_confidence' in strat and 0.0 <= strat['mode_confidence'] <= 1.0

    def test_transition_changes_with_gates():
        h = TrueH(use_submodes=False, use_bocpd=False)
        rb = make_buffer(n=5)
        # fixed gates baseline
        _, _ = h.update(rb, external_info_gain=0.3, external_plan_progress=0.1)
        T0 = h.transition_matrix.copy()

        # strong entry to TASK
        h.set_task_solving_gate(enter=10.0, exit=1.0)
        _, _ = h.update(rb + [{"position":[100,0], "place_doubt_step_count":0}], external_info_gain=0.3, external_plan_progress=0.9)
        T1 = h.transition_matrix.copy()

        i = h.state_index['EXPLORE']; j = h.state_index['TASK_SOLVING']
        assert T1[i, j] > T0[i, j], "enter gate should increase EXPLORE->TASK_SOLVING transition weight"


```

`hierarchical-nav/control_eval/NavigationSystem.py`:

```py

import time
import random
import numpy as np
from collections import deque
from collections import defaultdict
from nltk import PCFG
import math
import re
import heapq
import torch
from collections import namedtuple
from nltk.parse.generate import generate
from nltk.grammar import Nonterminal, Production
# --- optional dreamer_mg import (safe for test harness) ---
try:
    from dreamer_mg.world_model_utils import DictResizeObs, WMPlanner  # type: ignore
except Exception:
    # Minimal stubs so type hints & calls don't explode during testing
    class WMPlanner:  # pragma: no cover
        def astar_prims(self, *args, **kwargs):
            return []
    class DictResizeObs:  # pragma: no cover
        pass

State = namedtuple("State", ["x","y","d"])
DIR_VECS = [(1,0),(0,1),(-1,0),(0,-1)]
ACTIONS   = ["forward","left","right"]

class NavigationSystem:
    def __init__(self, memory_graph, get_current_pose_func, planner: WMPlanner | None = None  ):
        self.memory_graph      = memory_graph          # live reference
        self.get_current_pose  = get_current_pose_func # lambda ↦ (x,y,d)
        
        self.planner = planner 
        # ─ plan bookkeeping ─
        self.full_plan_tokens: list[tuple[int, int]] = []   # [(u,v), …]
        self.token_idx   = 0        # which edge
        self.token_prims: list[str] = []
        self.prim_idx    = 0
        self.plan_progress = -1.0   # -1=no plan, 0-1 executing, 1.1 done
        # ─ misc state you already had ─
        self.target_node_id   = None
        self.current_mode= None
        self.navigation_flags = {}
        self._prev_token_idx = -2
        self._prev_prim_idx  = -2
        self._stall_count    = 0         
        self._stall_limit    = 5      
        self._evade_injected = False
        self._evade_ticks_since_recalc = 0
        self._evade_recalc_every = 4  

        self._pose_hist: deque[tuple[float,float]] = deque(maxlen=8)
        
    def get_available_nodes_with_confidence(self, min_confidence=0.5):
        """Get nodes that are viable for navigation"""
        viable_nodes = []
        emap = self.memory_graph.experience_map
        
        for exp in emap.exps:
            # Check if node has sufficient confidence
            if hasattr(exp, 'confidence') and exp.confidence >= min_confidence:
                viable_nodes.append({
                    'id': exp.id,
                    'confidence': exp.confidence,
                    'pose': (exp.x_m, exp.y_m, getattr(exp, 'dir', 0))
                })
        
        return viable_nodes
    
    def check_navigation_feasibility(self):
        """Early check for navigation feasibility"""
        viable_nodes = self.get_available_nodes_with_confidence()
        
        # Check edge cases
        if len(viable_nodes) < 2:
            self.navigation_flags['insufficient_nodes'] = True
            return False
            
        if len(viable_nodes) == 0:
            self.navigation_flags['no_viable_nodes'] = True
            return False
            
        # Check if we have a current plan that's still valid
        if self.full_plan_tokens and self.target_node_id:
            target_still_viable = any(n['id'] == self.target_node_id for n in viable_nodes)
            if not target_still_viable:
                self.navigation_flags['target_became_unviable'] = True
                return False
                
        self.navigation_flags.clear()  # Clear flags if everything is okay
        return True
    
    def _weighted_prod_choice(self, prods):
        if random.random() < 0.8:                 # ε = 0.2 exploration
            return max(prods, key=lambda p: p.prob())
        weights = [p.prob() for p in prods]
        return random.choices(prods, weights=weights, k=1)[0]

    # ─────────────────────────────────────────────────────────────
    # Replacement for generate_plan_with_pcfg
    # ─────────────────────────────────────────────────────────────
    @staticmethod
    def to_onehot_list(act: str) -> list[int]:
        """'left'/'right'/'forward' → [0,0,1] / [0,1,0] / [1,0,0]"""
        mapping = {'forward': [1,0,0], 'right': [0,1,0], 'left': [0,0,1]}
        return mapping[act]
    
    def _pcfg_shortest_path(self, graph: dict[int, list[int]], start: int, goal: int):
        """Return one shortest path [start,...,goal] in edge-count metric, or None."""
        if start == goal:
            return [start]
        parent = {start: None}
        q = deque([start])
        while q:
            u = q.popleft()
            for v in graph.get(u, ()):
                if v in parent:
                    continue
                parent[v] = u
                if v == goal:
                    # reconstruct
                    path = [v]
                    while u is not None:
                        path.append(u); u = parent[u]
                    return list(reversed(path))
                q.append(v)
        return None
    
    def build_pcfg_from_memory(self, debug: bool = True) -> PCFG:
        """
        Build a PCFG that *deterministically* targets a single GOAL node chosen by
        a human-like 'sparse-zone' heuristic, gates links by confidence (1/0),
        permanently blacklists zero-confidence pairs, and *proposes* new links
        between nearby, currently unconnected nodes (distance threshold).
        The rest of the grammar structure is preserved.

        Side-effects:
        - self.link_blacklist: Set[frozenset({u,v})] is created/updated.
        - self.goal_node_id: the deterministically chosen goal node.
        - self.graph_used_for_pcfg: adjacency (dict[int, list[int]]) actually used.

        Compatibility:
        - generate_plan_with_pcfg() keeps working: we still start from 'NAVPLAN'
            and emit 'STEP_u_v' terminals; target is the last token's 'v'.

        Notes:
        - Node confidences are **ignored**.
        - Link confidences gate edges: getattr(link, "confidence", 1.0) ∈ {1,0}.
        """
        mg   = self.memory_graph
        emap = mg.experience_map
        start = mg.get_current_exp_id()

        # ---------- A) ensure permanent blacklist ---------------------------------
        if not hasattr(self, "link_blacklist") or self.link_blacklist is None:
            self.link_blacklist = set()   # set[frozenset({u,v})]

        # ---------- B) collect basic node list ------------------------------------
        exps = list(getattr(emap, "exps", []))
        if len(exps) == 0:
            raise RuntimeError("No experiences in experience_map")
        id2exp = {e.id: e for e in exps}

        # ---------- C) adjacency from CONFIDENT links + blacklist ------------------
        graph = self._pcfg_build_adjacency_with_confidence_and_inferred(
            exps=exps,
            start_id=start,
            blacklist=self.link_blacklist,
            # Heuristic default for MiniGrid-like spacing (~3.0 units per grid)
            infer_link_max_dist=float(getattr(self, "infer_link_max_dist", 3.4)),
            debug=debug
        )

        # Persist for inspection
        self.graph_used_for_pcfg = graph

        if debug:
            print("[PCFG DEBUG] adjacency (post-confidence+inferred):", {k: sorted(v) for k,v in graph.items()})

        # ---------- D) choose GOAL deterministically via sparse-zone heuristic -----
        goal = self._pcfg_select_goal_node_via_zones(
            exps=exps, start_id=start, graph=graph, debug=debug
        )
        self.goal_node_id = goal

        # If still None for any reason, fallback to farthest reachable node
        if goal is None:
            if debug:
                print("[PCFG DEBUG] Goal via zones unavailable → fallback to farthest reachable node.")
            dists = self._pcfg_bfs_all_dists(graph, start)
            if len(dists) > 0:
                far_node = max(dists.items(), key=lambda kv: (kv[1], kv[0]))[0]
                goal = far_node
            else:
                goal = start
            self.goal_node_id = goal

        # As an additional safeguard, if no path exists to 'goal', fallback.
        if not self._pcfg_is_reachable(graph, start, goal):
            if debug:
                print(f"[PCFG DEBUG] Goal {goal} unreachable from {start} → fallback to farthest reachable.")
            dists = self._pcfg_bfs_all_dists(graph, start)
            if len(dists) > 0:
                far_node = max(dists.items(), key=lambda kv: (kv[1], kv[0]))[0]
                goal = far_node
            else:
                goal = start
            self.goal_node_id = goal

        if debug:
            print(f"[PCFG DEBUG] Chosen GOAL node: {goal} (start={start})")

        # ---------- E) enumerate a few path candidates to GOAL ---------------------
        def all_paths(src, dst, k=16, depth=50):
            out, q = [], deque([[src]])
            seen_paths = set()
            while q and len(out) < k:
                p = q.popleft()
                u = p[-1]
                if u == dst:
                    tup = tuple(p)
                    if tup not in seen_paths:
                        out.append(p); seen_paths.add(tup)
                    continue
                if len(p) >= depth:
                    continue
                for nb in graph.get(u, ()):   # deterministic by sorted adj
                    if nb not in p:
                        q.append(p + [nb])
            return out

        # 1) Always try to get the shortest path first
        sp = self._pcfg_shortest_path(graph, start, goal)
        if debug:
            sp_len = (len(sp) - 1) if sp else None
            print(f"[PCFG DEBUG] shortest path edges: {sp_len}")

        # 2) Choose a depth cap adaptively
        #    Allow an instance override: self.pcfg_depth_cap, self.pcfg_k_paths, self.pcfg_depth_margin
        k_paths   = int(getattr(self, "pcfg_k_paths", 16))
        margin    = int(getattr(self, "pcfg_depth_margin", 4))
        if getattr(self, "pcfg_depth_cap", None):
            depth_cap = int(self.pcfg_depth_cap)
        else:
            if sp is not None:
                depth_cap = (len(sp) - 1) + max(margin, 1)
            else:
                depth_cap = len(exps) + max(margin, 1)   # safe upper bound on simple paths

        # 3) Collect paths: shortest (if any) + additional alternatives (deduped)
        paths = []
        if sp:
            paths.append(sp)
        extras = all_paths(start, goal, k=k_paths, depth=depth_cap)
        for p in extras:
            if not sp or p != sp:
                paths.append(p)

        if debug:
            print(f"[PCFG DEBUG] depth_cap={depth_cap}  total_paths={len(paths)}")

        # If still no path (unreachable), synthesize trivial one
        if not paths:
            paths = [[start]]
        # ---------- F) construct production rules (keep structure) -----------------
        rules = defaultdict(list)

        # NAVPLAN → PATH_goal (deterministic single option)
        rules["NAVPLAN"].append((f"PATH_{goal}", 1.0))

        # PATH_goal → STEP_*_* ...  (equal weight per enumerated path)
        for path in paths:
            step_tokens = []
            #if not self._at_node_exact(start):
                #step_tokens.append(f"STEP_{start}_{start}")  # self-edge only if away
            step_tokens += [f"STEP_{u}_{v}" for u, v in zip(path, path[1:])]
            rhs = " ".join(step_tokens) if step_tokens else f"STEP_{start}_{start}"
            rules[f"PATH_{goal}"].append((rhs, 1.0))

        # Every STEP_u_v becomes a terminal
        for lhs in list(rules.keys()):
            if lhs.startswith("PATH_"):
                for rhs, _ in rules[lhs]:
                    for tok in rhs.split():
                        if tok not in rules:
                            rules[tok].append((f"'{tok}'", 1.0))

        # ---------- G) serialise to NLTK PCFG -------------------------------------
        lines = []
        for lhs, prods in rules.items():
            Z = sum(p for _, p in prods) or 1.0
            for rhs, p in prods:
                lines.append(f"{lhs} -> {rhs} [{p/Z:.6f}]")

        grammar_src = "\n".join(lines)
        if debug:
            print("[PCFG DEBUG] Final grammar:\n" + grammar_src)

        return PCFG.fromstring(grammar_src)

    def _reset_token_prims(self):
        import torch
        if torch.is_tensor(self.token_prims):
            # keep type stable for downstream code
            self.token_prims = torch.empty((0, 3), dtype=self.token_prims.dtype, device=self.token_prims.device)
        else:
            self.token_prims = []
    # ──────────────────────────────────────────────────────────────────────────────
    # Helper methods (paste inside the same class)
    # ──────────────────────────────────────────────────────────────────────────────

    def _pcfg_is_reachable(self, graph: dict[int, list[int]], start: int, goal: int) -> bool:
        if start == goal:
            return True
        q, seen = deque([start]), {start}
        while q:
            u = q.popleft()
            for v in graph.get(u, ()):
                if v == goal:
                    return True
                if v not in seen:
                    seen.add(v)
                    q.append(v)
        return False

    def _pcfg_bfs_all_dists(self, graph: dict[int, list[int]], start: int) -> dict[int, int]:
        """BFS in edge-count metric; returns distances to all reachable nodes."""
        d = {start: 0}
        q = deque([start])
        while q:
            u = q.popleft()
            for v in graph.get(u, ()):
                if v not in d:
                    d[v] = d[u] + 1
                    q.append(v)
        return d

    def _pcfg_build_adjacency_with_confidence_and_inferred(
        self,
        exps,
        start_id: int,
        blacklist: set,
        infer_link_max_dist: float,
        debug: bool = False
    ) -> dict[int, list[int]]:
        """
        Build a directed adjacency graph from:
        (1) confident links (confidence==1) that are not blacklisted
        (2) inferred links for close-by unconnected nodes (below threshold),
            not blacklisted. Inferred links are added in *both* directions.

        Toggles (attributes on self, default False if absent):
        • pcfg_treat_all_links_confident : if True, treat *all* links as conf=1 and
            do NOT add new pairs to blacklist for this build.
        • pcfg_disable_inferred_links    : if True, skip proximity-based inferred links.
        • pcfg_ignore_blacklist          : if True, ignore blacklist both for existing
            links and inferred links (does *not* mutate the blacklist).
        """
        # Read toggles with safe defaults
        treat_all = bool(getattr(self, "pcfg_treat_all_links_confident", True))
        no_infer  = bool(getattr(self, "pcfg_disable_inferred_links", False))
        ign_bl    = bool(getattr(self, "pcfg_ignore_blacklist", True))

        graph = {e.id: [] for e in exps}
        def canon(u,v): return frozenset((u,v))

        dropped, added_black, added_inferred = 0, 0, 0

        # (1) Existing links with confidence & blacklist gating
        for e in exps:
            for lnk in getattr(e, "links", []):
                # neighbor id robustly
                v = None
                tgt = getattr(lnk, "target", None)
                if tgt is not None:
                    v = getattr(tgt, "id", None)
                if v is None:
                    v = getattr(lnk, "target_id", None)
                if v is None:
                    continue  # malformed link

                pair = canon(e.id, v)
                orig_conf = int(getattr(lnk, "confidence", 1))
                conf = 1 if treat_all else orig_conf

                if conf == 1 and (ign_bl or pair not in blacklist):
                    graph[e.id].append(v)
                else:
                    # Only mutate blacklist when we *aren't* in "treat_all" and *aren't* ignoring blacklist
                    if (orig_conf == 0) and (not treat_all) and (not ign_bl):
                        if pair not in blacklist:
                            blacklist.add(pair)
                            added_black += 1
                    dropped += 1
        # (2) Proximity-inferred links (optional) — **use only (x,y) coordinates**
        if not no_infer:
            emap = self.memory_graph.experience_map

            def _xy_of(e):
                # 1) Prefer explicit JSON-style fields
                x = getattr(e, "x", None)
                y = getattr(e, "y", None)
                if x is None or y is None:
                    # 2) Fallback to emap.get_pose(e.id) and take only (x,y)
                    xx, yy, _ = emap.get_pose(e.id)
                    x, y = xx, yy
                return float(x), float(y)

            pts = {e.id: _xy_of(e) for e in exps}
            ids = sorted(pts.keys())

            # Inference mode & knobs (safe defaults)
            infer_mode   = str(getattr(self, "pcfg_infer_mode", "mutual_knn"))  # 'mutual_knn' | 'knn' | 'radius'
            k_neighbors  = int(getattr(self, "pcfg_infer_k", 2))                 # K for (mutual_)kNN
            mutual_only  = bool(getattr(self, "pcfg_infer_mutual_only", True))   # only add if both u∈kNN(v) and v∈kNN(u)
            max_per_node = int(getattr(self, "pcfg_infer_max_per_node", k_neighbors))
            hard_radius  = float(infer_link_max_dist) if (infer_link_max_dist and infer_link_max_dist > 0) else float("inf")
            dbg_infer    = bool(getattr(self, "pcfg_infer_debug", False))

            # Precompute sorted neighbor candidates by Euclidean distance (x,y only)
            sorted_neighbors: dict[int, list[tuple[float, int]]] = {}
            for u in ids:
                ux, uy = pts[u]
                cand: list[tuple[float, int]] = []
                for v in ids:
                    if v == u:
                        continue
                    pair = canon(u, v)
                    if (not ign_bl) and (pair in blacklist):
                        continue
                    # Skip if already connected either way
                    if (v in graph.get(u, ())) or (u in graph.get(v, ())):
                        continue
                    vx, vy = pts[v]
                    d = math.hypot(ux - vx, uy - vy)
                    cand.append((d, v))
                cand.sort(key=lambda t: t[0])  # nearest first
                sorted_neighbors[u] = cand

            inferred_pairs = set()

            if infer_mode in ("knn", "mutual_knn"):
                # Select up to K neighbors per node under the hard radius
                topk_under_radius: dict[int, list[int]] = {}
                for u in ids:
                    lst = [v for (d, v) in sorted_neighbors[u] if d <= hard_radius]
                    topk_under_radius[u] = lst[:k_neighbors]

                if infer_mode == "knn":
                    # one-sided kNN (still under radius)
                    for u in ids:
                        for v in topk_under_radius[u][:max_per_node]:
                            inferred_pairs.add(canon(u, v))
                else:
                    # mutual-kNN under radius: u↔v only if each is in the other's top-K
                    for u in ids:
                        for v in topk_under_radius[u][:max_per_node]:
                            if u in topk_under_radius.get(v, ()):
                                inferred_pairs.add(canon(u, v))

            elif infer_mode == "radius":
                # Pure radius graph (original behavior, but still only (x,y))
                for i, u in enumerate(ids):
                    ux, uy = pts[u]
                    for v in ids[i+1:]:
                        pair = canon(u, v)
                        if (not ign_bl) and (pair in blacklist):
                            continue
                        if (v in graph.get(u, ())) or (u in graph.get(v, ())):
                            continue
                        vx, vy = pts[v]
                        d = math.hypot(ux - vx, uy - vy)
                        if d <= hard_radius:
                            inferred_pairs.add(pair)

            # Add inferred edges (undirected → add both directions)
            for pair in inferred_pairs:
                u, v = tuple(pair)
                graph[u].append(v)
                graph[v].append(u)

            added_inferred = len(inferred_pairs)

            if debug or dbg_infer:
                # Show a few of the longest inferred edges to catch outliers
                longest = sorted(
                    [(math.hypot(pts[u][0]-pts[v][0], pts[u][1]-pts[v][1]), u, v)
                    for (u, v) in [tuple(p) for p in inferred_pairs]],
                    reverse=True
                )[:5]
                if longest:
                    print("[PCFG DEBUG] inferred (longest first) [dist,u,v]:", longest)
                print(f"[PCFG DEBUG] infer_mode={infer_mode} k={k_neighbors} mutual={mutual_only} "
                    f"hard_radius={hard_radius:.3f} max_per_node={max_per_node}")

        # deterministic adjacency
        for u in graph:
            graph[u] = sorted(set(graph[u]))

        if debug:
            print(f"[PCFG DEBUG] flags: treat_all={treat_all}  no_infer={no_infer}  ignore_blacklist={ign_bl}")
            print(f"[PCFG DEBUG] dropped_links={dropped}  blacklisted_additions={added_black}  inferred_additions={added_inferred}")

        return graph


    def _pcfg_select_goal_node_via_zones(
        self,
        exps,
        start_id: int,
        graph: dict[int, list[int]],
        debug: bool = False
    ) -> int | None:
        """
        Choose a GOAL node by:
        1) Form bounding box of all nodes.
        2) Partition into a 3x3 grid of zones.
        3) Find the zone(s) with the fewest nodes (sparsest).
        4) Choose the node closest to the centroid of the sparsest zone
            (deterministic tie-breaker by node id).
        Fallbacks:
        - If too few nodes (<4) or degenerate bbox, return None.
        """
        nodes = [(e.id, float(getattr(e, "x_m", getattr(e, "x", 0.0))), float(getattr(e, "y_m", getattr(e, "y", 0.0)))) for e in exps]
        if len(nodes) < 4:
            
            return None

        xs = [x for _, x, _ in nodes]
        ys = [y for _, _, y in nodes]
        min_x, max_x = min(xs), max(xs)
        min_y, max_y = min(ys), max(ys)

        if not (math.isfinite(min_x) and math.isfinite(max_x) and math.isfinite(min_y) and math.isfinite(max_y)):
            return None
        if abs(max_x - min_x) < 1e-6 or abs(max_y - min_y) < 1e-6:
            return None

        # 3x3 grid partition
        Nx = 4
        Ny = 4
        step_x = (max_x - min_x) / Nx
        step_y = (max_y - min_y) / Ny

        # Assign nodes to cells
        cell_counts = [[0 for _ in range(Ny)] for _ in range(Nx)]
        for _, x, y in nodes:
            ix = min(Nx-1, int((x - min_x) / step_x))
            iy = min(Ny-1, int((y - min_y) / step_y))
            cell_counts[ix][iy] += 1

        # Find sparsest cells (min count)
        min_count = min(c for col in cell_counts for c in col)
        sparsest = [(ix, iy) for ix in range(Nx) for iy in range(Ny) if cell_counts[ix][iy] == min_count]

        # Prefer sparsest cell that is farthest from the *start* node position (centroid distance)
        # Get start coordinates
        sx, sy, _ = getattr(self.memory_graph.experience_map, "get_pose")(start_id)
        def cell_centroid(ix, iy):
            cx = min_x + (ix + 0.5) * step_x
            cy = min_y + (iy + 0.5) * step_y
            return cx, cy

        sparsest_sorted = sorted(
            sparsest,
            key=lambda ij: (
                # choose the cell whose centroid is *farthest* from start (to expand frontier)
                -math.hypot(cell_centroid(*ij)[0]-sx, cell_centroid(*ij)[1]-sy),
                ij[0], ij[1]
            )
        )
        best_cell = sparsest_sorted[0] if sparsest_sorted else None
        if best_cell is None:
            return None
        cx, cy = cell_centroid(*best_cell)

        # Choose actual node: nearest to that centroid
        best_node = None
        best_d = float("inf")
        for nid, x, y in nodes:
            d = (x - cx)**2 + (y - cy)**2
            if d < best_d or (d == best_d and (best_node is None or nid < best_node)):
                best_d = d
                best_node = nid

        # Make sure goal is not the current start unless trivial map; if it is, pick farthest node instead
        if best_node == start_id and len(nodes) > 1:
            dists = self._pcfg_bfs_all_dists(graph, start_id)
            if dists:
                best_node = max(dists.items(), key=lambda kv: (kv[1], kv[0]))[0]

        # Finally, avoid unreachable pick (caller still double-checks)
        return best_node
    def generate_plan_with_pcfg(self, grammar: PCFG, debug: bool = True) -> list[tuple[int, int]]:
        """
        Probabilistic sampler that respects the PCFG production weights.

        1. Recursively expand from start symbol 'NAVPLAN'
        2. Collect terminal tokens ('STEP_u_v')
        3. Tokenise into edge tuples, reset indices and progress counters
        """

        agenda = [Nonterminal("NAVPLAN")]
        sentence: list[str] = []

        if debug:
            print("\n[DBG] Starting PCFG expansion from 'NAVPLAN'")
            print(f"[DBG] Initial agenda: {agenda}")

        # --- 1. stochastic top-down expansion -------------
        while agenda:
            sym = agenda.pop()

            if debug:
                print(f"[DBG] Popped symbol: {sym}")

            if isinstance(sym, Nonterminal):
                prods = grammar.productions(lhs=sym)
                if not prods:
                    raise ValueError(f"No production for {sym}")
                prod = self._weighted_prod_choice(prods)
                rhs = list(reversed(prod.rhs()))  # push in reverse for L-to-R
                agenda.extend(rhs)

                if debug:
                    print(f"[DBG] Expanded {sym} using: {prod}")
                    print(f"[DBG] New agenda: {agenda}")
            else:
                sentence.append(str(sym))
                if debug:
                    print(f"[DBG] Added terminal: {sym}")

            # safety guard
            if len(sentence) > 200:
                raise RuntimeError("PCFG expansion runaway (>200 terminals)")

        plan_str = " ".join(sentence)
        if debug:
            print(f"[DBG] Final sentence: {plan_str}")

        # --- 2. tokenise STEP_u_v → (u,v) ----------------
        self.full_plan_tokens = self._tokenize_plan(plan_str)
        self.token_idx = self.prim_idx = 0
        self._reset_token_prims() 
        self.plan_progress = 0.0

        if debug:
            print(f"[DBG] Tokenized plan: {self.full_plan_tokens}")

        # --- 3. store final target node ID ---------------
        if self.full_plan_tokens:
            _, self.target_node_id = self.full_plan_tokens[-1]
        else:
            self.target_node_id = None

        if debug:
            print(f"[DBG] Target node ID: {self.target_node_id}")

        return self.full_plan_tokens
        
    def _tokenize_plan(self, full_plan: str) -> list[tuple[int, int]]:
        """Convert 'STEP_9_8 STEP_8_5 …' → [(9,8), (8,5), …]."""
        return [
            tuple(map(int, t.split('_')[1:3]))
            for t in full_plan.strip().split()
            if t.startswith('STEP_')
        ]
    
    def _load_edge_primitives(self, wm, belief, debug: bool = False):
        """
        Ensure `self.token_prims` is (re)filled for the *current* edge
        identified by `self.token_idx`.

        Policy:
        • If plan done → clear and return.
        • If primitives already queued → leave them (do NOT reload).
        • If EMPTY:
            A) If already at *target* v → LEAVE EMPTY and return
                (caller will skip the edge — avoids start==goal A*).
            B) If at *source* u → fetch edge path via get_primitives(u,v).
            C) Else → inject a detour A* from real pose to v.
        """
        import torch

        # 0) Plan finished?
        if self.token_idx >= len(self.full_plan_tokens):
            self.token_prims = []
            return

        # 1) Already have something queued? leave it alone.
        seq = getattr(self, "token_prims", [])
        if seq is not None:
            if torch.is_tensor(seq):
                if seq.numel() > 0:
                    return
            else:
                if len(seq) > 0:
                    return

        # 2) We only get here if token_prims is EMPTY
        u, v = self.full_plan_tokens[self.token_idx]
        if debug:
            print(f"[_load_edge_prims] need prims for edge {u}->{v}  at_token={self.token_idx}")

        # === SHORT-CIRCUIT: already at the TARGET node v? ===
        # Leave token_prims empty and let step_plan() advance the edge.
        if self._at_node_exact(v):
            if debug:
                print(f"[_load_edge_prims] already at target v={v} → leave empty (caller will skip)")
            self.token_prims = []
            return

        # A) Are we physically at the *source* node u?
        if self._at_node_exact(u):
            if debug: print(f"[_load_edge_prims] docked at u={u} → get_primitives(u,v)")
            prims = self.get_primitives(u, v, wm, belief)
            # normalize; allow empty
            if torch.is_tensor(prims):
                self.token_prims = [] if prims.numel() == 0 else prims
                if debug and prims.numel() == 0:
                    print("[_load_edge_prims] get_primitives returned EMPTY tensor")
            else:
                self.token_prims = prims if prims else []
                if debug and not prims:
                    print("[_load_edge_prims] get_primitives returned EMPTY list")
            return

        # B) Not at source node → inject detour from *real pose* to target v.
        pose = self.get_current_pose()
        if pose is None:
            if debug: print("[_load_edge_prims] no current pose! cannot detour → empty")
            self.token_prims = []
            return

        sx, sy, sd = pose
        gx, gy, gd = self.memory_graph.experience_map.get_pose(v)

        # Extra guard: if start==goal (pose drift/timing), leave empty and let caller skip.
        if int(round(sx)) == int(round(gx)) and int(round(sy)) == int(round(gy)) and int(round(sd)) == int(round(gd)):
            if debug:
                print(f"[_load_edge_prims] start==goal ({sx},{sy},{sd}) → leave empty (caller will skip)")
            self.token_prims = []
            return

        start = State(int(round(sx)), int(round(sy)), int(sd))
        goal  = State(int(round(gx)), int(round(gy)), int(gd))
        if debug: print(f"[_load_edge_prims] computing detour A* start={start} goal={goal}")

        detour = self._astar_to_node(wm, belief, start, v, debug=debug)
        if detour is None:
            detour = []
        empty = (torch.is_tensor(detour) and detour.numel() == 0) or (not torch.is_tensor(detour) and len(detour) == 0)

        if empty:
            if debug: print("[_load_edge_prims] detour A* FAILED → empty")
            self.token_prims = []
        else:
            if debug:
                ln = detour.shape[0] if torch.is_tensor(detour) else len(detour)
                print(f"[_load_edge_prims] detour injected, len={ln}")
            self.token_prims = detour


    def step_plan(self, wm, belief, debug: bool = True):
        """
        Emit *one* primitive towards executing the current plan.

        Cases:
        1) Normal edge primitives (we are at u; cached link path known).
        2) Detour primitives (we injected an A* seq to v).
        3) If empty/unreachable, try forced detour; else mark plan_progress=-1.0.
        Returns: ([primitive_payload], 1) or ([], 0) on failure/done.
        """
        import torch

        # --- helpers that are SAFE for tensors or lists -----------------------
        def _plen(seq):
            if seq is None: return 0
            if torch.is_tensor(seq): return int(seq.shape[0])
            try: return len(seq)
            except Exception: return 0

        def _pempty(seq):
            if seq is None: return True
            if torch.is_tensor(seq): return seq.numel() == 0
            try: return len(seq) == 0
            except Exception: return True

        # ----------------------------------------------------------------------
        # 0) no plan or finished?
        # ----------------------------------------------------------------------
        if self.token_idx >= len(self.full_plan_tokens):
            if debug: print("[step_plan] plan complete or empty")
            self.plan_progress = 1.1 if self.full_plan_tokens else -1.0
            self.navigation_flags['plan_complete'] = True 
            return [], 0

        # ----------------------------------------------------------------------
        # 1) ensure we have primitives for *current* edge
        # ----------------------------------------------------------------------
        self._load_edge_primitives(wm, belief, debug=debug)

        # If still empty → decide whether to skip edge (maybe we are at v) or detour/fail
        while True:
            if not _pempty(self.token_prims):
                break  # good; we have something to emit

            # nothing loaded: are we already at this edge’s *target* node?
            u, v = self.full_plan_tokens[self.token_idx]
            if self._at_node_exact(v):
                if debug: print(f"[step_plan] already at target node {v} → advance edge")
                self.token_idx += 1
                if self.token_idx >= len(self.full_plan_tokens):
                    self.plan_progress = 1.1
                    self.navigation_flags['plan_complete'] = True 
                    return [], 0
                self._load_edge_primitives(wm, belief, debug=debug)
                continue

            # not at v & still empty → try forced detour to v
            if debug: print("[step_plan] empty but not at target → force detour inject")
            pose = self.get_current_pose()
            if pose is None:
                if debug: print("[step_plan] no pose; cannot detour → fail")
                self.plan_progress = -1.0
                return [], 0

            sx, sy, sd = pose
            gx, gy, gd = self.memory_graph.experience_map.get_pose(v)
            start = State(int(round(sx)), int(round(sy)), int(sd))
            goal  = State(int(round(gx)), int(round(gy)), int(gd))

            # guard: if start==goal (timing/rounding), let the while-loop recheck skip
            if (start.x, start.y, start.d) == (goal.x, goal.y, goal.d):
                if debug: print("[step_plan] start==goal; recheck skip on next loop")
                # leave token_prims empty and loop to the _at_node_exact(v) path
                continue

            detour = self.planner.astar_prims(wm, belief, start, goal, verbose=debug)
            if detour is None:
                detour = []

            if (torch.is_tensor(detour) and detour.numel() == 0) or \
            (not torch.is_tensor(detour) and len(detour) == 0):
                if debug: print("[step_plan] forced detour FAILED → abort plan")
                self.plan_progress = -1.0
                return [], 0

            if debug:
                ln = detour.shape[0] if torch.is_tensor(detour) else len(detour)
                print(f"[step_plan] forced detour injected len={ln}")
            self.token_prims = detour
            self.prim_idx = 0
            self._evade_injected = True
            break

        # ----------------------------------------------------------------------
        # 2) serve ONE primitive from token_prims
        # ----------------------------------------------------------------------
        raw_prim = self.token_prims[self.prim_idx]
        self.prim_idx += 1

        # normalize to string + payload
        if torch.is_tensor(raw_prim):
            idx = int(raw_prim.argmax().item())
            prim_str = ("forward", "right", "left")[idx]
            prim_out = raw_prim.cpu().tolist()
        elif isinstance(raw_prim, (list, tuple)):
            try:
                import numpy as _np
                idx = int(_np.argmax(raw_prim))
                prim_str = ("forward", "right", "left")[idx]
            except Exception:
                prim_str = str(raw_prim)
            prim_out = list(raw_prim)
        elif isinstance(raw_prim, str):
            prim_str = raw_prim
            prim_out = raw_prim
        else:
            prim_str = str(raw_prim)
            prim_out = raw_prim
        # normalize to string + payload
        if torch.is_tensor(raw_prim):
            idx = int(raw_prim.argmax().item())
            prim_str = ("forward", "right", "left")[idx]
            prim_out = raw_prim.cpu().tolist()
        elif isinstance(raw_prim, (list, tuple)):
            try:
                import numpy as _np
                idx = int(_np.argmax(raw_prim))
                prim_str = ("forward", "right", "left")[idx]
            except Exception:
                prim_str = str(raw_prim)
            prim_out = list(raw_prim)
        elif isinstance(raw_prim, str):
            prim_str = raw_prim
            prim_out = raw_prim
        else:
            prim_str = str(raw_prim)
            prim_out = raw_prim

        self._last_served_prim = prim_str

        # === ISSUE ACCOUNTING (so navigation_grade can see progress) ===
        self._issue_seq = int(getattr(self, "_issue_seq", 0)) + 1
        # capture a baseline pose and the pose_hist length at issue time
        hist = getattr(self, "_pose_hist", ())
        self._issue_baseline_hist_len = len(hist)
        if len(hist) > 0:
            self._issue_baseline_pose = hist[-1]
            self._issue_baseline_valid = True
        else:
            # fallback if pose_hist empty: use current pose if available
            cur = self.get_current_pose()
            if cur is not None:
                self._issue_baseline_pose = (float(cur[0]), float(cur[1]), float(cur[2]))
                self._issue_baseline_valid = True
            else:
                self._issue_baseline_valid = False
        self._last_served_prim = prim_str

        # ----------------------------------------------------------------------
        # 3) finished this primitive sequence?
        # ----------------------------------------------------------------------
        if self.prim_idx >= _plen(self.token_prims):
            if getattr(self, "_evade_injected", False):
                if debug: print("[step_plan] detour drained; will re-evaluate edge next tick")
                self.token_prims = []   # leave empty; skip/advance handled next call
                self.prim_idx = 0
                self._evade_injected = False
            else:
                # completed planned edge path
                self.token_idx += 1
                self.token_prims = []
                self.prim_idx = 0

        # ----------------------------------------------------------------------
        # 4) recompute progress scalar (0..1; don't mark -1 here)
        # ----------------------------------------------------------------------
        done_edges = self.token_idx
        fraction = (self.prim_idx / max(_plen(self.token_prims), 1)) if not _pempty(self.token_prims) else 0
        self.plan_progress = (done_edges + fraction) / max(len(self.full_plan_tokens), 1)

        if debug:
            print(f"[step_plan] edge={done_edges}/{len(self.full_plan_tokens)} "
                f"prim={self.prim_idx}/{_plen(self.token_prims)} "
                f"progress={self.plan_progress:.3f}")

        return [prim_out], 1




    def new_plan_from_grammar(self, grammar: PCFG):
        sent = random.choice(list(generate(grammar, depth=50)))
        self.full_plan_tokens = self._tokenize_plan(" ".join(sent))
        self.token_idx = self.prim_idx = 0
        self._reset_token_prims() 
        self.plan_progress = 0.0
        # final target = last rhs node
        if self.full_plan_tokens:
            _, self.target_node_id = self.full_plan_tokens[-1]

    def progress_scalar(self) -> float:
        """-1 no plan, 0-1 executing, 1.1 finished."""
        return self.plan_progress
    def task_solve_mission(self) -> str | None:
        """
        Return the mission string, e.g., "go to red room".
        First tries an attribute you can set externally (self.task_mission),
        then a generic fallback.
        """
        ms = getattr(self, "task_mission", None)
        if isinstance(ms, str) and ms.strip():
            return ms
        # Fallback – you can replace this if your env feeds a mission string elsewhere
        return "go to red room"
    def _parse_color_from_mission(self, mission: str | None) -> str | None:
        """
        Extract a MiniGrid color token from free text. Returns UPPERCASE or None.
        """
        if not mission:
            return None
        # canonical MiniGrid palette (+ a few aliases)
        canon = {
            "RED":"RED", "GREEN":"GREEN", "BLUE":"BLUE",
            "YELLOW":"YELLOW", "PURPLE":"PURPLE", "GREY":"GREY", "GRAY":"GREY",
            "BROWN":"BROWN", "ORANGE":"ORANGE"
        }
        m = re.search(r"\b(red|green|blue|yellow|purple|grey|gray|brown|orange)\b", mission, re.I)
        return canon.get(m.group(0).upper(), None) if m else None



    def push_pose(self, pose):
        """pose = (x,y,θ) or dict with x,y"""
        if pose is None:
            return
        if isinstance(pose, dict):
            self._pose_hist.append((float(pose["x"]), float(pose["y"]),float(pose["θ"])))
        else:
            self._pose_hist.append((float(pose[0]), float(pose[1]),float(pose[2])))
    def _at_node_exact(self, node_id: int) -> bool:
        pose = self.get_current_pose()
        if pose is None:
            return False
        x, y, th = pose
        nx, ny, nth = self.memory_graph.experience_map.get_pose(node_id)
        return int(round(x)) == int(nx) and int(round(y)) == int(ny) 
    def _coalesce(self, *vals):
        """Return the first value that is not None."""
        for v in vals:
            if v is not None:
                return v
        return None

    def _next_hop_after(self, v: int) -> int | None:
        """Return w if the next token is v->w, else None."""
        i = int(getattr(self, "token_idx", 0))
        if i + 1 < len(self.full_plan_tokens):
            u2, w = self.full_plan_tokens[i + 1]
            if u2 == v:
                return w
        return None

    def _heading_from_to(self, from_id: int, to_id: int | None) -> int | None:
        """
        Grid heading 0:E,1:N,2:W,3:S from pose deltas (x,y only).
        Safe when to_id is None or positions are identical (returns None).
        """
        if to_id is None:
            return None
        fx, fy, _ = self.memory_graph.experience_map.get_pose(from_id)
        tx, ty, _ = self.memory_graph.experience_map.get_pose(to_id)
        dx = int(round(tx - fx))
        dy = int(round(ty - fy))
        if dx == 0 and dy == 0:
            return None
        # Prefer axis with larger magnitude; ties fall back to x-axis
        if abs(dx) >= abs(dy):
            return 0 if dx > 0 else 2
        else:
            return 1 if dy > 0 else 3

    def _astar_to_node(self, wm, belief, start: "State", v: int, debug: bool = False):
        """
        Run A* from 'start' to node v, picking a *preferred* goal heading:
        • face from v toward next hop (if any),
        • else keep start.d (avoid gratuitous spins at final goal),
        • else fall back to stored gd at v.
        """
        gx, gy, gd_stored = self.memory_graph.experience_map.get_pose(v)

        # Compute preferred heading safely (don’t use boolean `or` with ints)
        preferred = self._heading_from_to(v, self._next_hop_after(v))
        goal_dir = self._coalesce(preferred, int(start.d) % 4, int(gd_stored) % 4)
        if goal_dir is None:
            goal_dir = int(gd_stored) % 4  # super-safe fallback

        goal = State(int(round(gx)), int(round(gy)), int(goal_dir))
        if debug:
            print(f"[_astar_to_node] v={v} start={start} goal={goal} (stored_gd={gd_stored}, next={self._next_hop_after(v)})")
        return self.planner.astar_prims(wm, belief, start, goal, verbose=debug)

    def universal_navigation(self, submode: str, wm, belief) -> tuple[list[str], int]:
        dbg = getattr(self, "debug_universal_navigation", False)
        def _log(msg: str) -> None:
            if dbg:
                print(msg)

        cur_pose = self.get_current_pose()
        _log(f"[universal_navigation] submode={submode} pose={cur_pose} "
            f"tok={self.token_idx}/{len(self.full_plan_tokens)} "
            f"prim_idx={self.prim_idx} prog={self.plan_progress:.3f}")

        # detour bookkeeping
        if not hasattr(self, "_evade_injected"):           self._evade_injected = False
        if not hasattr(self, "_evade_ticks_since_recalc"): self._evade_ticks_since_recalc = 0
        if not hasattr(self, "_evade_recalc_every"):
            import random; self._evade_recalc_every = random.randint(3, 5)

        # helpers
        def _is_empty_path(path) -> bool:
            if path is None: return True
            if hasattr(path, "numel"):
                try: return path.numel() == 0
                except Exception: pass
            try: return len(path) == 0
            except Exception: return False

        def _prims_pending() -> bool:
            seq = getattr(self, "token_prims", [])
            if seq is None: return False
            try:
                import torch
                if torch.is_tensor(seq):
                    return self.prim_idx < int(seq.shape[0])
            except Exception:
                pass
            try:
                return self.prim_idx < len(seq)
            except Exception:
                return False

        def _fallback_turn() -> tuple[list[str], int]:
            import random
            turn = random.choice(["right", "left"])
            self._last_served_prim = turn
            fb = [self.to_onehot_list(turn)]
            _log(f"[navigate] STALL → issuing {fb}")
            return fb, 1

        def _current_target():
            if self.token_idx < len(self.full_plan_tokens):
                return self.full_plan_tokens[self.token_idx]
            return None

        def _plan_local_path_to_target(tgt_exp_id):
            pose = self.get_current_pose()
            if pose is None:
                _log("[navigate] no current pose → cannot A*; returning EMPTY")
                return []
            start = self._pose_to_state(pose)
            tgt_pose = self.memory_graph.experience_map.get_pose(tgt_exp_id)
            goal  = self._pose_to_state(tgt_pose)
            _log(f"[navigate] A* request start={start} goal={goal} (tgt={tgt_exp_id})")
            try:
                path = self._astar_to_node(wm, belief, start, tgt_exp_id, debug=dbg)
                if _is_empty_path(path): _log("[navigate] A* result: EMPTY")
                else:
                    try: ln = len(path)
                    except Exception: ln = "<tensor>"
                    _log(f"[navigate] A* result: ok len={ln}")
                return path
            except Exception as e:
                _log(f"[navigate] astar_prims error: {e}")
                return []

        def _highlevel_replan_from_here(penalize_pair: tuple[int,int] | None = None,
                                        penalize_node: int | None = None) -> bool:
            if penalize_node is not None:
                _log("[navigate] note: node-confidence penalties are deprecated; ignoring.")
            try:
                if penalize_pair is not None:
                    u, v = int(penalize_pair[0]), int(penalize_pair[1])
                    for a, b in ((u, v), (v, u)):
                        link = self._find_link(a, b)
                        if link is not None:
                            link.confidence = 0
                    if not hasattr(self, "link_blacklist") or self.link_blacklist is None:
                        self.link_blacklist = set()
                    self.link_blacklist.add(frozenset({u, v}))
            except Exception as e:
                _log(f"[navigate] warn: link penalty failed: {e}")

            try:
                _log("[navigate] High-level REPLAN via PCFG…")
                grammar = self.build_pcfg_from_memory()
                self.generate_plan_with_pcfg(grammar)
                _log(f"[navigate] NEW full_plan_tokens={self.full_plan_tokens}")
                # Reset low-level progress
                self.token_prims = []
                self.prim_idx = 0
                self._evade_injected = False
                self._evade_ticks_since_recalc = 0
                ok = len(self.full_plan_tokens) > 0
                _log(f"[navigate] REPLAN success={ok}")
                return ok
            except Exception as e:
                _log(f"[navigate] High-level replan failed: {e}")
                return False

        # 0) If no plan at all, try one replan
        if not self.full_plan_tokens or self.token_idx >= len(self.full_plan_tokens):
            _log("[navigate] No usable plan tokens at entry.")
            if _highlevel_replan_from_here():
                _log("[navigate] Replan provided tokens; continuing.")
            else:
                _log("[navigate] Replan failed; falling back to stall-turn.")
                return _fallback_turn()

        # 1) Current edge + guarded reset
        cur_edge = _current_target()
        if cur_edge is None:
            _log("[navigate] current_target() is None after (re)plan.")
            return _fallback_turn()
        src, tgt = cur_edge
        if not hasattr(self, "_edge_key") or (src, tgt) != self._edge_key:
            self._edge_key = (src, tgt)
            self._edge_evades = 0
            self.navigation_flags.pop('evade_request', None)
            self.navigation_flags.pop('replan_request', None)
            self.navigation_flags.pop('replan_bad_node', None)
        _log(f"[navigate] current edge: src={src} -> tgt={tgt} key={getattr(self, '_edge_key', None)}")

        # 1a) ### FAST PATH: if we already have primitives to execute, just step them
        if _prims_pending():
            _log("[navigate] prims pending → step_plan()")
            prims, n = self.step_plan(wm, belief, debug=dbg)
            if not prims or n == 0:
                return _fallback_turn()
            return prims, n

        # 2) stalled motion escalates
        if self.navigation_flags.get('stalled_motion', False):
            _log("[navigate] stalled_motion → escalate to REPLAN")
            ok = _highlevel_replan_from_here(penalize_pair=(src, tgt), penalize_node=tgt)
            self.navigation_flags.pop('stalled_motion', None)
            if not ok:
                return _fallback_turn()
            cur_edge = _current_target()
            if cur_edge is None:
                return _fallback_turn()
            src, tgt = cur_edge
            if (src, tgt) != self._edge_key:
                self._edge_key = (src, tgt)
                self._edge_evades = 0
                self.navigation_flags.pop('evade_request', None)
                self.navigation_flags.pop('replan_request', None)
                self.navigation_flags.pop('replan_bad_node', None)

        # 3) explicit REPLAN request from grader
        if self.navigation_flags.get('replan_request', False):
            bad_node = self.navigation_flags.get('replan_bad_node', tgt)
            _log(f"[navigate] REPLAN requested; penalize node {bad_node} and pair ({src},{tgt})")
            ok = _highlevel_replan_from_here(penalize_pair=(src, tgt), penalize_node=bad_node)
            self.navigation_flags.pop('replan_request', None)
            self.navigation_flags.pop('replan_bad_node', None)
            self.navigation_flags.pop('evade_request', None)
            if not ok:
                return _fallback_turn()
            cur_edge = _current_target()
            if cur_edge is None:
                return _fallback_turn()
            src, tgt = cur_edge

        # 4) EVade request → inject/refresh a local detour
        if self.navigation_flags.get('evade_request', False) and not self._evade_injected:
            _log("[navigate] EVADE requested → compute local A* detour to tgt")
            if self._at_node_exact(tgt):
                _log(f"[navigate] already at target node {tgt} → skip edge via step_plan")
                prims, n = self.step_plan(wm, belief, debug=dbg)
                self.navigation_flags.pop('evade_request', None)
                if prims and n:
                    return prims, n
                cur_edge = _current_target()
                if cur_edge is None:
                    return _fallback_turn()
                src, tgt = cur_edge

            detour = _plan_local_path_to_target(tgt)
            if _is_empty_path(detour):
                _log("[navigate] EVADE A* empty → escalate to REPLAN")
                ok = _highlevel_replan_from_here(penalize_pair=(src, tgt), penalize_node=tgt)
                self.navigation_flags.pop('evade_request', None)
                if not ok:
                    return _fallback_turn()
                cur_edge = _current_target()
                if cur_edge is None:
                    return _fallback_turn()
                src, tgt = cur_edge
            else:
                try: ln = len(detour)
                except Exception: ln = "<tensor>"
                _log(f"[navigate] injecting detour len={ln}")
                self.token_prims = detour
                self.prim_idx = 0
                self._evade_injected = True
                self._evade_ticks_since_recalc = 0
                import random; self._evade_recalc_every = random.randint(3, 5)
                prims, n = self.step_plan(wm, belief, debug=dbg)
                if not prims or n == 0:
                    return _fallback_turn()
                self.navigation_flags.pop('evade_request', None)
                return prims, n

        # 5) Active detour: keep stepping / refreshing
        if self._evade_injected:
            self._evade_ticks_since_recalc += 1
            _log(f"[navigate] stepping detour; ticks={self._evade_ticks_since_recalc}/{self._evade_recalc_every}")

            if self._at_node_exact(tgt):
                _log(f"[navigate] reached target node {tgt} → resume main plan")
                self._evade_injected = False
                self.token_prims = []
                self.prim_idx = 0
                prims, n = self.step_plan(wm, belief, debug=dbg)
                if not prims or n == 0:
                    return _fallback_turn()
                return prims, n

            if self._evade_ticks_since_recalc >= self._evade_recalc_every:
                _log("[navigate] periodic A* refresh due")
                self._evade_ticks_since_recalc = 0
                import random; self._evade_recalc_every = random.randint(3, 5)
                detour = _plan_local_path_to_target(tgt)
                if not _is_empty_path(detour):
                    self.token_prims = detour
                    self.prim_idx = 0

            prims, n = self.step_plan(wm, belief, debug=dbg)
            if not prims or n == 0:
                return _fallback_turn()
            return prims, n

        # 6) No detour active → compute fresh local path (only if NO prims pending)
        _log("[navigate] no active detour → compute local path to tgt")
        if self._at_node_exact(tgt):
            _log(f"[navigate] already at target node {tgt} → skip edge via step_plan")
            prims, n = self.step_plan(wm, belief, debug=dbg)
            if prims and n:
                return prims, n
            cur_edge = _current_target()
            if cur_edge is None:
                return _fallback_turn()
            src, tgt = cur_edge

        detour = _plan_local_path_to_target(tgt)
        if _is_empty_path(detour):
            _log("[navigate] A* to current target FAILED → penalize and REPLAN")
            ok = _highlevel_replan_from_here(penalize_pair=(src, tgt), penalize_node=tgt)
            if not ok:
                return _fallback_turn()
            cur_edge = _current_target()
            if cur_edge is None:
                return _fallback_turn()
            src, tgt = cur_edge
            detour = _plan_local_path_to_target(tgt)
            if _is_empty_path(detour):
                _log("[navigate] Local path still empty after REPLAN → fallback turn")
                return _fallback_turn()

        try: ln = len(detour)
        except Exception: ln = "<tensor>"
        _log(f"[navigate] injecting local path len={ln}")
        self.token_prims = detour
        self.prim_idx = 0
        self._evade_injected = True
        self._evade_ticks_since_recalc = 0
        import random; self._evade_recalc_every = random.randint(3, 5)

        prims, n = self.step_plan(wm, belief, debug=dbg)
        if not prims or n == 0:
            return _fallback_turn()
        return prims, n




    # ---- small helper to safely format ints in logs (avoids NameError if you paste into class scope) ----
    def _safe_int(self,x):
        try:
            return int(x)
        except Exception:
            return x


    
    
    def _crash_node_confidence(self, node_id):
        """Reduce confidence of a problematic node"""
        emap = self.memory_graph.experience_map
        for exp in emap.exps:
            if exp.id == node_id:
                if hasattr(exp, 'confidence'):
                    exp.confidence *= 0.5  # Reduce confidence by half
                else:
                    exp.confidence = 0.1  # Set low confidence
                break

    def generate_initial_plan(self, grammar: PCFG):
        """1-shot helper for the controller (kept for API parity)."""
        self.new_plan_from_grammar(grammar)

    
    def _pose_to_state(self, pose):
        """Convert pose tuple to State object"""
        # Assuming State class exists as in your A* implementation
        x, y, direction = pose
        return State(int(round(x)), int(round(y)), int(direction))
    
    
    
    
# Observer integration (to be added to your step function)

    
    def navigation_grade(self) -> float:
        """
        Debuggable grader that is robust to prim_idx/token resets.
        Progress is keyed off a monotonic _issue_seq set by step_plan().
        """
        dbg = getattr(self, "debug_navigation_grade", True)
        def _log(*args):
            if dbg:
                print("[GRADE]", *args)

        mode = getattr(self, 'current_mode', 'NAVIGATE')
        if mode != 'NAVIGATE':
            if hasattr(self, "_edge_evades"): self._edge_evades = 0
            self.navigation_flags.pop('evade_request', None)
            self.navigation_flags.pop('replan_request', None)
            self.navigation_flags.pop('replan_bad_node', None)
            _log("mode != NAVIGATE → 0.0")
            return 0.0

        if self.token_idx >= len(self.full_plan_tokens):
            _log(f"no tokens to grade: token_idx={self.token_idx} len={len(self.full_plan_tokens)} → -1.0")
            return -1.0

        # NEW: sequence-based progress (ignore prim_idx churn)
        issue_seq = int(getattr(self, "_issue_seq", 0))
        prev_seq  = int(getattr(self, "_prev_issue_seq", -1))
        if issue_seq == prev_seq:
            _log(f"no new issued primitive yet (issue_seq={issue_seq}) → 0.0")
            return 0.0
        self._prev_issue_seq = issue_seq

        # Need at least two pose samples
        if len(getattr(self, "_pose_hist", ())) < 2:
            self._stall_bad_motion = 0
            _log("pose_hist < 2 → optimistic 1.0")
            return 1.0

        prim = getattr(self, "_last_served_prim", None)
        if prim is None:
            _log("WARN: _last_served_prim is None → 0.0")
            return 0.0

        # Choose baseline: prefer the one captured at issue time
        hist_len = len(self._pose_hist)
        x1, y1, th1 = self._pose_hist[-2]
        baseline = "hist[-2]"
        if getattr(self, "_issue_baseline_valid", False):
            bx, by, bth = self._issue_baseline_pose
            # use it only if at least one new sample was appended since issue
            if getattr(self, "_issue_baseline_hist_len", 0) <= hist_len - 1:
                x1, y1, th1 = bx, by, bth
                baseline = "issued_baseline"

        x2, y2, th2 = self._pose_hist[-1]

        x1i, y1i, th1i = int(round(x1)), int(round(y1)), int(round(th1))
        x2i, y2i, th2i = int(round(x2)), int(round(y2)), int(round(th2))

        if (x1i, y1i, th1i) == (x2i, y2i, th2i):
            _log(f"IDENTICAL samples {x1i,y1i,th1i} → graded two post-step poses. "
                f"Order must be: issue → execute → push_pose(new) → grade.")
            return 0.0

        # Current edge
        if self.token_idx < len(self.full_plan_tokens):
            u, v = self.full_plan_tokens[self.token_idx]
        else:
            u = v = None

        if not hasattr(self, "_edge_key"):
            self._edge_key = (u, v)
            self._edge_evades = 0
        if (u, v) != getattr(self, "_edge_key", None):
            _log(f"edge changed {(u,v)} (prev={getattr(self,'_edge_key', None)}) → reset per-edge flags")
            self._edge_key = (u, v)
            self._edge_evades = 0
            self.navigation_flags.pop('evade_request', None)
            self.navigation_flags.pop('replan_request', None)
            self.navigation_flags.pop('replan_bad_node', None)

        # Expected outcome
        def _expected_after(pose, action):
            xx, yy, dd = pose
            if action == "forward":
                dx, dy = DIR_VECS[dd % 4]
                return (xx + dx, yy + dy, dd)
            elif action == "left":
                return (xx, yy, (dd - 1) % 4)
            elif action == "right":
                return (xx, yy, (dd + 1) % 4)
            else:
                return (xx, yy, dd)

        ex_x, ex_y, ex_th = _expected_after((x1i, y1i, th1i), prim)
        ok_pos = (x2i == ex_x and y2i == ex_y)
        ok_dir = (th2i == ex_th)

        _log(f"prim='{prim}'  tok={self.token_idx}/{len(self.full_plan_tokens)}  "
            f"edge={(u,v)}  baseline={baseline}")
        _log(f"pose prev=({x1i},{y1i},{th1i})  curr=({x2i},{y2i},{th2i})  "
            f"expected=({ex_x},{ex_y},{ex_th})  ok_pos={ok_pos} ok_dir={ok_dir}")
        _log(f"flags pre: evades={getattr(self,'_edge_evades',0)}  "
            f"evade_req={self.navigation_flags.get('evade_request')}  "
            f"replan_req={self.navigation_flags.get('replan_request')}")

        if prim == "forward":
            if ok_pos and ok_dir:
                self.navigation_flags.pop('evade_request', None)
                self._edge_evades = 0
                self._stall_bad_motion = 0
                _log("FORWARD success → 1.0")
                return 1.0
            else:
                self.navigation_flags['evade_request'] = True
                self._edge_evades = int(getattr(self, "_edge_evades", 0)) + 1
                self._stall_bad_motion = int(getattr(self, "_stall_bad_motion", 0)) + 1
                thr = int(getattr(self, "evades_replan_threshold", 2))
                if self._edge_evades >= thr and v is not None:
                    self.navigation_flags['replan_request'] = True
                    self.navigation_flags['replan_bad_node'] = v
                    self.navigation_flags.pop('evade_request', None)
                    self.plan_progress = -1.0
                    _log(f"FORWARD fail (evades={self._edge_evades} >= {thr}) → replan_request, plan_progress=-1.0 → 0.0")
                else:
                    _log(f"FORWARD fail (evades={self._edge_evades}/{thr}) → evade_request → 0.0")
                return 0.0

        if prim in ("left", "right"):
            if ok_dir and (x2i == x1i) and (y2i == y1i):
                self._stall_bad_motion = 0
                _log(f"TURN {prim} success → 1.0")
                return 1.0
            else:
                self._stall_bad_motion = int(getattr(self, "_stall_bad_motion", 0)) + 1
                if self._stall_bad_motion >= int(getattr(self, "_stall_limit", 5)):
                    self.navigation_flags['stalled_motion'] = True
                    self.plan_progress = -1.0
                    _log(f"TURN {prim} fail (stall={self._stall_bad_motion}) → stalled_motion, plan_progress=-1.0 → 0.0")
                else:
                    _log(f"TURN {prim} minor fail (stall={self._stall_bad_motion}) → 0.0")
                return 0.0

        _log(f"unknown primitive '{prim}' → 0.5")
        return 0.5


    def _safe(self, u, v):
        try: return (int(u), int(v))
        except Exception: return (u, v)


    def _at_start_node_exact(self, start_id: int) -> bool:
        """True iff current pose *exactly* matches the stored node pose."""
        pose = self.get_current_pose()
        if pose is None:
            return False

        x, y, th = pose
        nx, ny, nth = self.memory_graph.experience_map.get_pose(start_id)

        # compare after rounding to the same precision that was used to store poses
        return int(round(x)) == int(nx) and \
            int(round(y)) == int(ny) 
    
    
          
    def get_primitives(self, u: int, v: int,wm,belief) -> list[str]:
        """
        Return the best primitive sequence for traversing the edge u→v:

        0) If we’ve already stored a path in the ExperienceLink, return it.
        1) Otherwise, build a fresh A* plan from *our real pose* → node v.
        2) Cache it in the link and return.
        """
        emap = self.memory_graph.experience_map

        # 1) No cache → build an A* state from our current *real* pose
        real = self.get_current_pose()
        if real is None:
            raise RuntimeError("No last_real_pose available for A* start!")
        sx, sy, sd = real
        start = State(int(round(sx)), int(round(sy)), int(sd))

        # target node’s stored map pose
        gx, gy, gd = emap.get_pose(v)
        goal = State(int(round(gx)), int(round(gy)), int(gd))

        print(f"[get_primitives] no cache {u}->{v}, A* from {start} → {goal}")

        # 2) Run your egocentric‐aware A*:
        prims = self._astar_to_node(wm, belief, start, v, debug=True)
        print(f"[get_primitives] A* returned for {u}->{v}: {prims}")

        return prims


    def _find_link(self, u: int, v: int):
        """
        Scan your ExperienceMap for a u→v link; return it or None.
        """
        emap = self.memory_graph.experience_map
        for exp in emap.exps:
            if exp.id == u:
                for link in exp.links:
                    if link.target.id == v:
                        return link
        return None




    




# ──────────────────────────────────────────────────────────────────────────────
# Test harness: build surrogate cognitive graph from snapshot JSON and exercise
# build_pcfg_from_memory() + generate_plan_with_pcfg().
# Run:
#   python NavigationSystem.py --json /mnt/data/snapshot_t0480.json
# Options:
#   --infer-dist 3.4     (override inferred-link distance)
#   --seed 42            (deterministic link confidence synthesis)
# ──────────────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    import json, argparse, math, random, sys
    from collections import defaultdict

    # ----- Light-weight surrogate data structures to mimic your live objects ---
    class _SurLink:
        __slots__ = ("target", "confidence")
        def __init__(self, target, confidence=1):
            self.target = target
            self.confidence = int(confidence)

    class _SurExp:
        __slots__ = ("id", "x_m", "y_m", "dir", "links")
        def __init__(self, node):
            self.id   = int(node["id"])
            self.x_m  = float(node.get("x_m", node.get("x", 0.0)))
            self.y_m  = float(node.get("y_m", node.get("y", 0.0)))
            rp        = node.get("real_pose", None)
            self.dir  = int(rp[2]) if (isinstance(rp, (list, tuple)) and len(rp) >= 3) else int(node.get("dir", 0))
            self.links = []  # filled later

    class _SurExperienceMap:
        def __init__(self, exps):
            self.exps = exps
            self._pose = {e.id: (e.x_m, e.y_m, e.dir) for e in exps}
        def get_pose(self, exp_id: int):
            return self._pose[int(exp_id)]

    class _SurMemoryGraph:
        def __init__(self, exps, start_id):
            self.experience_map = _SurExperienceMap(exps)
            self._start_id = int(start_id)
        def get_current_exp_id(self) -> int:
            return self._start_id

    # ---------------------------- CLI -----------------------------------------
    ap = argparse.ArgumentParser(description="PCFG test harness over snapshot JSON")
    ap.add_argument("--json", default="/mnt/data/snapshot_t0480.json")
    ap.add_argument("--infer-dist", type=float, default=None,
                    help="Override inferred-link distance (if omitted, auto-derive)")
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    random.seed(args.seed)

    # ------------------------ Load snapshot -----------------------------------
    with open(args.json, "r") as f:
        snap = json.load(f)

    cog = snap.get("cog", {})
    nodes = cog.get("nodes", [])
    start_id = int(cog.get("current_exp_id", 0))

    if not nodes:
        print("[TEST] No nodes in snapshot; abort.", file=sys.stderr)
        sys.exit(2)

    # ----------------- Build surrogate experiences ----------------------------
    exps = [_SurExp(n) for n in nodes]
    id2exp = {e.id: e for e in exps}
    ids = sorted(id2exp.keys())

    # ---------- Synthesize a handful of explicit links with 0/1 confidence ----
    # Rule: connect each node → its next (by id) with conf=1, except every 10th
    # pair conf=0 (to verify permanent blacklisting). Also add the reverse link.
    dropped_pairs = []
    for i in range(len(ids) - 1):
        u, v = ids[i], ids[i+1]
        uexp, vexp = id2exp[u], id2exp[v]
        conf = 0 if ((u + v) % 10 == 0) else 1
        uexp.links.append(_SurLink(vexp, confidence=conf))
        vexp.links.append(_SurLink(uexp, confidence=conf))
        if conf == 0:
            dropped_pairs.append((u, v))

    # ------------------- Create memory graph ----------------------------------
    mg = _SurMemoryGraph(exps, start_id=start_id)

    # --------------- Current pose function (dock at start node) ---------------
    def _docked_pose():
        return mg.experience_map.get_pose(start_id)

    # --------------- Construct NavigationSystem instance ----------------------
    nav = NavigationSystem(memory_graph=mg, get_current_pose_func=_docked_pose, planner=None)

    # ------ Auto-derive a sensible inferred-link distance if not provided -----
    # Compute each node's nearest-neighbor distance; use 1.5× median as threshold.
    def _auto_infer_dist(exps_list):
        import statistics
        pts = [(e.x_m, e.y_m) for e in exps_list]
        dists = []
        for i, (xi, yi) in enumerate(pts):
            nd = float("inf")
            for j, (xj, yj) in enumerate(pts):
                if i == j: continue
                d = math.hypot(xi - xj, yi - yj)
                nd = min(nd, d)
            if math.isfinite(nd):
                dists.append(nd)
        if not dists:
            return 3.4
        med = statistics.median(dists)
        return 1.5 * med

    nav.infer_link_max_dist = float(args.infer_dist if args.infer_dist is not None else _auto_infer_dist(exps))

    # ----------------------------- RUN TEST -----------------------------------
    print("\n[TEST] ─────────────────────────────────────────────────────────────")
    print(f"[TEST] Nodes: {len(exps)}   Start ID: {start_id}   infer_link_max_dist: {nav.infer_link_max_dist:.3f}")
    if dropped_pairs:
        print(f"[TEST] Seeded {len(dropped_pairs)} zero-confidence links (will be blacklisted). Example: {dropped_pairs[:5]}")

    # Build grammar
    grammar = nav.build_pcfg_from_memory(debug=True)

    # Generate plan
    tokens = nav.generate_plan_with_pcfg(grammar, debug=True)

    # ----------------------------- SUMMARY ------------------------------------
    def _deg_stats(adj):
        degs = [len(v) for v in adj.values()]
        if not degs: return (0, 0, 0)
        return (min(degs), sum(degs)/len(degs), max(degs))

    adj = getattr(nav, "graph_used_for_pcfg", {})
    dmin, davg, dmax = _deg_stats(adj)

    print("\n[TEST] ===================== RESULT SUMMARY ========================")
    print(f"[TEST] Goal node id          : {getattr(nav, 'goal_node_id', None)}")
    print(f"[TEST] Plan tokens (u->v)    : {tokens}")
    print(f"[TEST] Token count           : {len(tokens)}")
    print(f"[TEST] Target node matches?  : {('YES' if (tokens and tokens[-1][1] == nav.goal_node_id) else 'NO/EMPTY')}")
    print(f"[TEST] Blacklist size        : {len(getattr(nav, 'link_blacklist', set()))}")
    print(f"[TEST] Adjacency |V|,|E|     : {len(adj)} nodes, {sum(len(v) for v in adj.values())} directed edges")
    print(f"[TEST] Degree stats (min/avg/max): {dmin:.0f}/{davg:.2f}/{dmax:.0f}")
    print("[TEST] =============================================================\n")
    # =========================== SECOND RUN ==================================
    # Scenario: treat every link as confidence==1, and DISABLE inferred links.
    # Blacklist remains honored (permanent) unless you explicitly override below.

    print("\n[TEST2] ────────────────────────────────────────────────────────────")
    print("[TEST2] Rebuilding PCFG with: all links treated as conf=1, no inferred links.")

    # Set toggles on the SAME nav instance (permanent blacklist remains in effect)
    nav.pcfg_treat_all_links_confident = True
    nav.pcfg_disable_inferred_links    = True
    # If you want to also ignore the blacklist in this pass, uncomment:
    # nav.pcfg_ignore_blacklist          = True

    grammar2 = nav.build_pcfg_from_memory(debug=True)
    tokens2  = nav.generate_plan_with_pcfg(grammar2, debug=True)

    adj2 = getattr(nav, "graph_used_for_pcfg", {})
    dmin2, davg2, dmax2 = _deg_stats(adj2)

    print("\n[TEST2] ===================== RESULT SUMMARY =======================")
    print(f"[TEST2] Goal node id          : {getattr(nav, 'goal_node_id', None)}")
    print(f"[TEST2] Plan tokens (u->v)    : {tokens2}")
    print(f"[TEST2] Token count           : {len(tokens2)}")
    print(f"[TEST2] Target node matches?  : {('YES' if (tokens2 and tokens2[-1][1] == nav.goal_node_id) else 'NO/EMPTY')}")
    print(f"[TEST2] Blacklist size        : {len(getattr(nav, 'link_blacklist', set()))}  (honored={not getattr(nav, 'pcfg_ignore_blacklist', False)})")
    print(f"[TEST2] Adjacency |V|,|E|     : {len(adj2)} nodes, {sum(len(v) for v in adj2.values())} directed edges")
    print(f"[TEST2] Degree stats (min/avg/max): {dmin2:.0f}/{davg2:.2f}/{dmax2:.0f}")
    print("[TEST2] =============================================================\n")
    print("\n[TEST3] ────────────────────────────────────────────────────────────")
    print("[TEST3] Rebuilding PCFG with: all links treated as conf=1, no inferred links.")

    # Set toggles on the SAME nav instance (permanent blacklist remains in effect)
    nav.pcfg_treat_all_links_confident = True
    nav.pcfg_disable_inferred_links    = False
    # If you want to also ignore the blacklist in this pass, uncomment:
    # nav.pcfg_ignore_blacklist          = True

    grammar3 = nav.build_pcfg_from_memory(debug=True)
    tokens3  = nav.generate_plan_with_pcfg(grammar2, debug=True)

    adj3 = getattr(nav, "graph_used_for_pcfg", {})
    dmin3, davg3, dmax3 = _deg_stats(adj3)

    print("\n[TEST2] ===================== RESULT SUMMARY =======================")
    print(f"[TEST2] Goal node id          : {getattr(nav, 'goal_node_id', None)}")
    print(f"[TEST2] Plan tokens (u->v)    : {tokens3}")
    print(f"[TEST2] Token count           : {len(tokens3)}")
    print(f"[TEST2] Target node matches?  : {('YES' if (tokens3 and tokens3[-1][1] == nav.goal_node_id) else 'NO/EMPTY')}")
    print(f"[TEST2] Blacklist size        : {len(getattr(nav, 'link_blacklist', set()))}  (honored={not getattr(nav, 'pcfg_ignore_blacklist', False)})")
    print(f"[TEST2] Adjacency |V|,|E|     : {len(adj3)} nodes, {sum(len(v) for v in adj3.values())} directed edges")
    print(f"[TEST2] Degree stats (min/avg/max): {dmin3:.0f}/{davg3:.2f}/{dmax3:.0f}")
    print("[TEST2] =============================================================\n")

```

`hierarchical-nav/control_eval/__init__.py`:

```py
from control_eval.keyboard_control_navigation import *

```

`hierarchical-nav/control_eval/arguments.py`:

```py
import argparse

parser = argparse.ArgumentParser(description='PyTorch Scalable Agent')

#==== ENV related arguments ====#
parser.add_argument("--env",
    type=str,
    help="gym environment to load",
    default='4-tiles-ad-rooms'
)
parser.add_argument("--seed",
    type=int,
    help="env seed generation, 1 seed 1 config",
    default=-1
)
parser.add_argument("--rooms_in_row",
    type=int,
    help="env number of rooms in row",
    default='3'
)
parser.add_argument("--rooms_in_col",
    type=int,
    help="env number of rooms in col",
    default='3'
)

#==== Model arguments ====#
parser.add_argument("--allo_config",
    type=str,
    help="path to the allocentric model to load as yaml",
    default='runs/GQN_V2_AD/v2_GQN_AD_conv7x7_bi/GQN.yml'
)    
parser.add_argument('--memory_config',
    type=str,
    default='navigation_model/Services/memory_service/memory_graph_config.yml',
    help="path to the memory_graph_config as yaml",
)
parser.add_argument('--memory_load',
    default=None,
    help="enter the path to .map memory, None by default",
)

parser.add_argument('--lookahead',
    type=int,
    default=5,
    help="default lookahead the agent use to navigate",
)
#==== TESTS arguments ====#
parser.add_argument("--test",
    help="Test we want to run.\
    'key: manual keyboard entry, \
    'exploration': exploration ",
    default='key',
    type= str
)

parser.add_argument("--video",
    action='store_true',
    help="should we record the current test",
    default=False
)

parser.add_argument("--save_dir",
    type=str, 
    required=False,
    help="saving directory for videos or other files",
    default="tests_outputs"
)

```

`hierarchical-nav/control_eval/automatic_env_run.py`:

```py
import logging
import traceback

from control_eval.input_output import (create_saving_directory,
                                       save_experiment_data, setup_video)
from Oracle_Astar_algo import Oracle_path
from navigation_model.visualisation_tools import record_video_frames

def env_definition(args): 
    ''' extract most relevant flags, just for order sake'''   
    env_room_size = int(args.env[0])
    env_row = args.rooms_in_row
    env_col = args.rooms_in_col
    max_steps = env_row * env_col * env_room_size *18
    
    env_definition = {'room_size': env_room_size, 'n_row': env_row, \
                    'n_col': env_col, 'max_steps':max_steps,\
                    'seed':args.seed}
        
    return env_definition

def run_test(step_controller, flags):
    env_def = env_definition(flags)
    env_details = str(env_def['room_size']) +'t_'+ str(env_def['n_row'])+'x'+str(env_def['n_col']) + '_s'+ str(env_def['seed'])
    home_saving_dir = create_saving_directory(flags.save_dir)
    exp_saving_dir = home_saving_dir + '/' + env_details   
    video_gridmap = setup_video(exp_saving_dir, env_details, flags.video, flags.test)
    final_data = {}
    final_data['Error_occured?'] = False
    oracle = Oracle_path(step_controller.env)
    
    try:
        if 'exploration'in flags.test :
            explo_path, final_data['oracle_exploration'] = oracle.steps_to_explore_env()
            print(explo_path, final_data['oracle_exploration'])
            final_data['exploration_success?'], final_data['visited_rooms'], video_gridmap, final_data['Error_occured?'] \
                = run_exploration(step_controller, env_def, oracle, video_gridmap)
            final_data['exploration_n_steps'] = step_controller.step_count()
            step_controller.save_created_map(exp_saving_dir + '/map_' + env_details)
        if 'goal' in flags.test:
            goal_path, final_data['oracle_steps_to_goal'] = oracle.steps_to_closest_goal()
            final_data['goal_in_map'] = list(oracle.goal_position[0])
            start_step = step_controller.step_count()
            final_data['goal_reached?'], final_data['goal_rooms_path'], video_gridmap, final_data['Error_occured?'] \
                = run_goal_seeking(step_controller, env_def, oracle, video_gridmap, final_data['goal_in_map'])
            final_data['steps_to_goal'] = step_controller.step_count() - start_step
            
    except:
        error_message = traceback.format_exc() 
        print('ERROR MESSAGE:',error_message)
        print('EXPERIMENT INTERRUPTED')  
        final_data['Error_occured?'] = True   
    
    finally:
        if video_gridmap:
            video_gridmap.close()
        final_data['last_p_agent_in_map'] = step_controller.agent_absolute_pose()
        save_experiment_data(final_data, env_details, flags.test, home_saving_dir)
        print('EXPERIMENT DONE')
        print('++++++++++++++++++++++++++++++++++++++++++++')
#============================== GOAL METHODS ============================================

def run_goal_seeking(step_controller:object, env_definition:dict, oracle:object, video_gridmap:object, goal_in_map:list):
    rooms_limits = oracle_rooms_limit(oracle.get_doors_poses_in_env())
    visited_rooms = [] 
    goal_reached= False
    
    prev_log_step = 0
    #Just to be sure we have a memory ready to use for goal search
    while step_controller.agent_situate_memory() < 0:   
        if step_controller.step_count() <= 1:     
            action = convert_string_to_env_action('f')
            step_controller.step(action)
        else:
            steps = step_controller.step_count()
            motion_data, agent_lost = step_controller.agent_explores(collect_data=True)
            visited_rooms = count_visited_rooms(step_controller.agent_absolute_pose(), rooms_limits, visited_rooms)
            video_gridmap = record_data(step_controller,video_gridmap, motion_data, env_definition, agent_lost, [visited_rooms], steps)
    print('Agent ready to search for goal')
    steps = step_controller.step_count()
    try:
        while steps <= env_definition['max_steps']:
            motion_data, agent_lost = step_controller.agent_seeks_goal(collect_data=True)
            agent_in_map = step_controller.agent_absolute_pose()
            visited_rooms = count_visited_rooms(agent_in_map, rooms_limits, visited_rooms)
            video_gridmap = record_data(step_controller,video_gridmap, motion_data, env_definition, agent_lost, visited_rooms, steps)
            
            steps = step_controller.step_count()
            if steps - prev_log_step >= 25:
                    logging.info('Goal seeking step:'+ str(steps)+'/'+ str(env_definition['max_steps']) + ','+str(len(visited_rooms)) + ' passed rooms up to now')
                    prev_log_step = steps
            goal_reached = goal_condition_filled(goal_in_map, agent_in_map)
            if goal_reached:
                print('The agent found the goal. GOAL REACHED')
                if video_gridmap:
                    #This is just because I can't see the last frame if i don't add a delay
                    memory_map_data = step_controller.get_memory_map_data()
                    frame = record_video_frames(motion_data[-1], env_definition, agent_lost, visited_rooms, memory_map_data, steps)
                    video_gridmap.append_data(frame)
                    frame = record_video_frames(motion_data[-1], env_definition, agent_lost, visited_rooms, memory_map_data, steps)
                    video_gridmap.append_data(frame)
                break

        return goal_reached, visited_rooms, video_gridmap, False

    except:
            error_message = traceback.format_exc() 
            print('ERROR MESSAGE:',error_message) #Both so that can be printed in txt
            logging.error(str(error_message))
            print('EXPERIMENT INTERRUPTED')     
            return goal_reached, visited_rooms, video_gridmap, True
    
def goal_condition_filled(goal_in_map, agent_im_map):
    """ are we on the goal position?"""
    if (goal_in_map[:2] == agent_im_map[:2]):
        return True
    return False
         
#============================== EXPLORATION METHODS =====================================
#TODO: Ideally it would be great to check if the agent has a wrong belief over the goal position and note it somewhere (or display the goal visual in the video)
def run_exploration(step_controller:object, env_definition:dict, oracle:object, video_gridmap:object):
    
        # tqdm_out = set_log() 
        rooms_limits = oracle_rooms_limit(oracle.get_doors_poses_in_env())
        print('rooms_limits', rooms_limits)
        visited_rooms = [] 
        full_exploration = False
        entered_last_room = 0

        action = convert_string_to_env_action('f')
        step_controller.step(action)
        
        steps = step_controller.step_count()
        prev_log_step = 0
        try:
            #while steps <= 2*(env_definition['max_steps']):
            while steps <= int(600):
                motion_data, agent_lost = step_controller.agent_explores(collect_data=True)
                visited_rooms = count_visited_rooms(step_controller.agent_absolute_pose(), rooms_limits, visited_rooms)
                video_gridmap = record_data(step_controller,video_gridmap, motion_data, env_definition, agent_lost, visited_rooms, steps)
                steps = step_controller.step_count()
                if steps - prev_log_step >= 25:
                        logging.info('Exploration step:'+ str(steps)+'/'+ str(env_definition['max_steps']) + ','+str(len(visited_rooms)) + ' visited rooms up to now')
                        prev_log_step = steps
                full_exploration = exploration_condition_filled(env_definition, visited_rooms)
                
                if full_exploration:
                    
                    print('all rooms have been visited')
                    if not step_controller.agent_lost() and entered_last_room > 2:
                        print('The agent got a grip of this last room. ENDING EXPLORATION')
                        if video_gridmap:
                            #This is just because I can't see the last frame if i don't add a delay
                            memory_map_data = step_controller.get_memory_map_data()
                            frame = record_video_frames(motion_data[-1], env_definition, agent_lost, visited_rooms, memory_map_data, steps)
                            video_gridmap.append_data(frame)
                            frame = record_video_frames(motion_data[-1], env_definition, agent_lost, visited_rooms, memory_map_data, steps)
                            video_gridmap.append_data(frame)
                        break
                    else:
                        entered_last_room += 1
        except:
            error_message = traceback.format_exc() 
            print('ERROR MESSAGE:',error_message) #Both so that can be printed in txt
            logging.error(str(error_message))
            print('EXPERIMENT INTERRUPTED')     
            return full_exploration, visited_rooms, video_gridmap, True
        
        return full_exploration, visited_rooms, video_gridmap, False

def exploration_condition_filled(env_definition:dict, visited_rooms:dict):
    num_rooms = env_definition['n_col'] * env_definition['n_row']
    return len(visited_rooms) == num_rooms
        
def count_visited_rooms(agent_pose:list, rooms_limits:dict, visited_rooms:list)->list:
    for room, dimensions in rooms_limits.items():
        
        if dimensions[0][0]  <= agent_pose[0] <= dimensions[0][1] and dimensions[1][0]  <= agent_pose[1] <= dimensions[1][1] :
            if room not in visited_rooms:
                print('exploration entering a new room', room)
                visited_rooms.append(room)
                break
    return visited_rooms

def oracle_rooms_limit(rooms_wt_door_poses:dict)-> dict:
    """Define the limits of each room"""
    rooms_limits = {}  
    
    for room in rooms_wt_door_poses.keys():
        rooms_limits[room] = [[min(rooms_wt_door_poses[room], key=lambda x: x[0])[0],max(rooms_wt_door_poses[room], key=lambda x: x[0])[0]],
                            [min(rooms_wt_door_poses[room], key=lambda x: x[1])[1], max(rooms_wt_door_poses[room], key=lambda x: x[1])[1]]]
    
    return rooms_limits


#================================ OTHER METHODS =========================================

def convert_string_to_env_action(action:str) -> int:
        if 'd' in action:
            action = 6
        if 'f' in action:
                action = 2
        elif 'r' in action:
            action = 1
        elif 'l' in action:
            action = 0
        else:
            raise 'unrecognised action to apply:'+str(action)
        return action

def record_data(step_controller,video_gridmap, motion_data, env_definition, agent_lost, visited_rooms, steps):
    if video_gridmap and motion_data: 
        memory_map_data = step_controller.get_memory_map_data()
        for data in motion_data:
            frame = record_video_frames(data, env_definition, agent_lost, visited_rooms, memory_map_data, steps)
            video_gridmap.append_data(frame)
            steps+= 1
    return video_gridmap
```

`hierarchical-nav/control_eval/input_output.py`:

```py
import io
import logging
import os
import time
from pathlib import Path

import h5py as h5
import torch
import yaml
from tqdm import tqdm

try:
    from pynput.keyboard import Controller, Key
except:
    pass
import csv


def load_h5(filename):
    """
    :param filename: Location to load data from (<path/to/file.h5>)
    """
    data = {}
    with h5.File(filename, "r") as f:
        for key in f.keys():
            data[key] = torch.tensor(f[key][:], dtype=torch.float32)
    return data


def press_keyboard(key:str = 'Key.enter')-> None:
        keyboard = Controller()     
        time.sleep(0.5)   
        keyboard.press(eval(key))
        time.sleep(0.005)
        keyboard.release(eval(key)) 

def setup_allocentric_config(allo_config:str)->dict:
    try:
        allo_config = yaml.safe_load(Path(allo_config).read_text())
    except Exception as e:
        raise Exception(e)
    allo_config['params'] = allo_config['log_dir']
    return allo_config

def setup_memory_config(memory_config:str)-> dict:
    try:
        memory_config = yaml.safe_load(Path(memory_config).read_text())
    except Exception as e:
        raise Exception(e)
    print("this is the memory config", memory_config)
    return memory_config

def load_memory(models_manager:object, old_memory:str) -> None:
    '''
    We re-use previously memorised experiences
    '''
    if old_memory is not None:
        try:
            models_manager.load_memory(old_memory)
        except Exception as e:
            raise Exception(e)

def save_memory(models_manager:object, saving_directory:str)-> None:
    """
    Save the memorised map in the desired directory
    """
    models_manager.save_memorised_map(saving_directory + '.map')

#TODO:MAKE THIS MORE MODULAR
def create_saving_directory(directory:str)-> str:
    """NOTE TIS IS PARTICULAR TO ONE INDIVIDUAL"""
    if os.path.exists('/Users/lab25/hierarchical-nav'):
        dir = '/Users/lab25/hierarchical-nav' 
    else:
        dir = '/Users/lab25/hierarchical-nav' 

    home_dir = dir + directory 
           
    create_directory(home_dir)

    return home_dir

def create_directory(directory:str)->bool:
    try:
        os.makedirs(directory)
        return True
    except FileExistsError:
        return False
    
def setup_video(saving_dir, env_details, record_video, run_test):
    import imageio
    if record_video:
        count = 0
        video_file = saving_dir + '/'+ run_test +'_'+env_details+'_'+str(count)+ ".mp4"
        while os.path.exists(video_file):
            count += 1
            video_file = saving_dir + '/'+ run_test +'_'+env_details+'_'+str(count)+ ".mp4"
        video_gridmap = imageio.get_writer(video_file, format='FFMPEG', fps=1)
    else:
        video_gridmap = False
    return video_gridmap

def get_user_input():
    user_pose = []
    for i in range(3):
        element = input(f"Enter element axis {i + 1}: ")
        user_pose.append(int(element))

    return user_pose
    
def save_experiment_data(data, env_details, test_type, save_dir):
    from collections import OrderedDict

    import numpy as np
    
    csv_name = save_dir + '/' + test_type +'_exps_results.csv'
    
    
    data['env'] = env_details
    # data['n_exps'], _, data['map_coverage'] = navigation_state
    # data['per_map_coverage'] = data['map_coverage']/max_visible_tiles
    
    
    if 'goal' in test_type:
        goal_diff_steps = np.subtract(data['goal_in_map'][:2],data['last_p_agent_in_map'][:2])
        data['agent_dist_goal'] = sum(abs(number) for number in goal_diff_steps)

        data['n_diff_goal_steps'] = data['steps_to_goal'] - data['oracle_steps_to_goal']

        # data['agent on goal'] = data['goal_in_map'] == data['last_p_agent_in_map'][:2]
        # header.extend(['goal_reached?', \
        #         'oracle_steps_to_goal', 'steps_to_goal', 'goal_in_map',  'last_p_agent_in_map', \
        #         'agent on goal', 'agent_dist_goal', 'n_diff_goal_steps' ])

    if 'exploration' in test_type:
        data['n_diff_explo_steps'] = data['exploration_n_steps'] - data['oracle_exploration'] 
        data['n_visited_rooms']= len(data['visited_rooms'])
    #to be sure it's always well ordered
    data = OrderedDict(sorted(data.items(), key=lambda t: t[0]))
    info,header = [], []
    print('in save_experiment_data')
    for key, value in data.items():
        print(key, value, type(value))
        header.append(key)
        info.append(value)
   
    print('header', header)
    print('info', info)
    #If we don't want to print the header each time (necessary to always save the same values type for 1 csv file)
    if not os.path.exists(csv_name):
        no_header = True
    else:
        no_header = False

    with open(csv_name, 'a+', encoding="UTF8") as file:
        writer = csv.writer(file)
        if no_header:
            writer.writerow(header)
        writer.writerow(info)


#================================ LOGS ==================================================
class TqdmToLogger(io.StringIO):
    """
        Output stream for TQDM which will output to logger module instead of
        the StdOut.
    """
    logger = None
    level = None
    buf = ''
    def __init__(self,logger,level=None):
        super(TqdmToLogger, self).__init__()
        self.logger = logger
        self.level = level or logging.INFO
    def write(self,buf):
        self.buf = buf.strip('\r\n\t ')
    def flush(self):
        self.logger.log(self.level, self.buf)

def set_log():
    logging.basicConfig(format='%(asctime)s [%(levelname)-8s] %(message)s')
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    tqdm_out = TqdmToLogger(logger,level=logging.INFO)
    return tqdm_out

```

`hierarchical-nav/control_eval/keyboard_control_navigation.py`:

```py
#!/usr/bin/env python3
#TESTS
import concurrent.futures
from typing import List, Dict, Any
from collections import defaultdict

import gym
import gym_minigrid 
#import rospy
#from sensor_msgs.msg import Image
#from cv_bridge import CvBridge

import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F
from skimage.metrics import structural_similarity as ssim
from scipy.spatial.distance import cosine
import cv2
import random
import math
import heapq
from collections import namedtuple
State = namedtuple("State", ["x","y","d"])
from nltk import PCFG
from nltk.parse.generate import generate
from gym_minigrid.wrappers import ImgActionObsWrapper, RGBImgPartialObsWrapper
from gym_minigrid.window import Window
from gym_minigrid.envs.aisle_door_rooms import AisleDoorRooms
from collections import deque
from copy import deepcopy
from control_eval.input_output import *
from collections import Counter
from navigation_model.Processes.motion_path_modules import action_to_pose

from env_specifics.minigrid_maze_wt_aisles_doors.minigrid_maze_modules import (
    is_agent_at_door, set_door_view_observation)

from navigation_model.Processes.AIF_modules import mse_elements
from navigation_model.Processes.exploitative_behaviour import \
    Goal_seeking_Minigrid
from navigation_model.Processes.explorative_behaviour import \
    Exploration_Minigrid
from navigation_model.Processes.manager import Manager
from navigation_model.Services.model_modules import no_vel_no_action
from navigation_model.visualisation_tools import (
    convert_tensor_to_matplolib_list, transform_policy_from_hot_encoded_to_str,
    visualise_image,visualize_replay_buffer)
from control_eval.NavigationSystem import NavigationSystem
from gym_minigrid.minigrid import Wall 
from dreamer_mg.world_model_utils import DictResizeObs ,WMPlanner
from control_eval.HierarchicalHMMBOCPD import HierarchicalBayesianController
from control_eval.NavigationSystem import NavigationSystem
from torchvision.utils import save_image
def print_keys_explanation():
    print('NOTE: the agent needs a first push forward to initialise the model with a first observation. \
          So please press up arrow at least once to start well')
    print('==========KEYBOARD MANUAL===========')
    print('a: allow lazy user to launch an autonomous set of motion pre-defined in code')
    print('g: the current pose and observations are set as goal')
    print('r: agent seeks a goal (all the find+reach goal process)')
    print('t: enter current place pose and obtain the agent prediction (you will obtain it at next step)')
    print('d: agent apply step done -which means no motion, however agent still process observations')
    print('enter: agent explore')
    print('s: save current memory map')
    print('right or left arrows: agent goes left or right in its reference frame')
    print('up arrow: agent goes forward in its reference frame')
    print('====================================')

DIR_VECS = [(1,0),(0,1),(-1,0),(0,-1)]
ACTIONS   = ["forward","left","right"]
class MinigridInteraction():
    def __init__(self, args, redraw_window:bool = True) -> None:
        env_type = 'Minigrid_maze_aisle_wt_doors' + args.env
        self.redraw_window = redraw_window

        #AUTO TEST 
        f_move_aisle = ['Key.up']*6
        enter_keys = ['Key.enter']*80
        self.auto_motion = f_move_aisle + ['Key.left'] + f_move_aisle + ['Key.up']*2 + ['Key.left']*1 + f_move_aisle + ['Key.up']*3 + ['Key.right']
        self.auto_move = False
        self.goal_test = []
        self.replay_buffer = []  # List to store recent state dictionaries.
        self.agent_current_pose=None
        self.buffer_size = 60
        ###3META
        self.hmm_bayes = HierarchicalBayesianController()
        self.hmm_bayes.key=self
        self.current_plan = []
        self.plan_step_idx = 0
        self.last_pose = None
       
        # ── HMM / meta-controller bookkeeping ─────────────────────────────
        self.current_mode: str      = "EXPLORE"        # start state
        self.current_submode: str   = "ego_allo"
        self.prev_mode: str | None  = None             # nothing to compare yet
        self.prev_submode: str | None = None
        self.mode_changed: bool     = False
        self.submode_changed: bool  = False
        self.hmm_stats: dict | None = None             # last update payload

        ###WMPLANNER
        '''CKPT = "dreamer_mg/dreamer/runs/mg_collision/20250704-220917/ckpt/iter00800.pt"'''
        CKPT ="dreamer_mg/runs/mg_collision/20250704-220917/ckpt/iter00800.pt"
        self.planner = WMPlanner(CKPT)
        print("✓ world-model loaded")
        self.wm=self.planner.wm
        self.belief=None
        self.prev_onehot=None
        #TODO: ALLOW USER TO SET ITS PREFERRED OB WTOUT ACCESSING CODE
        #Since 255 is the max value of any colour, we don't care about the above value of white
        self.preferred_colour_range = np.array([[235,235,235], [275,275,275]])

        # Minigrid HOT encoded actions
        forward = [1,0,0]
        right = [0,1,0]
        left = [0,0,1]
        mingrid_actions = [forward, right, left]

        #--- MODEL INIT ---#
        allo_config = setup_allocentric_config(args.allo_config)
        memory_config = setup_memory_config(args.memory_config)
        old_memory = args.memory_load

        self.models_manager = Manager(allo_config, memory_config, mingrid_actions, env= env_type, lookahead=args.lookahead,replay_buffer=self.replay_buffer)
        set_door_view_observation(self.models_manager)
        load_memory(self.models_manager,old_memory)

        #--- explorative_behaviour INIT ---#
        self.explorative_behaviour = Exploration_Minigrid(env_type, possible_actions = mingrid_actions, curiosity_temp= 100)

        self.exploitative_behaviour = Goal_seeking_Minigrid(env_type)

        #=== Tests variables ===#
        Minigrid_env_details = args.env[0] +'t_'+ str(args.rooms_in_row)+'x'+str(args.rooms_in_col) + '_s'+ str(args.seed)
        saving_dir = create_saving_directory(args.save_dir)
        self.saving_dir = saving_dir + '/' + Minigrid_env_details  
        create_directory(self.saving_dir) 
        Minigrid_env_details
        self.automatic_process = False

        #--- ENV INIT ---#
        self.env_name = 'MiniGrid-' + args.env + '-v0'
        #self.env_name= 'MiniGrid-ADRooms-Collision-v0'
        print(self.env_name)
        self.env = gym.make(self.env_name, rooms_in_row=args.rooms_in_row, rooms_in_col=args.rooms_in_col)
        import inspect, sys
        print("ENV CODE =", inspect.getfile(self.env.__class__))
        self.env = RGBImgPartialObsWrapper(self.env)
        self.env = ImgActionObsWrapper(self.env)
        self.window = Window(self.env_name)
        self.seed = args.seed

        self.nav_system = NavigationSystem(
                self.models_manager.memory_graph,
                lambda: self.agent_current_pose,
                planner=self.planner
            )
        self.nav_system.key=self

        self.reset()

        self.windows_key_handler()

#==================== MINIGRID ENV METHODS ====================#
    def windows_key_handler(self)-> None:
        ''' 
        Allows keyboard handling 
        only if we have a window display
        '''
        if self.redraw_window:
            print_keys_explanation()
            self.window.reg_key_handler(self.key_handler)        
            # Blocking event loop
            self.window.show(block=True) 

    def redraw(self) -> list:
        '''
        get observation, return whole world image
        '''
        img = self.env.render('rgb_array', tile_size=32)
        if self.redraw_window:
            self.window.show_img(img)
        return img

    def reset(self)->None:
        if self.seed != -1:
            self.env.seed(self.seed)
        obs = self.env.reset()
        if hasattr(self.env, 'mission'):
            print('Mission: %s' % self.env.mission)
            self.window.set_caption(self.env.mission)
        if self.redraw_window:
            self.redraw()

    def step_count(self)->int:
        return self.env.step_count + 1
    
    def agent_absolute_pose(self)->list:
        return [*self.env.agent_pos,self.env.agent_dir]

    def step(self,action)->bool:
        
        self.agent_step(action)
        self.redraw()

        print('____________')
        print()
        if self.auto_move:
            try:
                del self.auto_motion[0]
                if len(self.auto_motion) > 0 :
                    press_keyboard(self.auto_motion[0])
                else:
                    print('end of auto motion')
            except IndexError:
                pass
        return self.models_manager.agent_lost()

    def convert_hot_encoded_to_minigrid_action(self, action:list) -> int:
        if len(action) < 1 :
            print('No action, not moving')
            return self.env.actions.done
        if action[0] == 1:
                action = self.env.actions.forward
        elif action[1] == 1:
            action = self.env.actions.right
        elif action[2] == 1:
            action = self.env.actions.left
        else:
            raise Exception('unrecognised action to apply:'+str(action))
        return action
    def convert_minigrid_action_to_hot_encoded(self, action: int) -> list:
        if action == self.env.actions.forward:
            return [1, 0, 0]
        elif action == self.env.actions.right:
            return [0, 1, 0]
        elif action == self.env.actions.left:
            return [0, 0, 1]
        else:
            raise Exception("Unrecognized minigrid action: " + str(action))
    def convert_list_to_hot_encoded(self, actions_list: list) -> list:
        # Initialize an empty list for the one-hot encoded actions.
        hot_encoded_actions = []

        # Iterate over each action in the given list.
        for action in actions_list:
            # Use your conversion function to convert the action into one-hot encoded format.
            encoded_action = self.convert_minigrid_action_to_hot_encoded(action)
            hot_encoded_actions.append(encoded_action)

        return hot_encoded_actions    
        
    def key_handler(self,event:str) -> None:
        print('pressed', event.key)
        key = event.key

        if key == 'a':
            print('automotion')
            self.auto_move = True
            press_keyboard(self.auto_motion[0])
        
        if key == 'g':
            print('this pose and ob is the new goal')
            obs, _, _, _ = self.env.step(self.env.actions.done)
            self.goal_test = self.exploitative_behaviour.set_current_pose_as_goal(self.models_manager, obs['image'])

        if key == 'r':
            print('Agent seeks the goal, it will search for preferred colour range in pred + apply policy to reach it')
            self.agent_seeks_goal()
            if self.automatic_process:
                press_keyboard()
            return

        if key == 't':
            #VISUALISATION PREDICTION FOR A GIVEN POSE
            print('Enter coordinates x,y,theta of the pose \
                  from which you want the allo model img prediction')
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(get_user_input)
                user_pose = future.result()
            print("You want to know the predicted image for pose:", user_pose)
            pred_ob = self.models_manager.given_pose_prediction_visualisation(user_pose)
            obs, _, _, _ = self.env.step(self.env.actions.done)
            obs = self.models_manager.allocentric_process.allocentric_model.torch_observations(obs)
            visualise_image(obs['image'].unsqueeze(0), 'GT pose:'+ str(user_pose), fig_id = 101)
            obs_image = obs['image'].unsqueeze(0).unsqueeze(0).repeat(pred_ob.shape[0], 1,1,1,1)
            print(obs_image.shape, pred_ob.shape)
            print('MSE PRED', mse_elements(pred_ob, obs_image))
            print('_________')
            print()
        
        if key == 'd':
            print('Next step, do nothing')
            self.step(self.env.actions.done)
         
        if key == 'p':
            #Erase all plots from 10 to 300
            print('closing all plots rangin from 10 to 300')
            for id in range(10,1500):
                plt.close(id)
        if key == 'enter':
        #explorative_behaviour
            self.agent_explores()

            if self.automatic_process:
                press_keyboard()
            return

        #Manual keys:
        if key == 'left':
            self.step(self.env.actions.left)
            if self.automatic_process:
                press_keyboard()
            return 
        
        if key == 'right':
            self.step(self.env.actions.right)
            if self.automatic_process:
                press_keyboard()
            return 
        
        if key == 'up':
            self.step(self.env.actions.forward)
            if self.automatic_process:
                press_keyboard()
            return 
        if key == 's':
            save_memory(self.models_manager, self.saving_dir)
            return 

#==================== MODEL METHODS ====================#
    def agent_lost(self)->bool:
        return self.models_manager.agent_lost()
    
    def agent_situate_memory(self)->int:
        return self.models_manager.get_current_exp_id()
    
    def update_replay_buffer(self, state):
        if len(self.replay_buffer) >= self.buffer_size:
            self.replay_buffer.pop(0)  # Remove the oldest entry to keep the size fixed.
        self.replay_buffer.append(deepcopy(state))

    def extract_past_actions(self,replay_buffer):
        actions = []
        for state in replay_buffer:
            # Get the action from the state; use None as the default if not found.
            action = state.get('action', None)
            if action is not None:
                actions.append(action)
        return actions
    def extract_past_real_poses(self,replay_buffer):
        actions = []
        for state in replay_buffer:
            # Get the action from the state; use None as the default if not found.
            action = state.get('real_pose', None)
            if action is not None:
                actions.append(action)
        return actions
    def extract_past_poses(self,replay_buffer):
        actions = []
        for state in replay_buffer:
            # Get the action from the state; use None as the default if not found.
            action = state.get('imagined_pose', None)
            if action is not None:
                actions.append(action)
        return actions
    def action_to_past_pose(self, action, final_pose):
        DIR_TO_VEC = [
            [1, 0],    # 0: pointing right (positive X)
            [0, 1],    # 1: down (positive Y)
            [-1, 0],   # 2: left (negative X)
            [0, -1]    # 3: up (negative Y)
        ]

        initial_pose = final_pose.copy()
        if action[0] == 1:
            #print("action[0] == 1:")
            initial_pose[0] = final_pose[0] - DIR_TO_VEC[final_pose[2]][0]
            initial_pose[1] = final_pose[1] - DIR_TO_VEC[final_pose[2]][1]
        elif action[0] == -1:
            initial_pose[0] = final_pose[0] + DIR_TO_VEC[final_pose[2]][0]
            initial_pose[1] = final_pose[1] + DIR_TO_VEC[final_pose[2]][1]
        elif action[1] == 1:
            #print("action[1] == 1:")
            initial_pose[2] = (final_pose[2] - 1) % 4
        elif action[2] == 1:
            #print("action[2] == 1:")
            initial_pose[2] = (final_pose[2] + 1) % 4

        return initial_pose
    def extract_branch_paths(self, tree, current_path=None):
        if current_path is None:
            current_path = {"actions": [], "poses": [tree["pose"]]}
            #print(f">>> start at pose={tree['pose']}")

        kids = tree.get("children") or []
        if not kids:
            # We’re at a leaf
            L = len(current_path["actions"])
            #print(f"   leaf @ pose={tree['pose']} ➞ length={L}")
            return [current_path]

        paths = []
        for child in kids:
            action, pose = child["action"], child["pose"]
            new_path = {
                "actions": current_path["actions"] + [action],
                "poses":   current_path["poses"]   + [pose]
            }
            #print(f"  desc → action {action} ➞ pose {pose}  (so far: {len(new_path['actions'])} actions)")
            paths.extend(self.extract_branch_paths(child, new_path))

        return paths
    def build_decision_tree_all(self, current_pose, depth, max_depth, branch_history=None):
        # Initialize the branch history if necessary
        if branch_history is None:
            branch_history = [current_pose]
        
        # Create the current node.
        node = {
            "pose": current_pose,
            "action": None,   # Root node doesn't have an action that produced it.
            "children": [],
            "depth": depth
        }
        
        # Stop expansion if maximum depth is reached.
        if depth >= max_depth:
            return node

        # Define the available actions in your grid world.
        AVAILABLE_ACTIONS = [[1,0,0], [0,1,0], [0,0,1]]
        
        # For each action, simulate the new pose.
        for action in AVAILABLE_ACTIONS:
            # Compute new_pose using your forward update function.
            new_pose = action_to_pose(np.array(action), current_pose.copy())
            if any(np.array_equal(new_pose, p) for p in branch_history):    
                continue

            new_branch_history = branch_history + [new_pose]
            
            subtree = self.build_decision_tree_all(new_pose, depth + 1, max_depth, new_branch_history)
        
            # Create a child node that includes the action that led to new_pose.
            child_node = {
                "pose": new_pose,
                "action": action,   # Store this action here.
                "children": subtree["children"],  # Adopt the children from the subtree.
                "depth": depth + 1
            }
            # Optionally, include any additional fields from subtree into child_node.
            node["children"].append(child_node)
            
        return node
    def _recency_weight(self,age, tau, kind="exp"):
        """age = 0 means ‘just visited’.  Larger age → smaller weight."""
        if kind == "exp":            # exponential decay
            return np.exp(-age / tau)
        elif kind == "hyper":        # hyperbolic (1 / (1+age))
            return 1.0 / (1.0 + age)
        else:                        # constant (no decay)
            return 1.0
    def _rollout_poses(self, start_pose: np.ndarray,
                   actions: np.ndarray,
                   action_to_pose) -> List[np.ndarray]:
        """Re‑compute poses after the policy was shortened by collision trimming."""
        poses = [start_pose.copy()]
        cur   = start_pose.copy()
        for a in actions:
            cur = action_to_pose(a, cur.copy())
            poses.append(cur)
        return poses   

    
    def evaluate_branches(
    self,
    branches: List[Dict[str, Any]],     # [{actions, poses}, …] from extract_branch_paths
    past_poses: List[np.ndarray],
    tau: float = 50,
    decay: str = "hyper",
    *,
    eps: float = 0.5,
    max_keep: int | None = None,
    embed_fn=None
):
        """
        1. Feed **action sequences only** to egocentric collision check.
        2. Reconstruct pose sequences for the surviving (trimmed) policies.
        3. Score with recency penalty.
        4. Apply radius‑greedy diversity filter.
        5. Return list of (penalty, actions, poses) tuples OR just actions.
        """
        # ---- 0. default embedding on final (x,y,θ) pose ----
        if embed_fn is None:
            embed_fn = lambda poses: np.asarray(poses[-1], dtype=float)

        # ---- 1. collision + dedup first ----
        action_lists = [br["actions"] for br in branches]
        pruned_pols  = self.models_manager.get_plausible_policies(action_lists)
        #           -> list[torch.Tensor]   each shape (T,3)

        # ---- 2. rebuild pose traces so we can score ----
        # Build quick lookup: initial pose for each ORIGINAL branch
        # (all branches share the same start pose in typical tree; otherwise
        #  keep a parallel list or include start_pose in each dict)
        start_pose = branches[0]["poses"][0]

        rebuilt = []   # list of dicts with keys actions, poses
        for pol in pruned_pols:
            acts  = pol.cpu().numpy()              # (T,3)
            poses = self._rollout_poses(start_pose, acts, action_to_pose)
            rebuilt.append({"actions": acts, "poses": poses})

        # ---- 3. recency‑penalty scoring ----
        last_seen = {tuple(p[:2]): t            # 👈  keep only x,y !
             for t, p in enumerate(past_poses)}
        now = len(past_poses)
        current_key = tuple(start_pose[:2])     # (x0, y0)
        last_seen[current_key] = now 

        scored = []
        for br in rebuilt:
            penalty = 0.0
            for pose in br["poses"]:
                key = tuple(pose[:2])           # 👈  ignore θ during lookup
                if key in last_seen:
                    age     = now - last_seen[key]
                    penalty += self._recency_weight(age, tau, decay)
            scored.append((penalty, br))
        current_xy = np.asarray(start_pose[:2], dtype=float)
        scored.sort(key=lambda x: (x[0],              # 1️⃣ low penalty
                           np.linalg.norm(embed_fn(x[1]["poses"])[:2] - current_xy)))

        ALPHA = 6.0         # weight for recency‑penalty
        BETA  = 0.5         # weight for outward distance (tweak!)

        best_branch = None
        best_score  = np.inf

        for pen, br in scored:                       # kept comes out of radius‑greedy
            end_xy = np.asarray(br["poses"][-1][:2], dtype=float)
            dist   = np.linalg.norm(end_xy - current_xy)      # outward distance
            lin    = ALPHA * pen - BETA * dist               # lower is better
            print(pen,br)
            print("who is winning",ALPHA * pen,BETA * dist,)
            print(f"lin={lin:.10f}   best_score={best_score:.10f}")
            if lin < best_score:
                print("inside the best score",lin,br)
                best_score  = lin
                best_branch = br

        # what you return / store
        return_best_actions = torch.as_tensor(best_branch["actions"])
        
        return return_best_actions
        

    def compute_pose_history(self):
        # If the replay buffer is empty, return an empty list.
        if not self.replay_buffer:
            return []
        
        # Get the initial pose from the first state.
        first_state = self.replay_buffer[-1]
        if 'imagined_pose' in first_state and first_state['imagined_pose'] is not None:
            current_pose = list(first_state['imagined_pose'])
        else:
            current_pose = list(first_state.get('real_pose', [0, 0, 0]))  # assume pose format: [x, y, theta]
        
        pose_history = [current_pose.copy()]
        #print(pose_history)
        # Iterate through the replay buffer in forward order.
        # For each state, update the current pose using the stored action.
        for state in reversed(self.replay_buffer):
            action = state.get('action')
            hotenc_action = np.array(self.convert_minigrid_action_to_hot_encoded(action), dtype=np.int32)
            #print("Hot-encoded action:", hotenc_action, type(hotenc_action), hotenc_action.shape)
            if action is None:
                # If no action is specified, assume no movement.
                continue
            # Compute the updated pose: this function should update the pose given the action.
            current_pose = self.action_to_past_pose(hotenc_action, current_pose.copy())
            #print("fff",current_pose)
            # Append a copy of the updated pose to the trajectory history.
            pose_history.append(current_pose.copy())
        
        return pose_history
    def action_to_str(self,a):
        if hasattr(a, "value"):
            a = int(a.value)

        mapping = {0: "left", 1: "right", 2: "forward"}
        try:
            return mapping[a]
        except KeyError:
            raise ValueError("action must be 0, 1, or 2 (left, right, forward)")

    def agent_step(self, action) -> tuple[bool,dict]:
        print('step in world:', self.step_count())
        print('action to apply:',action)
        print("type",type(action))
        prev = self.last_pose
        #bridge = CvBridge()
        #img_msg = rospy.wait_for_message("/camera/depth_registered/rgb/image_raw", Image, timeout=10)
        #img_bgr = bridge.imgmsg_to_cv2(img_msg, desired_encoding='passthrough')
        #print(img_msg)
        #print(bridge,img_msg,img_bgr.shape)
        if self.belief==None:
            img0 = self.env.render(mode="rgb_array")
            obs = {"image":img0}  
            self.belief=self.planner.update_belief_from_obs(obs,self.belief,self.prev_onehot)
        obs, _, _, _ = self.env.step(action)
        print(obs.keys(),obs["pose"], obs["image"].shape)
        obs = no_vel_no_action(obs)
        act_wm=self.action_to_str(action)
        self.prev_onehot=self.planner.onehot(act_wm,self.wm)
        with torch.no_grad(): 
            self.belief=self.planner.update_belief_from_obs(obs,self.belief,self.prev_onehot)
            path2=[]
            decoded_image=self.planner.render_plan(self.wm,self.belief, path2, include_last=True)
        self.agent_current_pose=obs["pose"]
        self.nav_system.push_pose(obs["pose"])
        pos = obs['pose']
        self.last_pose = pos
        emap = self.models_manager.memory_graph.experience_map
        emap.last_link_action     = action
        emap.last_real_pose       = obs["pose"]
        emap.last_imagined_pose   = self.models_manager.get_best_place_hypothesis()['pose']
        prim = emap._map_raw_action(action)
        emap._recent_prims.append(prim)
        self.models_manager.digest(obs)
        self.belief=self.planner.update_belief_from_obs(obs,self.belief,self.prev_onehot)
        is_agent_at_door(self.models_manager,obs["image"], sensitivity=0.18)
        print("is agent at door",is_agent_at_door(self.models_manager,obs["image"], sensitivity=0.18))
        if self.models_manager.agent_lost():
            #we are lost? want the info_gain memory to be reset to 1 value
            slide_window = 1
        else:
            slide_window = 5
        info_gain = self.models_manager.get_best_place_hypothesis()['info_gain']
        self.explorative_behaviour.update_rolling_info_gain(info_gain, window=slide_window)
        current_state = {
        'node_id': self.agent_situate_memory(),
        'real_pose': obs["pose"],
        'imagined_pose':self.models_manager.get_best_place_hypothesis()['pose'],
        'real_image': obs['image'],   
        'imagined_image': self.models_manager.get_best_place_hypothesis()['image_predicted'],
        'action':action,
        'place_doubt_step_count':self.models_manager.allocentric_process.place_doubt_step_count()}

        self.update_replay_buffer(current_state)
        #####HIDDEN MARKOV####
        hmm_info_gain=self.info_gain(obs['image'],obs['pose'], self.replay_buffer, self.models_manager.memory_graph)
        raw_plan_prog=self.nav_system.navigation_grade()
        hmm_plan_progress = max(0.0, min(1.0, 0.5 if raw_plan_prog == -1 else float(raw_plan_prog)))
        current_mode,stats=self.hmm_bayes.update(self.replay_buffer,hmm_info_gain,hmm_plan_progress)
        print("############",current_mode,stats)
        print("############",stats)
        new_mode, new_submode = stats['most_likely_state']
        self.prev_mode = getattr(self, "current_mode", None)
        self.prev_submode = getattr(self, "current_submode", None)
        self.current_mode = new_mode
        self.current_submode = new_submode
        self.mode_changed = (new_mode != self.prev_mode)
        self.submode_changed = (new_submode != self.prev_submode)
        self.hmm_stats = stats  # For debugging, analysis

        print(f"🧠 [HMM] mode: {new_mode} → {new_submode}  | changed? {self.mode_changed}")
        
            #grammar = self.build_pcfg_from_memory()
            #self.print_adaptive_pcfg_plans(grammar)
        # --------------------------------------------------------------------------
        # ❸  BUILD A NEW obs DICT FOR DOWN-STREAM PLOTTING
        # --------------------------------------------------------------------------
        new_obs              = dict(obs)        # shallow copy of the env observation
        new_obs["decoded"]   = decoded_image    # you already add this

        # ---- NEW: attach everything the video overlay needs --------------------
        new_obs["recommended_mode"]     = new_mode
        new_obs["recommended_submode"]  = new_submode
        new_obs["mode_confidence"]      = stats.get("mode_probabilities",
                                                    {}).get(new_mode)
        sub_conf = None
        sub_probs = stats.get("submode_probabilities", {})
        if isinstance(sub_probs, dict):
            sub_conf = sub_probs.get(new_mode, {}).get(new_submode)
        new_obs["submode_confidence"] = sub_conf  
        new_obs["uncertainty"]          = stats.get("state_entropy") \
                                            or stats.get("uncertainty")
        new_obs["changepoint_mass"]     = stats.get("changepoint_probability")
        '''if self.step_count() == 400:
            print("DIS WHE ")
            self.env.Spawn_Obstacles(obstacle_rate=0.15)'''
        

        return self.models_manager.agent_lost(), new_obs, self.mode_changed or self.submode_changed
    
    def apply_policy(self, policy:list, n_actions:int, collect_data:bool=False)->tuple[list,bool]:
        motion_data = []
        agent_lost = False
        print("n_sctions",n_actions, policy,self.convert_hot_encoded_to_minigrid_action(policy[0]))
        #policy=[[1,0,0],[1,0,0],[1,0,0],[1,0,0],[0,0,1],[1,0,0],[1,0,0],[0,1,0],[1,0,0],[1,0,0]]
        for a in range(len(policy)):   
        #for a in range(n_actions):
            action = self.convert_hot_encoded_to_minigrid_action(policy[a])
            agent_lost, obs, change = self.agent_step(action)
            world_img = self.redraw()
            if collect_data:
                data = self.collect_data_models(obs)
                data['env_image'] = world_img
                data['ground_truth_ob'] = obs['image']
                motion_data.append(data)
        
            if change:
                print(f"Stopping policy: {self.prev_mode}")
                return motion_data, True
        return motion_data, agent_lost
    
    def agent_explores(self, collect_data:bool=False)->tuple[list,bool]:
        
        policy, n_actions = self.apply_exploration()
        motion_data,agent_lost = self.apply_policy(policy, n_actions, collect_data)
        return motion_data, agent_lost
    @staticmethod   
    def to_onehot_list(act: str) -> list[int]:
        """'left'/'right'/'forward' → [0,0,1] / [0,1,0] / [1,0,0] (Python list)"""
        mapping = {
            'forward': [1, 0, 0],
            'right'  : [0, 1, 0],
            'left'   : [0, 0, 1],
        }
        return mapping[act]

    def apply_exploration(self) -> tuple[list, int]:
        """
        Unified high-level controller.

        • Chooses a branch from the HMM’s most-likely (mode, submode)
        — stored in `self.prev_mode` / `self.prev_submode`
        • Falls back to the legacy “while-loop” exploration when inside
        EXPLORE/ego_allo-family sub-modes.
        • Preserves *all* previous side-effects (prints, counters, resets …).
        Returns
        -------
        (policy, n_actions)
            • policy  : list[np.ndarray]   one-hot action sequence
            • n_actions : int | list       whatever your behaviour functions use
        """

        # ------------------------------------------------------------------ #
        # 0. Read current HMM state (defaults are safe for the very 1st call)
        # ------------------------------------------------------------------ #
        mode, submode = (
            getattr(self, "current_mode", None)     or "EXPLORE",
            getattr(self, "current_submode", None)  or "ego_allo",
        )
        print(f"🧠 [HMM] mode:{mode}  submode={submode}", self.step_count())
        print(self.step_count())

        # ------------------------------------------------------------------ #
        # 1. RECOVER  –— was: “agent lost”
        # ------------------------------------------------------------------ #
        if mode == "RECOVER":
            if submode == "solve_doubt":
                print("Agent lost ➜ solve_doubt_over_place()")
                policy, n_action = self.explorative_behaviour.solve_doubt_over_place(
                    self.models_manager
                )
                return policy, n_action

            elif submode == "backtrack_safe":
                print("Recover ➜ backtrack to known safe node")
                # TODO: implement/back‐hook a back-tracking routine
                fallback = [self.to_onehot_list(random.choice(["right", "left"]))]
                print(f"[NAVIGATE] STALL → issuing {fallback}")
                return fallback, 1

        # ------------------------------------------------------------------ #
        # 2. NAVIGATE –— path-following & exploitation
        # ------------------------------------------------------------------ #
        if mode == "NAVIGATE":

            print(f"[NAVIGATE] submode={submode}  "
                f"plan_progress={self.nav_system.progress_scalar():.3f}")

            # ---------- 1. Early feasibility gate ----------
            if not self.nav_system.check_navigation_feasibility():
                print(f"[NAVIGATE] infeasible — flags={self.nav_system.navigation_flags}")
                self.navigation_flags = self.nav_system.navigation_flags
                fallback = [self.to_onehot_list(random.choice(["right", "left"]))]
                print(f"[NAVIGATE] STALL → issuing {fallback}")
                return fallback, 1

            # ---------- 2. Build first plan if none ----------
            if self.nav_system.progress_scalar() < 0.0:
                print("[NAVIGATE] no plan yet → building PCFG plan")
                grammar = self.nav_system.build_pcfg_from_memory()
                self.nav_system.generate_plan_with_pcfg(grammar)
                print(f"[NAVIGATE] new plan tokens={self.nav_system.full_plan_tokens}")

            # ---------- 3. Delegate to sub-mode handler ----------
            primitives, n_actions = self.nav_system.universal_navigation(
                submode, self.wm, self.belief
            )

            # ---------- 4. Stall safeguard ----------
            if n_actions == 0 or not primitives:
                fallback = [self.to_onehot_list(random.choice(["right", "left"]))]
                print(f"[NAVIGATE] universal_navigation returned empty "
                    f"→ STALL issuing {fallback}")
                return fallback, 1

            print(f"[NAVIGATE] primitives={primitives}  n_actions={n_actions}")
            return primitives, n_actions
        # ------------------------------------------------------------------ #
        # 3. TASK_SOLVING –— goal-directed behaviour
        # ------------------------------------------------------------------ #
        if mode == "TASK_SOLVING":
            if submode == "goal_directed":
                print("Task-solving ➜ explicit goal search")
                fallback = [self.to_onehot_list(random.choice(["right", "left"]))]
                print(f"[NAVIGATE] universal_navigation returned empty "
                    f"→ STALL issuing {fallback}")
                return fallback, 1

            elif submode == "systematic_search":
                print("Task-solving ➜ systematic sweep")
                # TODO
                fallback = [self.to_onehot_list(random.choice(["right", "left"]))]
                print(f"[NAVIGATE] universal_navigation returned empty "
                    f"→ STALL issuing {fallback}")
                return fallback, 1

            elif submode == "task_completion":
                print("Task-solving ➜ finalising task")
                # TODO
                allback = [self.to_onehot_list(random.choice(["right", "left"]))]
                print(f"[NAVIGATE] universal_navigation returned empty "
                    f"→ STALL issuing {fallback}")
                return fallback, 1
        
            
        # ------------------------------------------------------------------ #
        # 4. EXPLORE  –– handled by sub-modes
        # ------------------------------------------------------------------ #
        # Enter here whenever  mode == "EXPLORE"
        # ------------------------------------------------------------------ #

        print("==================================================== EXPLORE")

        policy, n_action = [], []

        # ────────────────────────────────────────────────────────────────
        # 4A. Sub-mode dispatch
        # ────────────────────────────────────────────────────────────────
        if submode == "ego_allo":
            # Basic ego↔allo oscillation (no look-ahead growth)
            if self.explorative_behaviour.is_agent_exploring():
                policy, n_action = self.explorative_behaviour.one_step_ego_allo_exploration(
                    self.models_manager
                )
            else:
                print("ego_allo: not currently exploring – no policy produced")

        elif submode == "ego_allo_lookahead":
            # Grow look-ahead horizon until a policy appears (old behaviour)
            max_extra = 5                     # ← keep original limit
            attempts  = 0
            while len(policy) == 0 and attempts <= max_extra:
                print(f"lookahead attempt {attempts} / {max_extra}")
                policy, n_action = self.explorative_behaviour.one_step_ego_allo_exploration(
                    self.models_manager
                )
                attempts += 1

        elif submode == "short_term_memory":
            print("short_term_memory: pushing out of STM comfort zone")
            policy, n_action = self.push_explo_stm()

        elif submode == "astar_directed":
            print("astar_directed: frontier-directed exploration (placeholder)")
            # TODO: implement an A* or frontier planner that returns (policy, n_actions)
            fallback = [self.to_onehot_list(random.choice(["right", "left"]))]
            print(f"[NAVIGATE] universal_navigation returned empty "
            "→ STALL issuing {fallback}")
            policy, n_action = fallback, 1

        else:
            print(f"[WARN] unknown EXPLORE sub-mode: {submode} – defaulting to ego_allo")
            if self.explorative_behaviour.is_agent_exploring():
                policy, n_action = self.explorative_behaviour.one_step_ego_allo_exploration(
                    self.models_manager
                )

        # ────────────────────────────────────────────────────────────────
        # 4B. Fallback if still empty •–––– keep agent moving
        # ────────────────────────────────────────────────────────────────
        if len(policy) == 0:
            print("No policy found after sub-mode logic – issuing egocentric poke")
            policy, n_action = self.explorative_behaviour.one_step_egocentric_exploration(
                self.models_manager
            )

        # ────────────────────────────────────────────────────────────────
        # 4C. Legacy EFE book-keeping (unchanged)
        # ────────────────────────────────────────────────────────────────
        policy_G = self.explorative_behaviour.get_latest_policy_EFE(reset_EFE=True)
        if policy_G is not None:
            info_gain_coeff = self.explorative_behaviour.rolling_coeff_info_gain()
            if self.explorative_behaviour.define_is_agent_exploring(
                info_gain_coeff, policy_G, threshold=1
            ):
                self.models_manager.reset_variable_lookahead_to_default()

        return policy, n_action
    def push_explo_stm(self):
            pose_history=self.compute_pose_history()
            self.extract_past_actions(self.replay_buffer)
            #print("nnn",a)
            #print(b)
            max_depth = 6
            tree_all = self.build_decision_tree_all(self.replay_buffer[-1]["imagined_pose"], 0, max_depth)
    
            paths=self.extract_branch_paths(tree_all)
            n_branches = len(paths)
            print(f"Number of branches: {n_branches}")
            actions_per_branch = [len(p['actions']) for p in paths]
            poses_per_branch   = [len(p['poses'])   for p in paths]

            # 3. Count how many branches fall into each length
            action_length_counts = Counter(actions_per_branch)
            pose_length_counts   = Counter(poses_per_branch)

            # 4. Print a nice summary
            print("Branches by # of actions:")
            for length, count in sorted(action_length_counts.items(), reverse=True):
                print(f"  {count} branch{'es' if count>1 else ''} with {length} action{'s' if length!=1 else ''}")

            print("\nBranches by # of poses:")
            for length, count in sorted(pose_length_counts.items(), reverse=True):
                print(f"  {count} branch{'es' if count>1 else ''} with {length} pose{'s' if length!=1 else ''}")
            top_diverse=self.evaluate_branches(paths,pose_history)
            print(top_diverse)
            print(top_diverse.tolist(),len(top_diverse))
            return top_diverse.tolist(),len(top_diverse)        

    def agent_seeks_goal(self, collect_data:bool=False)->tuple[list,bool]:
        motion_data, agent_lost = self.apply_goal_seeking(collect_data)
        goal = self.exploitative_behaviour.get_preferred_objective()
        if 'image' in goal and goal['image'] is not None:
            for step_data in motion_data:
                step_data['goal'] = goal['image']
   
        print('APPLIED REACH OBJECTIVE POLICY')
        return motion_data, agent_lost
    
    def apply_goal_seeking(self, collect_data:bool=False)->tuple[list,bool]:
        
        if self.models_manager.agent_lost():
            return self.agent_explores(collect_data)
            #DONE
        goal = self.exploitative_behaviour.get_preferred_objective()
        #DO THIS RESEARCH EVERY FEW POLICIES
        if not goal or goal['decay'] <= 0 :
            print('=================================== SEARCH GOAL ==========================')
            goal = self.exploitative_behaviour.define_preferred_objective(self.models_manager, self.preferred_colour_range)
        #TMP TEST VISUALISATION
        # if goal and goal['image'] is not None:
        #     if len(goal['image'].shape)<4:
        #         img = goal['image'].unsqueeze(0)
        #     else:
        #         img = goal['image']
        #     visualise_image(img, title='goal', fig_id=11)
        if not goal :
            print('=================================== NO GOALFOUND TO REACH ==========================')
            return self.agent_explores(collect_data)
            #DONE
        # elif 'pose' in goal and goal['pose'][:2] == self.models_manager.get_best_place_hypothesis()['pose'][:2]:
        #     #We are on the preferrence, nothing to do
        #     return {}, False
        
        elif goal['exp'] == self.models_manager.get_current_exp_id():
            print('=================================== GOAL FOUND IN PLACE ==========================')
            policy, error = self.exploitative_behaviour.go_to_observation_in_place(self.models_manager, goal.copy())
            if not error:
                if len(policy) > 0:
                    n_actions = 1
                else:
                    n_actions = 0
                
            else:
                #We failed to reach goal, we will retry process
                logging.warning('WARNING: apply_goal_seeking- Error in go_to_observation_in_place with policy' + str(transform_policy_from_hot_encoded_to_str(policy)))
                return [], False
            #NOTE: THIS CAN RETURN ERROR IF EGO MODEL SEE GOAL IN EXP, BUT FURTHER THAN LOOKAHEAD
            #THEN ALLO CALLED, AND ALLO WRONG PLACE REPRESENTATION. Anyway, likely a one time error pred as it happens.

        else:
            print('=================================== REACH EXP CONTAINING GOAL s==========================')
            path = self.exploitative_behaviour.apply_djikstra_over_memory(self.models_manager, goal)
            place_to_go = self.exploitative_behaviour.determine_location_to_reach_in_path(self.models_manager,path)
            print('Chosen place to go to reach objetive exp', place_to_go)
            if place_to_go:
                policy, n_actions = self.exploitative_behaviour.go_to_given_place(\
                    self.models_manager,place_to_go['current_exp_door_pose'])
            else:
                print('This place is not connected to any we know to lead toward the goal.')
                return self.agent_explores(collect_data)
        #The memory of the goal decay over time when reaching it, 
        # thus requiring refreshing of memory from time to time
        self.exploitative_behaviour.goal_memory_decay()
        motion_data,agent_lost = self.apply_policy(policy, n_actions, collect_data)
        return motion_data,agent_lost 
        
    def collect_data_models(self,obs)->dict:
        """collect latest data from manager """
        ''' this is for visualisation purposes only '''
        data = {}
        place_description = self.models_manager.get_best_place_hypothesis()
        data.update({k: v for k, v in place_description.items() if k != "pose"})
        if 'post' in data:
            del data['post']

        if "decoded" in obs and obs["decoded"] is not None:
            if isinstance(obs["decoded"], (list, tuple)):
                data["decoded"] = obs["decoded"]          # list is already fine for plotting
            else:
                data["decoded"] = convert_tensor_to_matplolib_list(obs["decoded"])
                if "decoded" in obs and obs["decoded"] is not None:
                    data["decoded"] = convert_tensor_to_matplolib_list(obs["decoded"])
                #----- just PLOT DATA -----#
        data['GP'] = self.models_manager.memory_graph.get_global_position() 
        data['exp'] = self.models_manager.memory_graph.get_current_exp_id()
        if 'mse' not in data :
            data['mse'] = -1
        if 'kl' not in data :
            data['kl'] = 0
        if 'info_gain' not in data :
            data['info_gain'] = 0
        #TEST
        # exp_ids, probs = self.exp_visualisation_proba(place_descriptors)
        # data['exp_ids'] = [exp_ids]
        # data['exp_prob'] = [probs]
        p = obs["pose"]

        # make an immutable copy
        if isinstance(p, np.ndarray):
            data['pose'] = p.copy()
        elif isinstance(p, list):
            data['pose'] = p[:]            # shallow copy of list
        else:                               # tuple or something immutable
            data['pose'] = tuple(p)
        p = obs["pose"]
        data['pose'] = p.copy() if hasattr(p, "copy") else p[:]
        print("[DBG collect] live pose", data['pose'])
        for key in ("recommended_mode", "recommended_submode",
            "mode_confidence", "submode_confidence",
            "uncertainty", "changepoint_mass"):
            if key in obs and obs[key] is not None:
                val = obs[key]
                if isinstance(val, np.ndarray):
                    data[key] = val.copy()
                elif isinstance(val, list):
                    data[key] = val[:]           # shallow copy of list
                else:                            # str, float, tuple … already immutable
                    data[key] = val

        return data
    
    def save_created_map(self, saving_dir:str)->None:
        save_memory(self.models_manager, saving_dir)
    
    def get_memory_map_data(self, dbg=True):
        """
        This is for plotting the memory map.
        Adds debug dumps of every link and deduplicates before returning.
        """
        mg = self.models_manager.memory_graph
        emap = mg.experience_map

        # 1) Prepare the empty payload
        memory_map_data = {
            'exps_GP': [], 
            'exps_decay': [], 
            'ghost_exps_GP': [],
            'ghost_exps_link': [], 
            'exps_links': []
        }

        # 2) Current experience and global pose
        memory_map_data['current_exp_id'] = mg.get_current_exp_id()
        memory_map_data['current_GP']     = mg.get_global_position()
        if memory_map_data['current_exp_id'] < 0:
            if dbg:
                print("[DBG] No current experience → empty map")
            return memory_map_data

        memory_map_data['current_exp_GP'] = mg.get_exp_global_position()

        # 3) DEBUG: dump every existing link in the graph
        if dbg:
            print("[DBG] Full link graph (exp → targets):")
            for exp in emap.exps:
                targets = [link.target.id for link in exp.links]
                print(f"   Exp {exp.id} → {targets}")

        # 4) Collect experience nodes
        for vc in mg.view_cells.cells:
            for exp in vc.exps:
                memory_map_data['exps_GP'].append([exp.x_m, exp.y_m])
                memory_map_data['exps_decay'].append(vc.decay)

        # 5) Ghost experiences + their links
        for ghost in emap.ghost_exps:
            memory_map_data['ghost_exps_GP'].append([ghost.x_m, ghost.y_m])
            for link in ghost.links:
                memory_map_data['ghost_exps_link'].append([ghost.x_m, ghost.y_m])
                memory_map_data['ghost_exps_link'].append([link.target.x_m, link.target.y_m])

        # 6) Real experience‐to‐experience links
        for exp in emap.exps:
            for link in exp.links:
                # skip ghost‐to‐ghost or ghost‐to‐real if any slipped through
                if not getattr(link.target, 'ghost_exp', False):
                    memory_map_data['exps_links'].append([link.target.x_m, link.target.y_m])
                    memory_map_data['exps_links'].append([exp.x_m, exp.y_m])

        # 7) DEBUG: show raw link list
        if dbg:
            raw = memory_map_data['exps_links']
            n_pairs = len(raw) // 2
            print(f"[DBG] Raw exps_links pairs: {n_pairs}, raw list (first 6 points) = {raw[:6]}")

        # 8) Deduplicate accidental duplicates
        clean = []
        seen = set()
        pts = memory_map_data['exps_links']
        # interpret as consecutive pairs [p0, p1, p2, p3, ...]
        for i in range(0, len(pts), 2):
            p0 = tuple(pts[i])
            p1 = tuple(pts[i+1])
            pair = (p0, p1)
            if pair not in seen:
                seen.add(pair)
                clean.extend([list(p0), list(p1)])
        memory_map_data['exps_links'] = clean

        # 9) DEBUG: show cleaned link list
        if dbg:
            n_clean = len(clean) // 2
            print(f"[DBG] Clean exps_links pairs: {n_clean}, cleaned list (first 6 points) = {clean[:6]}")

        return memory_map_data
    

    def build_pcfg_from_memory(self):
        mg      = self.models_manager.memory_graph
        emap    = mg.experience_map
        current = mg.get_current_exp_id()
        exps_by_dist = mg.get_exps_organised(current)
        all_exps     = emap.exps

        # 1) Abstract graph & distance priors
        graph = {e.id: [l.target.id for l in e.links] for e in all_exps}
        print("[PCFG DEBUG] graph:", graph)

        id_to_dist  = {}
        total_w = 0.0
        for d in exps_by_dist:
            dist = math.hypot(d['x'], d['y'])
            id_to_dist[d['id']] = dist
            total_w += 1.0/(dist + 1e-5)

        # 2) Top‐level: EXPLORE→NAVPLAN and NAVPLAN→GOTOₜ
        rules = defaultdict(list)
        rules['EXPLORE'].append(('NAVPLAN', 1.0))
        for tgt, dist in id_to_dist.items():
            p = (1.0/(dist+1e-5)) / total_w
            rules['NAVPLAN'].append((f'GOTO_{tgt}', p))
            rules[f'GOTO_{tgt}'].append((f'MOVESEQ_{current}_{tgt}', 1.0))

        # 3) BFS helper on abstract graph
        def find_paths(start, goal, max_depth=15, max_paths=10):
            paths, q = [], deque([[start]])
            while q and len(paths)<max_paths:
                path = q.popleft()
                if path[-1]==goal:
                    paths.append(path)
                elif len(path)<max_depth:
                    for nb in graph.get(path[-1],[]):
                        if nb not in path:
                            q.append(path+[nb])
            return paths

        # 4) Gather all abstract paths and their edges
        hop_edges = set()
        hopseqs   = {}
        for tgt in id_to_dist:
            paths = find_paths(current, tgt)
            hopseqs[tgt] = paths
            for path in paths:
                for u,v in zip(path, path[1:]):
                    hop_edges.add((u,v))
        hop_edges.add((current, current))

        # 4a) MOVESEQ_current→tgt → HOPSEQ_current→tgt
        for tgt in id_to_dist:
            lhs = f'MOVESEQ_{current}_{tgt}'
            rules[lhs].append((f'HOPSEQ_{current}_{tgt}', 1.0))

        # 4b) HOPSEQ_current→tgt → STEP_u_v … but *prefix* (current→current)
        for tgt, paths in hopseqs.items():
            lhs = f'HOPSEQ_{current}_{tgt}'
            if not paths and current!=tgt:
                # no path found
                rules[lhs].append((f'STEP_{current}_{tgt}', 1.0))
                hop_edges.add((current, tgt))
            else:
                w = 1.0/len(paths) if paths else 1.0
                for path in paths:
                    # *** here’s the dummy “first hop” ***
                    hops = [(current, current)] + list(zip(path, path[1:]))
                    seq = [f'STEP_{u}_{v}' for u,v in hops]
                    rhs = " ".join(seq)
                    rules[lhs].append((rhs, w))
                    print(f"[PCFG DEBUG] HOPSEQ {lhs} ← {hops} → {seq}")

        # 5) STEP_u_v → primitives OR fallback
        for (u,v) in hop_edges:
            lhs = f'STEP_{u}_{v}'
            prims = self.get_primitives(u, v)
            if prims:
                rhs = " ".join(f"'{p}'" for p in prims)
                rules[lhs].append((rhs, 1.0))
                print(f"[PCFG DEBUG] STEP_{u}_{v} → prims {prims}")
            else:
                rules[lhs].append((f"'step({u},{v})'", 1.0))
                print(f"[PCFG DEBUG] STEP_{u}_{v} → fallback 'step({u},{v})'")

        # 6) Optional hard‐coded extras
        hard = {
        f'MOVESEQ_{current}_18': [f"step({current},19)","step(19,18)"],
        f'MOVESEQ_{current}_3' : [f"step({current},5)","step(5,4)","step(4,3)"],
        }
        for lhs, steps in hard.items():
            if lhs not in rules:
                rhs = " ".join(f"'{s}'" for s in steps)
                rules[lhs].append((rhs, 1.0))
                print(f"[PCFG DEBUG] hardcoded {lhs} → {steps}")

        # 7) Assemble into PCFG
        pcfg_lines = []
        for lhs, prods in rules.items():
            total = sum(p for _,p in prods)
            for rhs,p in prods:
                pcfg_lines.append(f"{lhs} -> {rhs} [{p/total:.4f}]")

        grammar_src = "\n".join(pcfg_lines)
        print("[PCFG DEBUG] Final grammar:\n" + grammar_src)
        return PCFG.fromstring(grammar_src)
          
    def get_primitives(self, u: int, v: int) -> list[str]:
        """
        Return the best primitive sequence for traversing the edge u→v:

        0) If we’ve already stored a path in the ExperienceLink, return it.
        1) Otherwise, build a fresh A* plan from *our real pose* → node v.
        2) Cache it in the link and return.
        """
        emap = self.models_manager.memory_graph.experience_map

        # 0) do we already have a stored primitive path?
        link = self._find_link(u, v)
        if link and link.path_forward:
            print(f"[get_primitives] cached path for {u}->{v}: {link.path_forward}")
            return list(link.path_forward)

        # 1) No cache → build an A* state from our current *real* pose
        real = self.agent_current_pose 
        if real is None:
            raise RuntimeError("No last_real_pose available for A* start!")
        sx, sy, sd = real
        start = State(int(round(sx)), int(round(sy)), int(sd))

        # target node’s stored map pose
        gx, gy, gd = emap.get_pose(v)
        goal = State(int(round(gx)), int(round(gy)), int(gd))

        print(f"[get_primitives] no cache {u}->{v}, A* from {start} → {goal}")

        # 2) Run your egocentric‐aware A*:
        prims = self.astar_prims(
            start,
            goal,
            self.models_manager.egocentric_process,
            num_samples=5
        )

        print(f"[get_primitives] A* returned for {u}->{v}: {prims}")

        return prims


    def _find_link(self, u: int, v: int):
        """
        Scan your ExperienceMap for a u→v link; return it or None.
        """
        emap = self.models_manager.memory_graph.experience_map
        for exp in emap.exps:
            if exp.id == u:
                for link in exp.links:
                    if link.target.id == v:
                        return link
        return None




    # Cardinal motion vectors for dir ∈ {0=E,1=S,2=W,3=N}
    

    def heuristic(self, s: State, g: State) -> float:
        # Manhattan distance + minimal turn cost
        manh = abs(s.x - g.x) + abs(s.y - g.y)
        turn = min((s.d - g.d) % 4, (g.d - s.d) % 4)
        return manh + turn

    def astar_prims(
    self,
    start: State,
    goal:  State,
    egocentric_process,
    num_samples: int = 5
    ) -> list[str]:
        
        """
        A* in (x,y,dir)-space, but we skip ANY forward candidate
        whose entire prefix fails the egocentric collision check.
        """
        print(f"[A*] start={start}  goal={goal}")
        open_pq = [(self.heuristic(start, goal), 0, start, [])]  # (f, g, state, seq)
        g_score = { start: 0 }
        closed   = set()
        step = 0

        # helper: pack a list of 'forward'/'left'/'right' into a (T,3) float tensor
        def to_onehot_tensor(seq: list[str]) -> torch.Tensor:
            mapping = {'forward': [1,0,0], 'right': [0,1,0], 'left': [0,0,1]}
            return torch.tensor([mapping[a] for a in seq], dtype=torch.float32)

        while open_pq:
            f, g, (x,y,d), seq = heapq.heappop(open_pq)
            print(f"[A*][{step}] POP  state={State(x,y,d)}  g={g}  f={f}  seq={seq!r}")
            step += 1

            if (x,y,d) in closed:
                print("    SKIP (closed)")
                continue
            closed.add((x,y,d))

            # success only if we've reached the right cell and the correct orientation
            if (x, y, d) == (goal.x, goal.y, goal.d):
                print(f"[A*] reached goal at step {step} → seq={seq!r}")
                return seq

            for act in ("left","right","forward"):
                if act == "forward":
                    dx, dy = DIR_VECS[d]
                    nx, ny, nd = x + dx, y + dy, d
                elif act == "left":
                    nx, ny, nd = x, y, (d - 1) % 4
                else:  # right
                    nx, ny, nd = x, y, (d + 1) % 4

                ns = State(nx, ny, nd)
                print(f"    try {act!r} -> next={ns}", end="")

                if (nx,ny,nd) in closed:
                    print("   SKIP (closed)")
                    continue

                new_seq = seq + [act]

                # **egocentric** collision check
                tensor = to_onehot_tensor(new_seq)               # shape (T,3)
                safe   = egocentric_process.egocentric_policy_assessment(
                            tensor, num_samples=num_samples
                        )
                if safe.shape[0] < tensor.shape[0]:
                    print("   SKIP (ego-collision)")
                    continue

                # accept → push into open
                g2 = g + 1
                h2 = self.heuristic(ns, goal)
                f2 = g2 + h2
                old = g_score.get(ns, float("inf"))
                if g2 < old:
                    g_score[ns] = g2
                    print(f"   PUSH g={g2} h={h2} f={f2} seq={new_seq!r}")
                    heapq.heappush(open_pq, (f2, g2, ns, new_seq))
                else:
                    print(f"   SKIP (worse g: {g2} ≥ {old})")

        print("[A*] no path found → returning empty")
        return []
    
    

    # ---------------------------------------------------------------------
    #  TOP-LEVEL INFORMATION-GAIN WRAPPER
    # ---------------------------------------------------------------------
    def info_gain(
            self,
            current_image,
            current_pose,
            replay_buffer,
            view_cells_manager,
            device: str = "cpu",
            weights: dict | None = None,
    ):
        """
        Compute an information-gain score for the agent’s current observation,
        with abundant DEBUG prints along the way.
        """
        print(f"[INFO-GAIN DEBUG] replay_buffer size = {len(replay_buffer)}")

        if weights is None:
            weights = {"visual": 0.5, "spatial": 0.3, "temporal": 0.2}
        print(f"[INFO-GAIN DEBUG] weights      : {weights}")

        # normalise pose to dict --------------------------------------------------
        if isinstance(current_pose, (tuple, list)):
            current_pose = self._pose_to_dict(current_pose)

        # components --------------------------------------------------------------
        visual_novelty = self.calculate_visual_novelty(
            current_image, replay_buffer, device)
        print(f"[INFO-GAIN DEBUG] visual_novelty = {visual_novelty:.4f}")

        spatial_novelty = self.calculate_spatial_novelty(
            current_image, current_pose, view_cells_manager, device)
        print(f"[INFO-GAIN DEBUG] spatial_novelty = {spatial_novelty:.4f}")

        temporal_novelty = self.calculate_temporal_novelty(
            current_image, replay_buffer, device)
        print(f"[INFO-GAIN DEBUG] temporal_novelty = {temporal_novelty:.4f}")

        # weighted average --------------------------------------------------------
        info_gain_score = (
            weights["visual"]   * visual_novelty +
            weights["spatial"]  * spatial_novelty +
            weights["temporal"] * temporal_novelty
        )
        info_gain_score = float(np.clip(info_gain_score, 0.0, 1.0))

        print(f"[INFO-GAIN DEBUG] --> info_gain_score = {info_gain_score:.4f}")
        print("[INFO-GAIN DEBUG] =================================================\n")
        return info_gain_score


    # ---------------------------------------------------------------------
    #  VISUAL NOVELTY
    # ---------------------------------------------------------------------
    def calculate_visual_novelty(
            self,
            current_image,
            replay_buffer,
            device,
            top_k: int = 10,
    ):
        print("\n  [VISUAL DEBUG] -------------------------------")
        if not replay_buffer:
            print("  [VISUAL DEBUG] replay_buffer empty → novelty = 1.0")
            return 1.0

        current_features = self.extract_image_features(current_image, device)
        
        similarities = []
        for idx, state in enumerate(replay_buffer[-top_k:]):
            real_img = state.get("real_image")
            if real_img is None:
                continue
            real_feat = self.extract_image_features(real_img, device)
            sim = self.compute_feature_similarity(current_features, real_feat)
            similarities.append(sim)
            print(f"  [VISUAL DEBUG]   state {-top_k+idx}: sim_real={sim:.4f}")

        if not similarities:
            print("  [VISUAL DEBUG] no comparable images → novelty = 1.0")
            return 1.0

        max_sim = max(similarities)
        novelty = 1.0 - max_sim
        print(f"  [VISUAL DEBUG] max_similarity={max_sim:.4f}  → novelty={novelty:.4f}")
        return novelty


    # ---------------------------------------------------------------------
    #  SPATIAL NOVELTY
    # ---------------------------------------------------------------------
    def calculate_spatial_novelty(
            self,
            current_image,
            current_pose,
            view_cells_manager,
            device,
    ):
        print("\n  [SPATIAL DEBUG] ------------------------------")

        cells = getattr(getattr(view_cells_manager, "view_cells", None), "cells", [])
        if not cells:
            print("  [SPATIAL DEBUG] no view-cells → novelty = 1.0")
            return 1.0

        current_feat = self.extract_image_features(current_image, device)
        novelties = []

        for c_idx, cell in enumerate(cells):
            if not cell.exemplars:
                continue

            sims = []
            for ex in cell.exemplars[-3:]:
                ex_feat = self.extract_image_features(ex, device)
                sim = self.compute_feature_similarity(current_feat, ex_feat)
                sims.append(sim)

            if not sims:
                continue

            max_sim = max(sims)
            if max_sim > 0.7:
                print(f"  [SPATIAL DEBUG] current_pose={current_pose}  "
                f"cell_pose=({cell.x_pc:.2f},{cell.y_pc:.2f},{cell.th_pc:.2f})")
                dist = self.calculate_pose_distance(
                    current_pose,
                    {"x": cell.x_pc, "y": cell.y_pc, "theta": cell.th_pc})
                novelty = 0.8 if dist > 2.0 else 0.2
                novelties.append(novelty)
                print(f"  [SPATIAL DEBUG] cell {c_idx:3d}: "
                    f"max_sim={max_sim:.3f}  dist={dist:.2f}  novelty={novelty:.2f}")

        if not novelties:
            print("  [SPATIAL DEBUG] no visually similar cells → novelty = 0.6")
            return 0.6

        mean_novelty = float(np.mean(novelties))
        print(f"  [SPATIAL DEBUG] mean spatial novelty = {mean_novelty:.4f}")
        return mean_novelty


    # ---------------------------------------------------------------------
    #  TEMPORAL NOVELTY
    # ---------------------------------------------------------------------
    def calculate_temporal_novelty(
            self,
            current_image,
            replay_buffer,
            device,
            decay_factor: float = 0.9,
    ):
        
        if not replay_buffer:
            print("  [TEMPORAL DEBUG] replay_buffer empty → novelty = 1.0")
            return 1.0

        current_feat = self.extract_image_features(current_image, device)
        score = 1.0
        for i, state in enumerate(reversed(replay_buffer[-20:])):
            img = state.get("real_image")
            if img is None:
                continue
            feat = self.extract_image_features(img, device)
            sim = self.compute_feature_similarity(current_feat, feat)
            w = decay_factor ** i
            score *= (1.0 - sim * w * 0.5)

        score = float(np.clip(score, 0.0, 1.0))
        print(f"  [TEMPORAL DEBUG] final temporal novelty = {score:.4f}")
        return score


    # ---------------------------------------------------------------------
    #  FEATURE EXTRACTION (with heavy validation prints)
    # ---------------------------------------------------------------------
    def extract_image_features(self, img, device="cpu"):
        """
        Convert *anything* the code might hand us into the standard 64-D
        descriptor produced by `rgb56_to_template64`.

        Accepted inputs
        ----------------
        • 56×56×3 RGB/BGR   (numpy or torch)  ← main camera frame
        • (3,56,56)         (numpy or torch)  ← CHW layout
        • 1-D length-64     (numpy or torch)  ← already a template
        • list / tuple with the above inside
        """
        # ---------- unwrap list/tuple -----------------------------------
        if isinstance(img, (list, tuple)):
            img = np.asarray(img)

        # ---------- already a 64-D vector? ------------------------------
        if torch.is_tensor(img) and img.ndim == 1 and img.numel() == 64:
            vec = img.detach().cpu().float().numpy()
            return vec / (np.linalg.norm(vec) + 1e-8)

        if isinstance(img, np.ndarray) and img.ndim == 1 and img.size == 64:
            vec = img.astype(np.float32)
            return vec / (np.linalg.norm(vec) + 1e-8)

        # ---------- otherwise convert the 56×56×3 image -----------------
        vec64_torch = self.models_manager.rgb56_to_template64(img, device=device)
        vec64 = vec64_torch.detach().cpu().float().numpy()
        return vec64 / (np.linalg.norm(vec64) + 1e-8)


    # ---------------------------------------------------------------------
    #  SIMILARITY + POSE DISTANCE (unchanged, but with prints if desired)
    # ---------------------------------------------------------------------
    def compute_feature_similarity(self, f1, f2):
        if len(f1) != len(f2):
            return 0.5
        sim = 1.0 - cosine(f1, f2)
        return float(np.clip(sim, 0.0, 1.0))

    def _pose_to_dict(self, p):
        """
        Accepts:
            • {'x':…, 'y':…, 'theta':…}
            • (x, y, theta)   tuple / list
            • np.ndarray shape (3,)
            • torch.Tensor  shape (3,)
        Returns a plain Python dict with float values.
        """
        if isinstance(p, dict):
            return {"x": float(p["x"]), "y": float(p["y"]), "theta": float(p["theta"])}
        if isinstance(p, (list, tuple)) and len(p) == 3:
            return {"x": float(p[0]), "y": float(p[1]), "theta": float(p[2])}
        if isinstance(p, np.ndarray) and p.size == 3:
            return {"x": float(p[0]), "y": float(p[1]), "theta": float(p[2])}
        if torch.is_tensor(p) and p.numel() == 3:
            p = p.detach().cpu().float().tolist()
            return {"x": p[0], "y": p[1], "theta": p[2]}
        raise ValueError(f"_pose_to_dict: unsupported pose format {type(p)}")
    def calculate_pose_distance(self, pose1, pose2):
        p1 = self._pose_to_dict(pose1)
        p2 = self._pose_to_dict(pose2)

        pos = np.hypot(p1["x"] - p2["x"], p1["y"] - p2["y"])
        ang = abs(p1["theta"] - p2["theta"])
        ang = min(ang, 2 * np.pi - ang)
        return pos + 0.5 * ang
    
    def plan_progress_hybrid(self,current_pose, goal_pose, plan_adherence_score, time_efficiency):
        """
        Hybrid approach combining multiple factors
        """
        if current_pose is None or goal_pose is None:
            return 0.0
        
        # Distance component (0-1)
        distance_progress = self.calculate_distance_progress(current_pose, goal_pose)
        
        # Plan adherence component (0-1)
        adherence_progress = max(0.0, min(1.0, plan_adherence_score))
        
        # Time efficiency component (0-1)
        efficiency_progress = max(0.0, min(1.0, time_efficiency))
        
        # Weighted combination
        progress = (0.5 * distance_progress + 
                    0.3 * adherence_progress + 
                    0.2 * efficiency_progress)
        
        return min(1.0, max(0.0, progress))
    def plan_progress_placeholder(replay_buffer=None, current_pose=None, goal_pose=None):
        
        # Simple placeholder logic - returns moderate progress
        # You can replace this with your actual progress calculation
        
        if current_pose is None or goal_pose is None:
            return 0.5  # Default moderate progress when no position info
        
        # Very basic distance-based progress (replace with your logic)
        try:
            if isinstance(current_pose, (list, tuple)) and isinstance(goal_pose, (list, tuple)):
                dx = float(goal_pose[0]) - float(current_pose[0])
                dy = float(goal_pose[1]) - float(current_pose[1])
                distance = (dx**2 + dy**2)**0.5
                
                # Simple progress based on distance (assumes max distance of 10 units)
                progress = max(0.0, 1.0 - distance / 10.0)
                return min(1.0, progress)
        except (IndexError, TypeError, ValueError):
            pass
        
        # Fallback to moderate progress
        return 0.5
    '''def build_pcfg_from_memory(self):
        mg      = self.models_manager.memory_graph
        emap    = mg.experience_map
        current = mg.get_current_exp_id()
        exps_by_dist = mg.get_exps_organised(current)
        all_exps     = emap.exps

        # 1) Abstract graph & distance priors
        graph = {e.id: [l.target.id for l in e.links] for e in all_exps}
        print("[PCFG DEBUG] graph:", graph)

        id_to_dist  = {}
        total_weight = 0.0
        for d in exps_by_dist:
            dist = math.hypot(d['x'], d['y'])
            id_to_dist[d['id']] = dist
            total_weight += 1.0 / (dist + 1e-5)

        # 2) Top‐level productions
        rules = defaultdict(list)
        rules['EXPLORE'].append(('NAVPLAN', 1.0))
        for tgt, dist in id_to_dist.items():
            p = (1.0/(dist+1e-5)) / total_weight
            rules['NAVPLAN'].append((f'GOTO_{tgt}', p))
            rules[f'GOTO_{tgt}'].append((f'MOVESEQ_{current}_{tgt}', 1.0))

        # 3) BFS helper on abstract graph
        def find_paths(graph, start, goal, max_depth=15, max_paths=15):
            paths, queue = [], deque([[start]])
            while queue and len(paths) < max_paths:
                path = queue.popleft()
                if path[-1] == goal:
                    paths.append(path)
                elif len(path) < max_depth:
                    for nb in graph.get(path[-1], []):
                        if nb not in path:
                            queue.append(path + [nb])
            return paths

        # 4) MOVESEQ: only first hop uses get_primitives; rest use stored link.path_forward
        for tgt in id_to_dist:
            lhs = f"MOVESEQ_{current}_{tgt}"
            prods = []

            # a) try all abstract routes
            for path in find_paths(graph, current, tgt):
                seq = []
                hops = list(zip(path, path[1:]))
                # a) try all abstract routes, *prefixed* by (current→current)
                for path in find_paths(graph, current, tgt):
                    seq = []
                    # force a dummy “hop” that will get you from your real pose into your canonical
                    # current-node pose via get_primitives(current,current)
                    hops = [(current, current)] + list(zip(path, path[1:]))

                #  a.1) first hop: from real pose → first node
                for u, v in hops:
                    prims = self.get_primitives(u, v)
                    if prims:
                        seq.extend(prims)
                    else:
                        # fallback to a single step‐token if no stored or computed primitives
                        seq.append(f"step({u},{v})")


                # Flatten into a quoted‐terminals RHS
                rhs = " ".join(f"'{tok}'" for tok in seq)
                prods.append(rhs)
                print(f"[PCFG DEBUG] path {path} → prims {seq}")

            # b) if we got at least one flattened seq, normalize & emit
            if prods:
                w = 1.0/len(prods)
                for rhs in prods:
                    rules[lhs].append((rhs, w))
                continue

            # c) fallback if no abstract route found
            if current != tgt:
                rhs = f"'step({current},{tgt})'"
                rules[lhs].append((rhs, 1.0))
                print(f"[PCFG DEBUG] forced fallback for {lhs}")

        # 5) optional hard‐coded extras (unchanged)
        hard = {
            f'MOVESEQ_{current}_18': [f"step({current},19)","step(19,18)"],
            f'MOVESEQ_{current}_3' : [f"step({current},5)","step(5,4)","step(4,3)"],
        }
        for lhs, steps in hard.items():
            if lhs not in rules:
                rhs = " ".join(f"'{s}'" for s in steps)
                rules[lhs].append((rhs,1.0))
                print(f"[PCFG DEBUG] hardcoded {lhs} -> {steps}")

        # 6) Assemble PCFG
        pcfg_lines = []
        for lhs, prods in rules.items():
            total_p = sum(p for _,p in prods)
            for rhs,p in prods:
                pcfg_lines.append(f"{lhs} -> {rhs} [{p/total_p:.4f}]")

        grammar_src = "\n".join(pcfg_lines)
        print("[PCFG DEBUG] Final grammar:\n" + grammar_src)
        return PCFG.fromstring(grammar_src)

        
        def apply_exploration(self)->tuple[list,int]:
        print('====================================================explo')
        if self.models_manager.agent_lost():
            print('Agent lost, trying to determine best place hypothesis')
            policy, n_action = self.explorative_behaviour.solve_doubt_over_place(self.models_manager)
            print("solve doubt over place", policy, n_action)
            return policy, n_action
        
        ongoing_exploration_option = 'explore'
        print('exploring')
        if self.explorative_behaviour.is_agent_exploring(): #agent_exploring return the latest computed value
            policy, n_action = self.explorative_behaviour.one_step_ego_allo_exploration(self.models_manager)
            print("is agent exploring?",self.explorative_behaviour.is_agent_exploring())
            print(policy, n_action )
        else:
            policy, n_action = [],[]
        
              
        while len(policy) == 0:    
            print('++++++++++++++++++++++++ while len policy =0')   
            print('trying to increase lookahead by 1 to increase exploration range')
            lookahead_increased = self.models_manager.increase_lookahead(max=5)   
            if not lookahead_increased:
                print('lookahead at max value, search to return to another place')
                ongoing_exploration_option = 'change_memory_place'
              
            if ongoing_exploration_option == 'change_memory_place':
                print('Searching for another place to go to')
                print("this time for real",self.step_count())
                policy, n_action= self.push_explo_stm()  
                print('this time ++++++++++++++++++++++++',self.step_count()) 
                if place_to_go:
                    print('Searching How to go to the other place')
                    print("where are we going?",place_to_go['current_exp_door_pose'],place_to_go)
                    policy, n_action = self.exploitative_behaviour.go_to_given_place(\
                    self.models_manager,place_to_go['current_exp_door_pose'])
                
                else:
                    print('No place found to return to, using ego model')
                    ongoing_exploration_option = 'push_from_comfort_zone
                
            elif ongoing_exploration_option == 'explore':
                print('Exploring with allo model')
                policy, n_action = self.explorative_behaviour.one_step_ego_allo_exploration(self.models_manager)

            if ongoing_exploration_option == 'push_from_comfort_zone':
                print('Using ego model to push exploration')
                policy, n_action = self.explorative_behaviour.one_step_egocentric_exploration(self.models_manager)

        print('++++++++0 policy should be before this++++++++++++++++++++++++') 
        policy_G = self.explorative_behaviour.get_latest_policy_EFE(reset_EFE=True)
        print("policyG",policy_G ,self.explorative_behaviour.is_agent_exploring())
         #we get policy_G and then erase it from memory
        if policy_G is not None: 
            info_gain_coeff = self.explorative_behaviour.rolling_coeff_info_gain()
            print("info gain coeff", info_gain_coeff)
            if self.explorative_behaviour.define_is_agent_exploring(info_gain_coeff, policy_G, threshold=1):
                self.models_manager.reset_variable_lookahead_to_default()
        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')            
        return policy, n_action




if self.step_count()==7:
            #self.env.put_obj(Wall(), 14, 10) 
            sx,sy,sd=obs["pose"]
            gx,gy,gd=[-3,0,2]
            start= State(int(round(sx)), int(round(sy)), int(sd))
            print("START",start)
            goal = State(int(round(gx)), int(round(gy)), int(gd))
            path=self.planner.astar_prims(self.wm,self.belief, start, goal, verbose=True)
            print("PATHdbg/demo_rollout1.png",path)
            # ------------ render imagined rollout ------------------------------
            frames = self.planner.render_plan(self.wm,self.belief, path, include_last=True)
            Path("dbg").mkdir(exist_ok=True, parents=True)
            save_image(torch.stack(frames),
                    "dbg/demo_rollout1.png", nrow=len(frames),
                    normalize=True, scale_each=False)
            print("saved dbg/demo_rollout1.png.png")
            path2=self.astar_prims(start,goal,self.models_manager.egocentric_process)
            print("PATH",path,path2)
            gx,gy,gd=[6,2,3]
            start= State(int(round(sx)), int(round(sy)), int(sd))
            goal = State(int(round(gx)), int(round(gy)), int(gd))
            path=self.planner.astar_prims(self.wm,self.belief, start, goal, verbose=True)
            # ------------ render imagined rollout ------------------------------
            frames = self.planner.render_plan(self.wm,self.belief, path, include_last=True)
            Path("dbg").mkdir(exist_ok=True, parents=True)
            save_image(torch.stack(frames),
                    "dbg/demo_rollout2.png", nrow=len(frames),
                    normalize=True, scale_each=False)
            print("dbg/demo_rollout2.png",path)
            path2=self.astar_prims(start,goal,self.models_manager.egocentric_process)
            print(path,path2)
        if self.step_count()==4:
            sx,sy,sd=obs["pose"]
            gx,gy,gd=[-3,0,2]
            start= State(int(round(sx)), int(round(sy)), int(sd))
            print("START",start)
            goal = State(int(round(gx)), int(round(gy)), int(gd))
            path=self.planner.astar_prims(self.wm,self.belief, start, goal, verbose=True)
            # ------------ render imagined rollout ------------------------------
            frames = self.planner.render_plan(self.wm,self.belief, path, include_last=True)
            Path("dbg").mkdir(exist_ok=True, parents=True)
            save_image(torch.stack(frames),
                    "dbg/demo_rollout3.png", nrow=len(frames),
                    normalize=True, scale_each=False)
            print("PATHdbg/demo_rollout3.png",path)
            path2=self.astar_prims(start,goal,self.models_manager.egocentric_process)
            print("PATH",path,path2)
            gx,gy,gd=[6,2,3]
            start= State(int(round(sx)), int(round(sy)), int(sd))
            goal = State(int(round(gx)), int(round(gy)), int(gd))
            path=self.planner.astar_prims(self.wm,self.belief, start, goal, verbose=True)
            # ------------ render imagined rollout ------------------------------
            frames = self.planner.render_plan(self.wm,self.belief, path, include_last=True)
            Path("dbg").mkdir(exist_ok=True, parents=True)
            save_image(torch.stack(frames),
                    "dbg/demo_rollout4.png", nrow=len(frames),
                    normalize=True, scale_each=False)
            print("START",start)
            print("PATHdbg/demo_rollout4.png",path)
            path2=self.astar_prims(start,goal,self.models_manager.egocentric_process)
            print("PATH",path,path2)
        if self.step_count()==9:
            sx,sy,sd=obs["pose"]
            gx,gy,gd=[-3,0,2]
            start= State(int(round(sx)), int(round(sy)), int(sd))
            goal = State(int(round(gx)), int(round(gy)), int(gd))
            path=self.planner.astar_prims(self.wm,self.belief, start, goal, verbose=False)
            # ------------ render imagined rollout ------------------------------
            frames = self.planner.render_plan(self.wm,self.belief, path, include_last=True)
            Path("dbg").mkdir(exist_ok=True, parents=True)
            save_image(torch.stack(frames),
                    "dbg/demo_rollout5.5.png", nrow=len(frames),
                    normalize=True, scale_each=False)
            print("START",start)
            print("dbg/demo_rollout5.5.png",path)
            path2=self.astar_prims(start,goal,self.models_manager.egocentric_process)
            print(path,path2)
            gx,gy,gd=[6,2,3]
            start= State(int(round(sx)), int(round(sy)), int(sd))
            goal = State(int(round(gx)), int(round(gy)), int(gd))
            path=self.planner.astar_prims(self.wm,self.belief, start, goal, verbose=True)
            # ------------ render imagined rollout ------------------------------
            frames = self.planner.render_plan(self.wm,self.belief, path, include_last=True)
            Path("dbg").mkdir(exist_ok=True, parents=True)
            save_image(torch.stack(frames),
                    "dbg/demo_rollout5.png", nrow=len(frames),
                    normalize=True, scale_each=False)
            print("START",start)
            print("dbg/demo_rollout5.png",path)
            path2=self.astar_prims(start,goal,self.models_manager.egocentric_process)
            print(path,path2)
        if self.step_count()==11:
            sx,sy,sd=obs["pose"]
            gx,gy,gd=[-3,0,2]
            start= State(int(round(sx)), int(round(sy)), int(sd))
            goal = State(int(round(gx)), int(round(gy)), int(gd))
            path=self.planner.astar_prims(self.wm,self.belief, start, goal, verbose=False)
            # ------------ render imagined rollout ------------------------------
            frames = self.planner.render_plan(self.wm,self.belief, path, include_last=True)
            Path("dbg").mkdir(exist_ok=True, parents=True)
            save_image(torch.stack(frames),
                    "dbg/demo_rollout6.png", nrow=len(frames),
                    normalize=True, scale_each=False)
            print("dbg/demo_rollout6.png",path)
            path2=self.astar_prims(start,goal,self.models_manager.egocentric_process)
            print(path,path2)
            gx,gy,gd=[6,2,3]
            start= State(int(round(sx)), int(round(sy)), int(sd))
            goal = State(int(round(gx)), int(round(gy)), int(gd))
            path=self.planner.astar_prims(self.wm,self.belief, start, goal, verbose=False)
            path2=self.astar_prims(start,goal,self.models_manager.egocentric_process)
            print(path,path2) '''

            




    #-----------__MAIN -----------------------------------------------------------------------------------------------------------
    def main():
        try:
            args = parser.parse_args()
            test = MinigridInteraction(args)

        finally:
            pass

    if __name__ == "__main__":
    # instantiating the decorator
        main()
    
```

`hierarchical-nav/control_eval/meta_controller_system.py`:

```py
#meta_controller_system.py

import time
import random
import numpy as np
from collections import deque
from collections import defaultdict
from nltk import PCFG
import math
import heapq
import torch
from collections import namedtuple
from nltk.parse.generate import generate
State = namedtuple("State", ["x","y","d"])
DIR_VECS = [(1,0),(0,1),(-1,0),(0,-1)]
ACTIONS   = ["forward","left","right"]
# ---------------------------------------------------
# Utility placeholders (replace with real implementations)
# ---------------------------------------------------

def no_vel_no_action(obs):
    """Strip velocity/action from raw observation."""
    return obs

# A generic PCFG sampler stub (implement depth-limited top-down sampling)
def sample_from_pcfg(grammar: PCFG, symbol=None, max_depth=50):
    """
    A simple top-down stochastic PCFG sampler.
    """
    if symbol is None:
        symbol = grammar.start()

    def expand(sym, depth):
        # Terminal: no .symbol() method or depth exhausted
        if depth <= 0 or not hasattr(sym, 'symbol'):
            return [str(sym)]
        prods = grammar.productions(lhs=sym)
        probs = [p.prob() for p in prods]
        total = sum(probs)
        if total <= 0:
            return [str(sym)]
        weights = [p/total for p in probs]
        prod = random.choices(prods, weights=weights, k=1)[0]
        result = []
        for rhs_sym in prod.rhs():
            result += expand(rhs_sym, depth-1)
        return result

    return expand(symbol, max_depth)

# -------------------------------------
# Hierarchical Dynamic PCFG Builder
# -------------------------------------
class HierarchicalDynamicPCFG:
    """
    Three-mode PCFG builder: EXPLORE, NAVIGATE, RECOVER.
    Builds, evolves, and returns the active grammar.
    """
    MODES = ('EXPLORE', 'NAVIGATE', 'RECOVER')

    def __init__(self, key, pattern_detector=None, stats_collector=None):
        # key: the agent instance with models_manager, etc.
        self.memory_graph      = self.key.models_manager.memory_graph
        self.pattern_detector  = pattern_detector
        self.stats_collector   = stats_collector
        self.current_mode      = 'EXPLORE'
        self.level_grammars    = {}
        self.history           = []
        self.mg      = self.key.models_manager.memory_graph
        self.emap=self.mg.experience_map

    def set_mode(self, mode: str):
        assert mode in self.MODES, f"Unknown mode {mode}"
        self.current_mode = mode

    def get_grammar(self) -> PCFG:
        if self.current_mode not in self.level_grammars:
            # build on first request
            if self.current_mode == 'NAVIGATE':
                self.level_grammars['NAVIGATE'] = self.build_pcfg_from_memory()
            else:
                self.level_grammars[self.current_mode] = self.build_for_mode(self.current_mode)
        return self.level_grammars[self.current_mode]

    def build_for_mode(self, mode: str) -> PCFG:
        if mode == 'EXPLORE':
            return self._build_exploration_grammar()
        if mode == 'NAVIGATE':
            return self.build_pcfg_from_memory()
        if mode == 'RECOVER':
            return self._build_recover_grammar()
        raise ValueError(f"Unsupported mode: {mode}")

    def evolve(self, feedback: dict):
        # record event
        self.history.append((self.current_mode, time.time(), feedback))
        # update detectors/stats
        if self.stats_collector:
            self.stats_collector.update(feedback)
        if self.pattern_detector and 'sequence' in feedback:
            self.pattern_detector.update(feedback['sequence'])
        # rebuild
        self.level_grammars[self.current_mode] = self.build_for_mode(self.current_mode)

    def _build_exploration_grammar(self) -> PCFG:
        rules = []
        # Example: simple curiosity vs gap-fill
        rules.append("EXPLORE -> RANDOM_WALK [0.5]")
        rules.append("EXPLORE -> GAP_FILL [0.5]")
        rules.append("RANDOM_WALK -> 'forward' [1.0]")
        rules.append("GAP_FILL -> 'left' [0.5] 'forward' [0.5]")
        return PCFG.fromstring("\n".join(rules))

    def _build_recover_grammar(self) -> PCFG:
        rules = []
        # Example recover primitives
        rules.append("RECOVER -> RELOCALIZE [0.6]")
        rules.append("RECOVER -> BACKTRACK [0.4]")
        rules.append("RELOCALIZE -> 'scan' [1.0]")
        rules.append("BACKTRACK -> 'left' [0.5] 'forward' [0.5]")
        return PCFG.fromstring("\n".join(rules))
    

    def build_pcfg_from_memory(self):
        emap    = self.emap
        current = self.mg.get_current_exp_id()
        exps_by_dist = self.mg.get_exps_organised(current)
        all_exps     = emap.exps

        # 1) Abstract graph & distance priors
        graph = {e.id: [l.target.id for l in e.links] for e in all_exps}
        print("[PCFG DEBUG] graph:", graph)

        id_to_dist  = {}
        total_w = 0.0
        for d in exps_by_dist:
            dist = math.hypot(d['x'], d['y'])
            id_to_dist[d['id']] = dist
            total_w += 1.0/(dist + 1e-5)

        # 2) Top‐level: EXPLORE→NAVPLAN and NAVPLAN→GOTOₜ
        rules = defaultdict(list)
        rules['EXPLORE'].append(('NAVPLAN', 1.0))
        for tgt, dist in id_to_dist.items():
            p = (1.0/(dist+1e-5)) / total_w
            rules['NAVPLAN'].append((f'GOTO_{tgt}', p))
            rules[f'GOTO_{tgt}'].append((f'MOVESEQ_{current}_{tgt}', 1.0))

        # 3) BFS helper on abstract graph
        def find_paths(start, goal, max_depth=15, max_paths=10):
            paths, q = [], deque([[start]])
            while q and len(paths)<max_paths:
                path = q.popleft()
                if path[-1]==goal:
                    paths.append(path)
                elif len(path)<max_depth:
                    for nb in graph.get(path[-1],[]):
                        if nb not in path:
                            q.append(path+[nb])
            return paths

        # 4) Gather all abstract paths and their edges
        hop_edges = set()
        hopseqs   = {}
        for tgt in id_to_dist:
            paths = find_paths(current, tgt)
            hopseqs[tgt] = paths
            for path in paths:
                for u,v in zip(path, path[1:]):
                    hop_edges.add((u,v))
        hop_edges.add((current, current))

        # 4a) MOVESEQ_current→tgt → HOPSEQ_current→tgt
        for tgt in id_to_dist:
            lhs = f'MOVESEQ_{current}_{tgt}'
            rules[lhs].append((f'HOPSEQ_{current}_{tgt}', 1.0))

        # 4b) HOPSEQ_current→tgt → STEP_u_v … but *prefix* (current→current)
        for tgt, paths in hopseqs.items():
            lhs = f'HOPSEQ_{current}_{tgt}'
            if not paths and current!=tgt:
                # no path found
                rules[lhs].append((f'STEP_{current}_{tgt}', 1.0))
                hop_edges.add((current, tgt))
            else:
                w = 1.0/len(paths) if paths else 1.0
                for path in paths:
                    # *** here’s the dummy “first hop” ***
                    hops = [(current, current)] + list(zip(path, path[1:]))
                    seq = [f'STEP_{u}_{v}' for u,v in hops]
                    rhs = " ".join(seq)
                    rules[lhs].append((rhs, w))
                    print(f"[PCFG DEBUG] HOPSEQ {lhs} ← {hops} → {seq}")

        # 5) STEP_u_v → primitives OR fallback
        for (u,v) in hop_edges:
            lhs = f'STEP_{u}_{v}'
            prims = self.get_primitives(u, v)
            if prims:
                rhs = " ".join(f"'{p}'" for p in prims)
                rules[lhs].append((rhs, 1.0))
                print(f"[PCFG DEBUG] STEP_{u}_{v} → prims {prims}")
            else:
                rules[lhs].append((f"'step({u},{v})'", 1.0))
                print(f"[PCFG DEBUG] STEP_{u}_{v} → fallback 'step({u},{v})'")

        # 6) Optional hard‐coded extras
        hard = {
        f'MOVESEQ_{current}_18': [f"step({current},19)","step(19,18)"],
        f'MOVESEQ_{current}_3' : [f"step({current},5)","step(5,4)","step(4,3)"],
        }
        for lhs, steps in hard.items():
            if lhs not in rules:
                rhs = " ".join(f"'{s}'" for s in steps)
                rules[lhs].append((rhs, 1.0))
                print(f"[PCFG DEBUG] hardcoded {lhs} → {steps}")

        # 7) Assemble into PCFG
        pcfg_lines = []
        for lhs, prods in rules.items():
            total = sum(p for _,p in prods)
            for rhs,p in prods:
                pcfg_lines.append(f"{lhs} -> {rhs} [{p/total:.4f}]")

        grammar_src = "\n".join(pcfg_lines)
        print("[PCFG DEBUG] Final grammar:\n" + grammar_src)
        return PCFG.fromstring(grammar_src)
        
    def get_primitives(self, u: int, v: int) -> list[str]:
        """
        Return the best primitive sequence for traversing the edge u→v:

        0) If we’ve already stored a path in the ExperienceLink, return it.
        1) Otherwise, build a fresh A* plan from *our real pose* → node v.
        2) Cache it in the link and return.
        """
        emap = self.emap

        # 0) do we already have a stored primitive path?
        link = self._find_link(u, v)
        if link and link.path_forward:
            print(f"[get_primitives] cached path for {u}->{v}: {link.path_forward}")
            return list(link.path_forward)

        # 1) No cache → build an A* state from our current *real* pose
        real = self.key.agent_current_pose 
        if real is None:
            raise RuntimeError("No last_real_pose available for A* start!")
        sx, sy, sd = real
        start = State(int(round(sx)), int(round(sy)), int(sd))

        # target node’s stored map pose
        gx, gy, gd = emap.get_pose(v)
        goal = State(int(round(gx)), int(round(gy)), int(gd))

        print(f"[get_primitives] no cache {u}->{v}, A* from {start} → {goal}")

        # 2) Run your egocentric‐aware A*:
        egocentric_process = self.key.models_manager.egocentric_process
        prims = self.astar_prims(
            start,
            goal,
            egocentric_process=egocentric_process,
            num_samples=5
        )

        print(f"[get_primitives] A* returned for {u}->{v}: {prims}")

        return prims


    def _find_link(self, u: int, v: int):
        """
        Scan your ExperienceMap for a u→v link; return it or None.
        """
        emap = self.emap
        for exp in emap.exps:
            if exp.id == u:
                for link in exp.links:
                    if link.target.id == v:
                        return link
        return None
    
    def heuristic(self, s: State, g: State) -> float:
        # Manhattan distance + minimal turn cost
        manh = abs(s.x - g.x) + abs(s.y - g.y)
        turn = min((s.d - g.d) % 4, (g.d - s.d) % 4)
        return manh + turn

    def astar_prims(
    self,
    start: State,
    goal:  State,
    egocentric_process,
    num_samples: int = 5
    ) -> list[str]:
        """
        A* in (x,y,dir)-space, but we skip ANY forward candidate
        whose entire prefix fails the egocentric collision check.
        """
        print(f"[A*] start={start}  goal={goal}")
        open_pq = [(self.heuristic(start, goal), 0, start, [])]  # (f, g, state, seq)
        g_score = { start: 0 }
        closed   = set()
        step = 0

        # helper: pack a list of 'forward'/'left'/'right' into a (T,3) float tensor
        def to_onehot_tensor(seq: list[str]) -> torch.Tensor:
            mapping = {'forward': [1,0,0], 'right': [0,1,0], 'left': [0,0,1]}
            return torch.tensor([mapping[a] for a in seq], dtype=torch.float32)

        while open_pq:
            f, g, (x,y,d), seq = heapq.heappop(open_pq)
            print(f"[A*][{step}] POP  state={State(x,y,d)}  g={g}  f={f}  seq={seq!r}")
            step += 1

            if (x,y,d) in closed:
                print("    SKIP (closed)")
                continue
            closed.add((x,y,d))

            # success only if we've reached the right cell and the correct orientation
            if (x, y, d) == (goal.x, goal.y, goal.d):
                print(f"[A*] reached goal at step {step} → seq={seq!r}")
                return seq

            for act in ("left","right","forward"):
                if act == "forward":
                    dx, dy = DIR_VECS[d]
                    nx, ny, nd = x + dx, y + dy, d
                elif act == "left":
                    nx, ny, nd = x, y, (d - 1) % 4
                else:  # right
                    nx, ny, nd = x, y, (d + 1) % 4

                ns = State(nx, ny, nd)
                print(f"    try {act!r} -> next={ns}", end="")

                if (nx,ny,nd) in closed:
                    print("   SKIP (closed)")
                    continue

                new_seq = seq + [act]

                # **egocentric** collision check
                tensor = to_onehot_tensor(new_seq)               # shape (T,3)
                safe   = egocentric_process.egocentric_policy_assessment(
                            tensor, num_samples=num_samples
                        )
                if safe.shape[0] < tensor.shape[0]:
                    print("   SKIP (ego-collision)")
                    continue

                # accept → push into open
                g2 = g + 1
                h2 = self.heuristic(ns, goal)
                f2 = g2 + h2
                old = g_score.get(ns, float("inf"))
                if g2 < old:
                    g_score[ns] = g2
                    print(f"   PUSH g={g2} h={h2} f={f2} seq={new_seq!r}")
                    heapq.heappush(open_pq, (f2, g2, ns, new_seq))
                else:
                    print(f"   SKIP (worse g: {g2} ≥ {old})")

        print("[A*] no path found → returning empty")
        return []
    
    def print_adaptive_pcfg_plans(self,grammar, max_trials=5, initial_depth=4, max_depth_limit=25):
        print("[DEBUG] Attempting to generate plans adaptively...")

        depth = initial_depth
        while depth <= max_depth_limit:
            try:
                plans = list(generate(grammar, depth=depth))
                if plans:
                    print(f"[PLAN DEBUG] {len(plans)} plans found at depth {depth}:")
                    for i, plan in enumerate(plans):
                        print(f"  Plan {i+1}: {' '.join(plan)}")
                    #return plans  # Optionally return for use
                else:
                    print(f"[PLAN DEBUG] No plans at depth {depth}. Increasing depth...")
            except Exception as e:
                print(f"[PLAN DEBUG] Generation failed at depth {depth}: {e}")
            depth += 2  # Increase search depth gradually

        print("[PLAN DEBUG] No valid plans found up to max depth limit.")
        return []
    def replan(self):
        """Rebuild & adapt PCFG, sample new plan via self.key context."""
        base = self.build_pcfg_from_memory()
        ap = AdaptivePCFG(base)
        ap.apply_meta(self)
        grammar = ap.to_nltk_grammar(start=base.start())
        self.key.current_plan = sample_from_pcfg(grammar)
        print(f"REPLAN: new plan length={len(self.key.current_plan)}")

# -----------------------------
# Bayesian Progress Observer
# -----------------------------
class BayesianProgressObserver:
    """
    Belief state over modes: EXPLORE, NAVIGATE, RECOVER.
    """
    def __init__(self):
        self.beliefs = {'EXPLORE':0.33,'NAVIGATE':0.33,'RECOVER':0.34}

    def _likelihood(self, mode: str, fb: dict) -> float:
        # Define simple likelihoods
        if mode == 'EXPLORE':   return 0.9 if fb.get('info_gain',0)<0.5 else 0.1
        if mode == 'NAVIGATE':  return 0.9 if fb.get('progress',0)>0.2 else 0.2
        if mode == 'RECOVER':   return 0.9 if fb.get('agent_lost',False) else 0.1
        return 0.1

    def observe(self, feedback: dict) -> str:
        # Bayes update
        post = {}
        total = 0.0
        for m,prior in self.beliefs.items():
            like = self._likelihood(m, feedback)
            post[m] = prior * like
            total += post[m]
        if total>0:
            for m in post: post[m] /= total
        self.beliefs = post
        # return mode with max posterior
        return max(post, key=post.get)

# -----------------------------
# MetaController
# -----------------------------
class MetaController:
    """
    Supervises EXPLORE, NAVIGATE, RECOVER using PCFG + Bayes.
    """
    def __init__(self, memory_graph):
        self.pcfg_builder = HierarchicalDynamicPCFG(memory_graph)
        self.observer     = BayesianProgressObserver()
        self.current_plan = []

    def step(self, feedback: dict):
        # 1) Evolve PCFG
        self.pcfg_builder.evolve(feedback)
        # 2) Bayes to pick mode
        new_mode = self.observer.observe(feedback)
        if new_mode != self.pcfg_builder.current_mode:
            self.pcfg_builder.set_mode(new_mode)
        # 3) Sample plan
        grammar = self.pcfg_builder.get_grammar()
        self.current_plan = sample_from_pcfg(grammar)
        return self.current_plan


    # ---------------------------------------------------
    # Integration & Helpers (use self.key)
    # ---------------------------------------------------

    def measure_progress(self, prev, curr):
        if prev is None:
            return 0.0
        return min(1.0, np.linalg.norm(np.array(curr)-np.array(prev))/0.5)

    def handle_meta_action(self, action):
        """Dispatch actions on the attached `self.key` agent."""
        if self.key is None:
            raise RuntimeError("MetaController.key not attached")
        if action == 'SWITCH_TO_LOCAL_RECOVERY':
            self.key.start_astar_recovery()
        elif action in ('REPLAN_WITHOUT_CURRENT_NODE','EXECUTE_NEW_PLAN','REPLAN_WITH_NEW_INFORMATION'):
            self.replan()
        elif action == 'SWITCH_TO_EXPLORATION':
            self.key.apply_exploration()

    
# ---------------------------------------------------
class AdaptivePCFG:
    """
    Wraps a PCFG and applies meta-controller feedback.
    """
    def __init__(self, base_grammar: PCFG):
        self.rules = list(base_grammar.productions())
        self.excluded = set()
        self.adjustments = {}

    def exclude_node(self, node):
        self.excluded.add(node)

    def adjust_probability(self, node, factor):
        self.adjustments[node] = factor

    def normalize(self):
        groups = defaultdict(list)
        for r in self.rules:
            groups[r.lhs()].append(r)
        for lhs, rs in groups.items():
            total = sum(r.prob() for r in rs)
            if total > 0:
                for r in rs:
                    r._prob = r.prob()/total

    def apply_meta(self, meta: MetaController):
        # mark excluded nodes
        for n in meta.get_excluded_nodes():
            self.excluded.add(n)
        # store confidence factors
        for n, c in meta.get_node_confidence().items():
            self.adjustments[n] = c
        # adjust rule probs
        for r in self.rules:
            rep = str(r)
            if any(n in rep for n in self.excluded):
                r._prob = 0.0
            else:
                for n, c in self.adjustments.items():
                    if n in rep:
                        r._prob = r.prob()*c
        # normalize
        groups = defaultdict(list)
        for r in self.rules:
            groups[r.lhs()].append(r)
        for rs in groups.values():
            tot = sum(r.prob() for r in rs)
            if tot>0:
                for r in rs: r._prob = r.prob()/tot

    def to_nltk(self, start):
        lines = [f"{r.lhs()} -> {' '.join(map(str,r.rhs()))} [{r.prob():.6f}]"
                 for r in self.rules]
        return PCFG.fromstring("\n".join(lines), start=start)


    


# ---------------------------------------------------
# Agent Integration (MetaController Only)
# ---------------------------------------------------
class Agent:
    def __init__(self, env, models_manager, explorative_behaviour):
        self.env = env
        self.models_manager = models_manager
        self.explorative_behaviour = explorative_behaviour
        self.meta = MetaController()
        self.current_plan = []
        self.plan_idx = 0
        self.last_pose = None

    def agent_step(self, action):
        # Store previous pose for progress measurement
        prev = self.last_pose
        # Execute action
        obs, _, _, _ = self.env.step(action)
        obs = no_vel_no_action(obs)
        pos = obs['pose']
        self.last_pose = pos

        # Update environment models
        self.models_manager.digest(obs)
        info = self.models_manager.get_best_place_hypothesis()['info_gain']
        node = self.agent_situate_memory()

        # Measure progress
        prog = self.measure_progress(prev, pos)

        # Update MetaController
        meta_action = self.meta.update(
            plan_prog=prog,
            agent_lost=self.models_manager.agent_lost(),
            info_gain=info,
            current_node=node,
            current_pose=pos
        )

        # Handle any meta action
        if meta_action != 'CONTINUE':
            self.handle_meta_action(meta_action)

        return self.models_manager.agent_lost(), obs

    def measure_progress(self, prev, curr):
        """Compute normalized progress based on movement."""
        if prev is None:
            return 0.0
        dist = np.linalg.norm(np.array(curr) - np.array(prev))
        return min(1.0, dist / 0.5)

    def handle_meta_action(self, action):
        if action == 'SWITCH_TO_LOCAL_RECOVERY':
            self.start_astar_recovery()
        elif action.startswith('REPLAN'):
            self.replan()
        elif action == 'SWITCH_TO_EXPLORATION':
            self.apply_exploration()
        # 'CONTINUE' does nothing

    def apply_exploration(self):
        """
        Exploration stub updated to integrate with MetaController.
        Future extension: adapt exploration based on meta.current_state.
        """
        print('Exploration mode triggered by MetaController')
        return [], 0

class NavigationSystem:
    def __init__(self, memory_graph, get_primitives_func, astar_prims_func, get_current_pose_func):
        self.memory_graph = memory_graph
        self.get_primitives = get_primitives_func
        self.astar_prims = astar_prims_func
        self.get_current_pose = get_current_pose_func
        
        # Navigation state
        self.current_full_plan = None
        self.current_plan_tokens = []
        self.current_token_index = 0
        self.target_node_id = None
        self.navigation_flags = {}
        
    def get_available_nodes_with_confidence(self, min_confidence=0.5):
        """Get nodes that are viable for navigation"""
        viable_nodes = []
        emap = self.memory_graph.experience_map
        
        for exp in emap.exps:
            # Check if node has sufficient confidence
            if hasattr(exp, 'confidence') and exp.confidence >= min_confidence:
                viable_nodes.append({
                    'id': exp.id,
                    'confidence': exp.confidence,
                    'pose': (exp.x_m, exp.y_m, getattr(exp, 'dir', 0))
                })
        
        return viable_nodes
    
    def check_navigation_feasibility(self):
        """Early check for navigation feasibility"""
        viable_nodes = self.get_available_nodes_with_confidence()
        
        # Check edge cases
        if len(viable_nodes) <= 2:
            self.navigation_flags['insufficient_nodes'] = True
            return False
            
        if len(viable_nodes) == 0:
            self.navigation_flags['no_viable_nodes'] = True
            return False
            
        # Check if we have a current plan that's still valid
        if self.current_full_plan and self.target_node_id:
            target_still_viable = any(n['id'] == self.target_node_id for n in viable_nodes)
            if not target_still_viable:
                self.navigation_flags['target_became_unviable'] = True
                return False
                
        self.navigation_flags.clear()  # Clear flags if everything is okay
        return True
    
    def generate_plan_with_pcfg(self, grammar):
        """Generate a new plan using PCFG sampling"""
        # Sample from PCFG (you already have this logic)
        # This would use your existing build_pcfg_from_memory logic
        sampled_plan = self._sample_from_pcfg(grammar)
        
        # Parse the plan to extract target node
        self.target_node_id = self._extract_target_from_plan(sampled_plan)
        self.current_full_plan = sampled_plan
        
        # Tokenize the plan
        self.current_plan_tokens = self._tokenize_plan(sampled_plan)
        self.current_token_index = 0
        
        return sampled_plan
    
    def _tokenize_plan(self, full_plan):
        """Break full plan into individual step tokens"""
        # Example: "STEP_9_9 STEP_9_8 STEP_8_5 STEP_5_4 STEP_4_6"
        # Returns: ["STEP_9_9", "STEP_9_8", "STEP_8_5", "STEP_5_4", "STEP_4_6"]
        if isinstance(full_plan, str):
            tokens = full_plan.strip().split()
            return [token for token in tokens if token.startswith('STEP_')]
        return []
    
    def get_next_navigation_step(self):
        """Get the next step in the current plan"""
        if (self.current_plan_tokens and 
            self.current_token_index < len(self.current_plan_tokens)):
            
            current_token = self.current_plan_tokens[self.current_token_index]
            self.current_token_index += 1
            
            # Parse token to get source and target nodes
            # "STEP_9_8" -> source=9, target=8
            parts = current_token.split('_')
            if len(parts) >= 3:
                source_node = int(parts[1])
                target_node = int(parts[2])
                return source_node, target_node
                
        return None, None
    
    def universal_navigation_function(self, current_pose, objective, submode="plan_following"):
        """
        Universal function that handles all navigation submodes
        
        Args:
            current_pose: Current agent position
            objective: Target (can be pose or node_id)
            submode: "plan_following", "evade", or "replan"
            
        Returns:
            (primitives, n_actions)
        """
        if submode == "plan_following":
            return self._handle_plan_following(current_pose, objective)
            
        elif submode == "evade":
            return self._handle_evade(current_pose, objective)
            
        elif submode == "replan":
            return self._handle_replan(current_pose, objective)
            
        else:
            raise ValueError(f"Unknown submode: {submode}")
    
    def _handle_plan_following(self, current_pose, objective):
        """Handle normal plan following"""
        if isinstance(objective, int):  # objective is a node_id
            # Get primitives using your existing link-based system
            source_node, target_node = self.get_next_navigation_step()
            if source_node is not None and target_node is not None:
                primitives = self.get_primitives(source_node, target_node)
                return primitives, len(primitives)
        else:  # objective is a pose
            # Use A* for pose-to-pose navigation
            start_state = self._pose_to_state(current_pose)
            goal_state = self._pose_to_state(objective)
            primitives = self.astar_prims(start_state, goal_state, 
                                        egocentric_process=None, num_samples=5)
            return primitives, len(primitives)
            
        return [], 0
    
    def _handle_evade(self, current_pose, objective):
        """Handle obstacle evasion"""
        # Use A* to find path around obstacles
        start_state = self._pose_to_state(current_pose)
        
        if isinstance(objective, int):
            # Get pose of target node
            emap = self.memory_graph.experience_map
            target_pose = emap.get_pose(objective)
            goal_state = self._pose_to_state(target_pose)
        else:
            goal_state = self._pose_to_state(objective)
            
        # A* should naturally avoid obstacles
        primitives = self.astar_prims(start_state, goal_state,
                                    egocentric_process=None, num_samples=5)
        return primitives, len(primitives)
    
    def _handle_replan(self, current_pose, objective):
        """Handle replanning when current objective becomes unreachable"""
        if isinstance(objective, int):
            # Crash confidence of the problematic node
            self._crash_node_confidence(objective)
            
        # Generate new plan with updated confidence values
        grammar = self._rebuild_pcfg_with_updated_confidence()
        new_plan = self.generate_plan_with_pcfg(grammar)
        
        # Execute first step of new plan
        return self._handle_plan_following(current_pose, self.target_node_id)
    
    def _crash_node_confidence(self, node_id):
        """Reduce confidence of a problematic node"""
        emap = self.memory_graph.experience_map
        for exp in emap.exps:
            if exp.id == node_id:
                if hasattr(exp, 'confidence'):
                    exp.confidence *= 0.5  # Reduce confidence by half
                else:
                    exp.confidence = 0.1  # Set low confidence
                break
    
    def get_observer_data(self):
        """Provide data for the navigation observer"""
        current_pose = self.get_current_pose()
        
        observer_data = {
            'full_plan': self.current_full_plan,
            'plan_tokens': self.current_plan_tokens,
            'current_token_index': self.current_token_index,
            'target_node_id': self.target_node_id,
            'current_pose': current_pose,
            'navigation_flags': self.navigation_flags.copy(),
            'plan_progress': self.current_token_index / max(len(self.current_plan_tokens), 1)
        }
        
        return observer_data
    
    def _pose_to_state(self, pose):
        """Convert pose tuple to State object"""
        # Assuming State class exists as in your A* implementation
        x, y, direction = pose
        return State(int(round(x)), int(round(y)), int(direction))
    
    def _sample_from_pcfg(self, grammar):
        """Sample a plan from the PCFG"""
        # Your existing PCFG sampling logic here
        # This is a placeholder - use your actual implementation
        pass
    
    def _extract_target_from_plan(self, plan):
        """Extract target node ID from sampled plan"""
        # Parse the plan string to find the ultimate target
        # This depends on your PCFG structure
        pass
    
    def _rebuild_pcfg_with_updated_confidence(self):
        """Rebuild PCFG with updated node confidence values"""
        # Your existing build_pcfg_from_memory but considering confidence
        pass


# Updated apply_exploration method
def apply_exploration(self) -> tuple[list, int]:
    """
    Updated exploration method with navigation system integration
    """
    # Get current HMM state
    mode, submode = (
        getattr(self, "prev_mode", None) or "EXPLORE",
        getattr(self, "prev_submode", None) or "ego_allo",
    )
    print(f"[HMM] mode={mode}  submode={submode}")

    # Handle other modes (RECOVER, TASK_SOLVING, EXPLORE) as before...
    # [Previous code remains the same]

    # NAVIGATE mode with new implementation
    if mode == "NAVIGATE":
        # Initialize navigation system
        nav_system = NavigationSystem(
            self.models_manager.memory_graph,
            self.get_primitives,
            self.astar_prims,
            lambda: self.agent_current_pose
        )
        
        # Early feasibility check
        if not nav_system.check_navigation_feasibility():
            print("Navigation not feasible - returning flag for HMM")
            # Set flags for observer to detect and trigger mode change
            self.navigation_flags = nav_system.navigation_flags
            return [], []  # This should trigger mode change via observer
        
        # Check if we need to generate a new plan
        if nav_system.current_full_plan is None:
            grammar = self.build_pcfg_from_memory()
            nav_system.generate_plan_with_pcfg(grammar)
        
        # Handle submodes
        current_pose = self.agent_current_pose
        
        if submode == "plan_following":
            print("Navigate ➜ follow current plan")
            primitives, n_actions = nav_system.universal_navigation_function(
                current_pose, nav_system.target_node_id, "plan_following"
            )
            return primitives, n_actions

        elif submode == "evade":
            print("Evade obstacle")
            primitives, n_actions = nav_system.universal_navigation_function(
                current_pose, nav_system.target_node_id, "evade"
            )
            return primitives, n_actions

        elif submode == "replan":
            print("Objective unreachable lets replan")
            primitives, n_actions = nav_system.universal_navigation_function(
                current_pose, nav_system.target_node_id, "replan"
            )
            return primitives, n_actions

    # [Rest of the method remains the same for other modes]
    return [], []


# Observer integration (to be added to your step function)
def plan_progress_placeholder(self):
    """
    Enhanced plan progress observer for HMM
    """
    if hasattr(self, 'nav_system'):
        observer_data = self.nav_system.get_observer_data()
        
        # Calculate various metrics for HMM
        progress_metrics = {
            'plan_completion': observer_data['plan_progress'],
            'stuck_flag': 'insufficient_nodes' in observer_data['navigation_flags'],
            'replan_needed': 'target_became_unviable' in observer_data['navigation_flags'],
            'no_path_flag': 'no_viable_nodes' in observer_data['navigation_flags']
        }
        
        return progress_metrics
    
    return {'plan_completion': 0.0, 'stuck_flag': False, 'replan_needed': False, 'no_path_flag': False}



```

`hierarchical-nav/dommel_library/datasets/__init__.py`:

```py
from dommel_library.datasets.dataset_factory import dataset_factory

from dommel_library.datasets.dataset import (
    Dataset,
    SequenceDataset,
    ConcatDataset
)

from dommel_library.datasets.memory_pool import MemoryPool
from dommel_library.datasets.file_pool import FilePool


__all__ = [
    "dataset_factory",
    "Dataset",
    "SequenceDataset",
    "ConcatDataset",
    "MemoryPool",
    "FilePool",
]

```

`hierarchical-nav/dommel_library/datasets/dataset.py`:

```py
import numpy as np
import torch.utils.data as data
from functools import wraps

from dommel_library.datastructs import TensorDict


class Dataset(data.Dataset):
    """An abstract class representing a dommel Dataset

    Dommel datasets differ with PyTorch datasets in that they return
    TensorDicts instead of tensor tuples.

    In addition to inheriting from torch's Dataset, it also adds methods to
    sample a random batch of data. Sampling is implemented by default using
    a DataLoader, but you can also use your own DataLoader.
    """

    def __init__(self, transform=None, num_workers=0, **kwargs):
        self._num_workers = num_workers
        self._batch_size = 0
        self._dataloader = None
        self._itr = None
        self._transform = transform

    def __getitem__(self, index):
        if index >= len(self):
            raise IndexError(f"Index out of bounds: {index}")

        item = self._get_item(index)

        if self._transform:
            item = self._transform(item)

        return item

    def sample(self, batch_size=1):
        """returns a dictionary of tensors
        (shape = (batch_size, sequence_length))
        """
        if batch_size != self._batch_size:
            # reinstantiate the DataLoader if the batch size changes during
            # calls (should we better fix batch_size at construction?)
            self._batch_size = batch_size
            self._create_dataloader()
        try:
            x = next(self._itr)
        except StopIteration:
            self._create_dataloader()
            x = next(self._itr)
        if x[list(x.keys())[0]].shape[0] != self._batch_size:
            # recreate the dataloader and resample!
            self._create_dataloader()
            x = next(self._itr)
        return TensorDict(x)

    def _create_dataloader(self):
        self._dataloader = data.DataLoader(
            self,
            batch_size=self._batch_size,
            shuffle=True,
            num_workers=self._num_workers,
            drop_last=True,
        )
        self._itr = iter(self._dataloader)

    def _get_item(self, idx):
        """get item at index idx"""
        return NotImplementedError

    def __len__(self):
        """get the length of the dataset"""
        return NotImplementedError


class SequenceDataset(Dataset):
    """SequenceDataset is a dommel dataset that handles sequences of data

    Subsequences of sequence_length and stride can be sampled, and also
    sequence indices can be shuffled.

    If cutoff is False, the last subsequence with length < sequence_length
    is also kept.
    """

    def __init__(
        self,
        sequence_length=-1,
        sequence_stride=1,
        shuffle=False,
        cutoff=True,
        transform=None,
        num_workers=0,
        **kwargs,
    ):
        Dataset.__init__(self, transform, num_workers, **kwargs)
        self._sequence_length = sequence_length
        self._sequence_stride = sequence_stride
        self._cutoff = cutoff
        if self._sequence_stride == 0:
            raise ValueError("Invalid sequence_stride: 0")

        self._shuffle = shuffle
        self._update_table()

    def _load_sequences(self):
        """return a list with a key for each raw sequence
        and a list with their corresponding lengths"""
        return NotImplementedError

    def _load_sequence(self, key, indices):
        """return a tensordict with a key and indices"""
        return NotImplementedError

    def _update_table(self):
        self._sequences, sequence_lengths = self._load_sequences()
        if self._sequence_length > 0:
            extra = 0 if self._cutoff else 1
            total_indices = sum(
                (
                    max(i - self._sequence_length + extra + 1, 0)
                    for i in sequence_lengths
                )
            )
            # generate the table
            table = np.zeros(
                (total_indices, 1 + self._sequence_length), dtype=np.uint32
            )
            i = 0
            if not self._shuffle:
                for j in range(len(sequence_lengths)):
                    k = 0
                    while k + self._sequence_length <= sequence_lengths[j]:
                        indices = np.arange(k, k + self._sequence_length)
                        table[i] = np.array(
                            [j, *indices],
                            dtype=np.uint32,
                        )
                        i += 1
                        if self._sequence_stride > 0:
                            k += self._sequence_stride
                        else:
                            o = self._sequence_length + self._sequence_stride
                            k += o
                    if not self._cutoff:
                        indices = np.arange(k, sequence_lengths[j])
                        fill = np.empty(
                            (k + self._sequence_length - sequence_lengths[j]),
                            np.uint32,
                        )
                        fill[:] = np.iinfo(np.uint32).max
                        table[i] = np.array(
                            [j, *indices, *fill],
                            dtype=np.uint32,
                        )
                        i += 1
                self._lookup_table = table[0:i]
            else:
                for j in range(len(sequence_lengths)):
                    r = sequence_lengths[j] - self._sequence_length
                    for k in range(r):
                        indices = np.random.choice(
                            np.arange(0, sequence_lengths[j]),
                            self._sequence_length,
                            replace=False,
                        )
                        table[i] = np.array([j, *indices], dtype=np.uint32)
                        i += 1
                self._lookup_table = table
        else:
            table = {}
            for i in range(len(sequence_lengths)):
                length = sequence_lengths[i]
                table[i] = np.array([i] + list(np.arange(0, length)))
            self._lookup_table = table

    def __len__(self):
        return len(self._lookup_table)

    def _get_item(self, idx):
        lookup = self._lookup_table[idx]
        key = self._sequences[lookup[0]]
        indices = lookup[1:]
        indices = indices[np.where(indices != np.iinfo(np.uint32).max)]
        return self._load_sequence(key, indices)


class ConcatDataset(data.ConcatDataset, Dataset):
    """Wraps Pytorch ConcatDataset as a dommel dataset,
    providing the sample() method for convenience
    """

    def __init__(self, datasets):
        Dataset.__init__(self, None, 0)
        data.ConcatDataset.__init__(self, datasets)


def retry(func):
    """
    A Decorator to retry a function for a certain amount of attempts
    """

    @wraps(func)
    def wrapper(*args, **kwargs):
        attempts = 0
        max_attempts = 100
        last_error = ""
        while attempts < max_attempts:
            try:
                return func(*args, **kwargs)
            except (OSError, PermissionError) as e:
                attempts += 1
                last_error = e
        raise OSError(f"Retry failed: {last_error}")

    return wrapper

```

`hierarchical-nav/dommel_library/datasets/dataset_factory.py`:

```py
import re
import requests
import importlib
from pathlib import Path
from shutil import copyfile, rmtree
from zipfile import ZipFile

import dommel_library.datasets
from dommel_library.datasets.dataset import retry, ConcatDataset

import logging

logger = logging.getLogger(__name__)


def dataset_factory(
    location=None,
    destination=None,
    type="FilePool",
    sequence_length=-1,
    keys=None,
    transform=None,
    clean=False,
    keep_zip=False,
    backend=None,
    **kwargs,
):
    """
    Factory method for the creation of dommel datasets.
    :param location: Url or directory to load from. If None, will initialize
    an empty DictPool.
    :param type: String representing the dataset type to construct
    :param destination: If the dataset needs to copied to another location,
    or downloaded to a certain destination, provide target path here
    :param keys: List of sensor keys to load. Defaults to all keys.
    :param sequence_length: Length of subsequences.
    :param transform: List of data transforms.
    :param clean: Force re-download.
    :param keep_zip: Keep the .zip file after extraction.
    :param backend: parameter to register the backend for the dataset class
    :param kwargs: The parameters for the construction of the pools.
    """
    if isinstance(location, list):
        datasets = []
        for loc in location:
            d = dataset_factory(
                loc,
                destination,
                type,
                sequence_length,
                keys,
                transform,
                clean,
                keep_zip,
                **kwargs,
            )
            datasets.append(d)
        return ConcatDataset(datasets)

    if location:
        if "http" in location[:4]:
            location = download(location, destination, clean)

        if ".zip" in location[-4:]:
            location = unzip(location, destination, keep_zip)

        if destination and not location.startswith(destination):
            location = copy(location, destination)

        kwargs["directory"] = location

    kwargs["sequence_length"] = sequence_length
    kwargs["keys"] = keys

    if not backend:
        backend_module = dommel_library.datasets  # noqa: F
    else:
        backend_module = importlib.import_module(backend)  # noqa: F

    module = f"backend_module.{type}(**{repr(kwargs)}," f"transform=transform)"
    return eval(module)


@retry
def copy(file_location, destination=None):
    """
    Initializes the file transform
    :param file_location: Original location of the dataset.
    :param destination: Destination of the dataset, defaults to
    /tmp/data.
    """
    file_location = Path(file_location)
    if destination:
        destination = Path(destination)
    else:
        destination = Path("/tmp/data")
    # make sure destination exists
    destination.mkdir(parents=True)
    for file in file_location.iterdir():
        dest = destination / file.parts[-1]
        copyfile(file, dest)
    return str(destination)


def unzip(zip_location, destination=None, keep_zip=False):
    """
    Initializes the unzip transform file transform
    :param zip_location: Location of the zip file
    :param destination: Unzip destination. Defaults to /tmp/data
    :param keep_zip: keep the .zip file after extraction
    """
    logger.info("Unzipping dataset %s", zip_location)
    zip_location = Path(zip_location)

    if destination:
        destination = Path(destination)
    else:
        destination = Path("/tmp/data")
    # make sure destination exists
    destination.mkdir(parents=True, exist_ok=True)
    target_dir = destination / zip_location.name[:-4]

    with ZipFile(zip_location, "r") as zip_obj:
        needs_extraction = False
        for name in zip_obj.namelist():
            if not (target_dir / name).exists():
                needs_extraction = True

        if needs_extraction:
            zip_obj.extractall(target_dir)

    dataloc = target_dir

    # if the .zip contains a root dir with the same name
    # as the .zip name, we'll have it twice in the path
    # in that case, just move the target one up
    subdir = dataloc / zip_location.name[:-4]
    if subdir.exists():
        p = subdir.absolute()
        parent_dir = subdir.parents[1]
        tmp_p = parent_dir / (p.name + "_tmp")
        p.rename(tmp_p)
        tmp_p.rename(parent_dir / p.name)

    logger.info("Unzipped to %s", dataloc)

    if not keep_zip:
        zip_location.unlink()

    return str(dataloc)


def download(remote_location, destination=None, clean=False):
    """
    Initializes the file transform
    :param remote_location: URL to the dataset. Must end with (/download)
    to be valid.
    :param destination: Destination of the download operation. Defaults to
    /tmp/data.
    :param clean: Force redownload
    """
    if "http" not in remote_location[:4]:
        raise AssertionError("Invalid url")
    if destination:
        destination = Path(destination)
    else:
        destination = Path("/tmp/data")
    destination.mkdir(parents=True, exist_ok=True)

    with requests.get(remote_location, stream=True) as r:
        content_type = r.headers["content-type"]
        content_disposition = r.headers["content-disposition"]
        file_name = None
        if content_disposition is not None:
            file_name = get_filename_from_cd(content_disposition)

        if file_name is None:
            extension = content_type.split("/")[-1]
            file_name = f"file.{extension}"

        # if the target unzip directory exists, return this
        target_dir = destination / file_name[:-4]
        if target_dir.exists():
            if clean:
                rmtree(target_dir, ignore_errors=True)
            else:
                return str(target_dir)

        # if the .zip exists, return this
        destination /= file_name
        if destination.exists():
            if clean:
                destination.unlink()
            else:
                return str(destination)

        logger.info("Downloading dataset %s", remote_location)
        r.raise_for_status()
        with open(destination, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:  # filter out keep-alive new chunks
                    f.write(chunk)
        logger.info("Download saved at %s", destination)

    return str(destination)


def get_filename_from_cd(cd):
    """
    Get filename from content-disposition
    """
    if not cd:
        return None
    fname = re.findall("filename=(.+)", cd)
    if len(fname) == 0:
        return None

    fname = fname[0]
    # strip quotes if exist
    if '"' in fname:
        fname = fname.strip('"')

    return fname

```

`hierarchical-nav/dommel_library/datasets/file_pool.py`:

```py
import h5py
import torch
from pathlib import Path

import numpy as np

from dommel_library.datasets.dataset import SequenceDataset, retry
from dommel_library.datastructs import TensorDict

import logging

logger = logging.getLogger(__name__)


class FilePool(SequenceDataset):
    """ FilePool stores sequences directly in h5 files in a directory
    Collects all h5 files in a given directory and uses these to retrieve data
    """

    def __init__(
        self,
        directory=None,
        transform=None,
        keys=None,
        num_workers=0,
        compression=9,
        interval=None,
        sequence_length=-1,
        sequence_stride=1,
        shuffle=False,
        cutoff=True,
        **kwargs,
    ):
        if not directory:
            directory = "/tmp"
        self._dir = Path(directory)
        self._keys = keys
        self._compression = compression
        if not self._dir.exists():
            self._dir.mkdir(parents=True)

        # list h5 files in directory
        self._files = list(sorted(self._dir.glob("*.h5")))

        # Train val test split
        if interval is None:
            interval = [0, 1.0]

        ind_min = int(interval[0] * len(self._files))
        ind_max = int(interval[1] * len(self._files))
        self._files = self._files[ind_min:ind_max]

        SequenceDataset.__init__(self, sequence_length, sequence_stride,
                                 shuffle, cutoff, transform, num_workers)

    def _load_sequences(self):
        lengths = [FilePool._get_length(file) for file in self._files]
        return self._files, lengths

    def _load_sequence(self, key, indices):
        return self._read(key, indices)

    @retry
    def _read(self, path, indices):
        sorted_indices = np.argsort(indices)
        inverse_sorted_indices = np.argsort(sorted_indices)

        with h5py.File(path, "r") as f:
            if not self._keys:
                self._keys = [k for k in f.keys() if len(f[k].shape) > 0]

            d = {}
            for key in self._keys:
                if self._sequence_length > 0:
                    # indices must be in ascending order for h5py
                    value = f[key][indices[sorted_indices]]
                    if self._shuffle:
                        # undo the sort needed for getting files
                        value = value[inverse_sorted_indices]
                else:
                    if key not in f.keys():
                        raise Exception("Key " + key + " not in " + str(path))
                    value = f[key][:]

                if value.dtype == "uint16":
                    value = value.astype(np.int32)
                d[key] = torch.as_tensor(value)
            return TensorDict(d)

    def push(self, sequence):
        for key, value in sequence.items():
            value.detach()
            # we squeeze the batch dimension here
            # so it can be added automatically when sampling
            if not self._keys or key in self._keys:
                sequence[key] = value.squeeze(0)

        # store to pickle
        path = self._dir / f"{len(self._files) + 1}.h5"
        # Avoid overwriting an existing file
        index = 2
        while path.exists():
            path = self._dir / f"{len(self._files) + index}.h5"
            index += 1
        self._push(path, sequence)
        self._files.append(path)
        self._update_table()

    @staticmethod
    @retry
    def _get_length(file):
        with h5py.File(file, "r") as fp:
            i = 0
            key = list(fp.keys())[i]
            while len(fp[key].shape) == 0:
                i += 1
                key = list(fp.keys())[i]
            return fp[key].shape[0]

    @retry
    def _push(self, file, sequence):
        with h5py.File(file, "w") as f:
            for key, value in sequence.items():
                f.create_dataset(
                    key,
                    data=value,
                    compression="gzip",
                    compression_opts=self._compression,
                )

```

`hierarchical-nav/dommel_library/datasets/memory_pool.py`:

```py
import h5py
import os
import torch
import numpy as np
from collections import deque

from dommel_library.datasets.dataset import SequenceDataset, retry
from dommel_library.datastructs import TensorDict

import logging

logger = logging.getLogger(__name__)


class MemoryPool(SequenceDataset):
    """ MemoryPool stores sequences in a deque buffer in memory
    max_size specifies the maximum size of the buffer.
    The pool can be read / written from / to a directory of h5 files.
    """

    def __init__(
        self,
        max_size=None,
        transform=None,
        keys=None,
        num_workers=0,
        device="cpu",
        directory=None,
        interval=None,
        sequence_length=-1,
        sequence_stride=1,
        shuffle=False,
        cutoff=True,
        **kwargs,
    ):
        self._keys = keys
        self._buffer = deque(maxlen=max_size)
        self._device = torch.device(device)
        SequenceDataset.__init__(self, sequence_length, sequence_stride,
                                 shuffle, cutoff, transform, num_workers)
        if directory:
            self.load(directory, interval)

    def _load_sequences(self):
        keys = list(range(len(self._buffer)))
        lengths = [self._buffer[i].shape[0] for i in range(len(self._buffer))]
        return keys, lengths

    def _load_sequence(self, key, indices):
        raw_sequence = self._buffer[key]
        if indices.dtype.kind == "u":
            indices = indices.astype(np.int64)
        indices = torch.as_tensor(indices, dtype=torch.long)
        return raw_sequence[indices]

    def push(self, sequence):
        s = TensorDict({})
        for key, value in sequence.items():
            if self._keys is None or key in self._keys:
                value = value.detach()
                value = value.to(self._device)
                # we squeeze the batch dimension here (if present)
                # so it can be added automatically when sampling
                if value.shape[0] == 1:
                    value = value.squeeze(0)
                s[key] = value

        self._buffer.append(s)
        self._update_table()

    def dump(self, path, compression=9):
        if not os.path.exists(path):
            os.makedirs(path)

        for i in range(len(self._buffer)):
            file_name = str(i) + ".h5"
            out_path = os.path.join(path, file_name)
            sequence = self._buffer[i]
            with h5py.File(out_path, "w") as f:
                for key, value in sequence.items():
                    f.create_dataset(
                        key,
                        data=value,
                        compression="gzip",
                        compression_opts=compression,
                    )

    def load(self, path, interval=None):
        file_list = os.listdir(path)
        filtered_list = [item for item in file_list if ".h5" in item]

        # Train, validation split functionality
        if interval is None:
            interval = [0, 1.0]
        b = int(interval[0] * len(filtered_list))
        e = int(interval[1] * len(filtered_list))
        filtered_list = filtered_list[b:e]

        for name in filtered_list:
            file_path = os.path.join(path, name)
            d = self._load(file_path)
            self._buffer.append(TensorDict(d))

        self._update_table()
        return self

    @retry
    def _load(self, path):
        with h5py.File(path, "r") as f:
            # include all keys when no keys specified
            if not self._keys:
                keys = f.keys()
            else:
                keys = self._keys
            return {
                key: self._create_entry(value[:])
                for key, value in f.items()
                if key in keys
            }

    def _create_entry(self, value):
        value = value[:]
        if value.dtype == "uint16":
            value = value.astype(np.int32)
        return torch.as_tensor(value).to(self._device)

    def wrap(self, pool):
        for i in range(len(pool)):
            sequence = pool[i].unsqueeze(0)
            if self._keys is not None:
                filtered = {}
                for k, v in sequence.items():
                    if k in self._keys:
                        filtered[k] = v.detach().clone()
                sequence = TensorDict(filtered)
            self.push(sequence)
        return self

```

`hierarchical-nav/dommel_library/datasets/transforms.py`:

```py
import torch
from torch.nn.functional import interpolate

from dommel_library.datastructs import TensorDict


class RandomObservation:
    """Select random single observations from a sequence."""

    def __call__(self, sequence):
        observations = sequence["observation"]
        i = torch.randint(observations.shape[0], (1,))
        return observations[i, :].squeeze(0), i


class RandomSubsequence:
    """Select random subsequence from a sequence."""

    def __init__(self, length):
        self.length = length

    def __call__(self, sequence):
        subsequence = {}

        # assume all sequences have an action item to get the size
        actions = sequence["action"]
        size = actions.shape[0]

        if size - self.length <= 0:
            print(
                "Warning: sequence is not long enough"
                "to sample a subsequence..."
            )
            return sequence

        i = int(torch.randint(size - self.length, (1,)).item())
        for key, value in sequence.items():
            # unsqueeze a dimension in case we have a vector
            if len(value.shape) == 1:
                value = value.unsqueeze(-1)
            subsequence[key] = value[i: i + self.length, ...]

        return TensorDict(subsequence)


class Subsample:
    """ Subsample from sequence """

    def __init__(self, subsample_step):
        self.subsample_step = subsample_step

    def __call__(self, sequence):
        subsequence = {}
        for key, value in sequence.items():
            if len(value.shape) == 1:
                value = value.unsqueeze(-1)
            subsequence[key] = value[:: self.subsample_step, ...]
        return TensorDict(subsequence)


class Crop:
    """ Crop images in a sequence"""

    def __init__(self, start_point, end_point, keys):
        self.startx = start_point[0]
        self.starty = start_point[1]
        self.endx = end_point[0]
        self.endy = end_point[1]
        self.keys = keys

    def __call__(self, sequence):
        for key in self.keys:
            if key in sequence.keys():
                cropped_sequence = sequence[key][
                    :,  # noqa: W503,W504
                    :,  # noqa: W503,W504
                    self.startx: self.endx,  # noqa: W503,W504
                    self.starty: self.endy,  # noqa: W503,W504
                    ...,  # noqa: W503,W504
                ]
                sequence[key] = cropped_sequence
        return sequence


class ChannelFirst:
    """ switch tensors from channel last to channel first """

    def __init__(self, keys):
        self.keys = keys

    def __call__(self, sequence):
        for key in self.keys:
            if key in sequence.keys():
                # first dimension is sequence
                switched_sequence = sequence[key].permute(0, 3, 1, 2)
                sequence[key] = switched_sequence
        return sequence


class ToFloat:
    def __init__(self, keys=None):
        self.keys = keys
        pass

    def __call__(self, sequence):
        if not self.keys:
            keys = sequence.keys()
        else:
            keys = self.keys

        for key in keys:
            if key in sequence.keys():
                sequence[key] = sequence[key].float()
        return sequence


class Pad:
    """ adds padding to the data, changes the shape"""

    def __init__(self, pad_amount, keys):
        self.pad_amount = pad_amount
        self.keys = keys
        self.padder = torch.nn.ReplicationPad2d(pad_amount)

    def __call__(self, sequence):
        for key in self.keys:
            if key in sequence.keys():
                sequence[key] = self.padder(sequence[key]).detach()
        return sequence


class RescaleShift:
    """  shifts and rescales the data, but does not change the datashape
    """

    def __init__(self, scale, bias, keys):
        self.bias = bias
        self.scale = scale
        self.keys = keys

    def __call__(self, sequence):
        for key in self.keys:
            if key in sequence.keys():
                sequence[key] = sequence[key] * self.scale + self.bias
        return sequence


class Resize:
    """  changes the data dimensions
    """

    def __init__(self, size, keys, mode="nearest"):
        self.keys = keys
        self.size = size
        self.mode = mode

    def __call__(self, sequence):
        for key in self.keys:
            if key in sequence.keys():
                sequence[key] = sequence[key].unsqueeze(0)
                sequence[key] = interpolate(
                    sequence[key], size=self.size, mode=self.mode
                )
                sequence[key] = sequence[key].squeeze(0)
        return sequence


class Squeeze:
    """  squeeze a dimension
    """

    def __init__(self, dim=0, keys=None):
        self.dim = dim
        self.keys = keys

    def __call__(self, sequence):
        if self.keys is None:
            keys = sequence.keys()
        else:
            keys = self.keys

        for key in keys:
            if key in sequence.keys():
                sequence[key] = sequence[key].squeeze(self.dim)
        return sequence


class Unsqueeze:
    """  unsqueeze a dimension
    """

    def __init__(self, dim=0, keys=None):
        self.dim = dim
        self.keys = keys

    def __call__(self, sequence):
        if self.keys is None:
            keys = sequence.keys()
        else:
            keys = self.keys

        for key in keys:
            if key in sequence.keys():
                sequence[key] = sequence[key].unsqueeze(self.dim)
        return sequence

```

`hierarchical-nav/dommel_library/datasets/utils.py`:

```py
import torch
import h5py as h5


def store_h5(data_dictionary, filename, compression='lzf'):
    """
    :param data_dictionary: dictionary that contains the data you want to store
    :param filename: Location to store data as h5 (<path/to/file.h5>)
    :param compression: type of compression to use (default='lzf')
    """
    with h5.File(filename, "w") as f:
        for key in data_dictionary.keys():
            f.create_dataset(
                key, data=data_dictionary[key], compression=compression
            )


def load_h5(filename):
    """
    :param filename: Location to load data from (<path/to/file.h5>)
    """
    data = {}
    with h5.File(filename, "r") as f:
        for key in f.keys():
            data[key] = torch.tensor(f[key][:], dtype=torch.float32)
    return data

```

`hierarchical-nav/dommel_library/datastructs/__init__.py`:

```py
from dommel_library.datastructs.dict import Dict  # noqa: F401
from dommel_library.datastructs.tensor_dict import TensorDict  # noqa: F401
from dommel_library.datastructs.tensor_dict import cat  # noqa: F401
from dommel_library.datastructs.tensor_dict import stack  # noqa: F401

```

`hierarchical-nav/dommel_library/datastructs/dict.py`:

```py
class Dict(dict):
    """ Custom dictionary implementation
    Adds:
    dot.notation access to dictionary attributes
    update of nested dictionaries
    """

    def __getattr__(self, attr):
        try:
            return self.__getitem__(attr)
        except KeyError:
            raise AttributeError(attr)

    def __getitem__(self, key):
        try:
            val = dict.__getitem__(self, key)
            if type(val) is dict:
                val = Dict(val)
                self[key] = val
        except Exception:
            return None
        return val

    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

    def __getstate__(self):
        return self

    def __setstate__(self, state):
        self.update(state)

    def update(self, *dicts, **other):
        """ Update the self dict
        :param dicts: optional list of dictionaries to include
        :param other: key value pairs to include
        The original dict is overwritten left to right, so the right most dict
        overwrites te left most, and key values overwrite that again.
        """
        others = {}
        [others.update(d) for d in dicts]
        others.update(other)
        for k, v in others.items():
            if isinstance(v, dict):
                if k not in self.keys():
                    self[k] = Dict(v)
                else:
                    self[k].update(v)
            else:
                self[k] = v
        return None

    def dict(self):
        """ Convert to regular dict (e.g. to export to file) """
        result = dict()
        for k, v in self.items():
            if type(v) is Dict:
                v = v.dict()
            result[k] = v
        return result

```

`hierarchical-nav/dommel_library/datastructs/tensor_dict.py`:

```py
import torch
from dommel_library.datastructs.dict import Dict
from dommel_library.distributions import MultivariateNormal
class TensorDict(Dict):
    """ Separate datastructure for mapping from str to torch.Tensor
        Allow to index both on key and slice
    """

    def __getitem__(self, key):
        if isinstance(key, str):
            return dict.__getitem__(self, key)
        else:
            item = {}
            for k in self.keys():
                item[k] = dict.__getitem__(self, k)[key]
            return TensorDict(item)

    def to(self, device):
        for key, value in self.items():
            if value is not None:
                self[key] = value.to(device)
        return self

    def detach(self):
        for key, value in self.items():
            if value is not None:
                self[key] = value.detach()
        return self

    def squeeze(self, dim):
        """ Squeeze a dimension of the TensorDict """
        for key, value in self.items():
            self[key] = _check_type(value, value.squeeze(dim))
        return self

    def unsqueeze(self, dim):
        """ Unsqueeze a dimension of the TensorDict """
        for key, value in self.items():
            self[key] = _check_type(value, value.unsqueeze(dim))
        return self

    @property
    def shape(self):
        """ Get first two dimension shapes of the TensorDict values
        :return: first two dimensions, assuming these are batch and time
        """
        key = list(self.keys())[0]
        return tuple(self[key].shape[0: 2])


def cat(*dicts):
    """ Merge TensorDicts in the time dimension
    """
    if len(dicts) == 1:
        return dicts[0]
    merged = {}
    s1 = dicts[0]
    for i in range(1, len(dicts)):
        s2 = dicts[i]
        for key, value in s1.items():
            if key in s2:
                if value.shape[0] != s2[key].shape[0]:
                    # repeat in batch dimension if shapes are not the same
                    factor = int(s2[key].shape[0] / value.shape[0])
                    sizes = [factor]
                    for _ in range(len(value.shape) - 1):
                        sizes.append(1)
                    value = value.repeat(sizes)
                merged_value = torch.cat((value, s2[key]), dim=1)
                merged[key] = _check_type(value, merged_value)
            else:
                merged[key] = value
        s1 = merged

    return TensorDict(merged)


def stack(*dicts):
    """ Stack TensorDicts in the batch dimension """
    if len(dicts) == 1:
        return dicts[0]
    merged = {}
    s1 = dicts[0]
    for i in range(1, len(dicts)):
        s2 = dicts[i]
        for key, value in s1.items():
            if key in s2:
                merged_value = torch.cat((value, s2[key]), dim=0)
                merged[key] = _check_type(value, merged_value)
            else:
                merged[key] = value
        s1 = merged
    return TensorDict(merged)


def _check_type(value, result):
    """ Helper function to make sure distributions are also
    converted correctly """
    if value.__class__.__name__ != "Tensor":
        constructor = globals()[value.__class__.__name__]
        return constructor(result)
    return result

```

`hierarchical-nav/dommel_library/distributions/__init__.py`:

```py
from dommel_library.distributions.multivariate_normal import MultivariateNormal
from dommel_library.distributions.multivariate_normal import StandardNormal

__all__ = [
    "MultivariateNormal",
    "StandardNormal",
]

```

`hierarchical-nav/dommel_library/distributions/multivariate_normal.py`:

```py
import math
import numpy

import torch
from torch.distributions.kl import register_kl

"""
  We use custom classes for distributions that are also torch.Tensor instances
  This is a bit hacky, but allows to handle them nicely in TensorDict
"""


def MultivariateNormal(*t):
    if len(t) == 1:
        t = t[0]
    elif len(t) == 2:
        t = torch.cat(t, -1)

    # this is already a MultivariateNormal distribution, return
    if isinstance(t, InnerMultivariateNormal):
        return t

    # call type only once to make sure .__class__ is equals for objects
    if not hasattr(MultivariateNormal, "cls"):
        cls = type(
            "MultivariateNormal", (InnerMultivariateNormal, t.__class__), {}
        )
        setattr(MultivariateNormal, "cls", cls)

    t.__class__ = MultivariateNormal.cls
    return t


def StandardNormal(*dims):
    mu = torch.zeros(*dims, dtype=torch.float32)
    sigma = torch.ones(*dims, dtype=torch.float32)
    t = torch.cat((mu, sigma), -1)
    return MultivariateNormal(t)


class InnerMultivariateNormal:
    @property
    def variance(self):
        _, sigma = self._mu_sigma()
        return sigma ** 2

    @variance.setter
    def variance(self, value):
        self.stdev = torch.sqrt(value)

    @property
    def covariance_matrix(self):
        return self.variance.unsqueeze(1) * torch.eye(
            self.variance.size(-1)
        ).to(self.variance.device)

    @property
    def mean(self):
        mu, _ = self._mu_sigma()
        return mu

    @mean.setter
    def mean(self, value):
        split = self.shape[-1] // 2
        self[..., :split] = value

    @property
    def stdev(self):
        _, sigma = self._mu_sigma()
        return sigma

    @stdev.setter
    def stdev(self, value):
        split = self.shape[-1] // 2
        self[..., split:] = value

    def sample(self, no_samples=1):
        mu, sigma = self._mu_sigma()
        shape = mu.shape
        if no_samples > 1:
            shape = [no_samples] + list(shape)
        eps = torch.randn(shape).to(mu.device)
        return mu + eps * sigma

    def __mul__(self, other):
        mu1, sigma1 = self._mu_sigma()
        mu2, sigma2 = other._mu_sigma()
        mu = (mu1 * sigma2 ** 2) + (mu2 * sigma1 ** 2)
        mu /= sigma1 ** 2 + sigma2 ** 2
        sigma = sigma1 ** 2 * sigma2 ** 2
        sigma /= sigma1 ** 2 + sigma2 ** 2
        sigma = torch.sqrt(sigma)
        mul = torch.cat((mu, sigma), -1)
        return MultivariateNormal(mul)

    def log_prob(self, value):
        mu, sigma = self._mu_sigma()
        var = sigma ** 2 + 1e-12
        k = int(mu.size()[1])
        if len(sigma.size()) > 2 or len(value.size()) > 2:
            # our input has more than one dimension, vectorize it
            k = numpy.prod(mu.size()[1:])
            var = sigma.reshape(-1, k) ** 2
            mu = mu.reshape(-1, k)
            value = value.reshape(-1, k)

        # use the multivariate gaussian definition
        # we can simplify the determinant to a cumprod due to the fact that
        # sigma is diag(variance**(1/2))
        # we can simplify the (value - mu)^T * sigma^-1 * (value - mu)
        # to (value - mu) ** 2 * sigma (all elementswise) and summing over
        # them note that for sigma = 1 we get a loss similar mse
        # (up to a scalar k log(2*pi))
        term = k * numpy.log(2 * math.pi)
        term = torch.tensor(term).to(mu.device)
        log_det = torch.sum(var.log(), dim=1)
        nll = 0.5 * (
            log_det
            + term
            + torch.sum((value - mu).pow(2) / (var), dim=1)
        )
        return -nll

    # this is untested code!
    def entropy(self):
        _, sigma = self._mu_sigma()
        H = (
            0.5
            + 0.5 * math.log(2 * math.pi)
            + (torch.prod(sigma, dim=1) + 1e-12).log()
        )
        return H

    def _mu_sigma(self):
        split = self.shape[-1] // 2
        mu = self[..., 0:split]
        mu.__class__ = torch.Tensor
        sigma = self[..., split:]
        sigma.__class__ = torch.Tensor
        return mu, sigma

    def to(self, device):
        moved = self.__class__.__bases__[1].to(self, device)
        if isinstance(moved, InnerMultivariateNormal):
            return moved
        return MultivariateNormal(moved)

    def __getitem__(self, index):
        item = self.__class__.__bases__[1].__getitem__(self, index)
        return MultivariateNormal(item)


@register_kl(InnerMultivariateNormal, InnerMultivariateNormal)
def kl_imn_imn(p, q):
    sigma_ratio_squared = (p.stdev / (q.stdev + 1e-12)).pow(2)
    kl = 0.5 * (
        ((p.mean - q.mean) / (q.stdev + 1e-12)).pow(2)
        + sigma_ratio_squared
        - sigma_ratio_squared.log()
        - 1
    )
    return torch.sum(kl, dim=-1)

```

`hierarchical-nav/dommel_library/modules/__init__.py`:

```py
from dommel_library.modules.dommel_modules import *
from dommel_library.modules.visualize import * 


```

`hierarchical-nav/dommel_library/modules/dommel_modules.py`:

```py
import torch
from dommel_library.datastructs import TensorDict
from dommel_library.distributions.multivariate_normal import MultivariateNormal


def multivariate_distribution(mean: torch.Tensor, stdev: torch.Tensor=None) -> MultivariateNormal :
    if stdev is None:
        multivariate = MultivariateNormal(mean)
    else:
        multivariate = MultivariateNormal(mean, stdev)

    return multivariate

def tensor_dict(input: dict) -> TensorDict :
    return TensorDict(input)

```

`hierarchical-nav/dommel_library/modules/visualize.py`:

```py
import os
import torch
import imageio
import math
import numpy as np
from pyquaternion import Quaternion

from matplotlib import cm
import matplotlib as mpl

from dommel_library.datastructs import TensorDict
from dommel_library.distributions.multivariate_normal import InnerMultivariateNormal

if os.environ.get("MPL_BACKEND", "") != "":
    mpl.use(os.environ.get("MPL_BACKEND", ""))
elif os.environ.get("DISPLAY", "") == "":
    print("No display found. Using non-interactive Agg backend.")
    mpl.use("Agg")
import matplotlib.pyplot as plt  # noqa: E402
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401,E402


default_vis_mapping = {
    "image": ["image", "camera"],
    "intensity": ["intensity", "depth", "range_doppler", "range_azimuth"],
    "lidar": ["lidar", "laserscan"],
    "twist": ["twist", "odom"],
    "vector": ["vector"],
    "pose": ["pose"],
    "position": ["position"],
    "radar": ["radar"],
    "rpyt": ["rpyt"]
}


def visualize(observation, keys=None, fmt="torch", show=False, **kwargs):
    """ Visualize a dictionary of observations.

    Observations are provided as a dictionary with string keys
    and tensor (or numpy array) values.

    :param keys: the keys of the observation to visualize (default all)
    :param fmt: the output format of the image ("torch" or "numpy")
    (default "torch")
    :param show: whether to immediately show the image with matplotlib
    (default False)
    :param **kwargs: additional parameters
    :returns: an image tensor or numpy array.
    """
    if keys is not None:
        to_visualize = TensorDict({key: observation[key] for key in keys})
    else:
        to_visualize = observation

    vis_mapping = kwargs.get("vis_mapping", {})
    for k, v in default_vis_mapping.items():
        if k in vis_mapping:
            vis_mapping[k] = list(vis_mapping[k]) + v
        else:
            vis_mapping[k] = v

    result = {}
    for key, value in to_visualize.items():
        if "timestamp" in key:
            continue
        elif "parameters" in key:
            continue
        elif any([key.startswith(x) for x in vis_mapping["image"]]):
            result[key] = vis_image(value, fmt=fmt, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["lidar"]]):
            result[key] = vis_lidar(value, fmt=fmt, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["intensity"]]):
            result[key] = vis_intensity(value, fmt=fmt, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["twist"]]):
            result[key] = vis_twist(value, fmt=fmt, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["vector"]]):
            result[key] = vis_vector(value, fmt=fmt, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["pose"]]):
            result[key] = vis_poses(value, fmt=fmt, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["radar"]]):
            result[key] = vis_radar(value, fmt=fmt, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["rpyt"]]):
            result[key] = vis_rpyt(value, fmt=fmt, **kwargs)

    if show:
        show_grid(result, show=True, **kwargs)

    return TensorDict(result)


def export(observation, path, keys=None, **kwargs):
    """ Export observation visualization to file.
    """
    imgs = visualize(observation, keys, fmt="numpy", show=False, **kwargs)
    if len(imgs.keys()) == 1:
        frame = imgs[next(iter(imgs))]
    else:
        frame = show_grid(imgs, fmt="numpy")
    imageio.imwrite(path, frame)


def visualize_sequence(
    observations, keys=None, max_length=8, show=False, **kwargs
):
    """ Visualize a sequence of observations.

    Observations are provided as a dictionary with string keys and tensor
    (or numpy array) values. The tensors have a batch and time dimension.

    :param keys: the keys of the observation to visualize (default all)
    :param max_length: the max width of the grid (default None)
    :param show: whether to immediately show the image with matplotlib
    (default False)
    :param **kwargs: additional parameters
    :returns: an image tensor or numpy array. Time and batch dimensions are
    shown as consecutive frames in a grid.
    """
    if keys is not None:
        to_visualize = TensorDict({key: observations[key] for key in keys})
    else:
        to_visualize = observations

    vis_mapping = kwargs.get("vis_mapping", {})
    for k, v in default_vis_mapping.items():
        if k in vis_mapping:
            vis_mapping[k] = list(vis_mapping[k]) + v
        else:
            vis_mapping[k] = v

    result = {}
    for key, seq in to_visualize.items():
        if "timestamp" in key:
            continue
        elif "parameters" in key:
            continue
        elif any([key.startswith(x) for x in vis_mapping["image"]]):
            result[key] = vis_images(seq, max_length=max_length, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["lidar"]]):
            result[key] = vis_lidars(seq, max_length=max_length, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["intensity"]]):
            result[key] = vis_intensities(seq, max_length=max_length,
                                          **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["twist"]]):
            result[key] = vis_twists(seq, max_length=max_length, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["vector"]]):
            if isinstance(seq, InnerMultivariateNormal):
                mu = seq.mean
                sigma = seq.stdev
                result[key] = vis_vectors(mu, sigma,
                                          max_length=max_length, **kwargs)
            else:
                result[key] = vis_vectors(seq, max_length=max_length,
                                          **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["pose"]]):
            result[key] = vis_poses(seq, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["position"]]):
            result[key] = vis_positions(seq, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["radar"]]):
            result[key] = vis_radars(seq, **kwargs)
        elif any([key.startswith(x) for x in vis_mapping["rpyt"]]):
            result[key] = vis_rpyts(seq, **kwargs)

    if show:
        show_grid(result, show=True, **kwargs)

    return TensorDict(result)


def export_sequence(observations, path, keys=None, **kwargs):
    """ Export observation sequence visualization to file.
    """
    result = []
    if path[-3:] == "mp4":
        for i in range(observations.shape[1]):
            if observations.shape[0] == 1:
                imgs = visualize(observations[0, i, ...],
                                 keys, fmt="numpy", show=False, **kwargs)
            else:
                raise Exception("Only sequences with batch_size 1"
                                "can currently be exported as mp4")
            if len(imgs.keys()) == 1:
                frame = imgs[next(iter(imgs))]
            else:
                frame = show_grid(imgs, fmt="numpy")
            result.append(frame)
        imageio.mimwrite(path, result)
    else:
        imgs = visualize_sequence(observations, keys, show=False, **kwargs)
        frame = show_grid(imgs, fmt="numpy")
        imageio.imwrite(path, frame)


def vis_images(images, max_length=None, fmt="torch", show=False,
               padding=2, pad_value=0, **kwargs):
    """ Visualize a sequence of image data.

    Plots both numpy (HWC) and torch (CHW) formatted data.
    """
    images = numpyify(images)

    if len(images.shape) == 4:
        # add batch dimension
        images = np.expand_dims(images, 0)

    if images.shape[2] != 3 and images.shape[2] != 1:
        # numpy HWC format... permute dimensions to torch CHW format
        images = np.transpose(images, (0, 1, 4, 2, 3))

    if images.dtype == np.uint8:
        # convert to float, divide by 255
        images = images.astype(np.float32)
        images = images / 255

    # calculate number of images per row
    nrow = images.shape[1]
    if max_length is not None:
        if images.shape[1] > max_length:
            nrow = max_length

    # make grid
    shape = [images.shape[0] * images.shape[1]] + list(images.shape[2:])
    images = images.reshape(shape)
    img = make_grid(images.clip(0.0, 1.0), nrow=nrow,
                    padding=padding, pad_value=pad_value)
    # render using matplotlib if requested
    if show:
        to_show = np.transpose(img, (1, 2, 0))
        plt.imshow(to_show)
        plt.axis("off")
        plt.show()

    if fmt == "numpy":
        img = np.transpose(img, (1, 2, 0))
        img *= 255
        img = img.astype(np.uint8)
    elif fmt == "torch":
        img = torch.as_tensor(img)

    return img


def vis_image(image, fmt="torch", show=False,
              hflip=False, vflip=False, **kwargs):
    """ Visualize a single image.
    """
    img = numpyify(image)

    assert len(img.shape) == 3

    if img.shape[0] != 3 and img.shape[0] != 1:
        # numpy HWC format... permute dimensions to torch CHW format
        img = np.transpose(img, (2, 0, 1))

    if img.dtype == np.uint8:
        # convert to float, divide by 255
        img = img.astype(np.float32)
        img = img / 255

    if vflip:
        img = np.flip(img, 1)

    if hflip:
        img = np.flip(img, 2)

    img = np.clip(img, 0, 1)

    if show:
        to_show = np.transpose(img, (1, 2, 0))
        plt.imshow(to_show)
        plt.axis("off")
        plt.show()

    if fmt == "numpy":
        img = np.transpose(img, (1, 2, 0))
        img *= 255
        img = img.astype(np.uint8)
    elif fmt == "torch":
        img = torch.as_tensor(img)

    return img


def vis_intensities(maps, max_length=None, fmt="torch",
                    show=False, **kwargs):
    """ Visualize a sequence of 2D maps as intensities.
    """
    maps = numpyify(maps)
    # sometimes 2D maps have extra 1 dimension
    # to enable processing by CNN
    # this 1 dimension can be last in numpy format (HWC, C=1)
    if maps.shape[-1] == 1:
        maps = np.squeeze(maps, -1)

    # or can be the -3rd in torch format (CHW, C=1)
    # but be aware that it might also be a missing batch dimension
    # and a sequence lenght of 1
    if maps.shape[-3] == 1 and len(maps.shape) > 3:
        maps = np.squeeze(maps, -3)

    if len(maps.shape) == 3:
        # add batch dimension
        maps = np.expand_dims(maps, 0)

    # this is a slow loop... do something more clever
    # to plot everything in one go?
    batch = []
    for i in range(maps.shape[0]):
        sequence = []
        for j in range(maps.shape[1]):
            colormap = maps[i][j]
            img = vis_intensity(colormap)
            sequence.append(img)
        s = np.stack(sequence, axis=0)
        batch.append(s)
    b = np.stack(batch, axis=0)
    return vis_images(b, max_length, fmt, show)


def vis_intensity(colormap, fmt="torch", show=False,
                  hflip=False, vflip=False,
                  min_intensity=None, max_intensity=None,
                  **kwargs):
    """ Visualize a intensity map.
    """
    colormap = numpyify(colormap)
    # sometimes 2D maps have extra 1 dimension
    # to enable processing by CNN
    if colormap.shape[-1] == 1:
        colormap = np.squeeze(colormap, -1)

    assert len(colormap.shape) == 2

    if vflip:
        colormap = np.flip(colormap, 0)

    if hflip:
        colormap = np.flip(colormap, 1)

    if min_intensity is None:
        colormap = colormap - np.min(colormap)
    else:
        colormap = colormap - min_intensity

    if max_intensity is None:
        colormap = colormap / np.max(colormap)
    else:
        colormap = colormap / max_intensity

    viridis = cm.get_cmap("viridis", 256)
    colormap = viridis(colormap)[:, :, 0:3]
    return vis_image(colormap, fmt, show)


def vis_lidars(
    scans,
    angle_min=-np.pi / 2,
    angle_incr=None,
    max_range=6,
    render_lines=False,
    axis=False,
    max_length=None,
    fmt="torch",
    show=False,
    **kwargs
):
    """ Visualize lidar scans.

    :param angle_min: the angle of the first lidar ray (default -pi/2)
    :param angle_incr: the shift in angle in between rays
    (default None, the increments are calculated by assuming a scan range
    of angle_min, -angle_min)
    :param max_range: the maximal range to plot
    :param render_lines: render ray lines or not (default False)
    :param axis: whether to plot axis (default False)
    """
    scans = numpyify(scans)

    if len(scans.shape) == 2:
        # add batch dimension
        scans = np.expand_dims(scans, 0)

    batch = []
    for i in range(scans.shape[0]):
        sequence = []
        for j in range(scans.shape[1]):
            scan = scans[i][j]
            img = vis_lidar(scan, angle_min, angle_incr,
                            max_range, render_lines, axis)
            sequence.append(img)
        s = np.stack(sequence, axis=0)
        batch.append(s)
    b = np.stack(batch, axis=0)
    return vis_images(b, max_length, fmt, show, pad_value=1)


def vis_lidar(
    scan,
    angle_min=-1.5585244894,
    angle_incr=None,
    max_range=6,
    render_lines=False,
    axis=False,
    fmt="torch",
    show=False,
    **kwargs
):
    """ Visualize a lidar scan.

    :param angle_min: the angle of the first lidar ray (default -pi/2)
    :param angle_incr: the shift in angle in between rays
    (default None, the increments are calculated by assuming a scan range
    of angle_min, -angle_min)
    :param max_range: the maximal range to plot
    :param render_lines: render ray lines or not (default False)
    :param axis: whether to plot axis (default False)
    """
    scan = numpyify(scan)
    assert len(scan.shape) == 1
    fig = plt.figure(figsize=(4, 4), dpi=100)
    x = []
    y = []
    angle = angle_min
    if angle_incr is None:
        angle_incr = np.abs(angle_min) * 2 / (len(scan) + 1)
    for i in range(len(scan)):
        r = scan[i].item()
        x.append(-r * np.sin(angle))
        y.append(r * np.cos(angle))
        angle += angle_incr
    start_point = (0.0, 0.0)
    ax = plt.gca()
    ax.set_xlim(-max_range, max_range)
    ax.set_ylim(0, max_range)
    ax.set_xlabel("x (m)")
    ax.set_ylabel("y (m)")
    plt.axis(axis)
    if render_lines:
        for i in range(len(x)):
            line = mpl.lines.Line2D(
                [start_point[0], x[i]], [start_point[1], y[i]], lw=0.05
            )
            ax.add_line(line)
    plt.scatter(x, y, s=[1] * len(x))

    if show:
        plt.show()
    img = fig2img(fig, fmt)
    plt.close(fig)
    return img


def vis_twists(
    twists,
    max_linear=0.3,
    max_angular=2,
    max_length=None,
    fmt="torch",
    show=False,
    **kwargs
):
    """ Visualize Twist commands.

    :param max_linear: the maximal linear velocity to render
    :param max_angular: the maximal angular velocity to render
    """
    twists = numpyify(twists)

    if len(twists.shape) == 2:
        # add batch dimension
        twists = np.expand_dims(twists, 0)

    batch = []
    for i in range(twists.shape[0]):
        sequence = []
        for j in range(twists.shape[1]):
            twist = twists[i][j]
            img = vis_twist(twist, max_linear, max_angular)
            sequence.append(img)
        s = np.stack(sequence, axis=0)
        batch.append(s)
    b = np.stack(batch, axis=0)
    return vis_images(b, max_length, fmt, show, pad_value=1)


def vis_twist(
    twist, max_linear=0.5, max_angular=2, fmt="torch", show=False, **kwargs
):
    """ Visualize a Twist command.

    :param max_linear: the maximal linear velocity to render
    :param max_angular: the maximal angular velocity to render
    """
    fig, ax = plt.subplots()
    arrow_width = 0.1
    plt.xlim(-max_angular, max_angular)
    plt.ylim(-max_linear, max_linear)
    plt.axis("off")
    vx = twist[-5]
    vy = twist[-6]
    ax.annotate(
        "",
        xy=(vx, vy),
        xytext=(0, 0),
        arrowprops=dict(facecolor="black", shrink=arrow_width),
    )
    va = twist[-1]
    if va != 0:
        ax.annotate(
            "",
            xy=(-va, 0),
            xytext=(0, 0),
            arrowprops=dict(facecolor="black", shrink=arrow_width),
        )

    if show:
        plt.show()
    img = fig2img(fig, fmt)
    plt.close(fig)
    return img


def vis_rpyts(
    rpyts, max_length=None, fmt="torch", show=False, **kwargs
):
    """ Visualize a ActuatorControl command containig roll, pitch, yaw
    and thrust.
    """
    rpyts = numpyify(rpyts)

    if len(rpyts.shape) == 2:
        # add batch dimension
        rpyts = np.expand_dims(rpyts, 0)

    batch = []
    for i in range(rpyts.shape[0]):
        sequence = []
        for j in range(rpyts.shape[1]):
            rpyt = rpyts[i][j]
            img = vis_rpyt(rpyt, **kwargs)
            sequence.append(img)
        s = np.stack(sequence, axis=0)
        batch.append(s)
    b = np.stack(batch, axis=0)
    return vis_images(b, max_length, fmt, show, pad_value=1)


def vis_rpyt(
    rpyt, max_roll=0.3, max_pitch=0.3, max_yaw=0.2,
    fmt="torch", show=False, **kwargs
):
    """ Visualize a ActuatorControl command containig roll, pitch, yaw
    and thrust.
    """
    fig, ax = plt.subplots()
    arrow_width = 0.1
    plt.xlim(-1, 1)
    plt.ylim(-1, 1)
    plt.axis("off")
    roll = rpyt[0] / max_roll
    pitch = rpyt[1] / max_pitch
    yaw = rpyt[2] / max_yaw
    thrust = rpyt[3]
    if roll != 0 or pitch != 0:
        ax.annotate(
            "",
            xy=(roll, pitch),
            xytext=(0, 0),
            arrowprops=dict(facecolor="black", shrink=arrow_width),
        )

    if yaw != 0:
        ax.annotate(
            "",
            xy=(-yaw, 0),
            xytext=(0, 0),
            arrowprops=dict(facecolor="black", shrink=arrow_width),
        )

    if thrust != 0:
        ax.annotate(
            "",
            xy=(0.5, thrust),
            xytext=(0.5, 0),
            arrowprops=dict(facecolor="red", shrink=arrow_width),
        )

    if show:
        plt.show()
    img = fig2img(fig, fmt)
    plt.close(fig)
    return img


def vis_vectors(vectors, sigmas=None, max_length=None,
                fmt="torch", show=False, **kwargs):
    """ Visualize a batch of vectors.
    """
    vectors = numpyify(vectors)
    if sigmas is not None:
        sigmas = numpyify(sigmas)

    if len(vectors.shape) == 2:
        # add batch dimension
        vectors = np.expand_dims(vectors, 0)
        if sigmas:
            sigmas = np.expand_dims(sigmas, 0)

    batch = []  # batch can be a pre allocated numpy array
    for i in range(vectors.shape[0]):
        sigma = None
        if sigmas is not None:
            sigma = sigmas[i]

        img = vis_vector(vectors[i], sigma, **kwargs)
        batch.append(img)
    b = np.stack(batch, axis=0)
    return vis_images(b, max_length, fmt, show, pad_value=1)


def vis_vector(vector, sigma=None, ylim=None, fmt="torch",
               show=False, **kwargs):
    """ Visualize a sequence of vectors as line plots
    """
    vector = numpyify(vector)
    # if we still have a sequence of vectors, only plot the last?
    if len(vector.shape) == 3:
        vector = vector[-1, :, :]
    assert len(vector.shape) == 2
    fig, ax = plt.subplots()
    ax.set_xticklabels([])
    if ylim:
        ax.set_ylim(*ylim)
    for i in range(vector.shape[-1]):
        mu = vector[:, i]
        plt.plot(mu)
        if sigma is not None:
            sigma = numpyify(sigma)
            std = sigma[:, i]
            plt.fill_between(
                x=range(len(mu)), y1=mu - std, y2=mu + std, alpha=0.2
            )

    if show:
        plt.show()

    img = fig2img(fig, fmt)
    plt.close(fig)
    return img


def vis_poses(pose_vector, quaternion="XYZW", forward=[1, 0, 0],
              frame="Z_UP", quiver_length=1.0,
              xlim=None, ylim=None, zlim=None,
              fmt="torch", show=False, **kwargs):
    pose_vector = numpyify(pose_vector)
    if len(pose_vector.shape) == 1:
        pose_vector = np.expand_dims(pose_vector, 0)
    if len(pose_vector.shape) == 2:
        pose_vector = np.expand_dims(pose_vector, 0)

    fig = plt.figure(figsize=(8, 5))
    ax = fig.gca(projection="3d")
    for i in range(pose_vector.shape[0]):
        for j in range(pose_vector.shape[1]):
            p = pose_vector[i, j, :]
            x, y, z = p[:3]
            if p.shape[0] == 6 or quaternion == "log":
                qx, qy, qz = p[3:6]
                log_q = Quaternion(x=qx, y=qy, z=qz, w=0.0)
                rotation = Quaternion.exp(log_q)
            elif quaternion == "WXYZ":
                qw, qx, qy, qz = p[3:7]
                rotation = Quaternion(x=qx, y=qy, z=qz, w=qw)
            else:
                qx, qy, qz, qw = p[3:7]
                rotation = Quaternion(x=qx, y=qy, z=qz, w=qw)

            if frame == "Z_DOWN":
                # z-axes points downwards (i.e. pixhawk drone)
                rotation = rotation * Quaternion(axis=[1, 0, 0], angle=180)
            elif frame == "Z_IN":
                # z-axes points inwards the camera (i.e. t265 camera)
                rotation = rotation * Quaternion(axis=[1, 0, 0], angle=90)
            u, v, w = rotation.rotate(forward)
            ax.quiver(
                x, y, z, u, v, w,
                length=quiver_length, normalize=True,
                color='C' + str(i)
            )

    ax.plot([0], [0], [0], marker="x")
    ax.view_init(45, 95)
    ax.set_xlabel('$X$', fontsize=15)
    ax.set_ylabel('$Y$', fontsize=15)
    ax.set_zlabel('$Z$', fontsize=15)
    if xlim:
        ax.set_xlim(*xlim)
    if ylim:
        ax.set_ylim(*ylim)
    if zlim:
        ax.set_zlim(*zlim)

    if show:
        plt.show()

    img = fig2img(fig, fmt)
    plt.close(fig)
    return img


def vis_positions(positions, cross_latest=True, xlim=None, ylim=None,
                  fmt="torch", show=False, **kwargs):
    positions = numpyify(positions)
    if len(positions.shape) == 1:
        positions = np.expand_dims(positions, 0)
    if len(positions.shape) == 2:
        positions = np.expand_dims(positions, 0)
    if len(positions.shape) == 3 and positions.shape[-2] == 2:
        # besides positions also covariances provided
        # ignore for now
        positions = np.expand_dims(positions[:, 0, :], 0)

    fig = plt.figure(figsize=(8, 6))
    ax = fig.gca()
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    if xlim:
        ax.set_xlim(*xlim)
    if ylim:
        ax.set_ylim(*ylim)
    for i in range(positions.shape[0]):
        xs = positions[i, :, 0]
        ys = positions[i, :, 1]
        ax.scatter(xs, ys)

        if cross_latest:
            if len(xs) > 0:
                ax.plot(xs[-1], ys[-1], 'r', marker='x',
                        mew=4, markersize=12)

    if show:
        plt.show()

    img = fig2img(fig, fmt)
    plt.close(fig)
    return img


def vis_radars(sequence_adc_samples, max_length=None, fmt="torch",
               show=False, **kwargs):
    """ Visualize a sequence of raw adc samples as intensities.
    """
    sequence_adc_samples = numpyify(sequence_adc_samples)

    if len(sequence_adc_samples.shape) == 6:
        # add batch dimension
        sequence_adc_samples = np.expand_dims(sequence_adc_samples, 0)

    batch = []
    for i in range(sequence_adc_samples.shape[0]):
        sequence = []
        for j in range(sequence_adc_samples.shape[1]):
            adc_samples = sequence_adc_samples[i][j]
            img = vis_radar(adc_samples, **kwargs)
            sequence.append(img)
        s = np.stack(sequence, axis=0)
        batch.append(s)
    b = np.stack(batch, axis=0)
    return vis_images(b, max_length, fmt, show)


def vis_radar(adc_samples, fmt="torch", show=False, **kwargs):
    intensities = []
    for i in range(adc_samples.shape[1]):
        for j in range(adc_samples.shape[2]):
            intensities.append(adc_samples[:, i, j, :, 0])
            intensities.append(adc_samples[:, i, j, :, 1])

    intensities = np.stack(intensities)
    return vis_intensities(intensities, max_length=2,
                           fmt=fmt, show=show, **kwargs)


def fig2img(fig, fmt="torch"):
    """ Render a Matplotlib Figure into an image array.

    :param format: 'numpy' or 'torch'
    """
    fig.canvas.draw()

    # TODO prevent this from initializing the alpha channel
    s, (width, height) = fig.canvas.print_to_buffer()
    buf = np.frombuffer(s, np.uint8).reshape((height, width, 4))
    # we don't need the alpha channel
    buf = buf[:, :, 0:3]

    img = None
    if fmt == "torch":
        buf = buf.swapaxes(2, 0).swapaxes(1, 2)
        buf = buf / 255.0
        img = torch.as_tensor(buf)
    elif fmt == "numpy":
        img = buf
    else:
        raise Exception("Unsupported format: {}".format(fmt))

    return img


def show_grid(imgs, fmt="torch", show=False, **kwargs):
    """ Show a dictionary of images as a grid in matplotlib.
    """
    num_panes = len(imgs.keys())
    nrows = int(math.sqrt(num_panes))
    ncols = math.ceil(num_panes / nrows)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols)
    title = kwargs.get("title", None)
    if title is not None:
        fig.suptitle(title, fontsize=10)
    n_ax = 0
    for key in imgs.keys():
        if isinstance(axes, np.ndarray):
            if axes.ndim == 2:
                ax = axes[int(n_ax / axes.shape[1]), n_ax % axes.shape[1]]
            else:
                ax = axes[n_ax]
        else:
            ax = axes
        ax.axis("off")
        ax.set_title(key, {"fontsize": 8})
        to_show = np.transpose(imgs[key], (1, 2, 0))
        to_show = numpyify(to_show)
        ax.imshow(to_show)
        n_ax += 1

    # clear unused panes
    if n_ax < nrows * ncols:
        while n_ax < nrows * ncols:
            ax = axes[int(n_ax / axes.shape[1]), n_ax % axes.shape[1]]
            ax.axis("off")
            n_ax += 1

    if show:
        plt.show()

    img = fig2img(fig, fmt)
    plt.close(fig)
    return img


def make_grid(arr, nrow=8, padding=2, pad_value=0):
    """ Make a grid of images, similar to torchvision.make_grid
    """
    # we use the numpy methods since torch tensors also implement
    # the np.ndarray interface and we don't care about gradients here
    # this way make_grid will also work on h5 datasets and ndarrays
    # we will still respect the torch CHW convention to not break any earlier
    # code
    # ! NOTE that we don't support some optional torchvision make_grid
    # arguments

    # first get the input to a 4d array in BCHW format
    if len(arr.shape) == 2:
        # single gray_scale image
        arr = np.expand_dims(arr, axis=0)  # arr.shape has changed!
    if len(arr.shape) == 3:
        # single rgb image
        if arr.shape[0] == 1:
            new_shape = (3, arr.shape[1:])
            arr = np.reshape(arr, new_shape)
        arr = np.expand_dims(arr, axis=0)  # arr.shape has changed

    if len(arr.shape) == 4 and arr.shape[1] == 1:
        new_shape = (arr.shape[0], 3, *arr.shape[2:])
        arr = np.concatenate([arr, arr, arr], axis=1)
        arr = np.reshape(arr, new_shape)

    nmaps = arr.shape[0]
    xmaps = min(nrow, nmaps)
    if xmaps == 0:
        return np.zeros([3, 1, 1])
    ymaps = int(np.ceil(nmaps / xmaps))
    height, width = arr.shape[2] + padding, arr.shape[3] + padding
    num_channels = arr.shape[1]
    # create final image grid array in CHW format
    grid = np.full(
        (num_channels, height * ymaps + padding, width * xmaps + padding),
        pad_value,
        dtype=np.float32,
    )
    k = 0
    for y in range(ymaps):
        for x in range(xmaps):
            if k >= nmaps:
                break
            y_start = y * height + padding
            y_stop = y_start + (height - padding)
            x_start = x * width + padding
            x_stop = x_start + (width - padding)
            # remember grid is CHW mpt BCHW
            arr = numpyify(arr)
            grid[:, y_start:y_stop, x_start:x_stop] = arr[k]
            k += 1
    # finally convert grid to the same type is the input type
    if isinstance(arr, torch.Tensor):
        grid = torch.as_tensor(grid)
    return grid


def numpyify(arr):
    """ Convert to numpy array if torch Tensor.
    """
    try:
        return arr.detach().cpu().numpy()
    except AttributeError:
        return arr

```

`hierarchical-nav/dommel_library/nn/__init__.py`:

```py
from dommel_library.nn.film import FiLM, ConvFiLM
from dommel_library.nn.activation import Activation, get_activation
from dommel_library.nn.modules import (
    MLP,
    View,
    Reshape,
    Sample,
    Cat,
    Sum,
    Broadcast,
)
from dommel_library.nn.variational import (
    VariationalMLP,
    VariationalLayer,
    VariationalGRU,
    VariationalLSTM,
)
from dommel_library.nn.convolutions import (
    ConvPipeline,
    Interpolate,
    UpConvPipeline,
    CNN,
    UpCNN,
    Conv,
)
from dommel_library.nn.module_factory import module_factory, register_backend
from dommel_library.nn.summary import summary

__all__ = [
    "module_factory",
    "register_backend",
    "summary",
    "Sample",
    "View",
    "Reshape",
    "MLP",
    "FiLM",
    "ConvFiLM",
    "get_activation",
    "Activation",
    "Cat",
    "Sum",
    "Broadcast",
    "ConvPipeline",
    "Interpolate",
    "UpConvPipeline",
    "CNN",
    "UpCNN",
    "VariationalLayer",
    "VariationalMLP",
    "VariationalGRU",
    "VariationalLSTM",
    "Conv",
]

```

`hierarchical-nav/dommel_library/nn/activation.py`:

```py
import torch.nn as nn
import torch.nn.functional as F

from dommel_library.nn import module_factory


def get_activation(activation="Activation", **kwargs):
    if not activation:
        # no activation
        activation = "Identity"
    if isinstance(activation, str):
        # Search registered backends for the activation and create the module
        return module_factory.get_module_from_dict(
            {"type": activation, "args": kwargs}
        )
    else:
        # uniform way to also handle passing the objects directly
        return activation


class Activation(nn.Module):

    def __init__(self, **kwargs):
        nn.Module.__init__(self)

    def forward(self, x):
        return F.leaky_relu(x, negative_slope=0.02)

    def __repr__(self):
        return "LeakyReLU(negative_slope=0.02)"

```

`hierarchical-nav/dommel_library/nn/composable_module.py`:

```py
import torch

from dommel_library.datastructs.tensor_dict import TensorDict


class ComposableModule(torch.nn.Module):
    def __init__(self, modules):
        """
        :param modules: a list of modules that are used in the composable
            model. Each list element is a dictionary that describes the module
            using the following keys ["input", "output", "module"]
        """
        torch.nn.Module.__init__(self)

        self._module_list = modules

        # register modules for parameters(), to() and autograd
        for i, module in enumerate(self._module_list):
            self.add_module(f"module-{i}", module["module"])

    def forward(self, input_dict):
        """
        :param input_dict:
        :return: output_dict
        """
        output_dict = {}

        x = None
        for module in self._module_list:
            # Tries to get the value from the input dict, if it's not there,
            # it searches for the value in the output dict (from a previously
            # computed module), If that's not there, it takes the previously
            # computed value, as defined in last layer.

            module_args = []

            # Get input values
            input_keys = module.get("input", None)
            if input_keys is not None:
                # make it a list: unpacking can be done when calling the module
                if not isinstance(input_keys, list):
                    input_keys = [input_keys]
                for key in input_keys:
                    # add previous outputs using the "..." key
                    if key == "...":
                        if isinstance(x, tuple):
                            module_args += list(x)
                        else:
                            module_args.append(x)
                    else:
                        val = input_dict.get(key, output_dict.get(key, None))
                        module_args.append(val)
            else:
                if x is None:
                    module_type = module["type"]
                    message = (f"No valid input provided for {module_type}. "
                               f"Check your configuration for input keys.")
                    raise Exception(message)
                elif type(x).__name__ == "QuantTensor":
                    module_args.append(x)
                elif isinstance(x, tuple):
                    module_args += list(x)
                else:
                    module_args.append(x)

            # compute forward pass
            x = module["module"](*module_args)

            # store output of module in the dictionary of outputs
            output_keys = module.get("output", None)
            if output_keys is not None:
                if isinstance(output_keys, list):
                    for i, item in enumerate(x):
                        output_dict[output_keys[i]] = item
                else:
                    output_dict[output_keys] = x

        return TensorDict(output_dict)

```

`hierarchical-nav/dommel_library/nn/convolutions.py`:

```py
import math
import importlib
from numbers import Number

from torch import nn
from torch.nn import functional as F

from dommel_library.nn import get_activation, MLP
from dommel_library.nn import ConvFiLM


import matplotlib.pyplot as plt
import torch

class VisualizeLayer(nn.Module):

    def __init__(self, filename):
        nn.Module.__init__(self)
        self.filename = filename
        self.calib = False

    def forward(self, x):
        if not self.calib:
            b, c, h, w = x.shape
            print(x.shape)
            if (c > 7):
                fig, ax = plt.subplots(c//8, 8, figsize=(8 * 2, c//8 * 2))
                [a.axis('off') for a in ax.flatten()]
                if x.type() == 'torch.quantized.QUInt8Tensor':
                    x_p = torch.int_repr(x).numpy()
                    for bi in range(c//8):
                        for ci in range(8):
                            if c//8 == 1:
                                ax[ci].imshow(x_p[0, ci])
                                ax[ci].set_title(f"Ch: {ci}")
                            else:
                                ax[bi, ci].imshow(x_p[0, 8*bi + ci])
                                ax[bi, ci].set_title(f"Ch: {8*bi + ci}")
                    plt.savefig(self.filename, bbox_inches="tight")
                else:
                    for bi in range(c//8):
                        for ci in range(8):
                            if c//8 == 1:
                                ax[ci].imshow(x[0, ci])
                                ax[ci].set_title(f"Ch: {ci}")
                            else:
                                ax[bi, ci].imshow(x[0, 8*bi + ci])
                                ax[bi, ci].set_title(f"Ch: {8*bi + ci}")
                    plt.savefig(self.filename, bbox_inches="tight")
            else:
                fig, ax = plt.subplots(1, 2, figsize=(2 * 2, 1 * 2))
                [a.axis('off') for a in ax.flatten()]
                if x.type() == 'torch.quantized.QUInt8Tensor':
                    x_p = torch.int_repr(x).numpy()
                    for ci in range(2):
                        ax[ci].imshow(x_p[0, ci])
                        ax[ci].set_title(f"Ch: {ci}")
                else:
                    for ci in range(2):
                        ax[ci].imshow(x[0, ci])
                        ax[ci].set_title(f"Ch: {ci}")
                plt.savefig(self.filename, bbox_inches="tight")
            plt.show()
        return x

class Conv(nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        activation="Activation",
        batch_norm=False,
        condition_size=0,
        **kwargs
    ):
        """
        A basic convolutional building block consisting of a convolution with
        an activation, and optionally batch normalization and FiLM conditioning
        :param in_channels:  Length of the input channel.
        :param out_channels: Length of the output channel.
        :param activation: String indicating the activation function.
        :param batch_norm: Boolean to indicate whether to use batch norm.
        :param condition_size: Optional length for the FiLM conditioning.
        :param kwargs: Parameters to pass to conv2d from torch.
        """
        nn.Module.__init__(self)
        self.in_channels = in_channels
        self.out_channels = out_channels
        kernel_size = kwargs.get("kernel_size", 3)
        stride = kwargs.get("stride", 1)
        padding = kwargs.get("padding", (kernel_size - 1) // 2)
        bias = kwargs.get("bias", True)

        self.conv = nn.Conv2d(in_channels, out_channels,
                              kernel_size, stride, padding, bias=bias)
        self.act = get_activation(activation, **kwargs)
        self.batch_norm = nn.BatchNorm2d(out_channels) if batch_norm else None
        if condition_size > 0:
            self.film = ConvFiLM(condition_size, out_channels)
        else:
            self.film = None

    def forward(self, x, condition=None):
        y = self.conv(x)
        # a = VisualizeLayer('/home/idlab332/workspace/active-inference/filename_'+ str(self.in_channels)+'_'+ str(self.out_channels) )
        # a.forward(y)
        if self.batch_norm:
            y = self.batch_norm(y)
        if self.film:
            if condition is None:
                raise Exception("Condition input expected!")
            y = self.film(y, condition)
        y = self.act(y)
        return y


class ConvPipeline(nn.Module):
    """Builds a conv layer sequence that downscales the input tensor
    according to given channels, it will reshape the output to a vector
    """

    def __init__(
        self,
        input_shape,
        channels,
        block="Conv",
        kernel_size=None,
        stride=None,
        activation="Activation",
        batch_norm=False,
        condition_size=0,
        flatten=True,
        **kwargs
    ):
        """Creates a convolutional pipeline of conv blocks.
        The optional arguments for stride and kernel_size can be provided
        as either a single element -- meaning that all layers should share
        the same parameter -- or as a list, indicating that these values
        should be used. In that case a value must be provided for all layers.
        If no value is provided, a default value will be taken.
        The default kernel_size is 3.
        The default stride is 1 if subsequent channels are the same, or 2 if
        the number of channels changes.
        :param input_shape: The shape of the input. Must be CxHxW.
        :param channels: List of channel lengths the pipeline must go through.
        :param block: The type of block used, can also be a list.
        :param kernel_size: Optional list of kernel sizes.
        :param stride: Optional list of strides.
        :param activation: The activation function to be used.
        :param batch_norm: Flag to indicate the usage of batch norm.
        :param condition_size: Whether to use a film conditioning.
        :param flatten: Flatten the output to a single dimension vector.
        :param kwargs: Contains optional Conv block parameters.
        """
        nn.Module.__init__(self)
        self.input_shape = input_shape
        self.flatten = flatten

        self.channels = _parse_input_list(channels)
        self.blocks = _parse_input_list(block, None, len(channels))
        self.kernel_sizes = _parse_input_list(kernel_size, 3, len(channels))
        for k in self.kernel_sizes:
            if k % 2 == 0:
                raise ValueError(
                    "Even kernel sizes are not supported in ConvPipeline")

        self.strides = _parse_input_list(stride, None, len(channels))
        self.activations = _parse_input_list(activation, "Activation",
                                             len(channels))
        if "padding" in kwargs.keys():
            raise ValueError(
                "Custom padding is not supported in ConvPipeline")

        self.layers = nn.ModuleList()

        self.output_shape = input_shape[:]
        in_channels = input_shape[0]
        for i in range(len(self.channels)):
            block = self.blocks[i]
            b = block.split(".")
            if len(b) > 1:
                package = '.'.join(b[:-1])
                block = b[-1]
                block_module = importlib.import_module(package)  # noqa: F
                block = '.'.join(["block_module", block])

            out_channels = self.channels[i]
            kernel_size = self.kernel_sizes[i]
            stride = self.strides[i]
            if not stride:
                stride = 1 if in_channels == out_channels else 2
            activation = self.activations[i]
            module = (
                f"{block}("
                f"in_channels={in_channels},"
                f"out_channels={out_channels},"
                f"kernel_size={kernel_size},"
                f"stride={stride},"
                f"activation={repr(activation)},"
                f"batch_norm={batch_norm},"
                f"condition_size={condition_size},"
                f"**{kwargs})"
            )
            self.layers.append(eval(module))
            

            # adjust output shape
            self.output_shape[0] = out_channels
            self.output_shape[1] = math.ceil(self.output_shape[1] / stride)
            self.output_shape[2] = math.ceil(self.output_shape[2] / stride)

            # update in_channels for next
            in_channels = out_channels

        self.output_length = int(
            self.output_shape[0] * self.output_shape[1] * self.output_shape[2]
        )

    def forward(self, x, condition=None):
        y = x
        for layer in self.layers:
            y = layer(y, condition=condition)
        if self.flatten:
            y = y.reshape(-1, self.output_length)
        return y


class Interpolate(nn.Module):
    """Module for upsampeling a tensor in it's spacial dimensions.
    Provides a wrapper around torch.nn.F.interpolate .
    """

    def __init__(self, scale_factor=2, mode="nearest", **kwargs):
        """ Initializes the Interpolation step
        :param scale_factor: How much to upscale.
        :param mode: Interpolation method.
        """
        nn.Module.__init__(self)
        self.interp = F.interpolate
        self.scale_factor = scale_factor
        self.mode = mode

    def forward(self, x, *other):
        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode)
        return x


class UpConvPipeline(nn.Module):
    def __init__(
        self,
        output_shape,
        channels,
        block="Conv",
        kernel_size=None,
        interpolate=None,
        activation="Activation",
        batch_norm=False,
        condition_size=0,
        **kwargs
    ):
        """Creates an upsampeling conv pipeline, provides the opposite
        operation of a ConvPipeLine, if instantiated with the same parameters
        :param output_shape: The shape of the desired output. Must be CxHxW.
        :param channels: List of channel lengths the pipeline must go through.
        :param block: The type of block used, can also be a list.
        :param kernel_size: Optional list of kernel sizes.
        :param activation: The activation function to be used.
        :param interpolate: Optional list of interpolate factors.
        :param batch_norm: Flag to indicate the usage of batch norm.
        :param condition_size: Whether to use a film conditioning.
        :param kwargs: Contains optional Conv block parameters.
        """
        nn.Module.__init__(self)
        self._output_shape = output_shape

        self.channels = _parse_input_list(channels)
        self.blocks = _parse_input_list(block, None, len(channels))
        self.kernel_sizes = _parse_input_list(kernel_size, 3, len(channels))
        for k in self.kernel_sizes:
            if k % 2 == 0:
                raise ValueError(
                    "Even kernel sizes are not supported in UpConvPipeline")

        self.interpolate = _parse_input_list(interpolate, None, len(channels))
        self.activations = _parse_input_list(activation, "Activation",
                                             len(channels))
        if "padding" in kwargs.keys():
            raise ValueError(
                "Custom padding is not supported in UpConvPipeline")

        # calculate expected input shape
        in_channels = self.channels[0]
        height = output_shape[1]
        width = output_shape[2]
        #print('output height and width', height, width)
        channels.append(output_shape[0])

        self.layers = nn.ModuleList()
        for i in range(len(self.channels) - 1):
            block = self.blocks[i]
            b = block.split(".")
            if len(b) > 1:
                package = '.'.join(b[:-1])
                block = b[-1]
                block_module = importlib.import_module(package)  # noqa: F
                block = '.'.join(["block_module", block])

            in_channels = self.channels[i]
            out_channels = self.channels[i + 1]

            # check if we need to interpolate
            if not self.interpolate[i]:
                if in_channels != out_channels:
                    self.interpolate[i] = 2
                else:
                    self.interpolate[i] = 1
            if self.interpolate[i] > 1:
                mode = kwargs.get('interpolate_mode', 'nearest')
                self.layers.append(Interpolate(self.interpolate[i], mode))

            height = math.ceil(height / self.interpolate[i])
            width = math.ceil(width / self.interpolate[i])
            #print('updated height and width', in_channels, out_channels, height, width)

            kernel_size = self.kernel_sizes[i]
            activation = self.activations[i]
            module = (
                f"{block}("
                f"in_channels={in_channels},"
                f"out_channels={out_channels},"
                f"kernel_size={kernel_size},"
                f"stride=1,"
                f"activation={repr(activation)},"
                f"batch_norm={batch_norm},"
                f"condition_size={condition_size},"
                f"**{kwargs})"
            )
            self.layers.append(eval(module))

            # update in_channels for next
            in_channels = out_channels

        self.reshape_shape = [channels[0], height, width]
        #print('FINAL input height and width', height, width)

    def forward(self, z, condition=None):
        z = z.reshape(-1, *self.reshape_shape)
        for layer in self.layers:
            z = layer(z, condition)
        if z.shape[-1] != self._output_shape[-1]:
            z = z.narrow(-1, 0, self._output_shape[-1])
        if z.shape[-2] != self._output_shape[-2]:
            z = z.narrow(-2, 0, self._output_shape[-2])
        return z


class CNN(nn.Module):
    """Convenience wrapper around ConvPipeLine and MLP"""

    def __init__(
        self,
        input_shape,
        output_length,
        channels,
        hidden=None,
        activation="Activation",
        **kwargs
    ):
        """ Builds the CNN
         :param input_shape: The input shape in CxHxW format.
         :param output_length: The desired output vector length.
         :param channels: List of channel lengths the pipeline must go through.
         :param hidden: Optional list of hidden neuron lengths for the MLP.
         :param activation: The activation function for the pipeline
         :param kwargs: Optional parameters for the Conv2d and activation.
         """
        nn.Module.__init__(self)
        self._act = get_activation(activation, **kwargs)
        self._convs = ConvPipeline(
            input_shape, channels, activation=activation, **kwargs
        )
        self._fc = MLP(
            self._convs.output_length, output_length, hidden, activation
        )

    def forward(self, x):
        h = self._act(self._convs(x))
        h.reshape(-1, self._convs.output_length)
        y = self._fc(h)
        return y


class UpCNN(nn.Module):
    """Convenience wrapper around UpConvPipeline and MLP"""

    def __init__(
        self,
        input_length,
        output_shape,
        channels,
        hidden=None,
        activation="Activation",
        **kwargs
    ):
        """Initializes the InvConv
        :param input_length: The length of the input vector.
        :param output_shape: The desired output shape in CxHxW format.
        :param channels: The channels the upsampling must go through.
        :param hidden: optional list of hidden neurons of the MLP.
        :param activation: The activation function to use. Defaults to
        leaky_relu.
        :param kwargs: Optional parameters for the activation and conv2d
        """
        nn.Module.__init__(self)
        self._act = get_activation(activation, **kwargs)

        if hidden is not None:
            size = hidden.pop()
            self._fc = MLP(input_length, size, hidden, activation, **kwargs)
        else:
            # no linear layer size is provided
            # So just reshape when assuming doubling every layer
            c = channels[0]
            h = output_shape[1] // 2 ** len(channels)
            w = output_shape[2] // 2 ** len(channels)
            self._fc = nn.Linear(input_length, h * w * c)

        self._iconvs = UpConvPipeline(
            output_shape, channels, activation=activation, **kwargs
        )

    def forward(self, x):
        h = self._act(self._fc(x))
        y = self._iconvs(h)
        return y


def _parse_input_list(li, default=None, target_length=None):
    if not li and not isinstance(li, Number):
        # use default when li is None or empty string
        # but not when 0
        li = [default] * target_length
    elif isinstance(li, str):
        if "*" in li or "+" in li:
            # string contains operators, eval
            li = eval(li)
        else:
            # we want a list of strings, expand
            li = [li] * target_length
    elif not hasattr(li, "__iter__"):
        # single value, expand
        li = [li] * target_length
    elif target_length:
        # check for ellipsis
        for i in range(len(li)):
            if li[i] is Ellipsis or li[i] == "...":
                # replace and repeat item before or after
                if i > 0:
                    li = li[0:i] + [li[i - 1]] * \
                        (target_length + 1 - len(li)) + li[i + 1:]
                else:
                    li = [li[1]] * (target_length + 1 - len(li)) + li[1:]
                    break

        if len(li) != target_length:
            raise AssertionError(
                "When providing a list, it should"
                "contain as much entries as layers"
            )
    return li

```

`hierarchical-nav/dommel_library/nn/film.py`:

```py
import torch.nn as nn


class FiLM(nn.Module):
    """
    A Feature-wise Linear Modulation Layer from
    'FiLM: Visual Reasoning with a General Conditioning Layer'

    The input data (x) is scaled and a bias is added on a feature level,
    which are computed by a given condition (c).
    """

    def __init__(self, in_shape, out_shape):
        """
        :param in_shape: Shape of the condition c
        :param out_shape: Shape of the transforming data x
        """
        nn.Module.__init__(self)
        self.gamma = nn.Linear(in_shape, out_shape)
        self.beta = nn.Linear(in_shape, out_shape)

    def forward(self, x, c):
        """
        :param x: Input data to transform
        :param c: Condition that defines the transformation
        :return: Transformed data
        """
        v = self.gamma(c) * x.view(x.shape[0], -1) + self.beta(c)
        return v.view(x.shape)


class ConvFiLM(nn.Module):
    """
    In a convolutional layer, the FiLM transform is the same
    over all spatial locations but differs per feature.

    A Linear transformation (scaling and added bias) for each feature channel
    of the output of a convolution x is computed based on a condition value c.
    """

    def __init__(self, condition_shape, n_features):
        """
        :param condition_shape: shape of the condition parameter
        :param n_features: number of feature maps of the convolution output
            that will be transformed.
        """
        nn.Module.__init__(self)
        self.gamma = nn.Linear(condition_shape, n_features)
        self.beta = nn.Linear(condition_shape, n_features)

    def forward(self, x, c):
        """
        :param x: Data to transform
        :param c: Condition on which the transformation is based
        :return: transformed data
        """
        gamma = (
            self.gamma(c)
            .unsqueeze(2)
            .unsqueeze(3)
            .repeat(1, 1, x.shape[-2], x.shape[-1])
        )
        beta = (
            self.beta(c)
            .unsqueeze(2)
            .unsqueeze(3)
            .repeat(1, 1, x.shape[-2], x.shape[-1])
        )
        return gamma * x + beta

```

`hierarchical-nav/dommel_library/nn/module_factory.py`:

```py
import importlib

import torch.nn

# Has to import under a different name to avoid dependency issues with backends
from dommel_library import nn as dnn
from dommel_library.nn.composable_module import ComposableModule

import logging
import inspect

logger = logging.getLogger(__name__)

try:
    import brevitas.nn as qnn
    backends = [dnn, torch.nn, qnn]
except Exception:
    backends = [dnn, torch.nn]


def register_backend(backend):
    """
    Register a backend against the global backends in which is checked for
    modules
    :param backend: the new backend to add
    """
    global backends

    # import backend
    backend_module = importlib.import_module(backend)  # noqa: F
    backends.insert(0, backend_module)


def get_module_from_dict(module_dict):
    module_type = module_dict["type"]
    backend = module_dict.get("backend", None)

    b = module_type.split(".")
    if len(b) > 1:
        backend = '.'.join(b[:-1])
        module_type = b[-1]

    if backend is not None:
        backend_module = importlib.import_module(backend)  # noqa: F
        backend = "backend_module"
    else:
        for i, backend in enumerate(backends):
            if module_type in dir(backend):
                backend = f"backends[{i}]"
                break
        else:
            raise ImportError(
                f"{module_type} not found in "
                f"{[b.__name__ for b in backends]}"
            )
    sig = inspect.signature(eval(f"{backend}.{module_type}")).parameters.keys()
    if "kwargs" in sig:
        module = (
            f"{backend}.{module_type}(**{repr(module_dict.get('args', {}))})"
        )
    else:
        # If module does not take kwargs, parse additional arguments to only
        # pass matching
        module = f"{backend}.{module_type}("
        for k in sig:
            val = module_dict.get("args", {}).get(k, None)
            if val is not None:
                module += f"{k}={repr(val)},"
        module += ")"
    return eval(module)


def module_factory(modules, **kwargs):
    """
    :param modules: a list of dictionaries describing the different modules
        the dictionary should contain the keys ["input", "output", "module"]
        and the "module" should be a dictionary with the keys:
        ["type", "args", "backend"]
    :return: A ComposableModule() from the configuration list
    """
    for module_dict in modules:
        module_dict["module"] = get_module_from_dict(module_dict)
    return ComposableModule(modules)

```

`hierarchical-nav/dommel_library/nn/modules.py`:

```py
""" Module to aggregate our own neural modules """
import torch
from torch import nn

from dommel_library.nn.activation import get_activation


class View(nn.Module):
    """nn.Module for the PyTorch view method"""

    def __init__(self, shape, **kwargs):
        """
        :param shape: New shape to reshape the data into, should be a tuple
        """
        nn.Module.__init__(self)
        self._shape = shape

    def forward(self, x):
        return x.view(x.data.size(0), *self._shape)


class Reshape(nn.Module):
    """nn.Module for the PyTorch reshape method"""

    def __init__(self, shape, **kwargs):
        """
        :param shape: New shape to reshape the data into, should be a tuple
        Unlike view, this one uses reshape. Might be required in cases the
        input is not contiguous, for example after broadcasting.
        """
        nn.Module.__init__(self)
        self._shape = shape

    def forward(self, x):
        x = x.reshape(x.data.size(0), *self._shape)
        return x


class MLP(nn.Module):
    """Multi Layer Perceptron Module"""

    def __init__(
        self,
        num_inputs,
        num_outputs,
        hidden_layers=None,
        activation="Activation",
        **kwargs
    ):
        """Initializes a multilayer perceptron. Note that the final layer
        DOES NOT have an activation, if you have subsequent computations,
        you should add these yourself!
        :param num_inputs: Single int representing the amount of inputs.
        :param num_outputs: Single int representing the amount of outputs.
        :param hidden_layers: Optional list indicating the amount of
        intermediate neurons
        :param activation: The activation used in the pipeline
        :param kwargs: Optional key word args for the activation function
        """
        nn.Module.__init__(self)

        self.num_inputs = num_inputs
        self.num_outputs = num_outputs
        self.hidden_layers = hidden_layers
       
        self.layers = nn.ModuleList()
        if hidden_layers:
            self.layers.append(nn.Linear(self.num_inputs, hidden_layers[0]))
            self.layers.append(get_activation(activation, **kwargs))
        else:
            self.layers.append(nn.Linear(self.num_inputs, self.num_outputs))
        if hidden_layers:
            for i in range(0, len(hidden_layers) - 1):
                self.layers.append(
                    nn.Linear(hidden_layers[i], hidden_layers[i + 1])
                )
                self.layers.append(get_activation(activation, **kwargs))
            self.layers.append(nn.Linear(hidden_layers[-1], num_outputs))

    def forward(self, x):
        for i in range(len(self.layers)):
            x = self.layers[i](x)
        return x



class Sample(nn.Module):
    def __init__(self, **kwargs):
        """ Initialize a Sample block, which will just execute the wrapped
        distribution objects sample method. This allows the inclusion of the
        sample step in our yaml builder.
        """
        nn.Module.__init__(self)

    def forward(self, dist):
        return dist.sample()


class Cat(nn.Module):
    def __init__(self, dim=1):
        """Wrapper around torch.cat
        :param dim: The dimension on which to cat. 0 is batch dim, 1 is first
        data dim.
        """
        nn.Module.__init__(self)
        self.dim = dim

    def forward(self, *x):
        return torch.cat(x, dim=self.dim)


class Sum(nn.Module):
    def __init__(self, dim=1):
        """Wrapper around torch.sum
        :param dim: The dimension on which to cat. 0 is batch dim, 1 is first
        data dim.
        """
        nn.Module.__init__(self)
        self.dim = dim

    def forward(self, *x):
        return torch.stack(x, dim=self.dim).sum(dim=self.dim)


class Broadcast(nn.Module):
    def __init__(self, shape, position_encode=False):
        """Expand a vector into spatial dimensions
           :param shape: Spatial dimensions in H x W
           :param position_encode: Add extra 2 channels encoding
           spatial position
        """
        nn.Module.__init__(self)
        self.shape = shape
        self.position_encode = position_encode

    def forward(self, x):
        s = x.shape
        expanded = x.unsqueeze(-1).unsqueeze(-1).expand(*s, *self.shape)

        if not self.position_encode:
            return expanded
        else:
            hs = torch.linspace(-1, 1, steps=self.shape[0])
            ws = torch.linspace(-1, 1, steps=self.shape[1])
            h, w = torch.meshgrid(hs, ws)
            positions = torch.stack([h, w])
            for i in reversed(range(len(s) - 1)):
                positions.unsqueeze(0)
                positions = positions.expand(s[i], *positions.shape)
            positions = positions.to(expanded.device)

            return torch.cat([expanded, positions], dim=-3)

```

`hierarchical-nav/dommel_library/nn/summary.py`:

```py
import numpy as np
import torch.nn as nn

from collections import OrderedDict


def count_macs(module, input, output):
    if type(input).__name__ == "QuantTensor":
        input = input.value

    if type(output).__name__ == "QuantTensor":
        output = output.value

    if isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
        # for each conv output we need # kernel ops + bias
        # divided by number of output channels
        output_size = output.nelement()
        kernel_ops = module.weight.nelement()
        if module.bias is not None:
            kernel_ops += module.bias.nelement()

        return output_size * kernel_ops // module.out_channels

    elif isinstance(module, (nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d)):
        if isinstance(module.kernel_size, tuple):
            kernel_ops = np.prod(module.kernel_size)
        else:
            kernel_ops = module.kernel_size ** (len(input[0].shape) - 2)
        return kernel_ops * output.nelement() // output.shape[1]

    elif isinstance(module, nn.Linear):
        batch_size = input[0].size(0)
        total_ops = module.in_features * module.out_features
        if module.bias is not None:
            total_ops += module.bias.nelement()
        return total_ops * batch_size

    elif isinstance(module, nn.LSTMCell):
        hidden_size = module.hidden_size
        input_size = module.input_size
        batch_size = input[0].size(0)

        total_ops = 0
        # f_t = sigmoid( W_f z + b_f)
        total_ops += hidden_size * \
            (input_size + hidden_size + 1) * batch_size
        # i_t = sigmoid(W_i z + b_i)
        total_ops += hidden_size * \
            (input_size + hidden_size + 1) * batch_size
        # g_t = tanh(W_C z + b_C)
        total_ops += hidden_size * \
            (input_size + hidden_size + 1) * batch_size
        # o_t = sigmoid (W_o z + b_t)
        total_ops += hidden_size * \
            (input_size + hidden_size + 1) * batch_size
        # C_t = f_t * C_t-1 + i_t * g_t
        total_ops += 2 * hidden_size * batch_size
        # h_t = o_t * tanh(C_t)
        total_ops += hidden_size * 2 * batch_size
        return total_ops

    return 0


def get_shape(tensors):
    if type(tensors).__name__ == "QuantTensor":
        tensors = tensors.value
    if isinstance(tensors, (list, tuple)):
        shapes = [get_shape(t) for t in tensors]
        if len(shapes) == 1:
            return shapes[0]
        return shapes
    elif isinstance(tensors, dict):
        shapes = [get_shape(t) for _, t in tensors.items()]
        if len(shapes) == 1:
            return shapes[0]
        return shapes
    elif tensors is None:
        return []
    else:
        return list(tensors.shape)


def get_elements(tensors):
    if type(tensors).__name__ == "QuantTensor":
        tensors = tensors.value
    if isinstance(tensors, (list, tuple)):
        total = 0
        for t in tensors:
            total += get_elements(t)
        return total
    elif isinstance(tensors, dict):
        total = 0
        for _, t in tensors.items():
            total += get_elements(t)
        return total
    elif tensors is None:
        return 0
    else:
        return tensors.nelement()


def summary(model, inputs):
    """ summary
    prints a summary of the passed model
    :param model: nn.Module with the model to be summarized
    :param inputs: tensor or TensorDict with inputs
    """
    def register_hook(module):
        def hook(module, input, output):
            class_name = str(module.__class__).split(".")[-1].split("'")[0]
            module_idx = len(summary)

            m_key = "%s-%i" % (class_name, module_idx + 1)
            kernel_size = getattr(module, "kernel_size", None)
            if kernel_size is not None:
                if isinstance(kernel_size, int):
                    kernel_size = (kernel_size,)
                fmt = "x".join(["{}"] * len(kernel_size)).format(*kernel_size)
                m_key = "%s-%s-%i" % (class_name, fmt, module_idx + 1)

            summary[m_key] = OrderedDict()
            summary[m_key]["input_shape"] = get_shape(input)
            summary[m_key]["output_shape"] = get_shape(output)

            summary[m_key]["num_inputs"] = get_elements(input)
            summary[m_key]["num_outputs"] = get_elements(output)

            params = 0
            for attr in dir(module):
                if attr.startswith("weight"):
                    weight = getattr(module, attr)
                    if hasattr(weight, "shape"):
                        params += np.prod(list(getattr(module, attr).shape))
                        summary[m_key]["trainable"] = weight.requires_grad
                if attr.startswith("bias"):
                    bias = getattr(module, attr)
                    if hasattr(bias, "shape"):
                        params += np.prod(list(bias.shape))
            summary[m_key]["num_params"] = params
            summary[m_key]["num_macs"] = count_macs(module, input, output)

        if (
            not isinstance(module, nn.Sequential)
            and not isinstance(module, nn.ModuleList)
            and not (module == model)
        ):
            hooks.append(module.register_forward_hook(hook))

    # create properties
    summary = OrderedDict()

    # add "Input" layer
    summary["Input"] = OrderedDict()
    summary["Input"]["output_shape"] = get_shape(inputs)
    summary["Input"]["num_outputs"] = get_elements(inputs)
    summary["Input"]["num_params"] = 0
    summary["Input"]["num_macs"] = 0

    hooks = []

    # register hook
    model.apply(register_hook)

    # make a forward pass
    # print(x.shape)
    model(inputs)

    # remove these hooks
    for h in hooks:
        h.remove()

    # format column widths
    layer_length = 20
    shape_length = 20
    for layer in summary:
        ll = len(layer)
        if ll + 2 > layer_length:
            layer_length = ll + 2
        sl = len(str(summary[layer]["output_shape"]))
        if sl + 2 > shape_length:
            shape_length = sl + 2
    line_length = layer_length + shape_length + 28

    print("-" * line_length)
    line_new = ("{:>" + str(layer_length) + "}  "
                "{:>" + str(shape_length) + "} {:>12} {:>12}")\
        .format("Layer (type)", "Output Shape", "Param #", "MACs #")
    print(line_new)
    print("=" * line_length)
    total_macs = 0
    total_params = 0
    total_output = 0
    trainable_params = 0
    for layer in summary:
        line_new = ("{:>" + str(layer_length) + "} "
                    "{:>" + str(shape_length) + "} {:>12} {:>12}")\
            .format(
            layer,
            str(summary[layer]["output_shape"]),
            "{0:,}".format(summary[layer]["num_params"]),
            "{0:,}".format(summary[layer]["num_macs"]),
        )
        total_params += summary[layer]["num_params"]
        total_macs += summary[layer]["num_macs"]
        total_output += summary[layer]["num_outputs"]
        if "trainable" in summary[layer]:
            if summary[layer]["trainable"] is True:
                trainable_params += summary[layer]["num_params"]
        print(line_new)

    # assume 4 bytes/number (float on cuda).
    total_input_size = get_elements(inputs) * 4.0 / (1024 ** 2.0)
    # x2 for gradients
    total_output_size = abs(2.0 * total_output * 4.0 / (1024 ** 2.0))
    total_params_size = abs(total_params * 4.0 / (1024 ** 2.0))
    total_size = total_params_size + total_output_size + total_input_size

    non_trainable_params = total_params - trainable_params
    print("=" * line_length)
    print("Total MACs: {0:,}".format(total_macs))
    print("Total params: {0:,}".format(total_params))
    print("Trainable params: {0:,}".format(trainable_params))
    print("Non-trainable params: {0:,}".format(non_trainable_params))
    print("-" * line_length)
    print("Input size (MiB): %0.2f" % total_input_size)
    print("Forward/backward pass size (MiB): %0.2f" % total_output_size)
    print("Params size (MiB): %0.2f" % total_params_size)
    print("Estimated Total Size (MiB): %0.2f" % total_size)
    print("-" * line_length)

```

`hierarchical-nav/dommel_library/nn/variational.py`:

```py
import torch
from torch import nn
from torch.nn import functional as F

from dommel_library.distributions import MultivariateNormal
from dommel_library.datastructs import TensorDict
from dommel_library.nn import MLP


class VariationalMLP(MLP):
    """ Variational Multi Layer Perceptron
    A fully connected neural network outputting mu and sigma for each output
    """

    def __init__(
        self,
        num_inputs,
        num_outputs,
        hidden_layers=None,
        sigma_scale=1.0,
        sigma_offset=0.0,
        activation="Activation",
        **kwargs
    ):
        """Creates a MLP with mu and sigma outputs
        :param num_inputs: Amount of inputs, single int.
        :param num_outputs: The size of the output _space_, single int.
        :param hidden_layers: Amount of hidden neurons, list. Optional.
        :param sigma_scale: How much the sigma is to be rescaled.
        :param sigma_offset: How much the sigma is to be shifted.
        :param activation: The activation function for this module.
        :param kwargs: Optional arguments for MLP.
        """
        MLP.__init__(
            self,
            num_inputs,
            2 * num_outputs,
            hidden_layers,
            activation,
            **kwargs
        )
        self.sigma_scale = sigma_scale
        self.sigma_offset = sigma_offset

    def forward(self, x):
        x = MLP.forward(self, x)
        mu = x[:, : int(self.num_outputs / 2)]
        sigma = x[:, int(self.num_outputs / 2):]
        sigma = (
            self.sigma_scale * (F.softplus(sigma) + 1e-6) + self.sigma_offset
        )
        return MultivariateNormal(mu, sigma)


class VariationalLayer(nn.Module):
    """Module that takes a vector and outputs 2 vectors: mu and sigma"""

    def __init__(
        self,
        num_inputs,
        num_outputs,
        sigma_scale=1.0,
        sigma_offset=0.0,
        **kwargs
    ):
        """
        Creates a single linear layer with variational output
        :param num_inputs: The amount of inputs. Single int.
        :param num_outputs: The amount of outputs. Single int.
        :param sigma_scale: How much sigma must be rescaled.
        :param sigma_offset: How much simga must be shifted after scaling.
        """
        nn.Module.__init__(self)
        self.num_inputs = num_inputs
        self.num_outputs = num_outputs
        self.sigma_scale = sigma_scale
        self.sigma_offset = sigma_offset
        self.linear_mu = nn.Linear(num_inputs, num_outputs)
        self.linear_sigma = nn.Linear(num_inputs, num_outputs)

    def forward(self, y):
        mu = self.linear_mu(y)
        sigma = self.linear_sigma(y)
        sigma = (
            self.sigma_scale * (F.softplus(sigma) + 1e-6) + self.sigma_offset
        )
        return MultivariateNormal(mu, sigma)


class VariationalGRU(nn.Module):
    """Variational wrapper around a GRU block"""

    def __init__(self, num_inputs, num_outputs, hidden_layers=None, **kwargs):
        """Initialize the module
        :param num_inputs: Length of the input vector.
        :param num_outputs: Desired output length.
        :param hidden_layers: Optional list of hidden neurons for the cells.
        """
        nn.Module.__init__(self)
        self.num_inputs = num_inputs
        self.num_outputs = num_outputs
        self.hidden_layers = hidden_layers

        self.cells = nn.ModuleList()
        if not hidden_layers:
            hidden_layers = []
            self.output = VariationalLayer(num_inputs, num_outputs)
        else:
            self.output = VariationalLayer(hidden_layers[-1], num_outputs)
            self.cells.append(nn.GRUCell(self.num_inputs, hidden_layers[0]))

        for i in range(0, len(hidden_layers) - 1):
            self.cells.append(
                nn.GRUCell(hidden_layers[i], hidden_layers[i + 1])
            )

    def forward(self, x, hidden):
        next_hidden = {}
        for i in range(len(self.cells)):
            gru = self.cells[i]
            # TODO allow to namespace this to avoid collisions?!
            key = "gru" + str(i)
            if hidden is None:
                h = torch.zeros(x.shape[0], self.hidden_layers[i]).to(x.device)
            else:
                h = hidden[key]
            x = gru(x, h)
            next_hidden[key] = x

        dist = self.output(x)
        return dist, next_hidden


class VariationalLSTM(nn.Module):
    """Variational version of a LSTM block"""

    def __init__(self, num_inputs, num_outputs, hidden_layers=None, **kwargs):
        """Initializes the Module
        :param num_inputs: Input length.
        :param num_outputs: Desired output length.
        :param hidden_layers: Optional list of hidden layer neurons.
        """
        nn.Module.__init__(self)
        self.num_inputs = num_inputs
        self.num_outputs = num_outputs
        self.hidden_layers = hidden_layers

        self.cells = nn.ModuleList()
        if not hidden_layers:
            hidden_layers = []
            self.output = VariationalLayer(num_inputs, num_outputs)
        else:
            self.output = VariationalLayer(hidden_layers[-1], num_outputs)
            self.cells.append(nn.LSTMCell(self.num_inputs, hidden_layers[0]))

        for i in range(0, len(hidden_layers) - 1):
            self.cells.append(
                nn.LSTMCell(hidden_layers[i], hidden_layers[i + 1])
            )

    def forward(self, x, hidden):
        next_hidden = TensorDict({})
        for i in range(len(self.cells)):
            lstm = self.cells[i]
            # TODO allow to namespace this to avoid collisions?!
            hkey = "hidden" + str(i)
            ckey = "cell" + str(i)
            if hidden is None:
                h = torch.zeros(x.shape[0], self.hidden_layers[i]).to(x.device)
                c = torch.zeros(x.shape[0], self.hidden_layers[i]).to(x.device)
            else:
                h = hidden[hkey]
                c = hidden[ckey]

            x, c = lstm(x, (h, c))

            next_hidden[hkey] = x
            next_hidden[ckey] = c

        dist = self.output(x)
        return dist, next_hidden

```

`hierarchical-nav/dommel_library/setup.py`:

```py
from setuptools import setup, find_packages

with open("requirements.txt", "r") as req:
    requirements = req.read().splitlines()

setup(
    name="dommel_library",
    version="0.4.0",
    packages=find_packages(),
    install_requires=requirements,
)

```

`hierarchical-nav/dommel_library/train/__init__.py`:

```py
from dommel_library.train.loss_factory import loss_factory
from dommel_library.train.optimizer_factory import optimizer_factory
from dommel_library.train.trainer import Trainer

__all__ = ["loss_factory", "optimizer_factory", "Trainer"]

```

`hierarchical-nav/dommel_library/train/loss_factory.py`:

```py
from dommel_library.train.losses.elbo import NLLLoss, KLLoss
from dommel_library.train.losses.constraint import ConstraintLoss
from dommel_library.train.losses.loss_aggregate import LossAggregate

import torch.nn as nn  # noqa: F401

import logging

logger = logging.getLogger(__name__)


def loss_factory(**losses):
    """
    Factory method for creating a loss aggregate given a list of losses
    :param losses: dict mapping a name to a loss configuration specified by
    a dict with the keys: 'key', 'target', 'type'
    :return: A LossAggregate() object
    """
    for name, loss_dict in losses.items():
        loss_dict["name"] = name
        if loss_dict["type"] == "NLL":
            loss_dict["loss"] = NLLLoss(**loss_dict.get("args", {}))
        elif loss_dict["type"] == "KL":
            loss_dict["loss"] = KLLoss(**loss_dict.get("args", {}))
        elif loss_dict["type"] == "Constraint":
            # constraint on an other loss defined in `for`
            loss_to_constrain_dict = loss_dict.get("for", None)
            if loss_to_constrain_dict is None:
                logger.info(
                    "Specify a loss to constrain in `for` attribute"
                )
                raise ValueError

            loss_to_constrain_dict.update(
                key="prediction",
                target="expectation",
            )
            loss_to_constrain = loss_factory(**{
                name + "_constraint": loss_to_constrain_dict
            })

            constraint_params = loss_dict.get("args", None)
            if constraint_params is None:
                logger.info(
                    "Trying to create a constraint without setting"
                    " the parameters"
                )
                raise ValueError

            loss_dict["loss"] = ConstraintLoss(
                loss_to_constrain,
                name,
                **loss_dict.get("args", {}),
            )

            # by default don't batch average the Constraint,
            # as this should be handled in loss_to_constrain
            if "batch_average" not in loss_dict.keys():
                loss_dict["batch_average"] = False
        else:
            try:
                loss_module = "nn." + loss_dict["type"] + "Loss"
                module = (
                    loss_module
                    + "(**"  # noqa: W503
                    + repr(loss_dict.get("args",
                                         {"reduction": "sum"}))  # noqa: W503
                    + ")"  # noqa: W503
                )
                loss_dict["loss"] = eval(module)
            except Exception:
                logger.info("Loss type not recognized: " + loss_dict["type"])
                raise ValueError

    return LossAggregate(**losses)

```

`hierarchical-nav/dommel_library/train/losses/README.md`:

```md
# Losses 

The `dommel.losses.loss_factory.loss_factory(loss_dicts)` method can be used to create an aggregate loss function from a list of dictionaries describing the loss. 
Each loss dictionary should have the following keys: 
required:
- `type`: The type of loss, can be `MSE`, `KL`, `NLL` or `Constraint`
- `key`: contains the name of the predicted tensor
- `target`: contains the name of the target ground truth tensor
optional: 
- `weight`: weight of a loss term in the global loss
Only if `type` == `Constraint`:
- `constraint_parameters`: A dictionary containing the keyword arguments of the `Constraint` class, in order to create this directly from the config file. 
- `reconstruction`: The loss that should be used for reconstruction in the constraint. Can be `MSE` or `NLL`.

An example for this use is shown in `examples/mnist_vae.py` and `examples/mnist_ae.py`.

The loss for GECO can be defined as: 
```yaml
loss:
    - type: KL
      key: posterior 
      value: std_normal 
    - type: Constraint
      key: image
      value: image 
      reconstruction: NLL
      constraint_parameters: 
          tolerance: 180
```

___
A custom method for defining loss functions that can work together with the `Trainer` class. 
The loss function is able to compute a loss value and provide logs in the desired format to the trainer. 

The `CustomLoss` should therefore:
 - inherit from `Loss`
 - implement the `__call__` method which computes the loss value. 
 - By default the `logs` property will return an empty log object. A log object is used to log to tensorboard. A scalar should have the type `"scalar"` and an image the type `"image"`. 
 - The `trainable_parameters` property will return an empty list by default, if the loss functions should use the gradient, it should override this propety, as well as the `post_backprop()` method.


```

`hierarchical-nav/dommel_library/train/losses/__init__.py`:

```py
from dommel_library.train.losses.loss import Loss
from dommel_library.train.losses.log import Log

__all__ = ["Loss", "Log"]

```

`hierarchical-nav/dommel_library/train/losses/constraint.py`:

```py
import torch
import torch.nn.functional as F
from torch import nn
from torch.autograd import Function

from dommel_library.train.losses import Loss, Log

from dommel_library.datastructs.tensor_dict import TensorDict


class ConstraintLoss(Loss):
    def __init__(self, internal_loss, name, **constraint_dict):
        # name for logging
        self._name = name

        self._internal_loss = internal_loss
        self._constraint = Constraint(**constraint_dict)

        self._error = None
        self._last_value = None

    def __call__(self, prediction, expectation):
        self._error = self._internal_loss(
            TensorDict({"prediction": prediction}),
            TensorDict({"expectation": expectation}),
        )
        # [0] because from tensor of dim 1 to dim 0
        self._last_value = self._constraint(self._error)[0]
        return self._last_value

    @property
    def logs(self):
        log = Log()
        log.add(
            f"{self._name}/unconstrained",
            self._error.cpu().detach().numpy(),
            "scalar",
        )
        log.add(
            f"{self._name}/constrained",
            self._last_value.cpu().detach().numpy(),
            "scalar",
        )
        log.add(
            f"{self._name}/lambda",
            self._constraint.multiplier.cpu().detach().numpy(),
            "scalar",
        )
        log.add(
            f"{self._name}/tolerance",
            self._constraint.tolerance,
            "scalar",
        )
        return log

    def parameters(self):
        return list(self._constraint.parameters())

    def post_backprop(self):
        self._constraint.adjust_tolerance()
        try:
            self._internal_loss.post_backprop()
            # will fail if not a dommel loss
        except AttributeError:
            pass

    def to(self, *args, **kwargs):
        self._constraint.device = args[0]
        self._constraint.lambd_min = self._constraint.lambd_min.to(
            *args, **kwargs
        )
        self._constraint.lambd_max = self._constraint.lambd_max.to(
            *args, **kwargs
        )
        self._constraint.multiplier = self._constraint.multiplier.to(
            *args, **kwargs
        )
        self._constraint = self._constraint.to(*args, **kwargs)
        return self


class Constraint(nn.Module):
    def __init__(
        self,
        tolerance,
        lambda_min=0.0,
        lambda_max=20.0,
        lambda_init=1.0,
        alpha=0.99,
        adjust_tolerance=False,
        adjust_frequency=1000,
        adjust_tangent_threshold=0.2,
        adjust_tolerance_factor=1.01,
        device="cpu",
        **kwargs,
    ):
        nn.Module.__init__(self)
        self.moving_average = None
        self.tolerance = tolerance
        self.device = device
        self.lambd_min = torch.tensor([lambda_min], dtype=torch.float).to(
            self.device
        )
        self.lambd_max = torch.tensor([lambda_max], dtype=torch.float).to(
            self.device
        )
        self.multiplier = torch.tensor([lambda_init], dtype=torch.float).to(
            self.device
        )
        self.lambd = nn.Parameter(self._inv_squared_softplus(self.multiplier))
        self.alpha = alpha
        self.clamp = ClampFunction()
        self.prev_grad = None
        self.last_grad = None
        self.delta = adjust_frequency
        self.tangent = 0.0
        self.tangent_threshold = adjust_tangent_threshold
        self.tolerance_factor = adjust_tolerance_factor
        self.tolerance_fixed = not adjust_tolerance
        self.i = 0

        # variables required for online sleep
        self.constraint_is_hit = False
        self.constraint_has_been_hit = False

    def forward(self, value):
        constraint = value - self.tolerance
        self.constraint_is_hit = constraint < 0
        self.constraint_has_been_hit = self.constraint_has_been_hit or (
            constraint < 0)

        with torch.no_grad():
            if self.moving_average is None:
                self.moving_average = constraint
            else:
                self.moving_average = (
                    self.alpha * self.moving_average
                    + (1 - self.alpha) * constraint  # noqa: W503, W504
                )

        cost = constraint + (self.moving_average - constraint).detach()

        # we use squared softplus as in
        # https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules
        # /optimization_constraints.py
        # we also clamp the resulting values
        self.multiplier = self.clamp.apply(
            F.softplus(self.lambd) ** 2, self.lambd_min, self.lambd_max
        )

        return self.multiplier * cost

    def get_multiplier(self):
        return self.multiplier.item()

    def _inv_squared_softplus(self, x):
        sqrt = torch.sqrt(x)
        return torch.log(torch.exp(sqrt) - 1.0)

    def adjust_tolerance(self):
        # adjust tolerance
        if self.tolerance_fixed:
            return

        if self.i % self.delta == 0:
            if self.prev_grad is None:
                self.prev_grad = self.lambd.grad.item()
            elif self.last_grad is None:
                self.last_grad = self.lambd.grad.item()
            else:
                self.tangent = (self.last_grad - self.prev_grad) / self.delta
                self.prev_grad = self.last_grad
                self.last_grad = self.lambd.grad.item()
                # TODO which threshold and which tolerance adjustment?
                if self.last_grad > 0:
                    # no longer adjust tolerance once the loss reaches the
                    # current tolerance threshold
                    self.tolerance_fixed = True
                elif self.tangent < self.tangent_threshold:
                    self.tolerance *= self.tolerance_factor

        self.i += 1


class ClampFunction(Function):
    """
    Clamp a value between min and max.
    When the gradients push the value further away from the [min,max] range,
    set to zero
    When the gradients push the value back in the [min,max] range,
    let them flow through
    """

    @staticmethod
    def forward(ctx, lambd, min_value, max_value):
        ctx.save_for_backward(lambd, min_value, max_value)
        if lambd < min_value:
            return min_value
        elif lambd > max_value:
            return max_value
        else:
            return lambd

    @staticmethod
    def backward(ctx, lambd_grad):
        lambd, min_value, max_value = ctx.saved_tensors

        if lambd < min_value and lambd_grad < 0.0:
            grad = torch.tensor([0.0], device=lambd_grad.device)
        elif lambd > max_value and lambd_grad > 0.0:
            grad = torch.tensor([0.0], device=lambd_grad.device)
        else:
            grad = -lambd_grad
        return grad, None, None

```

`hierarchical-nav/dommel_library/train/losses/elbo.py`:

```py
import torch.nn as nn
from torch.distributions.kl import kl_divergence
from dommel_library.distributions import StandardNormal


class NLLLoss(nn.Module):
    def __init__(self, **kwargs):
        nn.Module.__init__(self)

    def forward(self, predicted, expected):
        """
        Computes the negative log likelihood
        :param predicted: should be a pytorch distributions object for the
            predicted distribution that implements the log_prob method
        :param expected: ground truth value
        :return: the negative log likelihood
        """
        log_prob = predicted.log_prob(expected)
        return -log_prob.sum()


class KLLoss(nn.Module):
    def __init__(self, **kwargs):
        nn.Module.__init__(self)

    def forward(self, predicted, expected=None):
        """
        KL divergence between the predicted distribution and the expected
            distribution
        :param predicted: pytorch distribution object
        :param expected: pytorch distribution object, if none is provided, a
            standard normal distribution is used
        :return: KL divergence
        """
        if expected is None:
            expected = StandardNormal(
                predicted.mean.shape).to(predicted.device)

        kl = kl_divergence(predicted, expected)
        return kl.sum()

```

`hierarchical-nav/dommel_library/train/losses/log.py`:

```py
import numpy as np

try:
    import wandb
except:  # noqa: E722
    pass


class Log:
    """
    A logger construct that stores intermediate values during training in a
    dictionary of lists.
    """

    def __init__(self):
        self._logs = {}

    @property
    def logs(self):
        return self._logs

    def add(self, key, value, data_type):
        """
        Add a new log to the logger object
        :param key: Name of the value
        :param value: Actual value to log
        :param data_type: Type of data, can be "scalar" or "image"
        """
        prev = self._logs.get(key, None)
        old_value = []
        if prev is not None:
            old_value = prev["value"]
            if prev["type"] != data_type:
                # Incompatible data types
                raise ValueError

        self._logs[key] = {
            "value": old_value + [value],
            "type": data_type,
        }

    def to_writer(self, writer, epoch):
        """
        Store logs of this objects in a specific tensorboard writer
        :param writer: the writer object
        :param epoch: The epoch
        """
        for key, value in self._logs.items():
            if value["type"] == "image":
                # only show the last image, otherwise tensorboard becomes too
                # large
                writer.add_image(key, value["value"][-1], epoch)
            elif value["type"] == "scalar":
                # Average the mean value for this scalar over the entire epoch
                writer.add_scalar(
                    key, np.mean(np.array(value["value"])), epoch
                )
            elif value["type"] == "text":
                writer.add_text(key, value["value"][0], epoch)

    def to_wandb(self, prefix):
        wandb_dict = {}
        for key, value in self._logs.items():
            if value["type"] == "image":
                wandb_dict[f"{prefix}/{key}"] = wandb.Image(value["value"][-1])
            if value["type"] == "scalar":
                # Average the mean value for this scalar over the entire epoch
                wandb_dict[f"{prefix}/{key}"] = np.mean(
                    np.array(value["value"])
                )
        return wandb_dict

    def __add__(self, other):
        """
        Add two log objects together
        :param other: other log object
        """
        for k, v in other.logs.items():
            self.add(k, v["value"], v["type"])
        return self

    def __iadd__(self, other):
        """
        Add two log objects together
        :param other: other log object
        """
        return self + other

```

`hierarchical-nav/dommel_library/train/losses/loss.py`:

```py
from dommel_library.train.losses.log import Log


class Loss:
    def __call__(self, predicted, expected):
        """
        Implements the loss function
        :param predicted: Dictionary of predicted tensors
        :param expected: Dictionary of ground_truth tensors
        :return: loss value on which .backward() can be called
        """
        raise NotImplementedError

    @property
    def logs(self):
        """
        :return: A Log object containing information of the training step
        """
        return Log()

    def parameters(self):
        """
        :return: A list of trainable parameters
        """
        return []

    def post_backprop(self):
        """
        If things need to be done after the backwards pass to the loss function
        e.g. updating tolerances in GECO loss functions...
        :return: nothing
        """
        pass

```

`hierarchical-nav/dommel_library/train/losses/loss_aggregate.py`:

```py
from dommel_library.train.losses.loss import Loss
from dommel_library.train.losses.log import Log


class LossAggregate(Loss):
    def __init__(self, **losses):
        """
        :param losses: dict of losses dictionaries.
            required keys: ['key', 'target', 'type', 'name']
            optional keys: ['weight']
            additional keys specific to the loss function can be provided.
        """
        self._losses = losses
        self._total_loss = 0
        self._log_values = {}

    def __call__(self, predictions, expectations):
        """
        Implements the loss function
        :param predicted: Dictionary of predicted tensors
        :param expected: Dictionary of ground_truth tensors
        :return: loss value on which .backward() can be called
        """
        self._total_loss = 0
        for _, loss_dict in self._losses.items():
            p_key = loss_dict["key"]
            t_key = loss_dict["target"]
            loss_name = loss_dict["name"]
            # print('loss aggegate p and t keys', p_key, t_key)
            # print(predictions.keys(), expectations.keys())
            prediction = predictions.get(p_key, None)
            target = expectations.get(t_key, predictions.get(t_key, None))

            # TODO: pass **kwargs, does not work for MSE
            loss_value = loss_dict["loss"](prediction, target)
            batch_average = loss_dict.get("batch_average", True)
            if batch_average:
                if len(predictions.shape) > 1:
                    loss_value = loss_value / predictions.shape[0]

            self._log_values[f"Loss/{loss_name}"] = loss_value

            weight = loss_dict.get("weight", 1)
            self._total_loss += weight * loss_value

        return self._total_loss

    def post_backprop(self):
        for _, loss_dict in self._losses.items():
            # will fail if the function is not a dommel Loss
            try:
                loss_dict["loss"].post_backprop()
            except AttributeError:
                continue

    def parameters(self):
        trainable_parameters = []
        for _, loss_dict in self._losses.items():
            # will fail if the function is not a dommel Loss
            try:
                trainable_parameters += loss_dict["loss"].parameters()
            except AttributeError:
                continue
        return trainable_parameters

    @property
    def logs(self):
        """
        :return: the logs for all components in this loss function
        """
        log = Log()
        # log all separate elements of loss
        for k, v in self._log_values.items():
            log.add(k, v.detach().cpu().numpy(), "scalar")
        log.add("Loss/loss", self._total_loss.detach().cpu().numpy(), "scalar")
        # If the implemented losses have additional logs
        for _, loss_dict in self._losses.items():
            # will fail if the function is not a dommel Loss
            try:
                log += loss_dict["loss"].logs
            except AttributeError:
                continue
        return log

    def to(self, *args, **kwargs):
        """
        Move all sub models to the desired device
        """
        for k, loss_dict in self._losses.items():
            try:
                # will fail if the loss does not have a .to() method
                loss_dict["loss"] = loss_dict["loss"].to(*args, **kwargs)
            except AttributeError:
                continue
        return self

```

`hierarchical-nav/dommel_library/train/optimizer_factory.py`:

```py
import torch.optim  # noqa: F401


def optimizer_factory(parameters, type, lr, **kwargs):
    """
    Factory method to construct an optimizer
    :param parameters: Trainable parameters
    :param optimizer_type: Name of the optimizer as defined in torch.optim
    :param lr: Learning rate for the chosen optimizer
    :return: an optimizer object
    """
    optimizer = eval("torch.optim." + type + "(parameters, lr=lr, **kwargs)")
    return optimizer

```

`hierarchical-nav/dommel_library/train/trainer.py`:

```py
import pathlib
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from dommel_library.train.losses import Log
from dommel_library.datastructs import Dict, TensorDict
from dommel_library.modules.visualize import visualize_sequence

from tqdm import tqdm
from numbers import Number

import logging
import numpy as np

logger = logging.getLogger(__name__)


class Trainer:
    def __init__(
        self,
        model,
        train_dataset,
        loss,
        optimizer,
        log_dir,
        val_dataset=None,
        scheduler=None,
        batch_size=10,
        device="cpu",
        log_epoch=1,
        log_debug=False,
        save_epoch=500,
        vis_epoch=None,
        vis_args=None,
        vis_batch_size=None,
        num_workers=0,
        verbose=True,
        clip_grad_norm=None,
        logging_platform="tensorboard",
        wandb_entity="dommel_library",
        experiment_config=None,
        ** kwargs,
    ):
        """
        Create a Trainer object
        :param model: The model to optimize.
        :param train_dataset: The train dataset to optimize on.
        Must be dommel Dataset.
        :param loss: The loss function to optimize.
        Must be a dommel LossAggregate.
        :param optimizer: The Pytorch optimizer.
        :param log_dir: Path to store the logs and models.
        :param val_dataset: Optionally, a validation dataset.
        :param scheduler: Optionally, a learning rate scheduler.
        :param batch_size: The mini-batch size for iterating the train set.
        :param device: The device to load the tensors and model
        on during training.
        :param log_epoch: After each how many epochs to log the loss.
        :param log_debug: Flag indicating whether to log grad and weight
        statistics for debugging.
        :param save_epoch: After each how many epochs to save the model.
        :param vis_epoch: After each how many epochs to log visualizations.
        This can be single number (i.e. similar to log_epoch), or a dict like
        {"before_rate": 1, "n": 10, "after_rate": 10}. In this example before
        the n=10th epoch every epoch is visualized, and after that each 10th.
        This to mitigate the overhead of extensive visualization in the logs.
        Default behavior is to log each log_epoch first 10 and then visualize
        10 times less frequent.
        :param vis_batch_size: Optionally, a different batch size for the
        batch that is visualized in the logs.
        :param num_workers: The number of workers of the DataLoaders
        :param verbose: Flag indicating whether to print tqdm.
        :param clip_grad_norm: Gradient clipping value, no gradient clipping
        when value is None
        :param logging_platform: Platform to use for logging the progress of
        training. Options are ["tensorboard", "wandb"]
        :param wandb_entity: Wandb entity/team
        :param experiment_config: experiment config dict to store in wandb
        :param kwargs: Other arguments.
        """
        self._device = device

        self._model = model.to(self._device)
        self._loss = loss.to(self._device)
        self._optimizer = optimizer
        self._scheduler = scheduler

        self._train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            drop_last=True,
        )

        # Create directories for logging
        log_dir = pathlib.Path(log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)

        self._model_dir = log_dir / "models"
        self._model_dir.mkdir(parents=True, exist_ok=True)
        self._optimizer_dir = log_dir / "optimizers"
        self._optimizer_dir.mkdir(parents=True, exist_ok=True)

        self._log_epoch = log_epoch
        self._log_debug = log_debug
        if isinstance(vis_epoch, dict):
            self._vis_epoch = Dict(vis_epoch)
        elif isinstance(vis_epoch, Number):
            self._vis_epoch = Dict(
                {"before_rate": vis_epoch, "n": 0, "after_rate": vis_epoch}
            )
        else:
            self._vis_epoch = Dict(
                {
                    "before_rate": self._log_epoch,
                    "n": self._log_epoch * 10,
                    "after_rate": self._log_epoch * 10,
                }
            )

        self._save_epoch = save_epoch

        self._vis_args = vis_args
        if self._vis_args is None:
            self._vis_args = dict()

        if not vis_batch_size:
            vis_batch_size = batch_size
        else:
            vis_batch_size = min(batch_size, vis_batch_size)
        self._visualize_batch = dict()
        self._visualize_batch["train"] = TensorDict(
            next(iter(self._train_loader))
        ).to(self._device)[0:vis_batch_size, ...]

        self._val_loader = None
        if val_dataset is not None:
            self._val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=num_workers,
                drop_last=True,
            )
            self._visualize_batch["val"] = TensorDict(
                next(iter(self._val_loader))
            ).to(self._device)[0:vis_batch_size, ...]

        self._tensorboard_logging = logging_platform == "tensorboard"
        if not self._tensorboard_logging:
            try:
                import wandb
                run = log_dir.name
                project = log_dir.parent.name
                wandb.init(project=project, entity=wandb_entity,
                           name=run, config=experiment_config)
                wandb.watch(self._model)
                self._log_callback = self._wandb_log_callback
            except Exception as err:
                # revert to tensorboard logging
                logger.warning(
                    f"Failed to initialize wandb: {err}, "
                    f"reverting to tensorboard logging.")
                self._tensorboard_logging = True

        if self._tensorboard_logging:
            train_log_dir = log_dir / "train"
            train_log_dir.mkdir(parents=True, exist_ok=True)
            self._train_writer = SummaryWriter(log_dir=train_log_dir)
            if self._val_loader is not None:
                val_log_dir = log_dir / "val"
                val_log_dir.mkdir(parents=True, exist_ok=True)
                self._val_writer = SummaryWriter(log_dir=val_log_dir)

            self._log_callback = self._tensorboard_log_callback

        self._verbose = verbose

        # Allow for gradient clipping if desired
        if clip_grad_norm is not None:
            self._clip = lambda x: nn.utils.clip_grad_norm_(
                x, max_norm=clip_grad_norm
            )
        else:
            # Avoid the if-check in _epoch -> map it to a no-op function
            self._clip = lambda x: x

    def _epoch(self):
        """
        Execute a single epoch of training
        :return: Log object containing information of the epoch step
        """
        logs = Log()
        self._model.train()
        for input_dict in tqdm(self._train_loader, disable=not self._verbose):
            input_dict = TensorDict(input_dict).to(self._device)
            self._optimizer.zero_grad()
            output_dict = self._model(input_dict)
            loss = self._loss(output_dict, input_dict)
            loss.backward()
            self._clip(self._model.parameters())
            self._optimizer.step()
            self._loss.post_backprop()
            logs += self._loss.logs
        return logs

    def _validate(self):
        """
        Execute a single pass over the validation set
        :return: Log object containing information of the validation step
        """
        logs = Log()
        self._model.eval()
        with torch.no_grad():
            for input_dict in tqdm(
                self._val_loader, disable=not self._verbose
            ):
                input_dict = TensorDict(input_dict).to(self._device)
                output_dict = self._model(input_dict)
                _ = self._loss(output_dict, input_dict)
                logs += self._loss.logs
        return logs

    def _save_model_callback(self, epoch):
        """
        Is called every epoch to store the model if epoch is a multiple of
        self._save_epoch
        :param epoch: current epoch during training
        """
        if epoch % self._save_epoch == 0:
            torch.save(
                self._model.state_dict(),
                self._model_dir / "model-{:04d}.pt".format(epoch),
            )
            torch.save(
                self._optimizer.state_dict(),
                self._optimizer_dir / "optim-{:04d}.pt".format(epoch),
            )
            if self._scheduler:
                torch.save(
                    self._scheduler.state_dict(),
                    self._optimizer_dir / "scheduler-{:04d}.pt".format(epoch),
                )

    def _log_visualization(self, train_logs, val_logs, epoch):
        """
        This method is called right before logging to tensorboard. This is
        where a user can create custom visualizations and add those to the
        log.
        :param train_logs:
        :param val_logs:
        :return: train_logs and val_logs with the new logs
        """
        with torch.no_grad():
            train_output = self._model(self._visualize_batch["train"])

        pred_images = visualize_sequence(train_output, **self._vis_args)
        for k, img in pred_images.items():
            train_logs.add("prediction/" + k, img, "image")

        if self._val_loader:
            with torch.no_grad():
                val_output = self._model(self._visualize_batch["val"])

            pred_images = visualize_sequence(val_output, **self._vis_args)
            for k, img in pred_images.items():
                val_logs.add("prediction/" + k, img, "image")

        return train_logs, val_logs

    def _tensorboard_log_callback(
        self, train_logs=None, val_logs=None, epoch=0
    ):
        """
        Generic tensorboard logger.
        Receives a dictionary with key: ( value, type )
        :param train_logs: logs should be written as above
        :param val_logs: logs should be written as above
        :param epoch: epoch number
        """
        vis_epoch = (
            self._vis_epoch.before_rate
            if epoch < self._vis_epoch.n
            else self._vis_epoch.after_rate
        )
        if epoch % vis_epoch == 0:
            # Get visualization on eval-batch
            train_logs, val_logs = self._log_visualization(
                train_logs, val_logs, epoch
            )

        if epoch % self._log_epoch == 0:
            if train_logs is not None:
                train_logs.to_writer(self._train_writer, epoch)
            if val_logs is not None:
                val_logs.to_writer(self._val_writer, epoch)

            # Visualize grads and values of the parameters
            if self._log_debug:
                for name, p in self._model.named_parameters():
                    self._train_writer.add_histogram(
                        f"{name.replace('.','/')}/value", p, epoch
                    )
                    self._train_writer.add_histogram(
                        f"{name.replace('.','/')}/grad", p, epoch
                    )

            # Visualize learning rate if there is a scheduler
            if self._scheduler:
                for param_group in self._optimizer.param_groups:
                    self._train_writer.add_scalar(
                        "learning_rate/", param_group["lr"], epoch
                    )

    def _wandb_log_callback(self, train_logs=None, val_logs=None, epoch=0):
        import wandb

        vis_epoch = (
            self._vis_epoch.before_rate
            if epoch < self._vis_epoch.n
            else self._vis_epoch.after_rate
        )
        if epoch % vis_epoch == 0:
            # Get visualization on eval-batch
            train_logs, val_logs = self._log_visualization(
                train_logs, val_logs, epoch
            )

        wandb_logs = {}
        if epoch % self._log_epoch == 0:
            if train_logs is not None:
                wandb_logs.update(train_logs.to_wandb(prefix="train"))
            if val_logs is not None:
                wandb_logs.update(val_logs.to_wandb(prefix="val"))

        # Visualize learning rate if there is a scheduler
        if self._scheduler:
            for param_group in self._optimizer.param_groups:
                wandb_logs.update(
                    {f"learning_rate/{param_group['lr']}": epoch}
                )

        wandb_logs["epoch"] = epoch
        wandb.log(wandb_logs)

    def _initial_log(self, start_epoch):
        # compute initial batch and log this
        # also visualize ground truth
        with torch.no_grad():
            train_logs = Log()
            output_dict = self._model(self._visualize_batch["train"])
            _ = self._loss(output_dict, self._visualize_batch["train"])
            train_logs += self._loss.logs

            # visualize ground truth as well
            dataset_images = visualize_sequence(
                self._visualize_batch["train"], **self._vis_args
            )
            for k, img in dataset_images.items():
                train_logs.add("ground_truth/" + k, img, "image")

            val_logs = None
            if self._val_loader:
                self._model.eval()
                val_logs = Log()
                output_dict = self._model(self._visualize_batch["val"])
                _ = self._loss(output_dict, self._visualize_batch["val"])
                val_logs += self._loss.logs

                dataset_images = visualize_sequence(
                    self._visualize_batch["val"], **self._vis_args
                )
                for k, img in dataset_images.items():
                    val_logs.add("ground_truth/" + k, img, "image")

            self._log_callback(train_logs, val_logs, start_epoch)

    def _schedule_callback(self, train_logs, val_logs):
        """
        Callback function for scheduling the optimizer learning rate
        :param train_logs: training logs can be used to determine new lr value
        :param val_logs: validation logs can be used to determine new lr value
        """
        if self._scheduler:
            self._scheduler.step()

    def load(self, checkpoint_epoch):
        """
        Load model and trainer
        :param checkpoint_epoch: Epoch from which to restore
        """
        model_path = self._model_dir / (f"model-{checkpoint_epoch:04d}.pt")

        models = [m for m in self._model_dir.glob("*.pt")]
        if model_path not in models:
            logger.warning(
                f"Model epoch is not stored, is it a multiple "
                f"of {self._save_epoch}?"
            )
            model_path = sorted(models)[-1]

        # load model weights
        model_state_dict = torch.load(model_path)
        self._model.load_state_dict(model_state_dict)

        # load optimizer weights
        optim_path = sorted([o for o in self._optimizer_dir.glob("*.pt")])[-1]
        optim_state_dict = torch.load(optim_path)
        self._optimizer.load_state_dict(optim_state_dict)

        # load scheduler weights
        if self._scheduler:
            scheduler_state_dict = torch.load(
                str(optim_path).replace("optim", "scheduler")
            )
            self._scheduler.load_state_dict(scheduler_state_dict)
        return int(str(model_path).split("-")[-1][:-3])

    def train(self, num_epochs, start_epoch=0):
        """
        Training loop
        :param num_epochs: number of epochs to train
        :param start_step: The step to start at, relevant for continuing
            training
        :return: nothing
        """
        # load a saved model
        if start_epoch != 0:
            start_epoch = self.load(start_epoch)

        # do initial logging (i.e. visualizing ground truth etc.)
        self._initial_log(start_epoch)

        # Do the train loop
        for epoch in range(start_epoch + 1, num_epochs + 1):
            logger.info(f"{epoch}/{num_epochs}")
            train_logs = self._epoch()
            logger.info(
                "Train loss: {}".format(
                    np.array(train_logs.logs["Loss/loss"]["value"])
                    .flatten()
                    .mean(),
                )
            )

            val_logs = None
            if self._val_loader:
                val_logs = self._validate()
                logger.info(
                    "Validation loss: {}".format(
                        np.array(val_logs.logs["Loss/loss"]["value"])
                        .flatten()
                        .mean(),
                    )
                )

            self._schedule_callback(train_logs, val_logs)
            self._log_callback(train_logs, val_logs, epoch)
            self._save_model_callback(epoch)

```

`hierarchical-nav/dreamer_mg/LICENSE`:

```
MIT License

Copyright (c) 2021 seolhokim

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`hierarchical-nav/dreamer_mg/README.md`:

```md
# EasyDreamer: A Simplified Version of the Dreamer Algorithm with Pytorch

## Introduction

In this repository, we've implemented a simplified version of the Dreamer algorithm, which is explained in detail in the paper [Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603). The main goal of Dreamer is to train a model that helps agents perform well in environments with high sample efficiency. We have implemented our version of Dreamer using PyTorch, which simplifies the process and makes the model more accessible to researchers and practitioners who are already familiar with the PyTorch framework. With this implementation, they can gain a deeper understanding of how the algorithm works and test their own ideas more efficiently, contributing to the advancement of research in this field.

We have also included a re-implementation of Plan2Explore, a model-based exploration method introduced in the paper [Planning to Explore via Self-Supervised World Models](https://arxiv.org/abs/2005.05960). Plan2Explore is designed to improve generalization about the model without any task-relevant information by using an unsupervised learning approach. Our PyTorch implementation of Plan2Explore is available in this repository.

### Differences from other implementations

Our implementation of Dreamer differs from others in several ways. Firstly, we separate the recurrent model from the other models to gain a better understanding of how deterministic processing works. Secondly, we align the naming conventions used in our implementation with those in the paper. Furthermore, modules are trained following the same pseudo code as outlined in the original Dreamer paper. Thirdly, we remove overshooting, which was crucial in Dreamer-v1 and model-based approaches but is no longer mentioned in Dreamer-v2 and v3, and is even omitted from official implementations. Lastly, we use a single-step lambda value calculation, which enhances readability at the expense of performance.


<hr/>

## Installation

To install the required dependencies, run the following command:

```
pip install -r requirements.txt
```

<hr/>

## run

To run the training process, use the following command:

### Dreamer
```
python main.py --config dmc-walker-walk
```
### Plan2Explore
```
python main.py --config p2e-dmc-walker-walk
```
<hr/>

## Architecture

### Dreamer
```
┌── dreamer
│   ├── algorithms
│   │   └── dreamer.py : Dreamer algorithm. Including the loss function and training loop
│   │   └── plan2explore.py : plan2explore algorithm. Including the loss function and training loop
│   ├── configs
│   │   └ : Contains hyperparameters for the training process and sets up the training environment
│   ├── envs
│   │   ├── envs.py : Defines the environments used in the Dreamer algorithm
│   │   └── wrappers.py : Modifies observations of the environments to make them more suitable for training
│   ├── modules
│   │   ├── actor.py : A linear network to generate action
│   │         └ input : deterministic and stochastic(state)
│   │         └ output : action
│   │   ├── critic.py : A linear network to generate value
│   │         └ input : deterministic and stochastic
│   │         └ output : value
│   │   ├── decoder.py : A convTranspose network to generate reconstructed image
│   │         └ input : deterministic and stochastic
│   │         └ output : reconstructed image
│   │   ├── encoder.py : A convolution network to generate embedded observation
│   │         └ input : image
│   │         └ output : embedded observation
│   │   ├── model.py : Contains the implementation of models
│   │       └ RSSM : Stands for "Recurrent State-Space Model"
│   │         └ RecurrentModel : A recurrent neural network to generate deterministic.
│   │           └ input : stochastic and deterministic and action
│   │           └ output : deterministic
│   │         └ TransitionModel : A linear network to generate stochastic. we call it as prior
│   │           └ input : deterministic
│   │           └ output : stochastic(prior)
│   │         └ RepresentationModel : A linear network to generate stochastic. we call it as posterior.
│   │           └ input : embedded observation and deterministic
│   │           └ output : stochastic(posterior)
│   │       └ RewardModel : A linear network to generate reward
│   │         └ input : deterministic and stochastic 
│   │         └ output : reward
│   │       └ ContinueModel : A linear network to generate continue flag(not done)
│   │         └ input : deterministic and stochastic
│   │         └ output : continue flag
│   │   └── one_step_model.py : A linear network to predict embedded observation # for plan2explore
│   │         └ input : deterministic and stochastic and action
│   │         └ output : embedded observation
│   └── utils
│       ├── buffer.py : Contains the replay buffer used to store and sample transitions during training
│       └── utils.py : Contains other utility functions
└── main.py : Reads the configuration file, sets up the environment, and starts the training process
```

<hr/>

## Todo

* discrete action space environment performance check
* code-coverage test
* dreamer-v2
* dreamer-v3

<hr/>

## Performance

### Dreamer

| Task                    | 20-EMA  |
|-------------------------|--------|
| ball-in-cup-catch        | 936.9  |
| walker-stand             | 972.8  |
| quadruped-walk           | 584.7  |
| cheetah-run              | 694.0  |
| cartpole-balance         | 831.2  |
| cartpole-swingup-sparse  | 219.3  |
| finger-turn_easy         | 805.1  |
| cartpole-balance-sparse  | 541.6  |
| hopper-hop               | 250.7  |
| walker-run               | 284.6  |
| reacher-hard             | 162.7  |
| reacher-easy             | 911.4  |
| acrobot-swingup          | 91.8   |
| finger-spin              | 543.5  |
| cartpole-swingup         | 607.8  |
| walker-walk              | 871.3  |

All reported results were obtained by running the experiments 3 times with different random seeds. Evaluation was performed after each interaction with the environment, and the reported performance metric is the 20-EMA (exponential moving average) of the cumulative reward in a single episode.

<hr/>

## References

* [Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603)
* [https://github.com/google-research/dreamer](https://github.com/google-research/dreamer)
* [https://github.com/danijar/dreamer](https://github.com/danijar/dreamer)
* [https://github.com/juliusfrost/dreamer-pytorch](https://github.com/juliusfrost/dreamer-pytorch)
* [https://github.com/yusukeurakami/dreamer-pytorch](https://github.com/yusukeurakami/dreamer-pytorch)
* [Planning to Explore via Self-Supervised World Models](https://arxiv.org/abs/2005.05960)
* [https://github.com/danijar/dreamerv2](https://github.com/danijar/dreamerv2)

```

`hierarchical-nav/dreamer_mg/bs.py`:

```py
"""def build_pcfg_from_memory(self,debug: bool = True) -> PCFG:
        mg    = self.memory_graph
        emap  = mg.experience_map
        start = mg.get_current_exp_id()

        # ---------- 1. adjacency list ----------------------------------
        graph = {e.id: [l.target.id for l in e.links] for e in emap.exps}
        if debug:
            print("[PCFG DEBUG] graph:", graph)

        # ---------- 2. confidence-weighted goal prior ------------------
        goals, Z_prior = {}, 0.0
        sx, sy, _ = emap.get_pose(start)
        for exp in emap.exps:
            if exp.id == start:
                continue
            conf = getattr(exp, "confidence", 1.0)
            dist = math.hypot(exp.x_m - sx, exp.y_m - sy) + 1e-5
            w    = conf / dist
            goals[exp.id] = w
            Z_prior      += w

        # ---------- 3. helper to enumerate a few paths ----------------
        def all_paths(src, dst, k=12, depth=15):
            out, q = [], deque([[src]])
            while q and len(out) < k:
                p = q.popleft()
                if p[-1] == dst:
                    out.append(p)
                elif len(p) < depth:
                    for nb in graph.get(p[-1], []):
                        if nb not in p:
                            q.append(p + [nb])
            return out

        # ---------- 4. construct production rules ---------------------
        rules = defaultdict(list)

        # 4-A.   NAVPLAN → PATH_t  (goal prior)
        for tgt, w in goals.items():
            rules["NAVPLAN"].append((f"PATH_{tgt}", w / Z_prior))

        # 4-B.   PATH_t  → STEP_*_* STEP_*_* ...
        for tgt in goals:
            for path in all_paths(start, tgt):
                # prepend self-edge STEP_s_s
                step_tokens = []
                if not self._at_node_exact(start):
                    step_tokens.append(f"STEP_{start}_{start}")   # only if we are *away*
                step_tokens += [f"STEP_{u}_{v}" for u, v in zip(path, path[1:])]
                rhs = " ".join(step_tokens)
                rules[f"PATH_{tgt}"].append((rhs, 1.0))       # equal weight

        # 4-C.   every STEP_u_v becomes a *terminal* symbol
        for lhs in list(rules.keys()):
            if lhs.startswith("PATH_"):
                for rhs, _ in rules[lhs]:
                    for tok in rhs.split():
                        if tok not in rules:                  # first encounter
                            rules[tok].append((f"'{tok}'", 1.0))

        # ---------- 5. serialise to NLTK PCFG --------------------------
        lines = []
        for lhs, prods in rules.items():
            Z = sum(p for _, p in prods)
            for rhs, p in prods:
                lines.append(f"{lhs} -> {rhs} [{p/Z:.4f}]")

        grammar_src = "\n".join(lines)
        if debug:
            print("[PCFG DEBUG] Final grammar:\n" + grammar_src)

        return PCFG.fromstring(grammar_src) """
```

`hierarchical-nav/dreamer_mg/bs2.py`:

```py
import numpy as np
from collections import defaultdict, deque
from typing import Dict, List, Tuple, Optional
import time, math
from scipy import stats

class EnhancedHMMBayesianObserver:
    """
    Enhanced HMM for mode transitions + proper BOCPD.
    Modes: EXPLORE, NAVIGATE, RECOVER, TASK_SOLVING
    """
    def __init__(self, modes: List[str] = None):
        if modes is None:
            modes = ['EXPLORE','NAVIGATE','RECOVER','TASK_SOLVING']
        self.modes       = modes
        self.n_modes     = len(modes)
        self.mode_to_idx = {m:i for i,m in enumerate(modes)}

        # 1) HMM params
        self.transition_matrix = self._init_transition_matrix()
        self.emission_params   = self._init_emission_params()

        # 2) HMM state
        self.mode_beliefs = np.ones(self.n_modes)/self.n_modes
        self.mode_history = deque(maxlen=100)

        # 3) Evidence tracking
        self.last_poses    = deque(maxlen=100)
        self.replay_buffer = deque(maxlen=80)
        self.new_nodes_created = deque(maxlen=50)
        self.plan_progress_history = deque(maxlen=30)
        self.last_significant_progress = time.time()
        self.stagnation_counter = 0

        # 4) attempt counters
        self.exploration_attempts = 0
        self.navigation_attempts  = 0
        self.recovery_attempts    = 0

        # 5) thresholds / params
        self.params = {
            'loop_threshold':20,
            'stagnation_time_threshold':30.0,
            'stagnation_step_threshold':50,
            'max_exploration_cycles':3,
            'max_navigation_cycles':2,
            'max_recovery_cycles':2
        }

        # 6) BOCPD state
        self.max_run_length      = 50
        self.run_length_dist     = np.zeros(self.max_run_length+1)
        self.run_length_dist[0]  = 1.0
        self.hazard_rate         = 1.0/25.0
        self.changepoint_threshold = 0.1

        # 7) history buffers
        self.evidence_buffer  = deque(maxlen=100)
        self.evidence_history = deque(maxlen=self.max_run_length)
        self.last_changepoint = 0

    def _init_transition_matrix(self) -> np.ndarray:
        return np.array([
            [0.75,0.15,0.05,0.05],
            [0.10,0.70,0.15,0.05],
            [0.20,0.20,0.55,0.05],
            [0.05,0.05,0.05,0.85],
        ])

    def _init_emission_params(self) -> Dict:
        return {
            'EXPLORE': {
                'info_gain':{'mean':0.4,'std':0.2},
                'progress': {'mean':0.2,'std':0.15},
                'stagnation':{'mean':0.3,'std':0.2},
                'lost_prob':0.2,
                'loop_prob':0.3,
                'exploration_productivity':{'mean':0.6,'std':0.25}
            },
            'NAVIGATE': {
                'info_gain':{'mean':0.1,'std':0.1},
                'progress': {'mean':0.6,'std':0.2},
                'stagnation':{'mean':0.2,'std':0.15},
                'lost_prob':0.15,
                'loop_prob':0.2,
                'navigation_progress':{'mean':0.7,'std':0.2}
            },
            'RECOVER': {
                'info_gain':{'mean':0.05,'std':0.05},
                'progress': {'mean':0.1,'std':0.1},
                'stagnation':{'mean':0.8,'std':0.2},
                'lost_prob':0.9,
                'loop_prob':0.7,
                'recovery_effectiveness':{'mean':0.4,'std':0.3}
            },
            'TASK_SOLVING': {
                'info_gain':{'mean':0.2,'std':0.1},
                'progress': {'mean':0.5,'std':0.2},
                'stagnation':{'mean':0.1,'std':0.1},
                'lost_prob':0.1,
                'loop_prob':0.1,
                'task_completion':{'mean':0.8,'std':0.2}
            }
        }

    # ————— Evidence metrics —————
    def detect_looping(self) -> float:
        if len(self.last_poses)<self.params['loop_threshold']:
            return 0.0
        counts = defaultdict(int)
        recent = list(self.last_poses)[-self.params['loop_threshold']:]
        for p in recent:
            pose = (round(p[0],1),round(p[1],1)) if len(p)>=2 else p
            counts[pose]+=1
        return min(1.0, max(counts.values())/len(recent))

    def detect_stagnation(self) -> float:
        dt = time.time()-self.last_significant_progress
        return max(min(1.0,dt/self.params['stagnation_time_threshold']),
                   min(1.0,self.stagnation_counter/self.params['stagnation_step_threshold']))

    def assess_exploration_productivity(self) -> float:
        if len(self.new_nodes_created)<5: return 0.5
        recent = list(self.new_nodes_created)[-self.params['exploration_productivity_window']:]
        rate = sum(recent)/len(recent)
        if len(self.last_poses)>=10:
            poses = list(self.last_poses)[-20:]
            unique = len({tuple(round(x,1) for x in p) for p in poses})
            div = unique/len(poses)
        else:
            div = 0.5
        return min(1.0,max(0.0,rate*0.6+div*0.4))

    def assess_navigation_progress(self,fb: Dict) -> float:
        prog=fb.get('plan_progress',0.0)
        self.plan_progress_history.append(prog)
        if len(self.plan_progress_history)<3: return prog
        rp = list(self.plan_progress_history)[-5:]
        trend = np.mean(np.diff(rp)) if len(rp)>1 else 0
        return min(1.0,max(0.0,prog*0.7+max(0,trend)*0.3))

    def assess_recovery_effectiveness(self,fb: Dict) -> float:
        doubt=fb.get('place_doubt_step_count',0)
        if hasattr(self,'last_doubt_count'):
            trend = self.last_doubt_count-doubt
            eff = min(1.0,max(0.0,trend/6.0))
        else:
            eff=0.5
        self.last_doubt_count=doubt
        return eff

    def should_transition_to_task_solving(self) -> float:
        total = (self.exploration_attempts+
                 self.navigation_attempts+
                 self.recovery_attempts)
        if total<3: return 0.1
        if (self.exploration_attempts>=self.params['max_exploration_cycles'] and
            self.navigation_attempts>=self.params['max_navigation_cycles']):
            return 0.8
        if self.exploration_attempts>=2 and len(self.new_nodes_created)>30:
            return 0.4
        return 0.2

    # ————— Emission models —————
    def compute_emission_likelihood(self, evidence: Dict, mode: str) -> float:
        params = self.emission_params[mode]
        ll = 0.0
        # continuous
        for k in ['info_gain','progress','stagnation']:
            if k in evidence and k in params:
                ll += stats.norm.logpdf(evidence[k],
                                        params[k]['mean'],
                                        params[k]['std'])
        # mode-specific
        if mode=='EXPLORE':
            if 'exploration_productivity' in evidence:
                p=evidence['exploration_productivity']
                pm=params['exploration_productivity']
                ll+=stats.norm.logpdf(p,pm['mean'],pm['std'])
            if 'loop_prob' in evidence:
                lp=evidence['loop_prob']
                ll+=stats.norm.logpdf(lp,params['loop_prob'],0.2)
        elif mode=='NAVIGATE':
            if 'navigation_progress' in evidence:
                npv=evidence['navigation_progress']
                pm=params['navigation_progress']
                ll+=stats.norm.logpdf(npv,pm['mean'],pm['std'])
        elif mode=='RECOVER':
            if 'recovery_effectiveness' in evidence:
                rev=evidence['recovery_effectiveness']
                pm=params['recovery_effectiveness']
                ll+=stats.norm.logpdf(rev,pm['mean'],pm['std'])
        elif mode=='TASK_SOLVING':
            if 'task_solving_readiness' in evidence:
                tr=evidence['task_solving_readiness']
                pm=params['task_completion']
                ll+=stats.norm.logpdf(tr,pm['mean'],pm['std'])
        # binary
        if 'agent_lost' in evidence:
            pl=params['lost_prob']
            ll+= (np.log(pl)   if evidence['agent_lost']
                  else np.log(1-pl))
        return np.exp(ll)

    # ————— BOCPD helpers —————
    def compute_run_length_specific_likelihood(self,
                                               evidence: Dict,
                                               run_length: int) -> float:
        if run_length==0:
            return self._compute_prior_likelihood(evidence)
        hist = list(self.evidence_history)
        if len(hist)<run_length:
            relevant = hist
        else:
            relevant = hist[-run_length:]
        if not relevant:
            return self._compute_prior_likelihood(evidence)

        logl = 0.0
        for var in ['info_gain','progress','stagnation',
                    'exploration_productivity',
                    'navigation_progress',
                    'recovery_effectiveness']:
            if var in evidence:
                vals = [e.get(var,0) for e in relevant if var in e]
                if len(vals)>=2:
                    m=np.mean(vals)
                    s=max(np.std(vals),1e-2)
                    s *= (1+1.0/run_length)
                    logl += stats.norm.logpdf(evidence[var],m,s)
                else:
                    # fallback to mode-weighted static
                    logl += np.log(self._compute_mode_weighted_likelihood(evidence,var)+1e-10)
        # combine with static emission
        static_mix = sum(self.mode_beliefs[i]*
                         self.compute_emission_likelihood(evidence,mode)
                         for i,mode in enumerate(self.modes))
        comb = 0.7*np.exp(logl) + 0.3*static_mix
        return max(comb,1e-10)

    def _compute_prior_likelihood(self,evidence:Dict)->float:
        # equally-weighted prior over modes
        return sum(0.25*self.compute_emission_likelihood(evidence,m)
                   for m in self.modes)

    def _compute_mode_weighted_likelihood(self,
                                          evidence:Dict,var:str)->float:
        tot = 0.0
        for i,mode in enumerate(self.modes):
            p=self.mode_beliefs[i]
            prm=self.emission_params[mode].get(var)
            if isinstance(prm,dict):
                tot += p*stats.norm.pdf(evidence[var],
                                        prm['mean'],prm['std'])
        return tot

    # ————— Buffer updates & HMM/BOCPD —————
    def update_mode_attempts(self,current_mode:str):
        if current_mode=='EXPLORE':   self.exploration_attempts+=1
        if current_mode=='NAVIGATE':  self.navigation_attempts+=1
        if current_mode=='RECOVER':   self.recovery_attempts+=1

    def update_evidence_buffers(self,evidence:Dict):
        if 'current_pose' in evidence:
            self.last_poses.append(evidence['current_pose'])
        if 'replay_step' in evidence:
            self.replay_buffer.append(evidence['replay_step'])
        if 'new_nodes' in evidence:
            self.new_nodes_created.append(evidence['new_nodes'])
        if evidence.get('progress',0)>0.1:
            self.last_significant_progress=time.time()
            self.stagnation_counter=0
        else:
            self.stagnation_counter+=1
        # **CRITICAL** push into evidence_history for BOCPD
        self.evidence_history.append(evidence.copy())

    def hmm_forward_step(self,evidence:Dict)->np.ndarray:
        # standard HMM predict+update
        e_likes = np.array([self.compute_emission_likelihood(evidence,m)
                            for m in self.modes])
        # Task-solving bump
        tsp = self.should_transition_to_task_solving()
        if tsp>0.5:
            idx=self.mode_to_idx['TASK_SOLVING']
            e_likes[idx]*=(1.0+tsp)
        pred = self.transition_matrix.T @ self.mode_beliefs
        newb = e_likes * pred
        return (newb/newb.sum()) if newb.sum()>0 else np.ones(self.n_modes)/self.n_modes

    def bocpd_update(self,evidence:Dict)->Tuple[bool,float]:
        pred = np.zeros(self.max_run_length+1)
        for r,mass in enumerate(self.run_length_dist):
            if mass>1e-10:
                pred[r] = self.compute_run_length_specific_likelihood(evidence,r)
        growth = self.run_length_dist[:-1]*(1-self.hazard_rate)*pred[:-1]
        cp_mass= np.sum(self.run_length_dist*self.hazard_rate*pred)
        newdist=np.zeros_like(self.run_length_dist)
        newdist[0]=cp_mass
        newdist[1:]=growth
        if newdist.sum()>0:
            newdist/=newdist.sum()
        self.run_length_dist=newdist
        return (cp_mass>self.changepoint_threshold), cp_mass

    def update(self,evidence:Dict)->Tuple[np.ndarray,bool]:
        self.update_evidence_buffers(evidence)
        enhanced = evidence.copy()
        enhanced.update({
            'loop_prob':self.detect_looping(),
            'stagnation_prob':self.detect_stagnation(),
            'exploration_productivity':self.assess_exploration_productivity(),
            'navigation_progress':self.assess_navigation_progress(evidence),
            'recovery_effectiveness':self.assess_recovery_effectiveness(evidence),
            'task_solving_readiness':self.should_transition_to_task_solving()
        })
        self.evidence_buffer.append(enhanced)
        # track attempts
        cur = self.modes[np.argmax(self.mode_beliefs)]
        self.update_mode_attempts(cur)
        # BOCPD
        cp_flag, cp_prob = self.bocpd_update(enhanced)
        if cp_flag:
            print(f"[BOCPD] Changepoint! p={cp_prob:.3f}")
            self.mode_beliefs = 0.5*self.mode_beliefs + 0.5*(np.ones(self.n_modes)/self.n_modes)
            self.last_changepoint = len(self.evidence_buffer)
        # HMM forward
        self.mode_beliefs = self.hmm_forward_step(enhanced)
        self.mode_history.append((time.time(),self.mode_beliefs.copy()))
        return self.mode_beliefs, cp_flag

    def get_mode_probabilities(self)->Dict[str,float]:
        return {m: self.mode_beliefs[i] for i,m in enumerate(self.modes)}

    def get_diagnostics(self)->Dict:
        recent = self.evidence_buffer[-1] if self.evidence_buffer else {}
        return {
            'mode_attempts':{
                'explore':self.exploration_attempts,
                'nav':self.navigation_attempts,
                'recov':self.recovery_attempts
            },
            'evidence_metrics':{
                'loop':recent.get('loop_prob',0),
                'stagnation':recent.get('stagnation_prob',0),
                'expl_prod':recent.get('exploration_productivity',0),
                'task_ready':recent.get('task_solving_readiness',0)
            },
            'buffer_stats':{
                'poses':len(self.last_poses),
                'replay':len(self.replay_buffer),
                'nodes':sum(self.new_nodes_created)
            },
            'bocpd_stats':{
                'run_length_dist':self.run_length_dist.copy(),
                'most_likely_r':int(np.argmax(self.run_length_dist)),
                'history_size':len(self.evidence_history)
            }
        }


# --------------------------------------------------
# PCFG Builder & Controller (unchanged from your skeleton)
# --------------------------------------------------
class EnhancedMixturePCFGBuilder:
    def __init__(self, key, hmm_observer: EnhancedHMMBayesianObserver):
        self.key          = key
        self.hmm_observer = hmm_observer
        self.memory_graph = key.models_manager.memory_graph
        self.emap         = self.memory_graph.experience_map
        self.submode_beliefs = {
            'EXPLORE': {'ego_allo':0.4,'ego_allo_lookahead':0.3,'short_term_memory':0.2,'astar_directed':0.1},
            'NAVIGATE':{'distant_node':0.4,'unvisited_priority':0.3,'plan_following':0.3},
            'RECOVER': {'solve_doubt':0.6,'backtrack_safe':0.4},
            'TASK_SOLVING':{'goal_directed':0.5,'systematic_search':0.3,'task_completion':0.2}
        }

    def update_submode_beliefs(self,evidence:Dict):
        # your RL-style update from before...
        current = max(self.hmm_observer.get_mode_probabilities().items(),
                      key=lambda x:x[1])[0]
        perf = evidence.get('performance_score',0.5)
        sub = evidence.get('active_submode')
        if sub and sub in self.submode_beliefs[current]:
            lr=0.1; cb=self.submode_beliefs[current][sub]
            if perf>0.6:   self.submode_beliefs[current][sub]=min(0.9,cb+lr*(1-cb))
            elif perf<0.3: self.submode_beliefs[current][sub]=max(0.1,cb-lr*cb)
            # renormalize
            tot=sum(self.submode_beliefs[current].values())
            for k in self.submode_beliefs[current]:
                self.submode_beliefs[current][k]/=tot

    def build_mixture_pcfg(self,use_soft:bool=True)->PCFG:
        mode_probs=self.hmm_observer.get_mode_probabilities()
        if use_soft:
            return self._build_soft_mixture_pcfg(mode_probs)
        else:
            best=max(mode_probs,key=mode_probs.get)
            return self._build_single_mode_pcfg(best)

    def _build_soft_mixture_pcfg(self,mode_probs):
        rules=[]
        s=sum(mode_probs.values())
        if s>0:
            for m,p in mode_probs.items():
                rules.append(f"START -> {m}_ROOT [{p/s:.4f}]")
        else:
            for m in mode_probs: rules.append(f"START -> {m}_ROOT [0.25]")
        rules+=self._build_explore_rules()
        rules+=self._build_navigate_rules()
        rules+=self._build_recover_rules()
        rules+=self._build_task_solving_rules()
        return PCFG.fromstring("\n".join(rules))

    def _build_single_mode_pcfg(self,mode):
        rules=[f"START -> {mode}_ROOT [1.0]"]
        if mode=='EXPLORE':   rules+=self._build_explore_rules()
        if mode=='NAVIGATE':  rules+=self._build_navigate_rules()
        if mode=='RECOVER':   rules+=self._build_recover_rules()
        if mode=='TASK_SOLVING':rules+=self._build_task_solving_rules()
        return PCFG.fromstring("\n".join(rules))

    def _build_explore_rules(self):
        p=self.submode_beliefs['EXPLORE']; s=sum(p.values())
        r=[f"EXPLORE_ROOT -> EXPLORE_{k.upper()} [{(v/s if s>0 else 0.25):.4f}]" for k,v in p.items()]
        r+=[
            "EXPLORE_EGO_ALLO -> 'forward' [0.6] | 'left' [0.2] | 'right' [0.2]",
            "EXPLORE_EGO_ALLO_LOOKAHEAD -> 'forward' 'forward' [0.4] | 'left' 'forward' [0.3] | 'right' 'forward' [0.3]",
            "EXPLORE_SHORT_TERM_MEMORY -> 'scan' [0.3] | 'forward' [0.4] | 'backtrack' [0.3]",
            "EXPLORE_ASTAR_DIRECTED -> 'plan_to_frontier' [1.0]"
        ]
        return r

    def _build_navigate_rules(self):
        p=self.submode_beliefs['NAVIGATE']; s=sum(p.values())
        r=[f"NAVIGATE_ROOT -> NAVIGATE_{k.upper()} [{(v/s if s>0 else 0.33):.4f}]" for k,v in p.items()]
        r+=[
            "NAVIGATE_DISTANT_NODE -> 'goto_distant' [1.0]",
            "NAVIGATE_UNVISITED_PRIORITY -> 'goto_unvisited' [1.0]",
            "NAVIGATE_PLAN_FOLLOWING -> 'follow_plan' [0.8] | 'replan' [0.2]"
        ]
        return r

    def _build_recover_rules(self):
        p=self.submode_beliefs['RECOVER']; s=sum(p.values())
        r=[f"RECOVER_ROOT -> RECOVER_{k.upper()} [{(v/s if s>0 else 0.5):.4f}]" for k,v in p.items()]
        r+=[
            "RECOVER_SOLVE_DOUBT -> 'scan' [0.4] | 'relocalize' [0.6]",
            "RECOVER_BACKTRACK_SAFE -> 'backtrack' [0.7] | 'return_to_known' [0.3]"
        ]
        return r

    def _build_task_solving_rules(self):
        p=self.submode_beliefs['TASK_SOLVING']; s=sum(p.values())
        r=[f"TASK_SOLVING_ROOT -> TASK_{k.upper()} [{(v/s if s>0 else 0.33):.4f}]" for k,v in p.items()]
        r+=[
            "TASK_GOAL_DIRECTED -> 'execute_task_plan' [0.8] | 'refine_task_plan' [0.2]",
            "TASK_SYSTEMATIC_SEARCH -> 'systematic_exploration' [0.6] | 'check_all_rooms' [0.4]",
            "TASK_TASK_COMPLETION -> 'complete_objective' [0.9] | 'verify_completion' [0.1]"
        ]
        return r


class EnhancedHybridBayesianController:
    def __init__(self, key, buffer_size:int=50):
        self.key           = key
        self.hmm_observer  = EnhancedHMMBayesianObserver()
        self.pcfg_builder  = EnhancedMixturePCFGBuilder(key,self.hmm_observer)
        self.performance_buffer = deque(maxlen=buffer_size)
        self.last_evidence     = {}
        self.use_soft_mixture  = True
        self.adaptation_enabled = True

    def extract_enhanced_evidence(self,agent_state,env_state,perf):
        e = {
            'timestamp':time.time(),
            'performance_score':perf.get('reward',0.5),
            'active_submode':agent_state.get('active_submode'),
            'info_gain':perf.get('info_gain',0.0),
            'progress':perf.get('plan_progress',0.0),
            'agent_lost':agent_state.get('place_doubt_step_count',0)>6,
            'place_doubt_step_count':agent_state.get('place_doubt_step_count',0),
            'new_nodes':env_state.get('new_nodes',0),
            'nodes_created':env_state.get('nodes_created_total',0),
            'plan_progress':perf.get('plan_progress',0.0),
            'has_navigation_goal':agent_state.get('has_navigation_goal',False),
            'path_blocked':env_state.get('path_blocked',False),
            'task_defined':agent_state.get('task_defined',False),
            'exploration_completeness':env_state.get('exploration_completeness',0.0)
        }
        return e

    def update(self,agent_state,env_state,perf):
        evidence = self.extract_enhanced_evidence(agent_state,env_state,perf)
        self.last_evidence = evidence
        self.performance_buffer.append(perf.get('reward',0.0))

        # 1) HMM+BOCPD
        beliefs, cp = self.hmm_observer.update(evidence)
        # 2) Submode adaptation
        if self.adaptation_enabled:
            self.pcfg_builder.update_submode_beliefs(evidence)
        # 3) PCFG
        pcfg = self.pcfg_builder.build_mixture_pcfg(self.use_soft_mixture)
        # 4) Diagnostics
        diag = {
            'mode_beliefs':self.hmm_observer.get_mode_probabilities(),
            'changepoint': cp,
            'last_evidence': evidence,
            **self.hmm_observer.get_diagnostics()
        }
        return pcfg, diag

    def get_current_strategy(self)->Tuple[str,float]:
        mp = self.hmm_observer.get_mode_probabilities()
        best,conf = max(mp.items(),key=lambda x:x[1])
        return best,conf

    def toggle_mixture_mode(self,use_soft:Optional[bool]=None):
        if use_soft is not None: self.use_soft_mixture = use_soft
        else:                   self.use_soft_mixture = not self.use_soft_mixture
        print(f"[Controller] {'soft' if self.use_soft_mixture else 'hard'} mixture")

    def print_status(self):
        best,conf = self.get_current_strategy()
        stats = self.hmm_observer.get_diagnostics()['buffer_stats']
        print(f"Dominant: {best} ({conf:.2f}); poses={stats['poses_tracked']}, replay={stats['replay_buffer_size']}")

```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0015/snapshot_t0015.json`:

```json
{
  "t_step": 15,
  "timestamp": "2025-09-14T20:54:33",
  "env": {
    "img_path": "dbg/cogmap/t0015/env_t0015.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0015/cog_t0015.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 0.9,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.30000000000000016,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.6000000000000001,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ]
    ],
    "current_exp_id": 2,
    "bbox": [
      0.9,
      3.1,
      -8.4,
      0.4
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0030/snapshot_t0030.json`:

```json
{
  "t_step": 30,
  "timestamp": "2025-09-14T20:36:39",
  "env": {
    "img_path": "dbg/cogmap/t0030/env_t0030.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0030/cog_t0030.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 0.7000000000000001,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 1.3999999999999997,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.9999999999999994,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.8,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ]
    ],
    "current_exp_id": 5,
    "bbox": [
      -6.45,
      3.45,
      -9.45,
      0.45
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0045/snapshot_t0045.json`:

```json
{
  "t_step": 45,
  "timestamp": "2025-09-14T20:37:10",
  "env": {
    "img_path": "dbg/cogmap/t0045/env_t0045.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0045/cog_t0045.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 0.20000000000000015,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.8999999999999988,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.30000000000000016,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.8,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ]
    ],
    "current_exp_id": 8,
    "bbox": [
      -14.85,
      3.85,
      -9.45,
      0.4500000000000006
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0060/snapshot_t0060.json`:

```json
{
  "t_step": 60,
  "timestamp": "2025-09-14T20:37:39",
  "env": {
    "img_path": "dbg/cogmap/t0060/env_t0060.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0060/cog_t0060.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 1.6999999999999993,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.6000000000000001,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 0.8,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.20000000000000015,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.6000000000000001,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ]
    ],
    "current_exp_id": 11,
    "bbox": [
      -15.9,
      3.9,
      -10.5,
      0.5000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0075/snapshot_t0075.json`:

```json
{
  "t_step": 75,
  "timestamp": "2025-09-14T20:38:13",
  "env": {
    "img_path": "dbg/cogmap/t0075/env_t0075.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0075/cog_t0075.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 1.199999999999998,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.10000000000000014,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 0.20000000000000015,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 0.8,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ]
    ],
    "current_exp_id": 14,
    "bbox": [
      -24.3,
      4.3,
      -17.85,
      0.8500000000000008
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0090/snapshot_t0090.json`:

```json
{
  "t_step": 90,
  "timestamp": "2025-09-14T20:38:40",
  "env": {
    "img_path": "dbg/cogmap/t0090/env_t0090.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0090/cog_t0090.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 1.6999999999999975,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 0.10000000000000014,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 2.1999999999999993,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 0.0,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 0.0,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 1.3877787807814457e-16,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 1.3,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 1.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ]
    ],
    "current_exp_id": 17,
    "bbox": [
      -24.3,
      4.3,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0105/snapshot_t0105.json`:

```json
{
  "t_step": 105,
  "timestamp": "2025-09-14T20:39:05",
  "env": {
    "img_path": "dbg/cogmap/t0105/env_t0105.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0105/cog_t0105.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 3.199999999999996,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.9,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.6999999999999984,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.8,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 0.0,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 0.0,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 2.8,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.4999999999999998,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 18,
        "x": -7.0,
        "y": -20.0,
        "decay": 0.10000000000000014,
        "real_pose": [
          -7,
          -20,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          14,
          22
        ]
      },
      {
        "id": 19,
        "x": -4.0,
        "y": -20.0,
        "decay": 0.5000000000000001,
        "real_pose": [
          -4,
          -20,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ]
    ],
    "current_exp_id": 19,
    "bbox": [
      -24.3,
      4.3,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0120/snapshot_t0120.json`:

```json
{
  "t_step": 120,
  "timestamp": "2025-09-14T20:39:30",
  "env": {
    "img_path": "dbg/cogmap/t0120/env_t0120.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0120/cog_t0120.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 1.6999999999999948,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 4.600000000000003,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 4.199999999999999,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 1.2999999999999985,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 18,
        "x": -7.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -7,
          -20,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          14,
          22
        ]
      },
      {
        "id": 19,
        "x": -4.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -20,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 20,
        "x": -1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          8,
          20
        ]
      },
      {
        "id": 21,
        "x": 2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          2,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          5,
          20
        ]
      },
      {
        "id": 22,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.9,
        "real_pose": [
          5,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ]
    ],
    "current_exp_id": 22,
    "bbox": [
      -24.4,
      6.4,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0135/snapshot_t0135.json`:

```json
{
  "t_step": 135,
  "timestamp": "2025-09-14T20:39:53",
  "env": {
    "img_path": "dbg/cogmap/t0135/env_t0135.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0135/cog_t0135.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 4.199999999999998,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.10000000000000014,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 1.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 3.1000000000000045,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 2.6999999999999984,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 18,
        "x": -7.0,
        "y": -20.0,
        "decay": 0.6000000000000001,
        "real_pose": [
          -7,
          -20,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          14,
          22
        ]
      },
      {
        "id": 19,
        "x": -4.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -20,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 20,
        "x": -1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          8,
          20
        ]
      },
      {
        "id": 21,
        "x": 2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          2,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          5,
          20
        ]
      },
      {
        "id": 22,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 24,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.40000000000000013,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ]
    ],
    "current_exp_id": 2,
    "bbox": [
      -24.45,
      7.45,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0150/snapshot_t0150.json`:

```json
{
  "t_step": 150,
  "timestamp": "2025-09-14T20:40:05",
  "env": {
    "img_path": "dbg/cogmap/t0150/env_t0150.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0150/cog_t0150.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 3.7000000000000006,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 2.699999999999999,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 3.1999999999999993,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 1.5999999999999999,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 1.6000000000000032,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 1.199999999999997,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 0.9,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 18,
        "x": -7.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -7,
          -20,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          14,
          22
        ]
      },
      {
        "id": 19,
        "x": -4.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -20,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 20,
        "x": -1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          8,
          20
        ]
      },
      {
        "id": 21,
        "x": 2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          2,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          5,
          20
        ]
      },
      {
        "id": 22,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 24,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ]
    ],
    "current_exp_id": 4,
    "bbox": [
      -24.45,
      7.45,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0165/snapshot_t0165.json`:

```json
{
  "t_step": 165,
  "timestamp": "2025-09-14T20:40:16",
  "env": {
    "img_path": "dbg/cogmap/t0165/env_t0165.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0165/cog_t0165.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 4.200000000000001,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 3.1999999999999975,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 1.699999999999998,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 1.0999999999999988,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 2.899999999999999,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 4.3,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 0.10000000000000281,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 0.0,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 18,
        "x": -7.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -7,
          -20,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          14,
          22
        ]
      },
      {
        "id": 19,
        "x": -4.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -20,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 20,
        "x": -1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          8,
          20
        ]
      },
      {
        "id": 21,
        "x": 2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          2,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          5,
          20
        ]
      },
      {
        "id": 22,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 24,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ]
    ],
    "current_exp_id": 7,
    "bbox": [
      -24.45,
      7.45,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0180/snapshot_t0180.json`:

```json
{
  "t_step": 180,
  "timestamp": "2025-09-14T20:40:29",
  "env": {
    "img_path": "dbg/cogmap/t0180/env_t0180.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0180/cog_t0180.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 3.7000000000000015,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 2.699999999999996,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.1999999999999976,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 1.3999999999999977,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 2.8,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 3.6000000000000014,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 1.0999999999999996,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 0.0,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 0.0,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 0.20000000000000015,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 18,
        "x": -7.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -7,
          -20,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          14,
          22
        ]
      },
      {
        "id": 19,
        "x": -4.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -20,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 20,
        "x": -1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          8,
          20
        ]
      },
      {
        "id": 21,
        "x": 2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          2,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          5,
          20
        ]
      },
      {
        "id": 22,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 24,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ]
    ],
    "current_exp_id": 7,
    "bbox": [
      -24.45,
      7.45,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0195/snapshot_t0195.json`:

```json
{
  "t_step": 195,
  "timestamp": "2025-09-14T20:40:40",
  "env": {
    "img_path": "dbg/cogmap/t0195/env_t0195.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0195/cog_t0195.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 6.200000000000005,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 2.199999999999995,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 3.2999999999999985,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 2.1,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 0.0,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 0.0,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 18,
        "x": -7.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -7,
          -20,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          14,
          22
        ]
      },
      {
        "id": 19,
        "x": -4.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -20,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 20,
        "x": -1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          8,
          20
        ]
      },
      {
        "id": 21,
        "x": 2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          2,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          5,
          20
        ]
      },
      {
        "id": 22,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 24,
        "x": 6.0,
        "y": -11.0,
        "decay": 1.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ]
    ],
    "current_exp_id": 5,
    "bbox": [
      -24.45,
      7.45,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0210/snapshot_t0210.json`:

```json
{
  "t_step": 210,
  "timestamp": "2025-09-14T20:40:52",
  "env": {
    "img_path": "dbg/cogmap/t0210/env_t0210.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0210/cog_t0210.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 5.70000000000001,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 1.6999999999999935,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 3.6000000000000014,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -2.0,
        "decay": 6.800000000000001,
        "real_pose": [
          -6,
          -2,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -9.0,
        "y": 0.0,
        "decay": 0.5999999999999993,
        "real_pose": [
          -9,
          0,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          2
        ]
      },
      {
        "id": 8,
        "x": -14.0,
        "y": 6.123233995736766e-16,
        "decay": 0.0,
        "real_pose": [
          -14,
          0,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -15.0,
        "y": -2.9999999999999996,
        "decay": 0.0,
        "real_pose": [
          -15,
          -3,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          22,
          5
        ]
      },
      {
        "id": 10,
        "x": -15.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -7,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          22,
          9
        ]
      },
      {
        "id": 11,
        "x": -15.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -15,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      },
      {
        "id": 12,
        "x": -21.0,
        "y": -10.0,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 13,
        "x": -22.0,
        "y": -13.0,
        "decay": 0.0,
        "real_pose": [
          -22,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          29,
          15
        ]
      },
      {
        "id": 14,
        "x": -23.0,
        "y": -17.0,
        "decay": 0.0,
        "real_pose": [
          -23,
          -17,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          3,
          2
        ]
      },
      {
        "id": 15,
        "x": -20.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -20,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          21
        ]
      },
      {
        "id": 16,
        "x": -17.0,
        "y": -19.0,
        "decay": 1.0,
        "real_pose": [
          -17,
          -19,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          24,
          21
        ]
      },
      {
        "id": 17,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 18,
        "x": -7.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -7,
          -20,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          14,
          22
        ]
      },
      {
        "id": 19,
        "x": -4.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -20,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 20,
        "x": -1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          8,
          20
        ]
      },
      {
        "id": 21,
        "x": 2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          2,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          5,
          20
        ]
      },
      {
        "id": 22,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 24,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -6.0,
        -2.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -9.0,
        0.0
      ],
      [
        -9.0,
        0.0
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -14.0,
        6.123233995736766e-16
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -2.9999999999999996
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -7.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -15.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -21.0,
        -10.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -22.0,
        -13.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -23.0,
        -17.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -20.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -17.0,
        -19.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -7.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -4.0,
        -20.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        -1.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        2.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ]
    ],
    "current_exp_id": 7,
    "bbox": [
      -24.45,
      7.45,
      -21.0,
      1.0000000000000007
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0225/snapshot_t0225.json`:

```json
{
  "t_step": 225,
  "timestamp": "2025-09-14T20:02:18",
  "env": {
    "img_path": "dbg/cogmap/t0225/env_t0225.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0225/cog_t0225.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 2.0999999999999974,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -1.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -1,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -10.0,
        "y": 8.881784197001252e-16,
        "decay": 2.4999999999999964,
        "real_pose": [
          -10,
          0,
          2
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          17,
          2
        ]
      },
      {
        "id": 8,
        "x": -15.0,
        "y": -0.9999999999999986,
        "decay": 0.0,
        "real_pose": [
          -15,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -20.0,
        "y": -0.9999999999999979,
        "decay": 0.0,
        "real_pose": [
          -20,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          3
        ]
      },
      {
        "id": 10,
        "x": -21.0,
        "y": -5.999999999999998,
        "decay": 0.39999999999999936,
        "real_pose": [
          -21,
          -6,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          28,
          8
        ]
      },
      {
        "id": 11,
        "x": -21.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 12,
        "x": -16.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -16,
          -10,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          23,
          12
        ]
      },
      {
        "id": 13,
        "x": -14.0,
        "y": -12.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -14,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          21,
          15
        ]
      },
      {
        "id": 14,
        "x": -14.0,
        "y": -15.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -14,
          -16,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          21,
          18
        ]
      },
      {
        "id": 15,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 16,
        "x": -9.0,
        "y": -20.0,
        "decay": 4.300000000000006,
        "real_pose": [
          -9,
          -20,
          0
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          22
        ]
      },
      {
        "id": 17,
        "x": -5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 18,
        "x": -2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -2,
          -18,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          9,
          20
        ]
      },
      {
        "id": 19,
        "x": 1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          20
        ]
      },
      {
        "id": 20,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 21,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 22,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -8.0,
        "decay": 2.5999999999999996,
        "real_pose": [
          6,
          -8,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 24,
        "x": 3.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -11,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 25,
        "x": -3.0,
        "y": -15.0,
        "decay": 0.0,
        "real_pose": [
          -3,
          -15,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          10,
          17
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ]
    ],
    "current_exp_id": 5,
    "bbox": [
      -22.35,
      7.35,
      -21.0,
      1.0000000000000009
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0240/snapshot_t0240.json`:

```json
{
  "t_step": 240,
  "timestamp": "2025-09-14T20:02:30",
  "env": {
    "img_path": "dbg/cogmap/t0240/env_t0240.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0240/cog_t0240.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 2.599999999999996,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.8,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -1.0,
        "decay": 1.0,
        "real_pose": [
          -6,
          -1,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -10.0,
        "y": 8.881784197001252e-16,
        "decay": 0.9999999999999952,
        "real_pose": [
          -10,
          0,
          2
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          17,
          2
        ]
      },
      {
        "id": 8,
        "x": -15.0,
        "y": -0.9999999999999986,
        "decay": 0.0,
        "real_pose": [
          -15,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -20.0,
        "y": -0.9999999999999979,
        "decay": 0.0,
        "real_pose": [
          -20,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          3
        ]
      },
      {
        "id": 10,
        "x": -21.0,
        "y": -5.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -6,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          28,
          8
        ]
      },
      {
        "id": 11,
        "x": -21.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 12,
        "x": -16.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -16,
          -10,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          23,
          12
        ]
      },
      {
        "id": 13,
        "x": -14.0,
        "y": -12.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -14,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          21,
          15
        ]
      },
      {
        "id": 14,
        "x": -14.0,
        "y": -15.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -14,
          -16,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          21,
          18
        ]
      },
      {
        "id": 15,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 16,
        "x": -9.0,
        "y": -20.0,
        "decay": 2.800000000000006,
        "real_pose": [
          -9,
          -20,
          0
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          22
        ]
      },
      {
        "id": 17,
        "x": -5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 18,
        "x": -2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -2,
          -18,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          9,
          20
        ]
      },
      {
        "id": 19,
        "x": 1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          20
        ]
      },
      {
        "id": 20,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.6000000000000001,
        "real_pose": [
          5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 21,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 22,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -8.0,
        "decay": 2.0999999999999983,
        "real_pose": [
          6,
          -8,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 24,
        "x": 3.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -11,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 25,
        "x": -3.0,
        "y": -15.0,
        "decay": 0.0,
        "real_pose": [
          -3,
          -15,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          10,
          17
        ]
      },
      {
        "id": 26,
        "x": -9.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          -8,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          10
        ]
      },
      {
        "id": 27,
        "x": -14.0,
        "y": -6.999999999999999,
        "decay": 0.40000000000000013,
        "real_pose": [
          -14,
          -8,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ]
    ],
    "current_exp_id": 13,
    "bbox": [
      -22.35,
      7.35,
      -21.0,
      1.0000000000000009
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0255/snapshot_t0255.json`:

```json
{
  "t_step": 255,
  "timestamp": "2025-09-14T20:02:53",
  "env": {
    "img_path": "dbg/cogmap/t0255/env_t0255.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0255/cog_t0255.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 4.099999999999996,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.30000000000000016,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -1.0,
        "decay": 0.49999999999999933,
        "real_pose": [
          -6,
          -1,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -10.0,
        "y": 8.881784197001252e-16,
        "decay": 2.4999999999999942,
        "real_pose": [
          -10,
          0,
          2
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          17,
          2
        ]
      },
      {
        "id": 8,
        "x": -15.0,
        "y": -0.9999999999999986,
        "decay": 1.9,
        "real_pose": [
          -15,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -20.0,
        "y": -0.9999999999999979,
        "decay": 0.0,
        "real_pose": [
          -20,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          3
        ]
      },
      {
        "id": 10,
        "x": -21.0,
        "y": -5.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -6,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          28,
          8
        ]
      },
      {
        "id": 11,
        "x": -21.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 12,
        "x": -16.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -16,
          -10,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          23,
          12
        ]
      },
      {
        "id": 13,
        "x": -14.0,
        "y": -12.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -14,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          21,
          15
        ]
      },
      {
        "id": 14,
        "x": -14.0,
        "y": -15.999999999999998,
        "decay": 1.9,
        "real_pose": [
          -14,
          -16,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          21,
          18
        ]
      },
      {
        "id": 15,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 16,
        "x": -9.0,
        "y": -20.0,
        "decay": 1.3000000000000047,
        "real_pose": [
          -9,
          -20,
          0
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          22
        ]
      },
      {
        "id": 17,
        "x": -5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 18,
        "x": -2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -2,
          -18,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          9,
          20
        ]
      },
      {
        "id": 19,
        "x": 1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          20
        ]
      },
      {
        "id": 20,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 21,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 22,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -8.0,
        "decay": 0.5999999999999975,
        "real_pose": [
          6,
          -8,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 24,
        "x": 3.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -11,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 25,
        "x": -3.0,
        "y": -15.0,
        "decay": 0.0,
        "real_pose": [
          -3,
          -15,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          10,
          17
        ]
      },
      {
        "id": 26,
        "x": -9.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          -8,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          10
        ]
      },
      {
        "id": 27,
        "x": -14.0,
        "y": -6.999999999999999,
        "decay": 0.0,
        "real_pose": [
          -14,
          -8,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ]
    ],
    "current_exp_id": 13,
    "bbox": [
      -22.35,
      7.35,
      -21.0,
      1.0000000000000009
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0270/snapshot_t0270.json`:

```json
{
  "t_step": 270,
  "timestamp": "2025-09-14T20:03:05",
  "env": {
    "img_path": "dbg/cogmap/t0270/env_t0270.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0270/cog_t0270.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 2.5999999999999948,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -1.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -1,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -10.0,
        "y": 8.881784197001252e-16,
        "decay": 0.999999999999993,
        "real_pose": [
          -10,
          0,
          2
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          17,
          2
        ]
      },
      {
        "id": 8,
        "x": -15.0,
        "y": -0.9999999999999986,
        "decay": 0.39999999999999936,
        "real_pose": [
          -15,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -20.0,
        "y": -0.9999999999999979,
        "decay": 0.10000000000000014,
        "real_pose": [
          -20,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          3
        ]
      },
      {
        "id": 10,
        "x": -21.0,
        "y": -5.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -6,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          28,
          8
        ]
      },
      {
        "id": 11,
        "x": -21.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 12,
        "x": -16.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -16,
          -10,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          23,
          12
        ]
      },
      {
        "id": 13,
        "x": -14.0,
        "y": -12.999999999999998,
        "decay": 1.3877787807814457e-16,
        "real_pose": [
          -14,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          21,
          15
        ]
      },
      {
        "id": 14,
        "x": -14.0,
        "y": -15.999999999999998,
        "decay": 0.39999999999999936,
        "real_pose": [
          -14,
          -16,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          21,
          18
        ]
      },
      {
        "id": 15,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 16,
        "x": -9.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          -20,
          0
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          22
        ]
      },
      {
        "id": 17,
        "x": -5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 18,
        "x": -2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -2,
          -18,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          9,
          20
        ]
      },
      {
        "id": 19,
        "x": 1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          20
        ]
      },
      {
        "id": 20,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 21,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 22,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -8.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -8,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 24,
        "x": 3.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -11,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 25,
        "x": -3.0,
        "y": -15.0,
        "decay": 0.0,
        "real_pose": [
          -3,
          -15,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          10,
          17
        ]
      },
      {
        "id": 26,
        "x": -9.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          -8,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          10
        ]
      },
      {
        "id": 27,
        "x": -14.0,
        "y": -6.999999999999999,
        "decay": 0.6000000000000001,
        "real_pose": [
          -14,
          -8,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ]
    ],
    "current_exp_id": 12,
    "bbox": [
      -22.35,
      7.35,
      -21.0,
      1.0000000000000009
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0285/snapshot_t0285.json`:

```json
{
  "t_step": 285,
  "timestamp": "2025-09-14T20:03:18",
  "env": {
    "img_path": "dbg/cogmap/t0285/env_t0285.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0285/cog_t0285.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 3.0999999999999934,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.6000000000000001,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -1.0,
        "decay": 1.6999999999999997,
        "real_pose": [
          -6,
          -1,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -10.0,
        "y": 8.881784197001252e-16,
        "decay": 1.9,
        "real_pose": [
          -10,
          0,
          2
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          17,
          2
        ]
      },
      {
        "id": 8,
        "x": -15.0,
        "y": -0.9999999999999986,
        "decay": 0.0,
        "real_pose": [
          -15,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -20.0,
        "y": -0.9999999999999979,
        "decay": 0.0,
        "real_pose": [
          -20,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          3
        ]
      },
      {
        "id": 10,
        "x": -21.0,
        "y": -5.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -6,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          28,
          8
        ]
      },
      {
        "id": 11,
        "x": -21.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 12,
        "x": -16.0,
        "y": -9.999999999999998,
        "decay": 1.8999999999999995,
        "real_pose": [
          -16,
          -10,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          23,
          12
        ]
      },
      {
        "id": 13,
        "x": -14.0,
        "y": -12.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -14,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          21,
          15
        ]
      },
      {
        "id": 14,
        "x": -14.0,
        "y": -15.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -14,
          -16,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          21,
          18
        ]
      },
      {
        "id": 15,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 16,
        "x": -9.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          -20,
          0
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          22
        ]
      },
      {
        "id": 17,
        "x": -5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 18,
        "x": -2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -2,
          -18,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          9,
          20
        ]
      },
      {
        "id": 19,
        "x": 1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          20
        ]
      },
      {
        "id": 20,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 21,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 22,
        "x": 6.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -8.0,
        "decay": 0.40000000000000013,
        "real_pose": [
          6,
          -8,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 24,
        "x": 3.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -11,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 25,
        "x": -3.0,
        "y": -15.0,
        "decay": 0.0,
        "real_pose": [
          -3,
          -15,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          10,
          17
        ]
      },
      {
        "id": 26,
        "x": -9.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          -8,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          10
        ]
      },
      {
        "id": 27,
        "x": -14.0,
        "y": -6.999999999999999,
        "decay": 0.0,
        "real_pose": [
          -14,
          -8,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ]
    ],
    "current_exp_id": 14,
    "bbox": [
      -22.35,
      7.35,
      -21.0,
      1.0000000000000009
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dbg/cogmap/t0300/snapshot_t0300.json`:

```json
{
  "t_step": 300,
  "timestamp": "2025-09-14T20:03:34",
  "env": {
    "img_path": "dbg/cogmap/t0300/env_t0300.png",
    "pixel_size": [
      768,
      1056
    ],
    "grid_size": [
      33,
      24
    ],
    "tile_size": 32
  },
  "cog": {
    "png_path": "dbg/cogmap/t0300/cog_t0300.png",
    "nodes": [
      {
        "id": 0,
        "x": 1.0,
        "y": 0.0,
        "decay": 3.599999999999992,
        "real_pose": [
          1,
          0,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          2
        ]
      },
      {
        "id": 1,
        "x": 3.0,
        "y": -5.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -5,
          3
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          4,
          7
        ]
      },
      {
        "id": 2,
        "x": 3.0,
        "y": -8.0,
        "decay": 0.9,
        "real_pose": [
          3,
          -8,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 3,
        "x": 0.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          0,
          -9,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          7,
          11
        ]
      },
      {
        "id": 4,
        "x": -4.0,
        "y": -9.0,
        "decay": 0.0,
        "real_pose": [
          -4,
          -9,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          1
        ]
      },
      {
        "id": 5,
        "x": -6.0,
        "y": -6.0,
        "decay": 0.0,
        "real_pose": [
          -6,
          -6,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          13,
          8
        ]
      },
      {
        "id": 6,
        "x": -6.0,
        "y": -1.0,
        "decay": 1.1999999999999984,
        "real_pose": [
          -6,
          -1,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "purple",
        "grid_xy": [
          1,
          0
        ]
      },
      {
        "id": 7,
        "x": -10.0,
        "y": 8.881784197001252e-16,
        "decay": 0.39999999999999936,
        "real_pose": [
          -10,
          0,
          2
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          17,
          2
        ]
      },
      {
        "id": 8,
        "x": -15.0,
        "y": -0.9999999999999986,
        "decay": 0.7999999999999993,
        "real_pose": [
          -15,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          0
        ]
      },
      {
        "id": 9,
        "x": -20.0,
        "y": -0.9999999999999979,
        "decay": 1.5,
        "real_pose": [
          -20,
          -1,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          27,
          3
        ]
      },
      {
        "id": 10,
        "x": -21.0,
        "y": -5.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -6,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          28,
          8
        ]
      },
      {
        "id": 11,
        "x": -21.0,
        "y": -9.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -21,
          -10,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          3,
          1
        ]
      },
      {
        "id": 12,
        "x": -16.0,
        "y": -9.999999999999998,
        "decay": 0.3999999999999989,
        "real_pose": [
          -16,
          -10,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          23,
          12
        ]
      },
      {
        "id": 13,
        "x": -14.0,
        "y": -12.999999999999998,
        "decay": 1.9999999999999996,
        "real_pose": [
          -14,
          -13,
          3
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          21,
          15
        ]
      },
      {
        "id": 14,
        "x": -14.0,
        "y": -15.999999999999998,
        "decay": 0.0,
        "real_pose": [
          -14,
          -16,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          21,
          18
        ]
      },
      {
        "id": 15,
        "x": -13.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -13,
          -20,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          2,
          2
        ]
      },
      {
        "id": 16,
        "x": -9.0,
        "y": -20.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          -20,
          0
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          22
        ]
      },
      {
        "id": 17,
        "x": -5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          -5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "blue",
        "grid_xy": [
          1,
          2
        ]
      },
      {
        "id": 18,
        "x": -2.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          -2,
          -18,
          0
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          9,
          20
        ]
      },
      {
        "id": 19,
        "x": 1.0,
        "y": -18.0,
        "decay": 0.0,
        "real_pose": [
          1,
          -18,
          0
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          6,
          20
        ]
      },
      {
        "id": 20,
        "x": 5.0,
        "y": -19.0,
        "decay": 0.0,
        "real_pose": [
          5,
          -19,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          0,
          2
        ]
      },
      {
        "id": 21,
        "x": 6.0,
        "y": -16.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -16,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "black",
        "grid_xy": [
          1,
          18
        ]
      },
      {
        "id": 22,
        "x": 6.0,
        "y": -11.0,
        "decay": 1.0999999999999994,
        "real_pose": [
          6,
          -11,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 23,
        "x": 6.0,
        "y": -8.0,
        "decay": 0.0,
        "real_pose": [
          6,
          -8,
          1
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 24,
        "x": 3.0,
        "y": -11.0,
        "decay": 0.0,
        "real_pose": [
          3,
          -11,
          3
        ],
        "place_kind": "ROOM",
        "room_color": "red",
        "grid_xy": [
          0,
          1
        ]
      },
      {
        "id": 25,
        "x": -3.0,
        "y": -15.0,
        "decay": 0.0,
        "real_pose": [
          -3,
          -15,
          1
        ],
        "place_kind": "CORRIDOR",
        "room_color": null,
        "grid_xy": [
          10,
          17
        ]
      },
      {
        "id": 26,
        "x": -9.0,
        "y": -7.0,
        "decay": 0.0,
        "real_pose": [
          -9,
          -8,
          2
        ],
        "place_kind": "DOOR",
        "room_color": "black",
        "grid_xy": [
          16,
          10
        ]
      },
      {
        "id": 27,
        "x": -14.0,
        "y": -6.999999999999999,
        "decay": 0.0,
        "real_pose": [
          -14,
          -8,
          2
        ],
        "place_kind": "ROOM",
        "room_color": "green",
        "grid_xy": [
          2,
          1
        ]
      }
    ],
    "links_xy": [
      [
        3.0,
        -5.0
      ],
      [
        1.0,
        0.0
      ],
      [
        1.0,
        0.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -5.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        0.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -6.0,
        -1.0
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -10.0,
        8.881784197001252e-16
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -15.0,
        -0.9999999999999986
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -20.0,
        -0.9999999999999979
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -5.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -21.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -14.0,
        -15.999999999999998
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -13.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -9.0,
        -20.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -5.0,
        -19.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        1.0,
        -18.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        5.0,
        -19.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -16.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        6.0,
        -8.0
      ],
      [
        3.0,
        -8.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        6.0,
        -11.0
      ],
      [
        3.0,
        -11.0
      ],
      [
        -2.0,
        -18.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -4.0,
        -9.0
      ],
      [
        -3.0,
        -15.0
      ],
      [
        -6.0,
        -6.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -9.0,
        -7.0
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -16.0,
        -9.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ],
      [
        -14.0,
        -12.999999999999998
      ],
      [
        -14.0,
        -6.999999999999999
      ]
    ],
    "current_exp_id": 12,
    "bbox": [
      -22.35,
      7.35,
      -21.0,
      1.0000000000000009
    ]
  },
  "spawn": {
    "x": 1,
    "y": 0,
    "dir": 0
  }
}
```

`hierarchical-nav/dreamer_mg/dreamer/algorithms/dreamer.py`:

```py
# dreamer/algorithms/dreamer.py
import logging, time, math, os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.utils import save_image

from dreamer.modules.model   import RSSM, RewardModel, ContinueModel
from dreamer.modules.encoder import Encoder
from dreamer.modules.decoder import Decoder
from dreamer.utils.utils     import (
    compute_lambda_values, create_normal_dist, DynamicInfos
)
from dreamer.modules.actor import Actor
from dreamer.utils.buffer    import ReplayBuffer
from pathlib import Path
import random
import gym_minigrid
from gym_minigrid.minigrid import Wall



# --------------------------------------------------------------------------- #
#  nice-looking logger                                                        #
# --------------------------------------------------------------------------- #
log = logging.getLogger("Dreamer")
if not log.handlers:          # only configure once
    h  = logging.StreamHandler()
    fmt = "[%(name)s] %(message)s"
    h.setFormatter(logging.Formatter(fmt))
    log.addHandler(h)
log.setLevel(os.environ.get("DREAMER_LOG_LVL", "INFO").upper())


# --------------------------------------------------------------------------- #
#                           D R E A M E R                                     #
# --------------------------------------------------------------------------- #
class Dreamer:
    # ..................................................................... #
    def __init__(self,
                 observation_shape,
                 discrete_action_bool,
                 action_size,
                 writer,
                 device,
                 config,
                 run_dir):
        t0 = time.time()
        self.run_dir=Path(run_dir) if run_dir else None
        self.device               = device
        self.action_size          = action_size
        self.action_dict = {"left": 0,
                            "right": 1,
                            "forward": 2}
        self.discrete_action_bool = discrete_action_bool
        self.writer               = writer
        self.log_dir = Path(run_dir)
        self.num_total_episode    = 0
        self.global_step = 0
        self.config               = config.parameters.dreamer

        # ---------- networks --------------------------------------------- #
        self.encoder           = Encoder(observation_shape, config).to(device)
        print("observation shape", observation_shape)
        self.decoder           = Decoder(observation_shape, config).to(device)
        self.rssm              = RSSM(action_size, config).to(device)
        self.reward_predictor  = RewardModel(config).to(device)
        #self.actor = Actor( discrete_action_bool,action_size, config).to(device)
        #self.debug_actor = True 
        if self.config.use_continue_flag:
            self.continue_predictor = ContinueModel(config).to(device)

        # ---------- replay buffer ---------------------------------------- #
        self.buffer = ReplayBuffer(observation_shape,
                                   action_size,
                                   device,
                                   config)

        # ---------- optimiser ------------------------------------------- #
        self.model_params = (list(self.encoder.parameters())   +
                             list(self.decoder.parameters())   +
                             list(self.rssm.parameters())      +
                             list(self.reward_predictor.parameters()))
        if self.config.use_continue_flag:
            self.model_params += list(self.continue_predictor.parameters())

        self.model_optimizer = torch.optim.Adam(
            self.model_params, lr=self.config.model_learning_rate
        )
        self.continue_criterion = nn.BCELoss()

        # ---------- helpers --------------------------------------------- #
        self.dynamic_learning_infos  = DynamicInfos(device)
        self.behavior_learning_infos = DynamicInfos(device)  # not used yet

        # ---------- debug summary ---------------------------------------- #
        def n_params(m): return sum(p.numel() for p in m.parameters())/1e6
        log.info("Initialised Dreamer | encoder %.1f M  rssm %.1f M  total %.1f M",
                 n_params(self.encoder), n_params(self.rssm),
                 sum(p.numel() for p in self.model_params)/1e6)
        log.info("device: %s | buffer capacity: %s",
                 device, f"{self.buffer.capacity:,}")
        log.info("setup done in %.2f s", time.time()-t0)

    # ..................................................................... #
    # placeholder until you plug actor-critic
    def behavior_learning(self, *_):
        return
    def _latest_ckpt_by_mtime(self, ckpt_dir: Path) -> Path | None:
        """
        Return the most recently *modified* checkpoint in ckpt_dir,
        or None if the directory is empty.
        """
        ckpts = sorted(ckpt_dir.glob("iter*.pt"),
                    key=lambda p: p.stat().st_mtime)
        return ckpts[-1] if ckpts else None

    # ..................................................................... #
    def train(self, env):
        # seed episodes --------------------------------------------------- #
        print(f"[Debug] seed_episodes = {self.config.seed_episodes}")
        '''need_seed = (len(self.buffer) < 1) and (getattr(self, "start_iter", 1) == 1)
        if need_seed:'''
        if len(self.buffer) < 1:
            self.environment_interaction(env, self.config.seed_episodes)
        ckpt_every = 50                       # iterations
        ckpt_dir   = self.run_dir / "ckpt"
        if ckpt_dir and not ckpt_dir.exists():
            ckpt_dir.mkdir(parents=True)

        # resume if a *.pt file already exists ----------------------------------
        latest = self._latest_ckpt_by_mtime(ckpt_dir)
        if latest:
            self._load_ckpt(latest)
        else:
            print("[Dreamer] no checkpoint found → fresh run")

        # main loop ------------------------------------------------------- #
        if not hasattr(self, "start_iter"):          # fresh run
            self.start_iter = 1
        for it in range(self.start_iter, self.config.train_iterations + 1):
            log.info("⇢ iteration %d/%d", it, self.config.train_iterations)

            # --- model training cycles ---------------------------------- #
            for c in range(1, self.config.collect_interval + 1):
                data = self.buffer.sample(
                    self.config.batch_size, self.config.batch_length
                )
                post, det = self.dynamic_learning(data)
                self.behavior_learning(post, det)

                if c % 10 == 0:
                    log.debug("   collect %d/%d  buffer len: %d",
                              c, self.config.collect_interval, len(self.buffer))
       
            # --- interact with env -------------------------------------- #
            self.environment_interaction(env,
                                         self.config.num_interaction_episodes)
            

            # --- evaluation --------------------------------------------- #
            if it % 10 == 0:
                self.evaluate(env)
            if it % ckpt_every == 0:
                self._save_ckpt(it)

    def _modules(self):
        # everything we want to store
        return dict(
            encoder=self.encoder,
            decoder=self.decoder,
            rssm=self.rssm,
            reward=self.reward_predictor
        )

    def _save_ckpt(self, it):
        state = {
            "iter" : it,
            "rng"  : torch.random.get_rng_state(),
            "opt"  : {k: o.state_dict() for k, o in {
                        "model": self.model_optimizer,
                        "actor": getattr(self, "actor_optimizer", None),
                        "critic":getattr(self,"critic_optimizer",None)}
                    .items() if o},
            "modules": {n: m.state_dict() for n, m in self._modules().items()},
        }
        path = self.run_dir / "ckpt" / f"iter{it:05d}.pt"
        torch.save(state, path)
        print(f"[ckpt] saved → {path.name}")

    def _load_ckpt(self, path: Path):
        """
        Load a checkpoint with shape-safe parameter matching.
        Any tensor whose shape differs from the current module’s tensor
        is *skipped* and reported, so the run can continue instead of crashing.
        """

        ckpt = torch.load(path, map_location=self.device)
        skipped = []                     # for reporting

        # --- 1. modules ----------------------------------------------------
        for name, module in self._modules().items():
            own_state   = module.state_dict()
            ckpt_state  = ckpt["modules"][name]

            # pick only tensors with identical shape
            ok_tensors = {
                k: v for k, v in ckpt_state.items()
                if k in own_state and v.shape == own_state[k].shape
            }
            module.load_state_dict({**own_state, **ok_tensors})
            for k in ckpt_state:
                if k not in ok_tensors:
                    skipped.append(f"{name}.{k}  "
                                f"ckpt{tuple(ckpt_state[k].shape)} "
                                f"≠ current{tuple(own_state[k].shape)}")

        # --- 2. optimizers -------------------------------------------------
        for n, opt in {
            "model":   getattr(self, "model_optimizer", None),
            "actor":   getattr(self, "actor_optimizer", None),
            "critic":  getattr(self, "critic_optimizer", None),
        }.items():
            if opt and n in ckpt["opt"]:
                try:
                    opt.load_state_dict(ckpt["opt"][n])
                except ValueError:          # param mismatch → reset optimiser
                    print(f"[Warn] optimiser '{n}' state skipped (shape mismatch)")
                    pass

        # --- 3. RNG & iteration counter -----------------------------------
        torch.random.set_rng_state(ckpt["rng"])
        self.start_iter = ckpt.get("iter", 0) + 1

        # --- 4. report -----------------------------------------------------
        print(f"[Dreamer] ✔ loaded {path.name}  → start_iter={self.start_iter}")
        if skipped:
            print("[Dreamer]   ⚠ skipped tensors:")
            for s in skipped:
                print("            ·", s)
    # ----------------------------------------------------------------------
    # ..................................................................... #
    def evaluate(self, env):
        self.environment_interaction(env,
                                     self.config.num_evaluate,
                                     train=False)

    # ..................................................................... #
    def dynamic_learning(self, data):
        # roll out through time ------------------------------------------ #
        prior, det = self.rssm.recurrent_model_input_init(len(data.action))
        print(len(data.action))
        print("prior",prior.shape, det.shape)
        print(data.observation.shape)
        
        data.embedded_observation = self.encoder(data.observation)
        print("image",data.embedded_observation.shape)

        for t in range(1, self.config.batch_length):
            #print(data.action[:, t-1])
            x=data.action[:, t-1]
            print(x.shape)
            print("action",data.action.shape)
            det = self.rssm.recurrent_model(prior, data.action[:, t-1], det)
            print("det",det.shape)
            prior_dist, prior = self.rssm.transition_model(det)
            post_dist, post   = self.rssm.representation_model(
                                    data.embedded_observation[:, t], det)
            print(data.embedded_observation[:, t].shape)
            print("jyuygyu")
            print(post.shape,prior.shape)

            self.dynamic_learning_infos.append(
                priors                = prior,
                prior_dist_means      = prior_dist.mean,
                prior_dist_stds       = prior_dist.scale,
                posteriors            = post,
                posterior_dist_means  = post_dist.mean,
                posterior_dist_stds   = post_dist.scale,
                deterministics        = det,
            )
            prior = post

        infos = self.dynamic_learning_infos.get_stacked()
        losses = self._model_update(data, infos)

        log.debug("   dynamic-loss: %.4f  (KL %.3f ‖ recon %.3f ‖ rew %.3f)",
                  losses["model"], losses["kl"], losses["recon"], losses["rew"])
        return infos.posteriors.detach(), infos.deterministics.detach()

    # ..................................................................... #
    def _model_update(self, data, infos):
        # ───────────────────────── reconstruction (image log-likelihood) ──────
        if not hasattr(self, "_printed_shapes"):
            b = 0                                         # look at batch element 0
            print("\n>>> DEBUG: shapes entering _model_update")
            print("posteriors      :", infos.posteriors.shape,
                "dtype", infos.posteriors.dtype)
            print("deterministics  :", infos.deterministics.shape,
                "dtype", infos.deterministics.dtype)

            # basic stats to make sure values aren’t all zero or huge
            print("posterior mean±std  :", infos.posteriors.mean().item(),
                "±", infos.posteriors.std().item())
            print("deterministic mean±std:", infos.deterministics.mean().item(),
                "±", infos.deterministics.std().item())

            # pass one pair through the decoder and print output range
            sample_img = self.decoder(
                            infos.posteriors[b:b+1, 0],      # (1, Z)
                            infos.deterministics[b:b+1, 0]   # (1, H)
                        ).mean
            print("decoder out shape :", sample_img.shape,
                "range", sample_img.min().item(), "…", sample_img.max().item())
            recon_dist  = self.decoder(infos.posteriors, infos.deterministics)
            print("decoder out shape :",recon_dist)
            #print(recon_dist.shape)
            self._printed_shapes = True
        recon_dist  = self.decoder(infos.posteriors, infos.deterministics)
        recon_loss  = recon_dist.log_prob(data.observation[:, 1:])          # <── moved up
        
        print("shapeeeee")
        print(data.observation[:, 1:].shape)
        # ───────────────────────── continue flag (optional) ───────────────────
        if self.config.use_continue_flag:
            cont_dist = self.continue_predictor(infos.posteriors,
                                                infos.deterministics)
            cont_loss = self.continue_criterion(cont_dist.probs,
                                                1 - data.done[:, 1:])

        # ───────────────────────── reward model  ───────────────────────────────
        rew_dist  = self.reward_predictor(infos.posteriors, infos.deterministics)
        rew_loss  = rew_dist.log_prob(data.reward[:, 1:])

        # ───────────────────────── KL divergence  ──────────────────────────────
        prior_dist = create_normal_dist(infos.prior_dist_means,
                                        infos.prior_dist_stds, event_shape=1)
        post_dist  = create_normal_dist(infos.posterior_dist_means,
                                        infos.posterior_dist_stds, event_shape=1)

        kl = torch.distributions.kl.kl_divergence(post_dist, prior_dist).mean()
        kl = torch.max(torch.tensor(self.config.free_nats, device=self.device), kl)

        # ───────────────────────── total model loss  ───────────────────────────
        model_loss = ( self.config.kl_divergence_scale * kl
                    - recon_loss.mean()
                    - rew_loss.mean() )
        if self.config.use_continue_flag:
            model_loss += cont_loss.mean()

        # ───────────────────────── optimisation  ───────────────────────────────
        self.model_optimizer.zero_grad()
        model_loss.backward()
        nn.utils.clip_grad_norm_(self.model_params,
                                self.config.clip_grad,
                                norm_type=self.config.grad_norm_type)
        self.model_optimizer.step()

        # ───────────────────────── bookkeeping / logging  ──────────────────────
        self.global_step += 1
        if self.writer is not None and self.global_step % 5 == 0:
            recon_img = recon_dist.mean[0, -1].clamp(0, 1)  # (3,H,W)
            self.writer.add_image("reconstruction", recon_img, self.global_step)

        # ─── tiny visual probe every 200 optimisation steps  ---------------
        # save to  runs/<TIMESTAMP>/recon/recon_00012.png  (auto-created dir)
        self._vis_counter = getattr(self, "_vis_counter", 0)
        xyx=5
        if getattr(self, "_vis_counter", 0) % xyx == 0:
           
            with torch.no_grad():                       # ← important: no grads!
                outdir = Path(self.run_dir) / "recon"
                outdir.mkdir(parents=True, exist_ok=True)

                # --- 1. ground truth & reconstruction at t = 1 ------------------
                t0_img = data.observation[0, 1]          # (3,64,64)   gt
                recon   = recon_dist.mean[0, 0]          # posterior-recon of the same

                # --- 2. one-step PRIOR (“fantasy”) ------------------------------
                prior_d = infos.deterministics[0, -1]    # deterministic @ last step
                prior_z = infos.priors       [0, -1]     # prior z (no obs)

                fantasy = self.decoder(prior_z[None],    # add batch dim (1, Z)
                                        prior_d[None]    # (1, H)
                                    ).mean.squeeze(0) # → (3,64,64)

                # --- 3. 4-step dreamed rollout under a hand-picked cmd list -----
                cmd = ["right", "left", "left", "forward"]
                act_idx = torch.tensor([self.action_dict[a] for a in cmd],
                                    device=self.device)
                onehots = F.one_hot(act_idx, num_classes=self.action_size).float()  # (T,3)

                z = prior_z.unsqueeze(0)     # (1,Z)
                d = prior_d.unsqueeze(0)     # (1,H)
                dreams = []
                for a in onehots:
                    a = a.unsqueeze(0)       # (1,3)
                    d = self.rssm.recurrent_model(z, a, d)
                    _, z = self.rssm.transition_model(d)  # PRIOR z_{t+1}
                    frame = self.decoder(z, d).mean.squeeze(0).cpu()
                    dreams.append(frame)

                recon_last = self.decoder(
                                infos.posteriors[0, -1:],
                                infos.deterministics[0, -1:]
                            ).mean.squeeze(0)

                grid1 = torch.stack([data.observation[0, -1].cpu(),   # GT at t = T
                                    recon_last.cpu(),                # posterior recon t = T
                                    fantasy.cpu()])                  # prior recon   t = T
                # --- save both grids (truth|recon|prior  & dreams) ---------------
                #grid1 = torch.stack([t0_img.cpu(), recon.cpu(), fantasy.cpu()])
                save_image(grid1, outdir / f"recon_{self._vis_counter//xyx:05d}.png",
                        nrow=3, normalize=True)

                grid2 = torch.stack([data.observation[0, -1].cpu()] + dreams)  # 1+len(cmd) frames
                save_image(grid2, outdir / f"dream_{self._vis_counter//xyx:05d}.png",
                        nrow=len(cmd)+1, normalize=True)

        self._vis_counter += 1
        
        if self.writer is not None :
            step = self._vis_counter                     # same counter as above
            self.writer.add_scalar("loss/model" ,  model_loss.item(), step)
            self.writer.add_scalar("loss/kl"    ,  kl.item()        , step)
            self.writer.add_scalar("loss/recon" , -recon_loss.mean().item(), step)
            self.writer.add_scalar("loss/reward", -rew_loss.mean().item(), step)

        return dict(model=model_loss.item(),
                    kl=kl.item(),
                    recon=-recon_loss.mean().item(),
                    rew=-rew_loss.mean().item())

    # ..................................................................... #
    @torch.no_grad()
    def environment_interaction(self, env,
                                num_episodes,
                                train: bool = True):

        mode = "train" if train else "eval"
        for epi in range(num_episodes):
            posterior, det = self.rssm.recurrent_model_input_init(1)
            action  = torch.zeros(1, self.action_size, device=self.device)

            obs     = env.reset()
            emb_obs = self.encoder(torch.tensor(obs, dtype=torch.float32,
                                                device=self.device))

            score, steps = 0.0, 0
            done = False
            
            SAFE_STEPS = 30
            while not done:
                det = self.rssm.recurrent_model(posterior, action, det)
                emb_obs = emb_obs.reshape(1, -1)
                _, posterior = self.rssm.representation_model(emb_obs, det)


                front_pos = env.front_pos           # (x, y) tuple
                front_cell = env.grid.get(*front_pos)
                SAFE_STEPS = 30
                # --------------------------------------------------------- choose an action

                if steps < SAFE_STEPS:
                    
                    if isinstance(front_cell, Wall):
                        # there's a wall ahead → turn
                        env_act = random.choice([0, 1])  # 0=left, 1=right
                    else:
                        env_act = 2                      # 2=forward
                else:
                    # after SAFE_STEPS, pure random
                    env_act = random.randrange(0,3)
              
                buffer_act = np.eye(self.action_size, dtype=np.float32)[env_act]
            
                next_obs, reward, done, _ = env.step(env_act)
                
                

                if train:
                    self.buffer.add(obs, buffer_act, reward, next_obs, done)

                score  += reward
                steps  += 1
                emb_obs = self.encoder(torch.tensor(next_obs, dtype=torch.float32,
                                                    device=self.device))
                obs     = next_obs

            # ---------- episode finished -------------------------------- #
            if train:
                self.num_total_episode += 1
                self.writer.add_scalar("training score", score,
                                       self.num_total_episode)

            log.info("  episode %d (%s)  score: %.2f  steps: %d",
                     self.num_total_episode if train else epi+1,
                     mode, score, steps)

        # ---------- evaluation summary ---------------------------------- #
        if not train:
            self.writer.add_scalar("test score", score, self.num_total_episode)
            log.info("≈ evaluate mean-score: %.3f over %d episodes",
                     score / num_episodes, num_episodes)


```

`hierarchical-nav/dreamer_mg/dreamer/algorithms/plan2explore.py`:

```py
import torch
import torch.nn as nn
import numpy as np

from dreamer.algorithms.dreamer import Dreamer
from dreamer.modules.actor import Actor
from dreamer.modules.critic import Critic
from dreamer.modules.one_step_model import OneStepModel
from dreamer.utils.utils import (
    compute_lambda_values,
    create_normal_dist,
    DynamicInfos,
)
from dreamer.utils.buffer import ReplayBuffer


class Plan2Explore(Dreamer):
    def __init__(
        self,
        observation_shape,
        discrete_action_bool,
        action_size,
        writer,
        device,
        config,
    ):
        super().__init__(
            observation_shape, discrete_action_bool, action_size, writer, device, config
        )
        self.config = self.config + config.parameters.plan2explore

        self.intrinsic_actor = Actor(discrete_action_bool, action_size, config).to(
            self.device
        )
        self.intrinsic_critic = Critic(config).to(self.device)

        self.one_step_models = [
            OneStepModel(action_size, config).to(self.device)
            for _ in range(self.config.num_ensemble)
        ]
        self.one_step_models_params = nn.ModuleList(self.one_step_models).parameters()
        self.one_step_models_optimizer = torch.optim.Adam(
            self.one_step_models_params, lr=self.config.one_step_model_learning_rate
        )

        self.intrinsic_actor_optimizer = torch.optim.Adam(
            self.intrinsic_actor.parameters(), lr=self.config.actor_learning_rate
        )
        self.intrinsic_critic_optimizer = torch.optim.Adam(
            self.intrinsic_critic.parameters(), lr=self.config.critic_learning_rate
        )

        self.intrinsic_actor.intrinsic = True
        self.actor.intrinsic = False

    def train(self, env):
        if len(self.buffer) < 1:
            self.environment_interaction(
                self.intrinsic_actor, env, self.config.seed_episodes
            )

        for iteration in range(self.config.train_iterations):
            for collect_interval in range(self.config.collect_interval):
                data = self.buffer.sample(
                    self.config.batch_size, self.config.batch_length
                )
                posteriors, deterministics = self.dynamic_learning(data)
                self.behavior_learning(
                    self.actor,
                    self.critic,
                    self.actor_optimizer,
                    self.critic_optimizer,
                    posteriors,
                    deterministics,
                )

                self.behavior_learning(
                    self.intrinsic_actor,
                    self.intrinsic_critic,
                    self.intrinsic_actor_optimizer,
                    self.intrinsic_critic_optimizer,
                    posteriors,
                    deterministics,
                )

            self.environment_interaction(
                self.intrinsic_actor, env, self.config.num_interaction_episodes
            )
            self.evaluate(self.actor, env)

    def evaluate(self, actor, env):
        self.environment_interaction(actor, env, self.config.num_evaluate, train=False)

    def dynamic_learning(self, data):
        prior, deterministic = self.rssm.recurrent_model_input_init(len(data.action))

        data.embedded_observation = self.encoder(data.observation)

        for t in range(1, self.config.batch_length):
            deterministic = self.rssm.recurrent_model(
                prior, data.action[:, t - 1], deterministic
            )
            prior_dist, prior = self.rssm.transition_model(deterministic)
            posterior_dist, posterior = self.rssm.representation_model(
                data.embedded_observation[:, t], deterministic
            )

            self.dynamic_learning_infos.append(
                priors=prior,
                prior_dist_means=prior_dist.mean,
                prior_dist_stds=prior_dist.scale,
                posteriors=posterior,
                posterior_dist_means=posterior_dist.mean,
                posterior_dist_stds=posterior_dist.scale,
                deterministics=deterministic,
            )

            prior = posterior

        infos = self.dynamic_learning_infos.get_stacked()
        self._model_update(data, infos)
        return infos.posteriors.detach(), infos.deterministics.detach()

    def _model_update(self, data, posterior_info):
        reconstructed_observation_dist = self.decoder(
            posterior_info.posteriors, posterior_info.deterministics
        )
        reconstruction_observation_loss = reconstructed_observation_dist.log_prob(
            data.observation[:, 1:]
        )
        if self.config.use_continue_flag:
            continue_dist = self.continue_predictor(
                posterior_info.posteriors, posterior_info.deterministics
            )
            continue_loss = self.continue_criterion(
                continue_dist.probs, 1 - data.done[:, 1:]
            )

        reward_dist = self.reward_predictor(
            posterior_info.posteriors.detach(), posterior_info.deterministics.detach()
        )
        reward_loss = reward_dist.log_prob(data.reward[:, 1:])

        prior_dist = create_normal_dist(
            posterior_info.prior_dist_means,
            posterior_info.prior_dist_stds,
            event_shape=1,
        )
        posterior_dist = create_normal_dist(
            posterior_info.posterior_dist_means,
            posterior_info.posterior_dist_stds,
            event_shape=1,
        )
        kl_divergence_loss = torch.mean(
            torch.distributions.kl.kl_divergence(posterior_dist, prior_dist)
        )
        kl_divergence_loss = torch.max(
            torch.tensor(self.config.free_nats).to(self.device), kl_divergence_loss
        )
        model_loss = (
            self.config.kl_divergence_scale * kl_divergence_loss
            - reconstruction_observation_loss.mean()
            - reward_loss.mean()
        )
        if self.config.use_continue_flag:
            model_loss += continue_loss.mean()

        self.model_optimizer.zero_grad()
        model_loss.backward()
        nn.utils.clip_grad_norm_(
            self.model_params,
            self.config.clip_grad,
            norm_type=self.config.grad_norm_type,
        )
        self.model_optimizer.step()

        predicted_feature_dists = [
            x(
                data.action[:, :-1],
                posterior_info.priors.detach(),
                posterior_info.deterministics.detach(),
            )
            for x in self.one_step_models
        ]
        one_step_model_loss = -sum(
            [
                x.log_prob(data.embedded_observation[:, 1:].detach()).mean()
                for x in predicted_feature_dists
            ]
        )

        self.one_step_models_optimizer.zero_grad()
        one_step_model_loss.backward()
        self.writer.add_scalar(
            "one step model loss", one_step_model_loss, self.num_total_episode
        )
        nn.utils.clip_grad_norm_(
            self.one_step_models_params,
            self.config.clip_grad,
            norm_type=self.config.grad_norm_type,
        )
        self.one_step_models_optimizer.step()

    def behavior_learning(
        self, actor, critic, actor_optimizer, critic_optimizer, states, deterministics
    ):
        """
        #TODO : last posterior truncation(last can be last step)
        posterior shape : (batch, timestep, stochastic)
        """
        state = states.reshape(-1, self.config.stochastic_size)
        deterministic = deterministics.reshape(-1, self.config.deterministic_size)

        # continue_predictor reinit
        for t in range(self.config.horizon_length):
            action = actor(state, deterministic)
            deterministic = self.rssm.recurrent_model(state, action, deterministic)
            _, state = self.rssm.transition_model(deterministic)
            self.behavior_learning_infos.append(
                priors=state, deterministics=deterministic, actions=action
            )

        self._agent_update(
            actor,
            critic,
            actor_optimizer,
            critic_optimizer,
            self.behavior_learning_infos.get_stacked(),
        )

    def _agent_update(
        self, actor, critic, actor_optimizer, critic_optimizer, behavior_learning_infos
    ):
        if actor.intrinsic:
            predicted_feature_means = [
                x(
                    behavior_learning_infos.actions,
                    behavior_learning_infos.priors,
                    behavior_learning_infos.deterministics,
                ).mean
                for x in self.one_step_models
            ]
            predicted_feature_mean_stds = torch.stack(predicted_feature_means, 0).std(0)

            predicted_rewards = predicted_feature_mean_stds.mean(-1, keepdims=True)

        else:
            predicted_rewards = self.reward_predictor(
                behavior_learning_infos.priors, behavior_learning_infos.deterministics
            ).mean
        values = critic(
            behavior_learning_infos.priors, behavior_learning_infos.deterministics
        ).mean

        if self.config.use_continue_flag:
            continues = self.continue_predictor(
                behavior_learning_infos.priors, behavior_learning_infos.deterministics
            ).mean
        else:
            continues = self.config.discount * torch.ones_like(values)

        lambda_values = compute_lambda_values(
            predicted_rewards,
            values,
            continues,
            self.config.horizon_length,
            self.device,
            self.config.lambda_,
        )

        actor_loss = -torch.mean(lambda_values)

        actor_optimizer.zero_grad()
        actor_loss.backward()
        nn.utils.clip_grad_norm_(
            actor.parameters(),
            self.config.clip_grad,
            norm_type=self.config.grad_norm_type,
        )
        actor_optimizer.step()

        value_dist = critic(
            behavior_learning_infos.priors.detach()[:, :-1],
            behavior_learning_infos.deterministics.detach()[:, :-1],
        )
        value_loss = -torch.mean(value_dist.log_prob(lambda_values.detach()))

        critic_optimizer.zero_grad()
        value_loss.backward()
        nn.utils.clip_grad_norm_(
            critic.parameters(),
            self.config.clip_grad,
            norm_type=self.config.grad_norm_type,
        )
        critic_optimizer.step()

    @torch.no_grad()
    def environment_interaction(self, actor, env, num_interaction_episodes, train=True):
        for epi in range(num_interaction_episodes):
            posterior, deterministic = self.rssm.recurrent_model_input_init(1)
            action = torch.zeros(1, self.action_size).to(self.device)

            observation = env.reset()
            embedded_observation = self.encoder(
                torch.from_numpy(observation).float().to(self.device)
            )

            score = 0
            score_lst = np.array([])
            done = False

            while not done:
                deterministic = self.rssm.recurrent_model(
                    posterior, action, deterministic
                )
                embedded_observation = embedded_observation.reshape(1, -1)
                _, posterior = self.rssm.representation_model(
                    embedded_observation, deterministic
                )
                action = actor(posterior, deterministic).detach()

                if self.discrete_action_bool:
                    buffer_action = action.cpu().numpy()
                    env_action = buffer_action.argmax()

                else:
                    buffer_action = action.cpu().numpy()[0]
                    env_action = buffer_action

                next_observation, reward, done, info = env.step(env_action)
                if train:
                    self.buffer.add(
                        observation, buffer_action, reward, next_observation, done
                    )
                score += reward
                embedded_observation = self.encoder(
                    torch.from_numpy(next_observation).float().to(self.device)
                )
                observation = next_observation
                if done:
                    if train:
                        self.num_total_episode += 1
                        self.writer.add_scalar(
                            "training score", score, self.num_total_episode
                        )
                    else:
                        score_lst = np.append(score_lst, score)
                    break
        if not train:
            evaluate_score = score_lst.mean()
            print("evaluate score : ", evaluate_score)
            self.writer.add_scalar("test score", evaluate_score, self.num_total_episode)

```

`hierarchical-nav/dreamer_mg/dreamer/configs/atari-boxing-v4.yml`:

```yml
#atari-boxing-v4.yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : atari-boxing-v4
    device : cuda
environment :
    benchmark : atari
    task_name : Boxing-v4
    seed : 0
    height : 64
    width : 64
    frame_skip : 4
    pixel_norm : True
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00004
        critic_learning_rate : 0.0001
        model_learning_rate : 0.0002
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 600
        stochastic_size : 600
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 

```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-acrobot-swingup.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : acrobot-swingup
    device : cuda
environment :
    benchmark : dmc
    domain_name : acrobot
    task_name : swingup
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-ball-in-cup-catch.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : ball-in-cup-catch
    device : cuda
environment :
    benchmark : dmc
    domain_name : ball_in_cup
    task_name : catch
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-cartpole-balance-sparse.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : cartpole-balance-sparse
    device : cuda
environment :
    benchmark : dmc
    domain_name : cartpole
    task_name : balance_sparse
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True

parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-cartpole-balance.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : cartpole-balance
    device : cuda
environment :
    benchmark : dmc
    domain_name : cartpole
    task_name : balance
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-cartpole-swingup-sparse.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : cartpole-swingup-sparse
    device : cuda
environment :
    benchmark : dmc
    domain_name : cartpole
    task_name : swingup_sparse
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-cartpole-swingup.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : cartpole-swingup
    device : cuda
environment :
    benchmark : dmc
    domain_name : cartpole
    task_name : swingup
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-cheetah-run.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : cheetah-run
    device : cuda
environment :
    benchmark : dmc
    domain_name : cheetah
    task_name : run
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-finger-spin.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : finger-spin
    device : cuda
environment :
    benchmark : dmc
    domain_name : finger
    task_name : spin
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True

parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-finger-turn-easy.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : finger-turn_easy
    device : cuda
environment :
    benchmark : dmc
    domain_name : finger
    task_name : turn_easy
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True

parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-finger-turn-hard.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : finger-turn_hard
    device : cuda
environment :
    benchmark : dmc
    domain_name : finger
    task_name : turn_hard
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-hopper-hop.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : hopper-hop
    device : cuda
environment :
    benchmark : dmc
    domain_name : hopper
    task_name : hop
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-hopper-stand.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : hopper-stand
    device : cuda
environment :
    benchmark : dmc
    domain_name : hopper
    task_name : stand
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-pendulum-swingup.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : pendulum-swingup
    device : cuda
environment :
    benchmark : dmc
    domain_name : pendulum
    task_name : swingup
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True

parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-quadruped-run.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : quadruped-run
    device : cuda
environment :
    benchmark : dmc
    domain_name : quadruped
    task_name : run
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-quadruped-walk.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : quadruped-walk
    device : cuda
environment :
    benchmark : dmc
    domain_name : quadruped
    task_name : walk
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-reacher-easy.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : reacher-easy
    device : cuda
environment :
    benchmark : dmc
    domain_name : reacher
    task_name : easy
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-reacher-hard.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : reacher-hard
    device : cuda
environment :
    benchmark : dmc
    domain_name : reacher
    task_name : hard
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True

parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-walker-run.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : walker-run
    device : cuda
environment :
    benchmark : dmc
    domain_name : walker
    task_name : run
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True

parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-walker-stand.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : walker-stand
    device : cuda
environment :
    benchmark : dmc
    domain_name : walker
    task_name : stand
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/dmc-walker-walk.yml`:

```yml
algorithm : dreamer-v1
operation :
    save : False
    log_dir : walker-walk
    device : cuda
environment :
    benchmark : dmc
    domain_name : walker
    task_name : walk
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True
    
parameters :
    dreamer :
        train_iterations : 1000
        horizon_length : 15
        batch_size : 50
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0006
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 2

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/configs/minigrid-default-temp.yml`:

```yml
algorithm: dreamer-v1

# ────────────────────────────────  RUNTIME  ────────────────────────────────
operation:
  save:      false              # if you want checkpoints set to true
  log_dir:   mg-test            # folder inside ./runs/
  device:    cpu               # "cpu" if you don’t have a GPU

# ──────────────────────────────  ENVIRONMENT  ──────────────────────────────
environment:
  benchmark:  minigrid
  task_name:  MiniGrid-ADRooms-Collision-v0
  rooms_row:  3                 # custom arguments your env builder needs
  rooms_col:  4
  height:     56                # the wrappers already give you 56×56×3
  width:      56
  frame_skip: 1
  pixel_norm: true              # normalise to [0,1] inside env wrapper
  seed:       0

# ───────────────────────────────  HYPER-PARAMS  ────────────────────────────
parameters:
  dreamer:
    # training loop ---------------------------------------------------------
    train_iterations:          5000
    horizon_length:            15
    batch_size:                32
    batch_length:              30
    collect_interval:          100
    num_interaction_episodes:  1
    seed_episodes:             2000
    num_evaluate:              3

    # objective -------------------------------------------------------------
    kl_divergence_scale:       1
    free_nats:                 3
    discount:                  0.99
    lambda_:                   0.95

    # optimisation ----------------------------------------------------------
    use_continue_flag:         false
    actor_learning_rate:       4.0e-5
    critic_learning_rate:      1.0e-4
    model_learning_rate:       2.0e-4
    grad_norm_type:            2
    clip_grad:                 100

    # model sizes -----------------------------------------------------------
    deterministic_size:        200
    stochastic_size:           34
    embedded_state_size:   1024

    buffer:
      capacity:               500000      # MiniGrid is cheap; smaller than Atari

    # ── building blocks ────────────────────────────────────────────────────
    encoder:                     # ConvNet that encodes 56×56×3 → latent
      depth:         32
      stride:         2
      kernel_size:    4
      activation:     ReLU        # ELU / SiLU also work

    decoder:                     # ConvNet for image reconstruction
      depth:         32
      stride:         2
      kernel_size:    5
      activation:     ReLU

    rssm:                        # Recurrent State-Space Model
      recurrent_model:
        hidden_size: 200
        activation:  ELU

      transition_model:
        hidden_size: 200
        num_layers:  2
        activation:  ELU
        min_std:     0.1

      representation_model:
        hidden_size: 200
        num_layers:  2
        activation:  ELU
        min_std:     0.1

    reward:
      hidden_size: 400
      num_layers:  2
      activation:  ELU

    continue_:
      hidden_size: 400
      num_layers:  3
      activation:  ELU

    agent:
      actor:
        hidden_size: 400
        min_std:     1.0e-4
        init_std:    5.0
        mean_scale:  5.0
        activation:  ELU
        num_layers:  2

      critic:
        hidden_size: 400
        activation:  ELU
        num_layers:  3
```

`hierarchical-nav/dreamer_mg/dreamer/configs/minigrid-default.yml`:

```yml
#minigrid-default
# ───────────────────────────────  TOP LEVEL  ────────────────────────────────
algorithm: dreamer-v1

# ────────────────────────────────  RUNTIME  ────────────────────────────────
operation:
  save:      false              # if you want checkpoints set to true
  log_dir:   mg-test            # folder inside ./runs/
  device:    cpu               # "cpu" if you don’t have a GPU

# ──────────────────────────────  ENVIRONMENT  ──────────────────────────────
environment:
  benchmark:  minigrid
  task_name:  MiniGrid-ADRooms-Collision-v0
  rooms_row:  3                 # custom arguments your env builder needs
  rooms_col:  4
  height:     56                # the wrappers already give you 56×56×3
  width:      56
  frame_skip: 1
  pixel_norm: true              # normalise to [0,1] inside env wrapper
  seed:       0

# ───────────────────────────────  HYPER-PARAMS  ────────────────────────────
parameters:
  dreamer:
    # training loop ---------------------------------------------------------
    train_iterations:          5000
    horizon_length:            15
    batch_size:                32
    batch_length:              30
    collect_interval:          100
    num_interaction_episodes:  1
    seed_episodes:             10
    num_evaluate:              3

    # objective -------------------------------------------------------------
    kl_divergence_scale:       1
    free_nats:                 1
    discount:                  0.99
    lambda_:                   0.95

    # optimisation ----------------------------------------------------------
    use_continue_flag:         false
    model_learning_rate:       2.0e-4
    grad_norm_type:            2
    clip_grad:                 100

    # model sizes -----------------------------------------------------------
    deterministic_size:        600
    stochastic_size:           600
    embedded_state_size:   1024

    buffer:
      capacity:               500000      # MiniGrid is cheap; smaller than Atari

    # ── building blocks ────────────────────────────────────────────────────
    encoder:                     # ConvNet that encodes 56×56×3 → latent
      depth:         32
      stride:         2
      kernel_size:    4
      activation:     ReLU        # ELU / SiLU also work

    decoder:                     # ConvNet for image reconstruction
      depth:         32
      stride:         2
      kernel_size:    5
      activation:     ReLU

    rssm:                        # Recurrent State-Space Model
      recurrent_model:
        hidden_size: 200
        activation:  ELU

      transition_model:
        hidden_size: 200
        num_layers:  2
        activation:  ELU
        min_std:     0.1

      representation_model:
        hidden_size: 200
        num_layers:  2
        activation:  ELU
        min_std:     0.1

    reward:
      hidden_size: 400
      num_layers:  2
      activation:  ELU
    agent :
      actor :
        hidden_size : 400
        min_std : 0.0001
        init_std : 5.
        mean_scale : 5
        activation : ELU
        num_layers : 2

      critic : 
          hidden_size : 400
          activation : ELU
          num_layers : 3 


  
```

`hierarchical-nav/dreamer_mg/dreamer/configs/p2e-dmc-walker-walk.yml`:

```yml
algorithm : plan2explore

operation :
    save : False
    log_dir : batch-15-walker-walk
    device : cuda
environment :
    benchmark : dmc
    domain_name : walker
    task_name : walk
    seed : 0
    visualize_reward : False
    from_pixels : True
    height : 64
    width : 64
    frame_skip : 2
    pixel_norm : True

parameters :
    plan2explore :
        num_ensemble : 10
        one_step_model_learning_rate : 0.0003
        
        one_step_model : 
            hidden_size : 400
            activation : ELU
            num_layers : 4
    dreamer :
        train_iterations : 4000
        horizon_length : 15
        batch_size : 15
        batch_length : 50
        collect_interval : 100
        num_interaction_episodes : 1
        seed_episodes : 5
        num_evaluate : 3
        
        kl_divergence_scale : 1
        free_nats : 3
        discount : 0.99
        lambda_ : 0.95
        
        use_continue_flag : False
        actor_learning_rate : 0.00008
        critic_learning_rate : 0.00008
        model_learning_rate : 0.0003
        grad_norm_type : 2
        clip_grad : 100
        
        deterministic_size : 200
        stochastic_size : 30
        embedded_state_size : 1024
        buffer :
            capacity : 5000000
        
        encoder : 
            depth : 32
            stride : 2
            kernel_size : 4
            activation : ReLU

        decoder : 
            depth : 32
            stride : 2
            kernel_size : 5
            activation : ReLU

        rssm :
            recurrent_model : 
                hidden_size : 200
                activation : ELU

            transition_model : 
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1

            representation_model:
                hidden_size : 200
                num_layers : 2
                activation : ELU
                min_std : 0.1
            
        reward :
            hidden_size : 400
            num_layers : 2
            activation : ELU

        continue_ :
            hidden_size : 400
            num_layers : 3
            activation : ELU
        
        agent :
            actor :
                hidden_size : 400
                min_std : 0.0001
                init_std : 5.
                mean_scale : 5
                activation : ELU
                num_layers : 4

            critic : 
                hidden_size : 400
                activation : ELU
                num_layers : 3 
```

`hierarchical-nav/dreamer_mg/dreamer/envs/envs.py`:

```py
import gym
import numpy as np
import os
from pathlib import Path
from gym import spaces

# --------------------------------------------------------------------------
# Make sure we have a ResizeObservation wrapper even on old Gym versions
# --------------------------------------------------------------------------
try:
    from gym.wrappers import ResizeObservation        # Gym >= 0.21
except (ImportError, AttributeError):
    # Gym < 0.21: roll our own ------------------------------------------------
    from gym import spaces
    import cv2

    class ResizeObservation(gym.ObservationWrapper):
        """
        Resize (H, W, C) observations to a user-defined (new_h, new_w).
        Works exactly like gym.wrappers.ResizeObservation that was added later.
        """
        def __init__(self, env, shape):
            super().__init__(env)
            if isinstance(shape, int):
                shape = (shape, shape)
            self.shape = tuple(shape)
            old_space = env.observation_space
            assert len(old_space.shape) == 3   # HWC
            self.observation_space = spaces.Box(
                low=old_space.low.min(),
                high=old_space.high.max(),
                shape=(self.shape[0], self.shape[1], old_space.shape[2]),
                dtype=old_space.dtype,
            )

        def observation(self, obs):
            # obs is H×W×C uint8 (after MiniGrid wrappers)
            obs = cv2.resize(obs, self.shape, interpolation=cv2.INTER_AREA)
            return obs
# --------------------------------------------------------------------------
from dreamer.envs.wrappers import ChannelFirstEnv, PixelNormalization, SkipFrame



def make_atari_env(task_name, skip_frame, width, height, seed, pixel_norm=True):
    env = gym.make(task_name)
    env = gym.wrappers.ResizeObservation(env, (height, width))
    env = ChannelFirstEnv(env)
    env = SkipFrame(env, skip_frame)
    if pixel_norm:
        env = PixelNormalization(env)
    env.seed(seed)
    return env


def get_env_infos(env):
    obs_shape = env.observation_space.shape
    if isinstance(env.action_space, gym.spaces.Discrete):
        discrete_action_bool = True
        action_size = env.action_space.n
    elif isinstance(env.action_space, gym.spaces.Box):
        discrete_action_bool = False
        action_size = env.action_space.shape[0]
    else:
        raise Exception
    return obs_shape, discrete_action_bool, action_size

def make_minigrid_env(
        task_name="MiniGrid-ADRooms-Collision-v0",
        rooms_row=3,
        rooms_col=4,
        frame_skip=1,
        height=64,
        width=64,
        pixel_norm=True,
        run_dir= Path
        
    ):
        """
        Returns a Dreamer-ready MiniGrid environment that emits 56×56×3 images.

        Parameters
        ----------
        task_name   : str   Name registered in your fork (e.g. MiniGrid-*-v0)
        rooms_row   : int   passed to env constructor
        rooms_col   : int
        seed        : int   RNG seed
        frame_skip  : int   >1 will SkipFrame just like Atari helper
        pixel_norm  : bool  apply PixelNormalization (-0.5…0.5 floats)
        """
        import gym_minigrid
        from gym_minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper
        env = gym.make(
            task_name,
            rooms_in_row=rooms_row,
            rooms_in_col=rooms_col
        )
        # (7×7×3) → (56×56×3) is already done by your custom wrappers inside
        env = ThreeActionWrapper(env)
        env = RGBImgPartialObsWrapper(env)   # adds "image" key
        env = ImgObsWrapper(env)             # drop everything except image

        env = gym.wrappers.ResizeObservation(env, (height, width))      # 64×64×3
        env = ChannelFirstEnv(env)                                     # 3×64×64

        if os.getenv("DREAMER_RENDER") == "1":
            out_dir = (run_dir or Path.cwd() / "videos") / "videos"
            out_dir.mkdir(parents=True, exist_ok=True)
            env = VideoEveryN(env, out_dir=out_dir, every=500)    
            env._video_prefix = task_name.replace("MiniGrid-", "")
            # --- generic Dreamer wrappers --------------------------------------
        if pixel_norm:
            env = PixelNormalization(env)            # float32 -0.5…0.5

        env.seed(1337)
        return env
class ThreeActionWrapper(gym.core.ActionWrapper):
    """
    Strip MiniGrid’s 7-action A PI down to LEFT / RIGHT / FORWARD.
    Incoming action ∈ {0,1,2}.  We just forward it unchanged.
    """
    def __init__(self, env):
        super().__init__(env)
        # overwrite the public action_space so that agents & Dreamer
        # know they can only output 3 discrete values
        self.action_space = spaces.Discrete(3)

    # ---- mapping new-space → original-space ---------------------------
    def action(self, act):
        # act is already int{0,1,2}.  In MiniGrid’s native space
        # left=0  right=1  forward=2, so we just pass it through.
        return int(act)

    # ---- mapping original-space → new-space (needed for .render() etc.)
    def reverse_action(self, act):
        # called by wrappers that may need to show keyboard hints, etc.
        # We only ever generate 0-2, so this is also identity.
        return int(act)
'''class EpisodicStats(gym.Wrapper):
    """
    Adds `info["episodic_return"]` and `info["episodic_length"]`
    on the final `step()` of every episode.
    """
    def __init__(self, env):
        super().__init__(env)
        self._R = 0.0
        self._L = 0

    def reset(self, **kw):
        self._R, self._L = 0.0, 0
        return self.env.reset(**kw)

    def step(self, action):
        obs, rew, done, info = self.env.step(action)
        self._R += rew
        self._L += 1
        if done:
            info["episodic_return"]  = self._R
            info["episodic_length"]  = self._L
        return obs, rew, done, info'''
import imageio, numpy as np, datetime as dt, pathlib, gym

class VideoEveryN(gym.Wrapper):
    """
    Records an MP4 of every *N*-th episode by calling env.render("rgb_array").
    Works with **any** Gym version.
    """
    def __init__(self, env, out_dir, every=500, prefix="episode"):
        super().__init__(env)
        self.every      = every
        self.ep_counter = 0
        self.out_dir    = pathlib.Path(out_dir)
        self.out_dir.mkdir(parents=True, exist_ok=True)

    def reset(self, **kw):
        obs = super().reset(**kw)
        self.frames = [self.render(mode="rgb_array")]
        return obs

    def step(self, action):
        obs, rew, done, info = super().step(action)
        self.frames.append(self.render(mode="rgb_array"))
        if done:
            self._maybe_write_video()
        return obs, rew, done, info

    # ------------------------------------------------------------------ #
    def _maybe_write_video(self):
        if self.ep_counter % self.every == 0:
            prefix = getattr(self, "_video_prefix", "episode")          # ← NEW line
            fname  = f"{prefix}_{self.ep_counter:05d}.mp4"
            path  = str(self.out_dir / fname)
            imageio.mimsave(path, self.frames, fps=10, macro_block_size=None)
            print(f"[video] saved {path}  ({len(self.frames)} f)")
        self.ep_counter += 1
```

`hierarchical-nav/dreamer_mg/dreamer/envs/wrappers.py`:

```py
import gym
import numpy as np


class ChannelFirstEnv(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        obs_space = self.observation_space
        obs_shape = obs_space.shape[-1:] + obs_space.shape[:2]
        self.observation_space = gym.spaces.Box(
            low=0, high=255, shape=obs_shape, dtype=np.uint8
        )

    def _permute_orientation(self, observation):
        # permute [H, W, C] array to [C, H, W] tensor
        observation = np.transpose(observation, (2, 0, 1))
        return observation

    def observation(self, observation):
        observation = self._permute_orientation(observation)
        return observation


class SkipFrame(gym.Wrapper):
    def __init__(self, env, skip):
        super().__init__(env)
        self._skip = skip

    def step(self, action):
        total_reward = 0.0
        for i in range(self._skip):
            obs, reward, done, info = self.env.step(action)
            total_reward += reward
            if done:
                break
        return obs, total_reward, done, info


class PixelNormalization(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)

    def _pixel_normalization(self, obs):
        return obs / 255.0 - 0.5

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        return self._pixel_normalization(obs), reward, done, info

    def reset(self):
        obs = self.env.reset()
        return self._pixel_normalization(obs)

```

`hierarchical-nav/dreamer_mg/dreamer/export_world_model.py`:

```py
import argparse, torch, importlib, pathlib

p = argparse.ArgumentParser()
p.add_argument("ckpt",  help="path/to/iterXXXXX.pt")
p.add_argument("--no-decoder",  action="store_true")
args = p.parse_args()

Device = "cpu"        # export on CPU

# ------------------------------------------------------------------ #
# 1)  load config + construct *matching* modules
ckpt = torch.load(args.ckpt, map_location=Device)
Dreamer = importlib.import_module("dreamer.algorithms.dreamer").Dreamer

# you need a config instance identical to training time
from dreamer.utils.utils import load_config
cfg = load_config("configs/minigrid-default.yml")       # or use ckpt["cfg"]

dummy = Dreamer((3,64,64), True, 3,            # observation shape, …
                writer=None, device=Device,
                config=cfg, run_dir=None)      # ← creates fresh modules

wanted = {"encoder", "rssm"}
if not args.no_decoder:
    wanted.add("decoder")

export = torch.nn.ModuleDict({k: Dreamer._modules(dummy)[k] for k in wanted})
for k in wanted:
    export[k].load_state_dict(ckpt["modules"][k])
export.eval()

out = pathlib.Path(args.ckpt).with_suffix(f"_{'-'.join(sorted(wanted))}.pt")
torch.save(export.state_dict(), out)
print("exported →", out)
```

`hierarchical-nav/dreamer_mg/dreamer/modules/actor.py`:

```py
import torch
import torch.nn as nn
from torch.distributions import TanhTransform

from dreamer.utils.utils import create_normal_dist, build_network


class Actor(nn.Module):
    def __init__(self, discrete_action_bool, action_size, config):
        super().__init__()
        self.config = config.parameters.dreamer.agent.actor
        self.discrete_action_bool = discrete_action_bool
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        action_size = action_size if discrete_action_bool else 2 * action_size

        self.network = build_network(
            self.stochastic_size + self.deterministic_size,
            self.config.hidden_size,
            self.config.num_layers,
            self.config.activation,
            action_size,
        )

    def forward(self, posterior, deterministic):
        x = torch.cat((posterior, deterministic), -1)
        x = self.network(x)
        if self.discrete_action_bool:
            dist = torch.distributions.OneHotCategorical(logits=x)
            action = dist.sample() + dist.probs - dist.probs.detach()
        else:
            dist = create_normal_dist(
                x,
                mean_scale=self.config.mean_scale,
                init_std=self.config.init_std,
                min_std=self.config.min_std,
                activation=torch.tanh,
            )
            dist = torch.distributions.TransformedDistribution(dist, TanhTransform())
            action = torch.distributions.Independent(dist, 1).rsample()
        return action

```

`hierarchical-nav/dreamer_mg/dreamer/modules/critic.py`:

```py
import torch
import torch.nn as nn
from dreamer.utils.utils import build_network, create_normal_dist, horizontal_forward


class Critic(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config.parameters.dreamer.agent.critic
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        self.network = build_network(
            self.stochastic_size + self.deterministic_size,
            self.config.hidden_size,
            self.config.num_layers,
            self.config.activation,
            1,
        )

    def forward(self, posterior, deterministic):
        x = horizontal_forward(
            self.network, posterior, deterministic, output_shape=(1,)
        )
        dist = create_normal_dist(x, std=1, event_shape=1)
        return dist

```

`hierarchical-nav/dreamer_mg/dreamer/modules/decoder.py`:

```py
import torch.nn as nn

from dreamer.utils.utils import (
    initialize_weights,
    horizontal_forward,
    create_normal_dist,
)


class Decoder(nn.Module):
    def __init__(self, observation_shape, config):
        super().__init__()
        self.config = config.parameters.dreamer.decoder
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        activation = getattr(nn, self.config.activation)()
        self.observation_shape = observation_shape

        self.network = nn.Sequential(
            nn.Linear(
                self.deterministic_size + self.stochastic_size, self.config.depth * 32
            ),
            nn.Unflatten(1, (self.config.depth * 32, 1)),
            nn.Unflatten(2, (1, 1)),
            nn.ConvTranspose2d(
                self.config.depth * 32,
                self.config.depth * 4,
                self.config.kernel_size,
                self.config.stride,
            ),
            activation,
            nn.ConvTranspose2d(
                self.config.depth * 4,
                self.config.depth * 2,
                self.config.kernel_size,
                self.config.stride,
            ),
            activation,
            nn.ConvTranspose2d(
                self.config.depth * 2,
                self.config.depth * 1,
                self.config.kernel_size + 1,
                self.config.stride,
            ),
            activation,
            nn.ConvTranspose2d(
                self.config.depth * 1,
                self.observation_shape[0],
                self.config.kernel_size + 1,
                self.config.stride,
            ),
        )
        self.network.apply(initialize_weights)

    def forward(self, posterior, deterministic):
        x = horizontal_forward(
            self.network, posterior, deterministic, output_shape=self.observation_shape
        )
        dist = create_normal_dist(x, std=1, event_shape=len(self.observation_shape))
        return dist

```

`hierarchical-nav/dreamer_mg/dreamer/modules/encoder.py`:

```py
import torch
import torch.nn as nn

from dreamer.utils.utils import (
    initialize_weights,
    horizontal_forward,
)


class Encoder(nn.Module):
    def __init__(self, observation_shape, config):
        super().__init__()
        self.config = config.parameters.dreamer.encoder

        activation = getattr(nn, self.config.activation)()
        self.observation_shape = observation_shape

        self.network = nn.Sequential(
            nn.Conv2d(
                self.observation_shape[0],
                self.config.depth * 1,
                self.config.kernel_size,
                self.config.stride,
            ),
            activation,
            nn.Conv2d(
                self.config.depth * 1,
                self.config.depth * 2,
                self.config.kernel_size,
                self.config.stride,
            ),
            activation,
            nn.Conv2d(
                self.config.depth * 2,
                self.config.depth * 4,
                self.config.kernel_size,
                self.config.stride,
            ),
            activation,
            nn.Conv2d(
                self.config.depth * 4,
                self.config.depth * 8,
                self.config.kernel_size,
                self.config.stride,
            ),
            activation,
        )
        self.network.apply(initialize_weights)

    def forward(self, x):
        return horizontal_forward(self.network, x, input_shape=self.observation_shape)

```

`hierarchical-nav/dreamer_mg/dreamer/modules/model.py`:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal

from dreamer.utils.utils import create_normal_dist, build_network, horizontal_forward


class RSSM(nn.Module):
    def __init__(self, action_size, config):
        super().__init__()
        self.config = config.parameters.dreamer.rssm

        self.recurrent_model = RecurrentModel(action_size, config)
        self.transition_model = TransitionModel(config)
        self.representation_model = RepresentationModel(config)

    def recurrent_model_input_init(self, batch_size):
        return self.transition_model.input_init(
            batch_size
        ), self.recurrent_model.input_init(batch_size)


class RecurrentModel(nn.Module):
    def __init__(self, action_size, config):
        super().__init__()
        self.config = config.parameters.dreamer.rssm.recurrent_model
        self.device = config.operation.device
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        self.activation = getattr(nn, self.config.activation)()

        self.linear = nn.Linear(
            self.stochastic_size + action_size, self.config.hidden_size
        )
        self.recurrent = nn.GRUCell(self.config.hidden_size, self.deterministic_size)

    def forward(self, embedded_state, action, deterministic):
        x = torch.cat((embedded_state, action), 1)
        x = self.activation(self.linear(x))
        x = self.recurrent(x, deterministic)
        return x

    def input_init(self, batch_size):
        return torch.zeros(batch_size, self.deterministic_size).to(self.device)


class TransitionModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config.parameters.dreamer.rssm.transition_model
        self.device = config.operation.device
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        self.network = build_network(
            self.deterministic_size,
            self.config.hidden_size,
            self.config.num_layers,
            self.config.activation,
            self.stochastic_size * 2,
        )

    def forward(self, x):
        x = self.network(x)
        prior_dist = create_normal_dist(x, min_std=self.config.min_std)
        prior = prior_dist.rsample()
        return prior_dist, prior

    def input_init(self, batch_size):
        return torch.zeros(batch_size, self.stochastic_size).to(self.device)


class RepresentationModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config.parameters.dreamer.rssm.representation_model
        self.embedded_state_size = config.parameters.dreamer.embedded_state_size
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        self.network = build_network(
            self.embedded_state_size + self.deterministic_size,
            self.config.hidden_size,
            self.config.num_layers,
            self.config.activation,
            self.stochastic_size * 2,
        )

    def forward(self, embedded_observation, deterministic):
        x = self.network(torch.cat((embedded_observation, deterministic), 1))
        posterior_dist = create_normal_dist(x, min_std=self.config.min_std)
        posterior = posterior_dist.rsample()
        return posterior_dist, posterior


class RewardModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config.parameters.dreamer.reward
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        self.network = build_network(
            self.stochastic_size + self.deterministic_size,
            self.config.hidden_size,
            self.config.num_layers,
            self.config.activation,
            1,
        )

    def forward(self, posterior, deterministic):
        x = horizontal_forward(
            self.network, posterior, deterministic, output_shape=(1,)
        )
        dist = create_normal_dist(x, std=1, event_shape=1)
        return dist


class ContinueModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config.parameters.dreamer.continue_
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        self.network = build_network(
            self.stochastic_size + self.deterministic_size,
            self.config.hidden_size,
            self.config.num_layers,
            self.config.activation,
            1,
        )

    def forward(self, posterior, deterministic):
        x = horizontal_forward(
            self.network, posterior, deterministic, output_shape=(1,)
        )
        dist = torch.distributions.Bernoulli(logits=x)
        return dist

```

`hierarchical-nav/dreamer_mg/dreamer/modules/one_step_model.py`:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal

from dreamer.utils.utils import create_normal_dist, build_network, horizontal_forward


class OneStepModel(nn.Module):
    def __init__(self, action_size, config):
        """
        For plan2explore
        There are several variations, but in our implementation,
        we use stochastic and deterministic actions as input and embedded observations as output
        """
        super().__init__()
        self.config = config.parameters.plan2explore.one_step_model
        self.embedded_state_size = config.parameters.dreamer.embedded_state_size
        self.stochastic_size = config.parameters.dreamer.stochastic_size
        self.deterministic_size = config.parameters.dreamer.deterministic_size

        self.action_size = action_size

        self.network = build_network(
            self.deterministic_size + self.stochastic_size + action_size,
            self.config.hidden_size,
            self.config.num_layers,
            self.config.activation,
            self.embedded_state_size,
        )

    def forward(self, action, stochastic, deterministic):
        stoch_deter = torch.concat((stochastic, deterministic), axis=-1)
        x = horizontal_forward(
            self.network,
            action,
            stoch_deter,
            output_shape=(self.embedded_state_size,),
        )
        dist = create_normal_dist(x, std=1, event_shape=1)
        return dist

```

`hierarchical-nav/dreamer_mg/dreamer/utils/buffer.py`:

```py
from dreamer.utils.utils import attrdict_monkeypatch_fix

attrdict_monkeypatch_fix()

from attrdict import AttrDict
import numpy as np
import torch

class ReplayBuffer(object):
    def __init__(self, observation_shape, action_size, device, config):
        self.config = config.parameters.dreamer.buffer
        self.device = device
        self.capacity = int(self.config.capacity)

        state_type = np.uint8 if len(observation_shape) < 3 else np.float32

        self.observation = np.empty(
            (self.capacity, *observation_shape), dtype=state_type
        )
        self.next_observation = np.empty(
            (self.capacity, *observation_shape), dtype=state_type
        )
        self.action = np.empty((self.capacity, action_size), dtype=np.float32)
        self.reward = np.empty((self.capacity, 1), dtype=np.float32)
        self.done = np.empty((self.capacity, 1), dtype=np.float32)

        self.buffer_index = 0
        self.full = False

    def __len__(self):
        return self.capacity if self.full else self.buffer_index

    def add(self, observation, action, reward, next_observation, done):
        self.observation[self.buffer_index] = observation
        self.action[self.buffer_index] = action
        self.reward[self.buffer_index] = reward
        self.next_observation[self.buffer_index] = next_observation
        self.done[self.buffer_index] = done

        self.buffer_index = (self.buffer_index + 1) % self.capacity
        self.full = self.full or self.buffer_index == 0

    def sample(self, batch_size, chunk_size):
        """
        (batch_size, chunk_size, input_size)
        """
        last_filled_index = self.buffer_index - chunk_size + 1
        assert self.full or (
            last_filled_index > batch_size
        ), "too short dataset or too long chunk_size"
        sample_index = np.random.randint(
            0, self.capacity if self.full else last_filled_index, batch_size
        ).reshape(-1, 1)
        chunk_length = np.arange(chunk_size).reshape(1, -1)

        sample_index = (sample_index + chunk_length) % self.capacity

        observation = torch.as_tensor(
            self.observation[sample_index], device=self.device
        ).float()
        next_observation = torch.as_tensor(
            self.next_observation[sample_index], device=self.device
        ).float()

        action = torch.as_tensor(self.action[sample_index], device=self.device)
        reward = torch.as_tensor(self.reward[sample_index], device=self.device)
        done = torch.as_tensor(self.done[sample_index], device=self.device)

        sample = AttrDict(
            {
                "observation": observation,
                "action": action,
                "reward": reward,
                "next_observation": next_observation,
                "done": done,
            }
        )
        return sample

```

`hierarchical-nav/dreamer_mg/dreamer/utils/utils.py`:

```py

def attrdict_monkeypatch_fix():
    import collections
    import collections.abc
    for type_name in collections.abc.__all__:
            setattr(collections, type_name, getattr(collections.abc, type_name))
attrdict_monkeypatch_fix()

import os
import os, datetime, pathlib, yaml, shutil
import torch
import torch.nn as nn
import torch.nn.functional as F

import yaml
from attrdict import AttrDict


def new_run_dir(base="./runs", exp_name="minigrid"):
    ts   = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    path = pathlib.Path(base) / exp_name / ts
    path.mkdir(parents=True, exist_ok=False)
    (path / "ckpt").mkdir()
    return path

def horizontal_forward(network, x, y=None, input_shape=(-1,), output_shape=(-1,)):
    batch_with_horizon_shape = x.shape[: -len(input_shape)]
    if not batch_with_horizon_shape:
        batch_with_horizon_shape = (1,)
    if y is not None:
        x = torch.cat((x, y), -1)
        input_shape = (x.shape[-1],)  #
    x = x.reshape(-1, *input_shape)
    x = network(x)

    x = x.reshape(*batch_with_horizon_shape, *output_shape)
    return x


def build_network(input_size, hidden_size, num_layers, activation, output_size):
    assert num_layers >= 2, "num_layers must be at least 2"
    activation = getattr(nn, activation)()
    layers = []
    layers.append(nn.Linear(input_size, hidden_size))
    layers.append(activation)

    for i in range(num_layers - 2):
        layers.append(nn.Linear(hidden_size, hidden_size))
        layers.append(activation)

    layers.append(nn.Linear(hidden_size, output_size))

    network = nn.Sequential(*layers)
    network.apply(initialize_weights)
    return network


def initialize_weights(m):
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
        nn.init.kaiming_uniform_(m.weight.data, nonlinearity="relu")
        nn.init.constant_(m.bias.data, 0)
    elif isinstance(m, nn.Linear):
        nn.init.kaiming_uniform_(m.weight.data)
        nn.init.constant_(m.bias.data, 0)


def create_normal_dist(
    x,
    std=None,
    mean_scale=1,
    init_std=0,
    min_std=0.1,
    activation=None,
    event_shape=None,
):
    if std == None:
        mean, std = torch.chunk(x, 2, -1)
        mean = mean / mean_scale
        if activation:
            mean = activation(mean)
        mean = mean_scale * mean
        std = F.softplus(std + init_std) + min_std
    else:
        mean = x
    dist = torch.distributions.Normal(mean, std)
    if event_shape:
        dist = torch.distributions.Independent(dist, event_shape)
    return dist


def compute_lambda_values(rewards, values, continues, horizon_length, device, lambda_):
    """
    rewards : (batch_size, time_step, hidden_size)
    values : (batch_size, time_step, hidden_size)
    continue flag will be added
    """
    rewards = rewards[:, :-1]
    continues = continues[:, :-1]
    next_values = values[:, 1:]
    last = next_values[:, -1]
    inputs = rewards + continues * next_values * (1 - lambda_)

    outputs = []
    # single step
    for index in reversed(range(horizon_length - 1)):
        last = inputs[:, index] + continues[:, index] * lambda_ * last
        outputs.append(last)
    returns = torch.stack(list(reversed(outputs)), dim=1).to(device)
    return returns


class DynamicInfos:
    def __init__(self, device):
        self.device = device
        self.data = {}

    def append(self, **kwargs):
        for key, value in kwargs.items():
            if key not in self.data:
                self.data[key] = []
            self.data[key].append(value)

    def get_stacked(self, time_axis=1):
        stacked_data = AttrDict(
            {
                key: torch.stack(self.data[key], dim=time_axis).to(self.device)
                for key in self.data
            }
        )
        self.clear()
        return stacked_data

    def clear(self):
        self.data = {}


def find_file(file_name: str) -> str:
    """
    Walk downward from the current working directory until we find *file_name*.

    If *file_name* contains path components (e.g. 'configs/foo.yml') we ignore
    them and match only the basename, so either of the following now works:

        find_file('configs/minigrid-default.yml')
        find_file('minigrid-default.yml')
    """
    cur_dir = os.getcwd()
    base_name = os.path.basename(file_name)   # <-- strip any leading path

    for root, dirs, files in os.walk(cur_dir):
        if base_name in files:                # compare with the stripped name
            return os.path.join(root, base_name)

    raise FileNotFoundError(
        f"File '{file_name}' not found in subdirectories of {cur_dir}"
    )

def get_base_directory():
    return "/".join(find_file("main.py").split("/")[:-1])


'''def load_config(config_path):
    if not config_path.endswith(".yml"):
        config_path += ".yml"
    config_path = find_file(config_path)
    with open(config_path) as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    return AttrDict(config)'''

import yaml, pprint
from pathlib import Path


def _merge_dicts(a: dict, b: dict) -> dict:
    """Recursively merge two dicts.  Keys in *b* override keys in *a*."""
    merged = dict(a)
    for k, v in b.items():
        if k in merged and isinstance(merged[k], dict) and isinstance(v, dict):
            merged[k] = _merge_dicts(merged[k], v)
        else:
            merged[k] = v
    return merged

def _to_attrdict(d):
    """Turn nested dicts into dot-accessible namespaces."""
    if isinstance(d, dict):
        return AttrDict({k: _to_attrdict(v) for k, v in d.items()})
    elif isinstance(d, list):
        return [_to_attrdict(x) for x in d]
    else:
        return d

def load_config(cfg_path: str):
    """
    Load YAML at *cfg_path*.
    If it contains a key `base_config: some_file.yml` we load that first
    and let the *current* file override it.
    Returns an AttrDict exactly like the original helper did.
    """

    cfg_path = Path(cfg_path).expanduser()
    if not cfg_path.suffix:
        cfg_path = cfg_path.with_suffix(".yml")
    assert cfg_path.exists(), f"Config file not found: {cfg_path}"

    # -------- 1. load the run-specific YAML ---------------------------------
    with cfg_path.open() as f:
        yaml_cfg = yaml.safe_load(f)

    # -------- 2. optionally load a base config ------------------------------
    base_cfg = {}
    if "base_config" in yaml_cfg:
        base_path = (cfg_path.parent / yaml_cfg["base_config"]).resolve()
        with base_path.open() as f:
            base_cfg = yaml.safe_load(f)

    # -------- 3. merge so YAML-file values override base --------------------
    merged_cfg = _merge_dicts(base_cfg, yaml_cfg)

    # -------- 4. debug prints ----------------------------------------------
    try:
        seed_nested = (
            merged_cfg["parameters"]["dreamer"]["seed_episodes"]
        )
    except KeyError:
        seed_nested = "N/A"

    print("\n[Debug-cfg] loaded YAML from", cfg_path)
    print("[Debug-cfg] seed_episodes after merge →", seed_nested, "\n")

    # -------- 5. return as AttrDict (dot-style access) ----------------------
    return _to_attrdict(merged_cfg)

```

`hierarchical-nav/dreamer_mg/dreamer_mg.egg-info/PKG-INFO`:

```
Metadata-Version: 2.4
Name: dreamer_mg
Version: 0.0.1
License-File: LICENSE
Requires-Dist: torch>=2.1
Requires-Dist: numpy>=1.22
Requires-Dist: pyyaml
Requires-Dist: tqdm
Requires-Dist: tensorboard
Requires-Dist: gym>=0.17.0
Dynamic: license-file
Dynamic: requires-dist

```

`hierarchical-nav/dreamer_mg/main.py`:

```py
# ---------- main.py (after the imports) ---------------------------------
from pathlib import Path
from datetime import datetime
import shutil
import argparse
import os

os.environ["MUJOCO_GL"] = "egl"

from torch.utils.tensorboard import SummaryWriter
from dreamer.algorithms.dreamer import Dreamer
from dreamer.algorithms.plan2explore import Plan2Explore
from dreamer.utils.utils import load_config, new_run_dir
from dreamer.envs.envs import (
    make_atari_env, make_minigrid_env, get_env_infos
)


def main(config_file: str | None, run_dir_arg: str | None):
    """
    If `run_dir_arg` is given we resume; otherwise we start a fresh run that
    stores its checkpoints in a new timestamped directory.
    """

    # ── 1. Choose the run directory ────────────────────────────────────
    if run_dir_arg:                                  # → resume
        run_dir = Path(run_dir_arg).expanduser()
        assert run_dir.exists(), f"{run_dir} does not exist"
        config_file = run_dir / "config.yml"         # use stored config
        print(f"[Debug] using run_dir’s config: {config_file}")
    else:                                            # → fresh run
        assert config_file, "--config is required when not resuming"
        run_dir = new_run_dir(exp_name="mg_collision")
        shutil.copy(config_file, run_dir / "config.yml")
        print(f"[Debug] copying  {config_file}  →  {run_dir/'config.yml'}")
    # ------------------------------------------------------------------
    print("\n[Debug] ────────────────────────────────────────────────")
    print(f"[Debug] CLI  --config  = {config_file}")
    print(f"[Debug] CLI  --run_dir = {run_dir_arg}")
    # ------------------------------------------------------------------
    # ── 2. Load config (now guaranteed to match the checkpoints) ───────
    config = load_config(str(config_file))
    print("[Debug] seed_episodes (root)                 =", getattr(config, "seed_episodes", None))
    print("[Debug] seed_episodes in config.dreamer      =", getattr(getattr(config, "dreamer", None), "seed_episodes", None))
    print("[Debug] seed_episodes in parameters.dreamer  =",
        getattr(config.parameters.dreamer, "seed_episodes", None))
    # ── 3. Make environment ────────────────────────────────────────────
    if config.environment.benchmark == "minigrid":
        env = make_minigrid_env(
            task_name  = config.environment.task_name,
            rooms_row  = config.environment.rooms_row,
            rooms_col  = config.environment.rooms_col,
            frame_skip = config.environment.frame_skip,
            pixel_norm = config.environment.pixel_norm,
            run_dir    = run_dir
        )
    else:
        raise ValueError(f"Unknown benchmark: {config.environment.benchmark}")

    obs_shape, discrete_action, action_size = get_env_infos(env)
    writer   = SummaryWriter(run_dir)
    device   = config.operation.device

    # ── 4. Instantiate algorithm ───────────────────────────────────────
    if config.algorithm == "dreamer-v1":
        agent = Dreamer(
            obs_shape, discrete_action, action_size,
            writer, device, config, run_dir
        )
    elif config.algorithm == "plan2explore":
        agent = Plan2Explore(
            obs_shape, discrete_action, action_size,
            writer, device, config
        )
    else:
        raise ValueError(f"Unknown algorithm: {config.algorithm}")

    # ── 5. Train / resume training ─────────────────────────────────────
    agent.train(env)


# ---------- CLI ---------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config",
        type=str,
        default="dreamer/configs/minigrid-default.yml",
        help="Path to YAML config file (ignored when --run_dir is given)",
    )
    parser.add_argument(
        "--run_dir",
        type=str,
        default=None,
        help="Existing run directory to resume (e.g. runs/mg_collision/20250704-220917)",
    )
    args = parser.parse_args()
    main(args.config, args.run_dir)
# -----------------------------------------------------------------------
'''import os, argparse
os.environ["MUJOCO_GL"] = "egl"
from pathlib import Path          #  ←  add this import
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
import shutil
from dreamer.algorithms.dreamer import Dreamer
from dreamer.algorithms.plan2explore import Plan2Explore
from dreamer.utils.utils import load_config, get_base_directory, new_run_dir

# ⬇️  NEW: include make_minigrid_env
from dreamer.envs.envs import (
    make_atari_env, make_minigrid_env, get_env_infos
)


def main(config_file):
    config = load_config(config_file)
    log_dir = (
        get_base_directory()
        + "/runs/"
        + datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        + "_"
        + config.operation.log_dir
    )
    run_dir = new_run_dir(exp_name="mg_collision")
    shutil.copy(config_file, run_dir / "config.yml")   # save exact cfg

def main(config_file, run_dir_arg):
    # ① choose run directory
    if run_dir_arg is not None:                           # ← resume path supplied
        run_dir = Path(run_dir_arg).expanduser()
        assert run_dir.exists(), f"{run_dir} does not exist"
        config_file = run_dir / "config.yml"              # re-load the *saved* cfg
    else:                                                 # ← fresh run
        run_dir = new_run_dir(exp_name="mg_collision")    # makes YYMMDD-HHMMSS dir
        shutil.copy(config_file, run_dir / "config.yml")

    # ② load config (now guaranteed to match the checkpoints)
    config = load_config(str(config_file))



    writer   = SummaryWriter(run_dir)
    device = config.operation.device

    if config.environment.benchmark == "minigrid":          # ⬅️ NEW
        env = make_minigrid_env(
            task_name   = config.environment.task_name,
            rooms_row   = config.environment.rooms_row,
            rooms_col   = config.environment.rooms_col,
            frame_skip  = config.environment.frame_skip,
            pixel_norm  = config.environment.pixel_norm,
            run_dir=run_dir

        )
    else:
        raise ValueError(f"Unknown benchmark: {config.environment.benchmark}")
    obs_shape, discrete_action_bool, action_size = get_env_infos(env)
    print(env.action_space)

    
    if config.algorithm == "dreamer-v1":
        agent = Dreamer(
            obs_shape, discrete_action_bool, action_size, writer, device, config,run_dir
        )
    elif config.algorithm == "plan2explore":
        agent = Plan2Explore(
            obs_shape, discrete_action_bool, action_size, writer, device, config
        )
    agent.train(env)



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
    "--run_dir",
    type=str,
    default=None,
    help="Existing run directory to resume (e.g. runs/mg_collision/20250704-220917)",)
    args = parser.parse_args()
    main(args.config, args.run_dir)'''
```

`hierarchical-nav/dreamer_mg/overlay_cog_env.py`:

```py
#!/usr/bin/env python3
"""
overlay_cog_env.py

Batch (CLI) usage examples:

# 1) Use the pre-rendered transparent cog PNG (fast path)
python overlay_cog_env.py \
  --snapshot dbg/cogmap/t0042/snapshot_t0042.json \
  --out dbg/cogmap/t0042/overlay_manual.png \
  --alpha 0.65 --deg 90 --mirror x --scale 1.0 --tx 0 --ty 0

# 2) Re-draw vectors from JSON (ignores cog.png), with new node styling
python overlay_cog_env.py \
  --snapshot dbg/cogmap/t0042/snapshot_t0042.json \
  --out dbg/cogmap/t0042/overlay_vectors.png \
  --alpha 0.75 --deg -30 --scale 1.2 --tx 3 --ty -2 --vector

Interactive usage (recommended for manual fitting):

python overlay_cog_env.py \
  --snapshot dbg/cogmap/t0042/snapshot_t0042.json \
  --interactive --out dbg/cogmap/t0042/overlay_interactive.png

Controls (interactive):
- Move: ← ↑ ↓ → buttons (step = 0.5 grid units)
- Rotate: ⟲ -5°, ⟳ +5°, and ±90° buttons
- Mirror: toggle X, toggle Y
- Sliders: Scale (0.25–3.0), Alpha (0–1)
- Save: writes current view to --out (or auto-named alongside snapshot)
- Reset: returns to initial params (deg=0, tx=ty=0, scale=1, no mirroring)
"""

import json
import argparse
from pathlib import Path
import numpy as np

# --- Pure-logic helpers (no pyplot/backend here) ---

def load_snapshot(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def build_affine(deg: float, mirror: str, scale: float, tx: float, ty: float):
    """
    Returns (sx, sy, deg, tx, ty) components; actual Affine2D is built inside draw code.
    mirror ∈ {"none","x","y","xy","yx"}.
    """
    sx, sy = 1.0, 1.0
    m = (mirror or "none").lower()
    if m in ("x", "xy", "yx"):
        sx *= -1.0
    if m in ("y", "xy", "yx"):
        sy *= -1.0
    return sx * scale, sy * scale, float(deg), float(tx), float(ty)


# --- Batch drawing paths (non-interactive) ---

def overlay_with_raster(ax, env, cog, deg, mirror, scale, tx, ty, alpha, mpl_modules):
    mpimg, transforms = mpl_modules
    env_img = mpimg.imread(env["img_path"])
    Wc, Hc = env["grid_size"]
    ax.imshow(env_img, origin="lower", extent=(0, Wc, 0, Hc))
    ax.set_xlim(0, Wc); ax.set_ylim(0, Hc); ax.set_aspect("equal", "box")

    if not cog.get("png_path"):
        print("[overlay] No cog PNG available; falling back to --vector draw. Use --vector.")
        return

    bbox = cog.get("bbox", [0, 1, 0, 1])
    x0, x1, y0, y1 = bbox
    cog_img = mpimg.imread(cog["png_path"])

    sx, sy, d, tx, ty = build_affine(deg, mirror, scale, tx, ty)
    T = transforms.Affine2D().scale(sx, sy).rotate_deg(d).translate(tx, ty)

    ax.imshow(
        cog_img, origin="lower", extent=(x0, x1, y0, y1),
        alpha=alpha, transform=T + ax.transData, interpolation="bilinear",
    )

def overlay_with_vectors(ax, env, cog, deg, mirror, scale, tx, ty, alpha, annotate_ids, mpl_modules):
    mpimg, transforms, LineCollection = mpl_modules
    env_img = mpimg.imread(env["img_path"])
    Wc, Hc = env["grid_size"]
    ax.imshow(env_img, origin="lower", extent=(0, Wc, 0, Hc))
    ax.set_xlim(0, Wc); ax.set_ylim(0, Hc); ax.set_aspect("equal", "box")

    # Build transform
    sx, sy, d, tx, ty = build_affine(deg, mirror, scale, tx, ty)
    T = transforms.Affine2D().scale(sx, sy).rotate_deg(d).translate(tx, ty)

    def apply_T(X):
        X = np.asarray(X, dtype=float)
        if X.ndim == 1:
            X = X[None, :]
        return T.transform(X)

    # Links (as a LineCollection)
    links = cog.get("links_xy", []) or []
    segs = []
    for i in range(0, len(links) - 1, 2):
        a = np.asarray(links[i], dtype=float)
        b = np.asarray(links[i+1], dtype=float)
        A = apply_T(a)[0]; B = apply_T(b)[0]
        segs.append([A, B])
    if segs:
        lc = LineCollection(segs, colors="k", linewidths=1.6, alpha=alpha, zorder=2)
        ax.add_collection(lc)

    # Nodes (white fill, black edge circles)
    from matplotlib import patheffects as pe
    nodes = cog.get("nodes", []) or []
    if nodes:
        pts = np.array([[n["x"], n["y"]] for n in nodes], dtype=float)
        P = apply_T(pts)
        ax.scatter(P[:,0], P[:,1],
                   s=120, marker="o",
                   facecolors="white", edgecolors="black",
                   linewidths=1.5, alpha=alpha, zorder=3)
        # Always annotate ids (if missing, show index)
        for i, (n, p) in enumerate(zip(nodes, P)):
            nid = n.get("id")
            label = str(nid if (nid is not None) else i)
            txt = ax.text(p[0], p[1], label,
                          fontsize=10, color="black",
                          ha="center", va="center", zorder=4,
                          alpha=min(1.0, alpha+0.2))
            # White halo for legibility
            txt.set_path_effects([pe.withStroke(linewidth=2.5, foreground="white")])

    # Current node marker
    cur_id = cog.get("current_exp_id", None)
    if (cur_id is not None) and nodes:
        for (n, p) in zip(nodes, P):
            if n.get("id") == cur_id:
                ax.plot([p[0]], [p[1]], marker="x", color="r", mew=2, ms=9, alpha=min(1.0, alpha+0.1), zorder=5)
                break


# --- Interactive app ---

def run_interactive(snapshot_path: str, out_path: str|None, annotate_ids: bool):
    import matplotlib
    # Use default interactive backend; DO NOT force Agg here.
    import matplotlib.pyplot as plt
    from matplotlib.transforms import Affine2D
    import matplotlib.image as mpimg
    from matplotlib.widgets import Button, Slider
    from matplotlib.collections import LineCollection
    import time

    snap = load_snapshot(snapshot_path)
    env, cog = snap["env"], snap["cog"]
    Wc, Hc = env["grid_size"]

    # Prepare base data
    nodes = cog.get("nodes", []) or []

    # Points in COG (emap units)
    pts_cog = np.array([[n["x"], n["y"]] for n in nodes], dtype=float) if nodes else np.zeros((0,2))

    # Points in REAL (env cell coords from real_pose[:2]); fallback to COG if missing
    pts_real = []
    for i, n in enumerate(nodes):
        rp = n.get("real_pose")
        if isinstance(rp, (list, tuple)) and len(rp) >= 2:
            pts_real.append([float(rp[0]), float(rp[1])])
        else:
            # fallback to COG so we never lose a node
            pts_real.append([float(n.get("x", 0.0)), float(n.get("y", 0.0))])
    pts_real = np.array(pts_real, dtype=float) if nodes else np.zeros((0,2))

    # Links provided as coordinate pairs in COG space; build two versions:
    links = cog.get("links_xy", []) or []
    seg_pairs_cog = np.array(links, dtype=float).reshape(-1,2,2) if len(links) >= 2 else np.zeros((0,2,2))

    # Remap COG link endpoints to REAL by snapping to nearest node (in COG)
    def remap_links_to_real(seg_pairs_cog, pts_cog, pts_real):
        if len(seg_pairs_cog) == 0 or len(pts_cog) == 0:
            return np.zeros((0,2,2))
        out = []
        for (a, b) in seg_pairs_cog:
            # nearest node in COG for each endpoint
            ia = int(np.argmin(np.sum((pts_cog - a)**2, axis=1)))
            ib = int(np.argmin(np.sum((pts_cog - b)**2, axis=1)))
            A = pts_real[ia] if ia >= 0 else a
            B = pts_real[ib] if ib >= 0 else b
            out.append([A, B])
        return np.array(out, dtype=float)

    seg_pairs_real = remap_links_to_real(seg_pairs_cog, pts_cog, pts_real)
    # State
    state = {
        "deg": 0.0, "mirror_x": False, "mirror_y": False,
        "scale": 1.0, "tx": 0.0, "ty": 0.0, "alpha": 0.65,
        "step": 0.5,"coord_mode": "cog",
    }

    # Figure + axes layout
    fig = plt.figure(figsize=(9.5, 7.0), dpi=120)
    ax = fig.add_axes([0.06, 0.12, 0.73, 0.84])     # main plot
    # Controls area (bottom row)
    ax_btn_left  = fig.add_axes([0.82, 0.78, 0.07, 0.06])
    ax_btn_up    = fig.add_axes([0.90, 0.86, 0.07, 0.06])
    ax_btn_down  = fig.add_axes([0.90, 0.78, 0.07, 0.06])
    ax_btn_right = fig.add_axes([0.98, 0.78, 0.07, 0.06])

    ax_btn_r_m5  = fig.add_axes([0.82, 0.66, 0.07, 0.06])
    ax_btn_r_p5  = fig.add_axes([0.90, 0.66, 0.07, 0.06])
    ax_btn_r_m90 = fig.add_axes([0.82, 0.58, 0.07, 0.06])
    ax_btn_r_p90 = fig.add_axes([0.90, 0.58, 0.07, 0.06])

    ax_btn_mx    = fig.add_axes([0.82, 0.46, 0.07, 0.06])
    ax_btn_my    = fig.add_axes([0.90, 0.46, 0.07, 0.06])

    ax_sld_scale = fig.add_axes([0.82, 0.36, 0.23, 0.03])
    ax_sld_alpha = fig.add_axes([0.82, 0.30, 0.23, 0.03])

    ax_btn_reset = fig.add_axes([0.82, 0.18, 0.10, 0.06])
    ax_btn_save  = fig.add_axes([0.95, 0.18, 0.10, 0.06])
    ax_btn_coords = fig.add_axes([0.82, 0.24, 0.23, 0.06])

    # Draw env background
    env_img = mpimg.imread(env["img_path"])
    im_env = ax.imshow(env_img, origin="lower", extent=(0, Wc, 0, Hc), zorder=0)
    ax.set_xlim(0, Wc); ax.set_ylim(0, Hc); ax.set_aspect("equal", "box")
    ax.axis("off")

    # Artists (links as LineCollection, nodes as scatter)
    from matplotlib import patheffects as pe

    # Artists (links as LineCollection, nodes as scatter)
    link_coll = LineCollection([], colors="k", linewidths=1.8, zorder=2, alpha=state["alpha"])
    ax.add_collection(link_coll)

    scat = ax.scatter([], [], s=120, marker="o", facecolors="white",
                      edgecolors="black", linewidths=1.5,
                      alpha=state["alpha"], zorder=3)

    # Always-on numeric labels (with white halo)
    text_labels = []
    for i, _ in enumerate(nodes):
        txt = ax.text(0, 0, "", fontsize=10, ha="center", va="center",
                      color="black", alpha=min(1.0, state["alpha"]+0.2), zorder=4)
        txt.set_path_effects([pe.withStroke(linewidth=2.5, foreground="white")])
        text_labels.append(txt)
    # Current node
    cur_id = cog.get("current_exp_id", None)
    cur_mark, = ax.plot([], [], marker="x", color="r", mew=2, ms=9,
                        alpha=min(1.0, state["alpha"]+0.1), zorder=5)

    def mirror_string():
        if state["mirror_x"] and state["mirror_y"]: return "xy"
        if state["mirror_x"]: return "x"
        if state["mirror_y"]: return "y"
        return "none"

    def apply_transform():
        sx, sy, d, tx, ty = build_affine(
            deg=state["deg"], mirror=mirror_string(),
            scale=state["scale"], tx=state["tx"], ty=state["ty"]
        )
        T = Affine2D().scale(sx, sy).rotate_deg(d).translate(tx, ty)
        return T

    def redraw():
        # transform
        T = apply_transform()

        # pick coord set
        if state["coord_mode"] == "real":
            local_pts = pts_real
            local_segs = seg_pairs_real
        else:
            local_pts = pts_cog
            local_segs = seg_pairs_cog

        # links
        if len(local_segs):
            A = local_segs.reshape(-1, 2)
            B = T.transform(A).reshape(-1, 2, 2)
            link_coll.set_segments(B)
            link_coll.set_alpha(state["alpha"])
        else:
            link_coll.set_segments([])

        # nodes
        if len(local_pts):
            P = T.transform(local_pts)
            scat.set_offsets(P)
            scat.set_alpha(state["alpha"])
            for i, n in enumerate(nodes):
                label = str(n.get("id", i))
                text_labels[i].set_text(label)
                if len(P) > i:
                    text_labels[i].set_position((P[i,0], P[i,1]))
                    text_labels[i].set_alpha(min(1.0, state["alpha"]+0.2))
        else:
            scat.set_offsets(np.zeros((0,2)))
            for txt in text_labels:
                txt.set_alpha(0.0)

        # current node (mark by id if we have it; uses whichever coord set is active)
        cur_id = cog.get("current_exp_id", None)
        if (cur_id is not None) and len(nodes) and len(local_pts):
            # find index by id; fallback to same index if missing id
            idx = None
            for i, n in enumerate(nodes):
                if n.get("id") == cur_id:
                    idx = i; break
            if idx is not None and idx < len(local_pts):
                Pc = T.transform(local_pts[idx])
                cur_mark.set_data([Pc[0]], [Pc[1]])
                cur_mark.set_alpha(min(1.0, state["alpha"]+0.1))
            else:
                cur_mark.set_data([], [])
        else:
            cur_mark.set_data([], [])

        fig.canvas.draw_idle()

    # --- Widgets ---
    btn_left  = Button(ax_btn_left,  "←")
    btn_up    = Button(ax_btn_up,    "↑")
    btn_down  = Button(ax_btn_down,  "↓")
    btn_right = Button(ax_btn_right, "→")

    btn_r_m5  = Button(ax_btn_r_m5,  "⟲ -5°")
    btn_r_p5  = Button(ax_btn_r_p5,  "⟳ +5°")
    btn_r_m90 = Button(ax_btn_r_m90, "−90°")
    btn_r_p90 = Button(ax_btn_r_p90, "+90°")

    btn_mx = Button(ax_btn_mx, "Mirror X")
    btn_my = Button(ax_btn_my, "Mirror Y")

    sld_scale = Slider(ax_sld_scale, "Scale", 0.25, 3.0, valinit=state["scale"], valstep=0.01)
    sld_alpha = Slider(ax_sld_alpha, "Alpha", 0.0, 1.0, valinit=state["alpha"], valstep=0.01)

    btn_reset = Button(ax_btn_reset, "Reset")
    btn_save  = Button(ax_btn_save,  "Save")
    btn_coords = Button(ax_btn_coords, "Coords: COG")
    def toggle_coords(_):
        state["coord_mode"] = "real" if state["coord_mode"] == "cog" else "cog"
        btn_coords.label.set_text(f"Coords: {state['coord_mode'].upper()}")
        redraw()
    btn_coords.on_clicked(toggle_coords)

    # --- Callbacks ---
    def move(dx, dy):
        state["tx"] += dx; state["ty"] += dy; redraw()

    def rotate(dd):
        state["deg"] = (state["deg"] + dd) % 360.0; redraw()

    def toggle_mx(_):
        state["mirror_x"] = not state["mirror_x"]; redraw()

    def toggle_my(_):
        state["mirror_y"] = not state["mirror_y"]; redraw()

    def on_scale(val):
        state["scale"] = float(val); redraw()

    def on_alpha(val):
        state["alpha"] = float(val); redraw()

    def reset(_=None):
        state.update({"deg":0.0,"mirror_x":False,"mirror_y":False,
                      "scale":1.0,"tx":0.0,"ty":0.0,"alpha":0.65})
        sld_scale.set_val(state["scale"])
        sld_alpha.set_val(state["alpha"])
        redraw()

    def save(_=None):
        # Determine output path
        out = out_path
        if not out:
            base = Path(snapshot_path).parent
            stamp = time.strftime("%Y%m%d_%H%M%S")
            out = base / f"overlay_{stamp}.png"
        else:
            out = Path(out)
            out.parent.mkdir(parents=True, exist_ok=True)
        # Save current view
        ax.axis("off")
        fig.savefig(out, bbox_inches="tight", pad_inches=0)
        print(f"[overlay interactive] wrote {out}")

    # Wire buttons
    btn_left.on_clicked(lambda _ : move(-state["step"], 0))
    btn_right.on_clicked(lambda _ : move( state["step"], 0))
    btn_up.on_clicked(lambda _ : move(0,  state["step"]))
    btn_down.on_clicked(lambda _ : move(0, -state["step"]))

    btn_r_m5.on_clicked(lambda _ : rotate(-5))
    btn_r_p5.on_clicked(lambda _ : rotate(+5))
    btn_r_m90.on_clicked(lambda _ : rotate(-90))
    btn_r_p90.on_clicked(lambda _ : rotate(+90))

    btn_mx.on_clicked(toggle_mx)
    btn_my.on_clicked(toggle_my)

    sld_scale.on_changed(on_scale)
    sld_alpha.on_changed(on_alpha)

    btn_reset.on_clicked(reset)
    btn_save.on_clicked(save)

    # Keyboard shortcuts (optional quality-of-life)
    def on_key(event):
        if event.key == "left":   move(-state["step"], 0)
        elif event.key == "right":move( state["step"], 0)
        elif event.key == "up":   move(0,  state["step"])
        elif event.key == "down": move(0, -state["step"])
        elif event.key == "r":    rotate(+5)
        elif event.key == "R":    rotate(-5)
        elif event.key == "x":    toggle_mx(None)
        elif event.key == "y":    toggle_my(None)
        elif event.key == "s":    save(None)
        elif event.key == "0":    reset(None)
    fig.canvas.mpl_connect("key_press_event", on_key)

    # Initial draw
    redraw()
    fig.suptitle("Overlay Interactive — arrows=move, r/R=rotate ±5°, x/y=mirror, s=save, 0=reset", fontsize=10)
    plt.show()


# --- CLI ---

def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--snapshot", required=True, type=str)
    ap.add_argument("--out", type=str, default=None, help="Output PNG path (used by batch and interactive Save)")
    ap.add_argument("--deg", type=float, default=0.0, help="Rotation in degrees (CCW)")
    ap.add_argument("--mirror", type=str, default="none", choices=["none", "x", "y", "xy", "yx"])
    ap.add_argument("--scale", type=float, default=1.0, help="Uniform scale on cog units")
    ap.add_argument("--tx", type=float, default=0.0, help="Translation in env grid units (x)")
    ap.add_argument("--ty", type=float, default=0.0, help="Translation in env grid units (y)")
    ap.add_argument("--alpha", type=float, default=0.65, help="Cog translucency")
    ap.add_argument("--vector", action="store_true", help="Re-draw graph from nodes/links (ignores cog.png)")
    ap.add_argument("--annotate-ids", action="store_true", help="Annotate node ids when using vector draw")
    ap.add_argument("--interactive", action="store_true", help="Open interactive window for live fitting & saving")
    return ap.parse_args()

def main():
    args = parse_args()

    if args.interactive:
        # Interactive mode (GUI backend)
        run_interactive(args.snapshot, args.out, annotate_ids=args.annotate_ids)
        return

    # Batch mode: we can force Agg safely here
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
    from matplotlib.transforms import Affine2D
    import matplotlib.image as mpimg
    from matplotlib.collections import LineCollection

    snap = load_snapshot(args.snapshot)
    env, cog = snap["env"], snap["cog"]

    fig, ax = plt.subplots(1, 1, dpi=160)
    if args.vector or not cog.get("png_path"):
        overlay_with_vectors(
            ax, env, cog, args.deg, args.mirror, args.scale, args.tx, args.ty, args.alpha,
            annotate_ids=args.annotate_ids,
            mpl_modules=(mpimg, Affine2D, LineCollection)
        )
    else:
        overlay_with_raster(
            ax, env, cog, args.deg, args.mirror, args.scale, args.tx, args.ty, args.alpha,
            mpl_modules=(mpimg, Affine2D)
        )

    ax.axis("off")
    out_path = args.out or (Path(args.snapshot).with_name("overlay.png"))
    Path(out_path).parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path, bbox_inches="tight", pad_inches=0)
    plt.close(fig)
    print(f"[overlay] wrote {out_path}")

if __name__ == "__main__":
    main()

```

`hierarchical-nav/dreamer_mg/quick_watch.py`:

```py
import time
import gym_minigrid, gym
import random
from gym_minigrid.minigrid import Wall

env = gym.make(
    "MiniGrid-ADRooms-Collision-v0",
    rooms_in_row=3,
    rooms_in_col=4
)
print(env.action_space)
print(env.action_space)

obs = env.reset()
done = False

FPS = 6                    # frames per second you want to SEE
dt  = 3.0 / FPS            # seconds per frame

env.render(tile_size=64)   # show the initial state once
SAFE_STEPS=5
steps=0
while not done:
    steps+=1
    front_pos = env.front_pos           # (x, y) tuple
    front_cell = env.grid.get(*front_pos)
    if steps < SAFE_STEPS:
            print("safe",steps)
            if isinstance(front_cell, Wall):
                
                        # there's a wall ahead → turn
                env_act = random.choice([0, 1])  # 0=left, 1=right
            else:
                env_act = 2                      # 2=forward
    else:
        # after SAFE_STEPS, pure random
        env_act = random.randrange(0,3)           # TODO: your policy
    
    obs, reward, done, info = env.step(env_act)    # legacy 4-tuple API
    env.render(tile_size=64)                      # draw AFTER the step
    time.sleep(dt)                                # slow things down

env.close()
```

`hierarchical-nav/dreamer_mg/setup.py`:

```py
from setuptools import setup, find_packages
from pathlib import Path


this_dir = Path(__file__).parent
setup(
    name="dreamer_mg",
    version="0.0.1",
    packages=find_packages(),         # finds dreamer_mg and sub-pkgs
    include_package_data=True,         # include *.yml configs
    install_requires=[
        # core
        "torch>=2.1",
        "numpy>=1.22",
        "pyyaml",
        "tqdm",
        "tensorboard",
        # environment deps (gym already required by MiniGrid fork)
        "gym>=0.17.0",
    ],
)
```

`hierarchical-nav/dreamer_mg/test1.py`:

```py
# Fixed decoder testing functions for world_model_utils.py
import yaml, torch, pathlib, heapq, random, numpy as np
from collections import namedtuple
from types import SimpleNamespace  
from dreamer.modules.encoder import Encoder
from dreamer.modules.model   import RSSM
from dreamer.modules.decoder import Decoder 
from attrdict import AttrDict 
import gym_minigrid
from gym_minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper
from gym.wrappers import ResizeObservation
from dreamer.envs.wrappers import ChannelFirstEnv
import matplotlib.pyplot as plt
def save_decoder_fixed(wm, belief_zd, obs, step_name=""):
    """
    Fixed version that properly uses current belief and handles tensor formats
    """
    from pathlib import Path
    from torchvision.utils import save_image
    import torch
    import numpy as np
    
    OUTDIR = Path("recon_demo")
    OUTDIR.mkdir(exist_ok=True)
    
    # Get current belief state (this is the key fix!)
    z_current, d_current = belief_zd
    
    # Plan for imagination
    plan = ["forward", "left", "right", "forward", "forward"]
    onehots = torch.stack([onehot(a, wm) for a in plan]).unsqueeze(0)  # (1, K, action_dim)
    
    # Start imagination from CURRENT belief, not initial state
    zs, ds = [z_current], [d_current]
    
    # Roll out in imagination (prior predictions)
    with torch.no_grad():
        for t in range(len(plan)):
            d_next = wm.rssm.recurrent_model(zs[-1], onehots[:, t], ds[-1])
            _, z_next = wm.rssm.transition_model(d_next)  # sample from prior
            zs.append(z_next)
            ds.append(d_next)
    
    # Decode all states (current + imagined future)
    recons = []
    with torch.no_grad():
        for z, d in zip(zs, ds):
            # Decoder expects (batch, z_dim) and (batch, d_dim)
            recon_dist = wm.decoder(z, d)
            recon_img = recon_dist.mean.squeeze(0).cpu()  # (3, 64, 64)
            recon_img = torch.clamp(recon_img, 0, 1)  # Ensure valid range
            recons.append(recon_img)
    
    # Get current observation as tensor
    truth = obs["image"].astype(np.float32).transpose(2,0,1) / 255.0  # (3,64,64)
    truth_tensor = torch.tensor(truth)
    
    # Combine truth + reconstructions
    all_images = [truth_tensor] + recons
    grid = torch.stack(all_images, dim=0)  # (K+2, 3, 64, 64)
    
    # Upscale for better visibility
    grid_big = torch.nn.functional.interpolate(
        grid, size=256, mode="bilinear", align_corners=False
    )
    
    # Save with step identifier
    fname = OUTDIR / f"recon_{step_name}_{len(list(OUTDIR.glob('recon_*.png'))):03d}.png"
    save_image(grid_big, fname, nrow=len(plan)+2, normalize=False)
    print(f"[demo] saved reconstruction → {fname}")

# ─────────────────────────────── constants ────────────────────────────
State    = namedtuple("State", ["x", "y", "d"])          # planner state
DIR_VECS = [(1,0), (0,1), (-1,0), (0,-1)]                # 0:right 1:down …

DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"

# ══════════════════════════════════════════════════════════════════════
# 1.  LOADING  (one-liner:  wm = load_world_model("…/iter0500.pt") )
# ══════════════════════════════════════════════════════════════════════

def load_world_model2(ckpt_path: str,
                     config_yaml: str = "dreamer/configs/minigrid-default-temp.yml"):
    """
    Returns an object with .encoder and .rssm that exactly match *any*
    Dreamer checkpoint – no manual YAML tweaks required.

    • Reads sizes directly from the ckpt:
        in_dim  = W.shape[1]  of recurrent_model.linear.weight
        stoch   = (W₂.shape[0]) // 2      (# outputs / 2)
        act_sz  = in_dim - stoch
    • Injects the discovered numbers into a *copy* of the yaml dict.
    """
    import copy, yaml, torch
    ckpt = torch.load(ckpt_path, map_location=DEVICE)

    # ---------- 1. discover sizes from weight shapes -------------------
    Wrec = ckpt["modules"]["rssm"]["recurrent_model.linear.weight"]      # (H, in)
    H, in_dim        = Wrec.shape
    Wtrans = ckpt["modules"]["rssm"]["transition_model.network.2.weight"]# (2*Z, H)
    stoch_sz         = Wtrans.shape[0] // 2
    act_size         = in_dim - stoch_sz            # the checkpoint’s action space

    # ---------- 2. clone & patch the yaml ------------------------------
    
    cfg = yaml.safe_load(open(config_yaml))
    cfg = copy.deepcopy(cfg)                         # keep original intact

    cfg = AttrDict(cfg)  
    cfg['parameters']['dreamer']['deterministic_size'] = H
    cfg['parameters']['dreamer']['stochastic_size']    = stoch_sz

    enc  = Encoder((3, 64, 64), cfg).to(DEVICE)
    dec = Decoder((3, 64, 64), cfg).to(DEVICE)
    rssm = RSSM(action_size=act_size, config=cfg).to(DEVICE)

    enc .load_state_dict(ckpt["modules"]["encoder"]);  enc .eval()
    rssm.load_state_dict(ckpt["modules"]["rssm"   ]);  rssm.eval()
    dec.load_state_dict(ckpt["modules"]["decoder"]); dec.eval()

    wm = lambda: None
    wm.encoder = enc
    wm.rssm    = rssm
    wm.decoder = dec
    wm.action_size = act_size                        # handy later
    wm.idx = {                           #  ← canonical MiniGrid ids
        "left":   0,
        "right":  1,
        "forward":2,
        "pickup": 3,
        "drop":   4,
        "toggle": 5,
        "done":   6,    }
    print("rssm expects action dim:", wm.action_size)
    return wm
def onehot(action_name: str, wm):
    v = torch.zeros(wm.action_size, device=DEVICE)
    idx = wm.idx.get(action_name, None)
    if idx is not None:
        v[idx] = 1.
    return v

def test_current_reconstruction(wm, belief_zd, obs):
    """
    Test how well the current belief reconstructs the current observation
    """
    import torch
    from torchvision.utils import save_image
    
    z_current, d_current = belief_zd
    
    with torch.no_grad():
        # Reconstruct current state
        recon_dist = wm.decoder(z_current, d_current)
        recon_img = recon_dist.mean.squeeze(0).cpu()  # (3, 64, 64)
        
        
        # Get ground truth
        truth = obs["image"].astype(np.float32).transpose(2,0,1) / 255.0
        truth_tensor = torch.tensor(truth)
        
        # Side by side comparison
        comparison = torch.stack([truth_tensor, recon_img], dim=0)
        comparison_big = torch.nn.functional.interpolate(
            comparison, size=256, mode="bilinear", align_corners=False
        )
        
        save_image(comparison_big, "current_recon_test.png", nrow=2, normalize=False)
        print("Saved current reconstruction test → current_recon_test.png")
        
        # Print reconstruction quality metrics
        mse = torch.nn.functional.mse_loss(recon_img, truth_tensor)
        print(f"Reconstruction MSE: {mse.item():.6f}")
@torch.no_grad()
def wm_update_belief(wm,
                     prev_z_d: tuple[torch.Tensor, torch.Tensor],
                     frame_rgb: np.ndarray,
                     prev_action_onehot: torch.Tensor | None):
    """
    Update (z,d) belief given *one* observation & the *previous* action.

    Parameters
    ----------
    prev_z_d          tuple(z,d) from the previous step OR  None on first step
    frame_rgb         np.ndarray  (3,64,64)   current observation
    prev_action_onehot  torch.Tensor shape (1,3)  one-hot of action_t-1
                         →  pass None right after reset()

    Returns
    -------
    z_t, d_t  :  the new posterior latent tensors (no grads)
    """
    if prev_z_d is None:                               # first frame
        # dummy recurrent input (batch=1)
        zprev, dprev = wm.rssm.recurrent_model_input_init(1)
    else:
        zprev, dprev = prev_z_d

    # recurrent model if we have an *action that already happened*
    if zprev is not None and prev_action_onehot is not None:
        dprev = wm.rssm.recurrent_model(zprev,
                                        prev_action_onehot.unsqueeze(0),  # (1,7)
                                        dprev)

    # encode observation & run representation model
    img = torch.tensor(frame_rgb, dtype=torch.float32,
                       device=DEVICE).unsqueeze(0)
    emb = wm.encoder(img).view(1, -1)
    _, zt = wm.rssm.representation_model(emb, dprev)   # posterior

    return zt.detach(), dprev.detach()


def step_by_step_reconstruction_test(wm, env, num_steps=5):
    """
    Complete test that steps through environment and shows reconstructions
    """
    import time
    from pathlib import Path
    from torchvision.utils import save_image
    import torch
    
    OUTDIR = Path("step_by_step_recon")
    OUTDIR.mkdir(exist_ok=True)
    
    # Reset environment
    obs = env.reset()
    belief_zd = None
    prev_act_1h = None
    
    all_comparisons = []
    
    for step in range(num_steps):
        print(f"\n=== Step {step} ===")
        
        # Update belief from observation
        belief_zd = wm_update_belief(wm, belief_zd, 
                                   obs["image"].transpose(2,0,1) / 255.0, 
                                   prev_act_1h)
        
        # Test current reconstruction
        z_current, d_current = belief_zd
        with torch.no_grad():
            recon_dist = wm.decoder(z_current, d_current)
            recon_img = recon_dist.mean.squeeze(0).cpu()
            
            
            # Ground truth
            truth = obs["image"].astype(np.float32).transpose(2,0,1) / 255.0
            truth_tensor = torch.tensor(truth)
            
            # Store comparison
            comparison = torch.stack([truth_tensor, recon_img], dim=0)
            all_comparisons.append(comparison)
            
            # Calculate and print metrics
            mse = torch.nn.functional.mse_loss(recon_img, truth_tensor)
            print(f"Step {step} Reconstruction MSE: {mse.item():.6f}")
        
        # Take action and get next observation
        action = env.actions.forward  # or choose randomly
        prev_act_1h = onehot("forward", wm)
        obs, _, done, _ = env.step(action)
        
        if done:
            break
    
    # Save all comparisons as a grid
    if all_comparisons:
        # Stack all comparisons vertically
        full_grid = torch.cat(all_comparisons, dim=0)  # (2*num_steps, 3, 64, 64)
        full_grid_big = torch.nn.functional.interpolate(
            full_grid, size=256, mode="bilinear", align_corners=False
        )
        
        save_image(full_grid_big, OUTDIR / "full_sequence.png", 
                  nrow=2, normalize=False)
        print(f"Saved full sequence → {OUTDIR / 'full_sequence.png'}")


# Updated main function with fixed decoder testing
if __name__ == "__main__":
    import time, gym, gym_minigrid
    from gym_minigrid.wrappers import RGBImgPartialObsWrapper, ImgActionObsWrapper
    import matplotlib.pyplot as plt
    from torchvision.utils import save_image
    import os, itertools
    from pathlib import Path
    import time, gym, gym_minigrid
    from gym_minigrid.wrappers import RGBImgPartialObsWrapper, ImgActionObsWrapper
    from world_model_utils import onehot, DictResizeObs         # already defined
    import matplotlib.pyplot as plt
    from torchvision.utils import save_image
    import os, itertools
    from pathlib import Path

    CKPT = "runs/mg_collision/20250630-165944/ckpt/iter01210.pt"
    wm   = load_world_model2(CKPT)
    print(f"[demo] loaded WM from {CKPT}")
    
    # Setup environment
    env = gym.make("MiniGrid-4-tiles-ad-rooms-v0", rooms_in_row=3, rooms_in_col=4)
    env = RGBImgPartialObsWrapper(env)
    env = ImgActionObsWrapper(env) 
    env = DictResizeObs(env, (64, 64))
    
    seed = 218
    env.seed(seed)
    obs = env.reset()
    obs, _, done, _ = env.step(env.actions.forward)
    env.render(tile_size=64)
    
    # Initialize belief
    belief_zd = None
    prev_act_1h = None
    
    # Update belief from first observation
    belief_zd = wm_update_belief(wm, belief_zd, 
                               obs["image"].transpose(2,0,1) / 255.0, 
                               prev_act_1h)
    
    print("\n=== Testing Current Reconstruction ===")
    test_current_reconstruction(wm, belief_zd, obs)
    
    print("\n=== Testing Imagination from Current State ===")
    save_decoder_fixed(wm, belief_zd, obs, "initial")
    
    # Take a few steps and test again
    for step in range(3):
        prev_act_1h = onehot("forward", wm)
        obs, _, done, _ = env.step(env.actions.forward)
        belief_zd = wm_update_belief(wm, belief_zd, 
                                   obs["image"].transpose(2,0,1) / 255.0, 
                                   prev_act_1h)
        env.render(tile_size=64)
        time.sleep(0.5)
    
    print(f"\n=== After {3} Steps ===")
    test_current_reconstruction(wm, belief_zd, obs)
    save_decoder_fixed(wm, belief_zd, obs, "after_steps")
    
    # Run full step-by-step test
    print("\n=== Full Step-by-Step Test ===")
    step_by_step_reconstruction_test(wm, env, num_steps=5)
    
    env.close()
```

`hierarchical-nav/dreamer_mg/unified_test_runner.py`:

```py
import importlib, sys, pathlib
from dataclasses import dataclass, field
from typing import List, Optional, Tuple, Dict, Any
from importlib import util
# or
from importlib.util import spec_from_file_location, module_from_spec

# point "dreamer" to dreamer_mg.dreamer
pkg_path = pathlib.Path(__file__).parent / "dreamer"
spec = importlib.util.spec_from_file_location("dreamer", pkg_path / "__init__.py")
dreamer_pkg = importlib.util.module_from_spec(spec)
spec.loader.exec_module(dreamer_pkg)
sys.modules["dreamer"] = dreamer_pkg
import yaml, torch, pathlib, heapq, random, numpy as np
from collections import namedtuple
from types import SimpleNamespace  
from dreamer.modules.encoder import Encoder
from dreamer.modules.model   import RSSM
from dreamer.modules.decoder import Decoder 
from dreamer.modules.model import RewardModel
from attrdict import AttrDict 
import gym_minigrid
from gym_minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper
from gym.wrappers import ResizeObservation
from dreamer.envs.wrappers import ChannelFirstEnv
import matplotlib.pyplot as plt
import torch.nn.functional as F 
import textwrap, pprint, itertools
import math
from PIL import Image, ImageDraw, ImageFont   # Pillow
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import torchvision.utils as vutils
import torchvision.transforms.functional as TF 
from torchvision.utils import save_image
from pathlib import Path
from itertools import islice
from torchvision.utils import save_image
 # --- Cognitive graph harness ---------------------------------------------------
from dataclasses import dataclass
from viz_help import (
    VideoGridmap,
    record_video_frames,
    append_vis_frame_unified,
    safe_get_memory_map_data_from,
    prepare_for_imshow,
    compute_mse,
)
from typing import Optional, List, Tuple, Any, Dict
import numpy as np
import sys as _sys


_REPO_ROOT = str(Path(__file__).resolve().parents[1])  # one level up from dreamer_mg/
if _REPO_ROOT not in _sys.path:
    _sys.path.insert(0, _REPO_ROOT)

# Optional deps used in the template encoder; all guarded.
try:
    import cv2  # for resize / gradients if available
except Exception:
    cv2 = None
try:
    import yaml
except Exception:
    yaml = None

# ================== BEGIN: KBrain integration (paste once) ==================
# Imports required by the controller + nav system + manager
from collections import deque
import numpy as np, random
from control_eval.HierarchicalHMMBOCPD import HierarchicalBayesianController
from control_eval.NavigationSystem import NavigationSystem
from navigation_model.Processes.manager import Manager
from navigation_model.Processes.explorative_behaviour import Exploration_Minigrid
# If you later want goal seeking, uncomment the next line:
# from navigation_model.Processes.exploitative_behaviour import Goal_seeking_Minigrid
from navigation_model.Services.model_modules import no_vel_no_action
from control_eval.input_output import setup_allocentric_config, setup_memory_config, load_memory
from env_specifics.minigrid_maze_wt_aisles_doors.minigrid_maze_modules import set_door_view_observation
from scipy.spatial.distance import cosine  # used by info-gain helpers

class KBrain:
    """
    Thin adapter that brings in:
      • HierarchicalBayesianController (HMM+BOCPD)
      • NavigationSystem
      • apply_exploration()  → returns one-hot policy list and n_actions
      • post_step_update()   → updates HMM state after YOUR step/belief/cog updates

    It assumes YOUR script already:
      • keeps a shared replay_buffer (deque of dict states),
      • updates Dreamer belief with planner.{onehot,update_belief_from_obs,...},
      • updates the cognitive map via planner.update_cog(...).
    """
    def __init__(self,
                 env,
                 planner,
                 *,
                 lookahead: int,
                 k_recent:int,
                 metric:str,
                 debug_print,
                 viz_tree,
                 viz_every,
                 viz_outdir,
                 memcfg_path: str | None,
                 replay_buffer: deque,
                 
                 seed: int | None = None):
        self.env = env
        self.planner = planner
        self.wm = planner.wm
        self.belief = None          # YOU set it before first step in your main
        self.prev_onehot = None     # YOU maintain it in your main
        self.agent_current_pose = None
        self._replay_buffer_source = replay_buffer
        self.K_recent=k_recent
        self.lookahead=lookahead
        self.metric=metric
        self.debug_print=debug_print
        self.viz_tree=viz_tree
        self.viz_outdir=viz_outdir
        self.viz_every=viz_every
        # ── HMM / meta-controller state
        self.hmm_bayes = HierarchicalBayesianController()
        self.current_mode: str = "EXPLORE"
        self.current_submode: str = "base"  # neutral placeholder; new HMM is mode-only
        self.prev_mode: str | None = None
        self.prev_submode: str | None = None
        self.mode_changed: bool = False
        self.submode_changed: bool = False
        self.hmm_stats: dict | None = None

        # ── Manager + configs (allocentric + memory)
        #     We pass a minimal, robust set: possible_actions = [[1,0,0],[0,1,0],[0,0,1]]
        forward = [1,0,0]
        right   = [0,1,0]
        left    = [0,0,1]
        mingrid_actions = [forward, right, left]
        self.planner=planner

        # ── Navigation system hooks into the memory_graph and your planner
        self.nav_system = NavigationSystem(
            self.planner.cog.mg,
            lambda: self.agent_current_pose,
            planner=self.planner
        )
        
        mg = planner.cog.mg
        self.nav_system = NavigationSystem(
            mg,
            lambda: self.agent_current_pose,
            planner=self.planner
        )
        self.nav_system.debug_universal_navigation = True

    # -------------------- action encodings --------------------
    @staticmethod
    def _onehot_to_name(v: list[int]) -> str | None:
        if not v: return None
        if v[0] == 1: return "forward"
        if v[1] == 1: return "right"
        if v[2] == 1: return "left"
        return None
    @property
    def replay_buffer(self):
        """Always return the *current* buffer. Supports a passed-in deque or a callable."""
        src = self._replay_buffer_source
        return src() if callable(src) else src

    def set_replay_buffer_source(self, source):
        """Optional runtime switch: pass a deque or a callable returning a deque."""
        self._replay_buffer_source = source
    @staticmethod
    def _name_to_onehot(name: str) -> list[int]:
        mapping = {'forward': [1,0,0], 'right': [0,1,0], 'left': [0,0,1]}
        return mapping.get(name, [1,0,0])
    @staticmethod
    def to_onehot_list(act: str) -> list[int]:
        """'left'/'right'/'forward' → [0,0,1] / [0,1,0] / [1,0,0]"""
        mapping = {'forward': [1,0,0], 'right': [0,1,0], 'left': [0,0,1]}
        return mapping[act]
    def _decode_planner_result(self, result) -> str | None:
        """
        Accepts:
          • (best_action: str, scores: dict, top_paths: list)
          • torch.Tensor of shape (T,3) one-hot
          • list[str] plan
        Returns a single action name or None.
        """
        import torch
        if isinstance(result, tuple) and len(result) >= 1 and isinstance(result[0], str):
            return result[0]

        seq = []
        if isinstance(result, torch.Tensor):
            # decode one-hot (T,3)
            idx_to_act = {0: "forward", 1: "right", 2: "left"}
            for row in result.detach().cpu():
                j = int(row.argmax().item())
                seq.append(idx_to_act[j])
        elif isinstance(result, (list, tuple)) and all(isinstance(a, str) for a in result):
            seq = list(result)

        return seq[0] if seq else None

    def convert_hot_encoded_to_minigrid_action(self, onehot: list[int]) -> int:
        if not onehot:
            return self.env.actions.done
        if onehot[0] == 1:
            return self.env.actions.forward
        if onehot[1] == 1:
            return self.env.actions.right
        if onehot[2] == 1:
            return self.env.actions.left
        raise ValueError(f"Unrecognized onehot action: {onehot}")

    # -------------------- brains: policy selection --------------------
    def apply_exploration(self, start_state, t_step: int) -> str:
        """
        Run FIRST each step. Returns a single action name to execute now.
        """
        mode, submode = self.current_mode, self.current_submode

        # 1) RECOVER (simple heuristic)
        if mode == "RECOVER":
            return random.choice(["right", "left"])

        # 2) NAVIGATE (delegate to NavigationSystem)
        if mode == "NAVIGATE":
            print(f"[NAVIGATE] submode={submode}  "
                f"plan_progress={self.nav_system.progress_scalar():.3f}")

            # ---------- 1. Early feasibility gate ----------
            if not self.nav_system.check_navigation_feasibility():
                print(f"[NAVIGATE] infeasible — flags={self.nav_system.navigation_flags}")
                self.navigation_flags = self.nav_system.navigation_flags
                fallback = [self.to_onehot_list(random.choice(["right", "left"]))]
                print(f"[NAVIGATE] STALL → issuing {fallback}")
                return fallback, 1

            # ---------- 2. Build first plan if none ----------
            if self.nav_system.progress_scalar() < 0.0:
                print("[NAVIGATE] no plan yet → building PCFG plan")
                grammar = self.nav_system.build_pcfg_from_memory()
                self.nav_system.generate_plan_with_pcfg(grammar)
                print(f"[NAVIGATE] new plan tokens={self.nav_system.full_plan_tokens}")

            # ---------- 3. Delegate to sub-mode handler ----------
            primitives, n_actions = self.nav_system.universal_navigation(submode, self.wm, self.belief)
            if not primitives or n_actions == 0:
                return random.choice(["right", "left"])
            act = self._onehot_to_name(primitives[0])
            return act or random.choice(["right", "left"])

        # 3) TASK_SOLVING (placeholder until integrated)
        if mode == "TASK_SOLVING":
            print(f"[TASK] submode={submode}")

            # ---- Barrier: only enter task-solving if we've discovered all rooms.
            # Prefer strict room-count using env_definition (n_row, n_col) if available.
            # Otherwise fall back to “we at least have seen the mission color in memory”.
            def _task_gate_all_rooms_env():
                """
                Gate TASK_SOLVING strictly by the environment's own room-visit bookkeeping.
                Uses:
                - env.unwrapped.get_visited_rooms_order()  → authoritative discovery history
                - env.unwrapped.rooms_in_row / rooms_in_col → target coverage
                """
                e = getattr(self.env, "unwrapped", self.env)

                # 1) Pull visited rooms from the env (authoritative)
                visited_room_ids = set()
                try:
                    ord_list = list(e.get_visited_rooms_order())  # [{'room': (col,row), 'color': str, ...}, ...]
                    for rec in ord_list:
                        rid = rec.get("room")
                        if isinstance(rid, tuple) and len(rid) == 2:
                            visited_room_ids.add((int(rid[0]), int(rid[1])))
                        elif isinstance(rid, list) and len(rid) == 2:
                            visited_room_ids.add((int(rid[0]), int(rid[1])))
                except Exception as ex:
                    print(f"[TASK][gate] get_visited_rooms_order() unavailable: {ex}")
                    ord_list = []

                # Optional: if the env exposes the private set, use it too (faster / exact)
                try:
                    vset = getattr(e, "_visited_rooms_set", None)
                    if isinstance(vset, set) and len(vset) > 0:
                        # entries are already (col,row) tuples
                        visited_room_ids |= {tuple(v) if isinstance(v, (list, tuple)) else v for v in vset}
                except Exception:
                    pass

                # 2) Target: total rooms from env metadata
                try:
                    n_row = int(getattr(e, "rooms_in_row"))
                    n_col = int(getattr(e, "rooms_in_col"))
                    total = n_row * n_col
                except Exception as ex:
                    print(f"[TASK][gate] rooms_in_row/rooms_in_col missing: {ex}")
                    # Last resort to avoid deadlock: assume at least the number we've seen
                    total = max(1, len(visited_room_ids))

                # 3) Decide
                visited = len(visited_room_ids)
                ready = (visited >= total)
                print(f"[TASK][gate] rooms covered: {visited}/{total} → {'READY' if ready else 'NOT READY'}")
                return ready, {"visited": visited, "target": total}


            ready, gate_info = _task_gate_all_rooms_env()
            if not ready:
                print(f"[TASK] Not ready to enter TASK_SOLVING (gate={gate_info}) → fallback random turn.")
                return random.choice(["right", "left"])

            # ---- Same early feasibility gate as NAVIGATE
            if not self.nav_system.check_navigation_feasibility():
                print(f"[TASK] infeasible — flags={self.nav_system.navigation_flags}")
                self.navigation_flags = self.nav_system.navigation_flags
                return random.choice(["right", "left"])

            # ---- Build task plan (first time only), using the task-specific PCFG
            if self.nav_system.progress_scalar() < 0.0:
                mission = self.nav_system.task_solve_mission()  # e.g., "go to red room"
                color   = self.nav_system._parse_color_from_mission(mission)
                if not color:
                    print(f"[TASK] Could not parse mission color from: {mission!r} → fallback turn.")
                    return random.choice(["right", "left"])

                grammar = self.nav_system.build_task_pcfg_from_memory(color, debug=True)
                self.nav_system.generate_plan_with_pcfg(grammar)
                print(f"[TASK] new plan tokens={self.nav_system.full_plan_tokens}  mission={mission}  color={color}")

            # ---- Delegate to the same sub-mode handler you use for NAVIGATE
            primitives, n_actions = self.nav_system.universal_navigation(submode, self.wm, self.belief)
            if not primitives or n_actions == 0:
                return random.choice(["right", "left"])

            act = self._onehot_to_name(primitives[0])
            return act or random.choice(["right", "left"])

        # 4) EXPLORE (default): call your novelty A* and choose the *next* action
        result = self.planner.novelty_astar_plan(
            wm=self.wm,
            belief_zd=self.belief,
            start_state=start_state,
            lookahead=self.lookahead,
            replay_buffer=self.replay_buffer,
            K_recent=self.K_recent,
            metric=self.metric,
            verbose=self.debug_print,
            viz_tree=self.viz_tree,
            viz_outdir=self.viz_outdir,
            viz_tag=f"t{int(t_step):02d}",
            # uncertainty-rollout knobs (keep your current defaults)
            w_p2e=1.0,
            p2e_K=5,
            p2e_mode="temporal",
            p2e_metric="l2",
            p2e_roll_from="parent"
        )
        action_name = self._decode_planner_result(result)
        if action_name is None:
            print( "EXPLORE FALLBACK",action_name)
            action_name=random.choice(["right", "left"])
        return action_name

       

    # -------------------- post-step update (HMM + bookkeeping) --------------------
    def post_step_update(self, obs: dict, belief_zd):
        """
        Call this RIGHT AFTER you step + update belief + update_cog in your loop.
        It updates nav pose, computes info-gain & plan-progress, and updates HMM.
        """
        # track pose for NavigationSystem callbacks
        self.agent_current_pose = tuple(obs["pose"]) if "pose" in obs else None
        try:
            self.nav_system.push_pose(self.agent_current_pose)
        except Exception:
            pass

        # HMM signals
        #hmm_info_gain = self.info_gain(obs.get('image'), obs.get('pose'), self.replay_buffer, self.planner.emap)
        hmm_info_gain=None
        self.nav_system.current_mode=self.current_mode
        print(self.nav_system.current_mode)
        raw_plan_prog = self.nav_system.navigation_grade()
        print("NAV GRADE",raw_plan_prog)
        hmm_plan_progress = max(0.0, min(1.0, 0.0 if raw_plan_prog == -1 else float(raw_plan_prog)))
        print("NAV GRADE",hmm_plan_progress)
        rb = self.replay_buffer
        # update mode (new HMM may not have submodes)
        _, stats = self.hmm_bayes.update(rb, hmm_info_gain, hmm_plan_progress)

        ml = stats.get('most_likely_state', None)
        if isinstance(ml, (list, tuple)):
            new_mode = ml[0]
            new_submode = ml[1] if len(ml) > 1 else "base"
        else:
            new_mode = str(ml) if ml is not None else "EXPLORE"
            new_submode = "base"

        self.prev_mode, self.prev_submode = self.current_mode, self.current_submode
        self.current_mode, self.current_submode = new_mode, new_submode
        self.mode_changed = (new_mode != self.prev_mode)
        self.submode_changed = (new_submode != self.prev_submode)
        self.hmm_stats = stats

        # Keep the print; submode is 'base' under the mode-only HMM
        print(f"🧠 [HMM] mode: {new_mode} → {new_submode}  | changed? {self.mode_changed}")
    # -------------------- info-gain (visual + spatial + temporal) --------------------
    # Top-level wrapper
    def info_gain(self, current_image, current_pose, replay_buffer, view_cells_manager,
              device: str = "cpu",
              weights: dict | None = None) -> float:
        """
        Simplified [0,1] info-gain signal for the HMM:
        • spatial novelty only (view-cells + pose mismatch)
        Rationale:
        – 'visual novelty' (feature similarity vs buffer) is noisy and
            not predictive of being stuck in our setting.
        – 'temporal novelty' was just visual novelty with decay: redundant.
        – Loop/stagnation signals are handled centrally by the HMM.
        """
        spatial_novelty = self.calculate_spatial_novelty(
            current_image, current_pose, view_cells_manager, device
        )
        return float(np.clip(spatial_novelty, 0.0, 1.0))

 

    # Spatial novelty via view-cells + pose mismatch (lightweight proxy)
    def calculate_spatial_novelty(self, current_image, current_pose, view_cells_manager, device) -> float:
        try:
            # Access your experience map through manager
            emap = view_cells_manager.experience_map
            # If there are no cells, it's new
            if len(getattr(emap, "view_cells", [])) == 0:
                return 1.0
            # Heuristic: if we are far from the most similar view-cell, mark novel
            novelties = []
            for c_idx, cell in enumerate(getattr(emap, "view_cells", [])):
                # many repos store feature vectors in cell.template64
                if not hasattr(cell, "template64"):
                    continue
                sim = 1.0 - cosine(self.extract_image_features(current_image, device), np.asarray(cell.template64))
                if sim > 0.7:
                    dist = self.calculate_pose_distance(current_pose, {"x": cell.x_pc, "y": cell.y_pc, "theta": cell.th_pc})
                    novelties.append(0.8 if dist > 2.0 else 0.2)
            if not novelties:
                return 1.0
            return float(np.clip(np.mean(novelties), 0.0, 1.0))
        except Exception:
            return 0.5

    # Temporal novelty (recent repeats are less novel)
    

    # Feature extraction helper (delegates to manager’s rgb56_to_template64 when possible)
    def extract_image_features(self, img, device="cpu"):
        # unify inputs
        import torch
        if isinstance(img, (list, tuple)):
            img = np.asarray(img)
        if isinstance(img, np.ndarray) and img.ndim == 3 and img.shape[-1] == 3:
            # assume HWC in [0..255]
            pass
        elif isinstance(img, torch.Tensor):
            x = img.detach().cpu().float()
            if x.ndim == 4 and x.shape[0] == 1:
                img = x[0].permute(1,2,0).numpy()
            elif x.ndim == 3 and x.shape[0] in (1,3):
                img = x.permute(1,2,0).numpy()
            else:
                img = np.asarray(x)
        elif isinstance(img, np.ndarray) and img.ndim == 1 and img.size == 64:
            vec = img.astype(np.float32)
            return vec / (np.linalg.norm(vec) + 1e-8)

        # prefer your repo’s learned template extractor if available
        try:
            vec64_torch = self.planner.emap.rgb_to_template64(img, device=device)
            vec64 = vec64_torch.detach().cpu().float().numpy()
            return vec64 / (np.linalg.norm(vec64) + 1e-8)
        except Exception:
            # fallback: mean-pooled RGB histogram-ish
            img = img.astype(np.float32)
            h, w, _ = img.shape
            pooled = img.reshape(h*w, 3).mean(axis=0)
            vec = np.tile(pooled / (np.linalg.norm(pooled) + 1e-8), 21)[:64]
            return vec / (np.linalg.norm(vec) + 1e-8)

    # Pose helpers
    def calculate_pose_distance(self, p1, p2) -> float:
        # accepts tuple/list (x,y,dir) or dict with x,y,theta
        def _norm_pose(p):
            if isinstance(p, dict):
                return float(p["x"]), float(p["y"]), float(p.get("theta", p.get("dir", 0.0)))
            x,y,d = p
            return float(x), float(y), float(d)
        x1,y1,t1 = _norm_pose(p1); x2,y2,t2 = _norm_pose(p2)
        return float(np.hypot(x1-x2, y1-y2) + 0.25*abs(t1 - t2))

    def compute_feature_similarity(self, f1, f2) -> float:
        try:
            sim = 1.0 - cosine(f1, f2)
            return float(np.clip(sim, 0.0, 1.0))
        except Exception:
            return 0.5




# Robust imports for your MemoryGraph + config loader
MemoryGraph = None
setup_memory_config = None
_import_errs = []
for modpath, name in [
    ("navigation_model.Services.memory_service.memory_graph", "MemoryGraph"),
    ("navigation_model.Services.memory_service.memory_graph.memory_graph", "MemoryGraph"),
    ("navigation_model.Services.memory_service", "memory_graph"),
]:
    try:
        mod = __import__(modpath, fromlist=[name])
        MemoryGraph = getattr(mod, name) if hasattr(mod, name) else getattr(mod, "MemoryGraph")
        break
    except Exception as e:
        _import_errs.append((modpath, str(e)))

if setup_memory_config is None:
    for modpath in [
        "control_eval.input_output",
        "navigation_model.control_eval.input_output",
        "control_eval.io",
    ]:
        try:
            mod = __import__(modpath, fromlist=["setup_memory_config"])
            setup_memory_config = getattr(mod, "setup_memory_config")
            break
        except Exception as e:
            _import_errs.append((modpath, str(e)))

import gym
import cv2
class DictResizeObs(gym.ObservationWrapper):
    def __init__(self, env, out_hw=(64,64)):
        super().__init__(env)
        self.out_hw = out_hw              # (H,W)

    def observation(self, obs):
        assert isinstance(obs, dict) and "image" in obs, \
               "expect dict with 'image' key"
        img = obs["image"]                               # (H,W,3) uint8
        img = cv2.resize(img, self.out_hw[::-1],
                         interpolation=cv2.INTER_AREA)
        obs["image"] = img
        return obs
if __name__ == "__main__":
    
    import time, random, numpy as np, torch
    import gym, gym_minigrid
    from collections import deque
    from itertools import islice
    from gym_minigrid.minigrid import Wall
    from gym_minigrid.wrappers import RGBImgPartialObsWrapper, ImgActionObsWrapper
    from world_model_utils import DictResizeObs, WMPlanner, State
    try:
        from world_model_utils import DualPathCollage  # if you have it
    except Exception:
        class DualPathCollage:
            def __init__(self, **kw): pass
            def add_step(self, **kw): pass
            def finalize(self): pass
    # ---------- config ----------
    def decode_policy_onehot(onehot_tensor: torch.Tensor) -> list[str]:
        """
        Fallback decoder if novelty_astar_plan returns a (T,3) plan.
        """
        idx_to_act = {0: "forward", 1: "right", 2: "left"}
        if onehot_tensor is None or onehot_tensor.numel() == 0:
            return []
        acts = []
        for row in onehot_tensor.cpu():
            j = int(row.argmax().item())
            acts.append(idx_to_act[j])
        return acts
    def dreamer_decode_from_belief(planner, wm, belief_zd, *, to_01: bool = True):
        import torch
        from torch.distributions import Distribution

        if belief_zd is None:
            return None

        z, h = belief_zd
        with torch.no_grad():
            out = wm.decoder(z, h)
            x = out.mean if isinstance(out, Distribution) else out
            x = torch.as_tensor(x).detach().float()
            if x.ndim == 4 and x.shape[0] == 1:
                x = x[0]
            if x.ndim == 2:
                x = x.unsqueeze(0).repeat(3, 1, 1)
            elif x.ndim == 3 and x.shape[0] not in (1, 3) and x.shape[-1] in (1, 3):
                x = x.permute(2, 0, 1).contiguous()
            elif x.ndim == 3 and x.shape[0] == 1:
                x = x.repeat(3, 1, 1)
            if to_01:
                x = (x + 0.5).clamp(0.0, 1.0)
            return x.cpu()

    def append_to_replay_buffer(buf: deque, obs: dict, action_int: int, belief_zd, node_id):
        """
        Store Dreamer belief, embeddings, decoded prediction, and enhanced predictions.
        """
        z_embed = planner.dreamer_embed_fn(belief_zd)  # (Z,) flattened image embedding in [0,1]
        decoded = dreamer_decode_from_belief(planner, wm, belief_zd)  # (3,H,W) or None
        enhanced_preds = planner.build_enhanced_perception(
            wm, belief_zd,
            combos=[('left',), ('right',), ('left','left'), ('right','right')]
        )
        pose  = obs.get("pose")
        image = obs.get("image")
        if isinstance(pose, np.ndarray):            pose = pose.copy()
        elif isinstance(pose, (list, tuple)):       pose = list(pose)
        elif isinstance(pose, dict):                pose = dict(pose)
        # (else leave as-is)

        if isinstance(image, np.ndarray):           image = image.copy()
        elif hasattr(image, "copy"):                image = image.copy()  # e.g., PIL.Image
        # (else leave as-is)
        entry = {
            "node_id": node_id,
            "real_pose": pose,
            "imagined_pose": None,
            "real_image": image,
            "imagined_image": None,
            "action": int(action_int),
            "dreamer_z": z_embed,
            "belief_zd": belief_zd,
            "decoded_image": decoded,
            "enhanced_preds": enhanced_preds,
        }
        print("NODE ID", node_id, entry["node_id"])
        buf.append(entry)

    def visualize_policy_probe(planner, wm, belief_zd, obs, action_names, t_step,
                               out_dir="dbg", include_start=True):
        import torch
        from pathlib import Path
        from torchvision.utils import save_image

        if not action_names:
            return None

        real_frame = torch.as_tensor(obs["image"]).permute(2, 0, 1).float() / 255.0 - 0.5
        frames_pred = planner.render_plan(wm, belief_zd, action_names, include_last=True)
        frames = [real_frame.detach().cpu()] + [f.detach().cpu() for f in frames_pred]
        Path(out_dir).mkdir(parents=True, exist_ok=True)

        short = "".join(a[0].upper() for a in action_names)
        out_path = f"{out_dir}/probe_t{t_step:02d}_{short}.png"
        save_image(torch.stack(frames), out_path, nrow=len(frames), normalize=True, scale_each=False)
        print(f"saved {out_path}")
        return out_path

    def save_replay_image_history(replay_buffer, planner, wm, t_step, Ns=(5,10,15,20), out_dir="dbg/history"):
        import torch
        from pathlib import Path
        from torchvision.utils import save_image

        Path(out_dir).mkdir(parents=True, exist_ok=True)

        def to_chw_real(img_hwc):
            t = torch.as_tensor(img_hwc).permute(2,0,1).float() / 255.0 - 0.5
            return t

        for N in Ns:
            tail = list(islice(reversed(replay_buffer), 0, N))
            if not tail:
                continue
            tail = list(reversed(tail))

            reals, preds = [], []
            for st in tail:
                ri = st.get("real_image", None)
                bi = st.get("belief_zd", None)
                if ri is None:
                    continue
                reals.append(to_chw_real(ri))
                di = st.get("decoded_image", None)
                if di is None and bi is not None:
                    di = dreamer_decode_from_belief(planner, wm, bi)
                if di is None:
                    di = reals[-1]
                preds.append(di)

            if not reals:
                continue

            frames = reals + preds
            out_path = f"{out_dir}/history_t{t_step:02d}_N{len(reals)}.png"
            save_image(torch.stack(frames), out_path, nrow=len(reals), normalize=True, scale_each=False)
            print(f"saved {out_path}")

    def infer_env_definition(env) -> dict:
        e = getattr(env, "unwrapped", env)
        try:
            n_row = int(getattr(e, "rooms_in_row", 1))
            n_col = int(getattr(e, "rooms_in_col", 1))
        except Exception:
            n_row, n_col = 1, 1
        return {"n_row": n_row, "n_col": n_col, "max_steps": int(N_STEPS)}
    
    CKPT        = "runs/mg_collision/20250704-220917/ckpt/iter05000.pt"
    N_STEPS     = 300            # run the novelty policy for this many env steps
    LOOKAHEAD   = 7             # A* novelty horizon
    K_RECENT    = 30            # how many recent embeddings to compare against
    METRIC      = "cos"         # "kl" | "cos" | "l2"
    DEVICE      = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Debug/vis knobs
    DEBUG_PRINT          = True   # keep tree/top-K/action scores logs
    VIZ_TREE             = False   # save decision tree collage from novelty_astar_plan
    VIZ_EVERY            = 15      # save imagined rollout strip and replay history every k steps (set 1 to save each step)
    VIZ_OUTDIR           = "dbg"  # where to write images
    SAVE_HISTORY_LENGTHS = (15,20,30,45, 60, 90, 120,150,180,210,240,270,300,330,360,390,410,440,470,500,530,560,590)

    # ---------- boot world model & env --------
    from pathlib import Path
    ROOT = Path(__file__).resolve().parents[1]  # repo root (…/hierarchical-nav/)
    MEMCFG = ROOT / "navigation_model/Services/memory_service/memory_graph_config.yml"

    planner = WMPlanner(ckpt=CKPT,device=str(DEVICE),memory_config=str(MEMCFG))
    wm = planner.wm
    print("✓ world-model loaded")
    

    env = gym.make("MiniGrid-4-tiles-ad-rooms-v0", rooms_in_row=3, rooms_in_col=4, max_steps=None)
    env.seed(218)
    env = RGBImgPartialObsWrapper(env)
    env = ImgActionObsWrapper(env)
    env = DictResizeObs(env, (64, 64))
    planner.env=env

    env_definition = infer_env_definition(env)
    visited_rooms: list[tuple[int, int]] = []   # keep if you later want room tracking
    VIDEO_OUT = Path("dbg/unified_runner.mp4")
    VIDEO_OUT.parent.mkdir(parents=True, exist_ok=True)
    video_gridmap = VideoGridmap(str(VIDEO_OUT), fps=10)
    print(f"✓ video writer @ {VIDEO_OUT}")
    
    # ---------- init replay & belief ----------
    replay_buffer = deque(maxlen=30)
    obs = env.reset()
    # --- at episode start
    episode_id=1
    brain = KBrain(
        env=env,
        planner=planner,
        lookahead=LOOKAHEAD,
        k_recent=K_RECENT,
        metric=METRIC,
        debug_print=DEBUG_PRINT,
        viz_tree=VIZ_TREE,
        viz_every=VIZ_EVERY,
        viz_outdir=VIZ_OUTDIR,

        memcfg_path=str(MEMCFG),         # your memory_graph_config.yml
        replay_buffer=lambda: replay_buffer)


    collage = DualPathCollage(
        out_dir="dbg/collages",
        tag=f"ep{episode_id:03d}",
        tile=64,          # match your Dreamer decode size if 64x64
        cols=20,          # tweak to taste
        live_write=True,  # set False if you only want the final PNGs
        live_every=1,     # write after every step when live
        annotate_idx=True,
        planner=planner,  # so it can call dreamer_decode_from_belief
        wm=wm
    )


    # bootstrap belief from first observation
    frame = obs["image"].transpose(2,0,1) / 255.0 - 0.5
    belief = planner.wm_update_belief(wm, prev_z_d=None, frame_rgb=frame, prev_action_onehot=None)

    # take one forward step to match your current flow, then update belief and seed buffer
    obs, _, done, _ = env.step(env.actions.forward)
    prev_act_1h = planner.onehot("forward", wm)
    belief = planner.update_belief_from_obs(obs, belief, prev_act_1h)
    append_to_replay_buffer(replay_buffer, obs, env.actions.forward, belief, None)
    pose_xyz = tuple(obs["pose"])
    print(obs["image"].shape)
    collage.add_step(real_img=obs["image"], belief_zd=belief)
    brain.belief = belief
    brain.post_step_update(obs, belief)
    planner.update_cog(obs["image"], prev_act_1h,pose_xyz, place_post=None)
    print(planner.get_cog_nodes())

    # maps between env ints and names
    act_to_name = {
        env.actions.left: "left",
        env.actions.right: "right",
        env.actions.forward: "forward",
    }
    name_to_act = {v: k for k, v in act_to_name.items()}

    try:
        print("\n=== Novelty-driven rollout (every step) ===")
        for t in range(1, N_STEPS + 1):
            start = State(*obs["pose"])

            print("STEP¡¡¡¡",t)
            action_name = brain.apply_exploration(start_state=start, t_step=t)  # [PATCH]
            env_act = name_to_act[action_name]

            # --- take the step ---
            next_obs, _, done, _ = env.step(env_act)
            if t == 1:
                print("[DIAG] obs['image']:", type(next_obs.get("image")), 
                    getattr(next_obs.get("image"), "shape", None))
            # --- update belief with observed frame and prev action ---
            prev_act_1h = planner.onehot(action_name, wm)
            planner.emap.last_real_pose= obs["pose"]
            #brain.nav_system.push_pose(obs["pose"])
            belief = planner.update_belief_from_obs(next_obs, belief, prev_act_1h)
            pose_xyz = tuple(next_obs["pose"])
            collage.add_step(real_img=next_obs["image"], belief_zd=belief)
            planner.update_cog(next_obs["image"], prev_act_1h,pose_xyz, place_post=None)
            if planner.emap.current_exp is not None:
                e = planner.emap.current_exp
                print(f"[PLACE] Exp{e.id} at {e.grid_xy}: {e.place_kind}"
                    + (f" ({e.room_color})" if e.room_color else ""))
                node_id=e.id
                print("NODE ID", node_id)
            else:
                node_id=None
            # --- push transition to replay buffer (includes decoded prediction) ---
            print("NODE ID", node_id)
            append_to_replay_buffer(replay_buffer, next_obs, env_act, belief,node_id)
            brain.belief = belief
            brain.post_step_update(next_obs, belief)
            # --- Build per-step data payload for the recorder ---
            try:
                # Full world view (fallback to obs image)
                env_img = None
                if hasattr(env, "render"):
                    try:
                        env_img = env.render(mode="rgb_array")
                    except TypeError:
                        env_img = env.render()  # some envs ignore mode kwarg
                if env_img is None:
                    env_img = next_obs.get("image", obs.get("image", None))

                # Dreamer decode (you already had dreamer_decode_from_belief above)
                decoded_img = dreamer_decode_from_belief(planner, wm, belief, to_01=True)

                # Optional predicted image: keep stub for wiring later
                predicted_img = None  # e.g., planner.render_prediction(...)

                # Extract positions for the side panel
                try:
                    GP = planner.emap.get_global_position() if hasattr(planner, "emap") else None
                except Exception:
                    GP = None

                data_for_frame = {
                    "env_image": env_img,
                    "ground_truth_ob": next_obs.get("image"),
                    "decoded": decoded_img,                 # will be shown if not None
                    "image_predicted": predicted_img,       # stub; safe if None
                    "mse": compute_mse(next_obs.get("image"), decoded_img) if decoded_img is not None else 0.0,
                    "GP": GP,
                    "pose": next_obs.get("pose"),
                    # HMM block (stubs — fill these when you wire your HMM stats)
                    "recommended_mode": brain.current_mode,
                    # "mode_confidence": 0.0,
                    # "submode_confidence": 0.0,
                    # "uncertainty": 0.0,
                    # "changepoint_mass": 0.0,
                }
                print("[DIAG] frame slots:",
                    "env", type(env_img), getattr(env_img, "shape", None),
                    "gt", type(next_obs.get("image")), getattr(next_obs.get("image"), "shape", None),
                    "dec", type(decoded_img), getattr(decoded_img, "shape", None))
                visited_rooms = env.unwrapped.get_visited_rooms_order()
                append_vis_frame_unified(
                    planner_like=planner,
                    video_gridmap=video_gridmap,
                    data=data_for_frame,
                    env_definition=env_definition,
                    visited_rooms=visited_rooms,
                    step_count=t,
                    seconds_per_step=1.0,
                )
                
            except Exception as e:
                print("[RECORDER] frame failed:", e)
            # --- debug prints / tree / top-K paths ---
            if DEBUG_PRINT:
                print(f"[t={t:02d}] chose action → {action_name}")
                # if internal records exist, echo them (kept from your probe block)
                tree = getattr(planner, "_last_decision_tree", None)
                if hasattr(planner, "_last_top_paths"):
                    print("\n=== TOP-K PATHS USED FOR VOTING ===")
                    for i, rec in enumerate(planner._last_top_paths, 1):
                        a0 = rec["seq"][0] if rec["seq"] else "∅"
                        print(f"#{i:02d} G={rec['gain']:.3f} first={a0:<7} depth={rec['depth']} "
                            f"seq={rec['seq']} pruned={rec.get('pruned_reason')}")
                if hasattr(planner, "_last_action_scores"):
                    print("Action vote scores:", getattr(planner, "_last_action_scores"))

            # --- visualizations (policy rollout + history strips) ---
            if (t % max(1, VIZ_EVERY)) == 0:
                # visualize 1-step policy just for local context; if you want longer, call
                # planner.rank_paths(...) to grab the best sequence and pass it in.
                visualize_policy_probe(planner, wm, belief, next_obs, [action_name], t_step=t,
                                    out_dir=VIZ_OUTDIR)
                save_replay_image_history(replay_buffer, planner, wm, t_step=t,
                                        Ns=SAVE_HISTORY_LENGTHS, out_dir=f"{VIZ_OUTDIR}/history")
                planner.save_cog_map_snapshot(t_step=t, out_dir=f"{VIZ_OUTDIR}/cogmap")
                planner.save_cog_map_vs_env(env, t_step=t, out_dir="dbg/cogmap", tile_size=32, dpi=160)
            

                # advance obs pointer
            obs = next_obs

            # --- handle episode end robustly ---
            if done:
                obs = env.reset()
                frame = obs["image"].transpose(2,0,1) / 255.0 - 0.5
                belief = planner.wm_update_belief(wm, prev_z_d=None, frame_rgb=frame, prev_action_onehot=None)
                obs, _, done, _ = env.step(env.actions.forward)
                prev_act_1h = planner.onehot("forward", wm)
                belief = planner.update_belief_from_obs(obs, belief, prev_act_1h)
                replay_buffer.clear()
                collage.finalize()
                append_to_replay_buffer(replay_buffer, obs, env.actions.forward, belief)
    except KeyboardInterrupt:
        print("\n[RUN] Interrupted by user (Ctrl+C). Finalizing video...")

    except Exception as e:
        import traceback
        print("[RUN] Exception:", e)
        print(traceback.format_exc())

    finally:
        # Try to “hold” the last frame to make the tail visible, like your prior scripts
        try:
            if 'data_for_frame' in locals():
                # build map data one more time
                mem = safe_get_memory_map_data_from(planner)
                last = record_video_frames(
                    data_for_frame, env_definition, agent_lost=False,
                    visited_rooms=visited_rooms, memory_map_data=mem, step_count=t
                )
                video_gridmap.append_data(last)
                video_gridmap.append_data(last)
        except Exception as e:
            print("[FINALIZE] Could not append last frame twice:", e)
        finally:
            video_gridmap.close()
            print(f"[RUN] Steps written: {t}, expected duration ≈ {t / 10:.2f}s")
            print(f"✓ video saved → {VIDEO_OUT}")
    
    print("\nDone. (Executed novelty policy at every step.)")
```

`hierarchical-nav/dreamer_mg/viz_help.py`:

```py
#!/usr/bin/env python3
"""
unified_test_runner.py

Standalone utilities for step-by-step video recording (env + GT obs + Dreamer decode +
HMM panel + experience map + pose panel), designed to be reused across scripts.

Typical usage:

    from unified_test_runner import (
        VideoGridmap, append_vis_frame_unified, record_video_frames,
        safe_get_memory_map_data_from, compute_mse
    )

    video = VideoGridmap("dbg/unified_runner.mp4", fps=10)
    env_definition = {"n_row": 5, "n_col": 5, "max_steps": 600}
    visited_rooms = []

    # ... inside your control loop:
    data = {
        "env_image": env_img,                 # HxWx3 uint8 or compatible tensor
        "ground_truth_ob": obs["image"],      # HxWx3
        "decoded": decoded_img,               # optional
        "image_predicted": predicted_img,     # optional
        "mse": compute_mse(obs["image"], decoded_img) if decoded_img is not None else 0.0,
        "GP": GP,                             # optional: global pose
        "pose": obs.get("pose"),              # optional: local pose
        # Optional HMM block:
        # "recommended_mode": hmm_mode,
        # "recommended_submode": hmm_submode,
        # "mode_confidence": 0.87,
        # "submode_confidence": 0.66,
        # "uncertainty": 0.42,
        # "changepoint_mass": 0.07,
    }

    append_vis_frame_unified(
        planner_like=planner,                 # must expose get_memory_map_data() OR planner.cog/.emap
        video_gridmap=video,
        data=data,
        env_definition=env_definition,
        visited_rooms=visited_rooms,
        step_count=t
    )

    # on exit (even if Ctrl+C):
    video.close()

Notes:
- This file is intentionally headless-safe (sets matplotlib backend to 'Agg' when needed).
- `safe_get_memory_map_data_from()` reconstructs memory_map_data if planner doesn’t expose it directly.
"""

from __future__ import annotations

import os
import math
from typing import Any, Dict, List, Tuple, Optional
import os, shutil, subprocess


# Headless-friendly Matplotlib
import matplotlib
if os.environ.get("DISPLAY", "") == "":
    matplotlib.use("Agg")
if os.environ.get("DISPLAY", "") == "":
    matplotlib.use("Agg")  # headless-safe
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas

import matplotlib.pyplot as plt
import numpy as np

# OpenCV is optional but strongly recommended for MP4 writing
try:
    import cv2
except Exception:
    cv2 = None  # we’ll raise a helpful error on writer init if it’s missing


# ------------------------------ Public API ---------------------------------

__all__ = [
    "VideoGridmap",
    "dbg_stats",
    "prepare_for_imshow",
    "get_image_plot",
    "plot_MSE_bar",
    "plot_visited_rooms",
    "plot_memory_map",
    "print_positions",
    "record_video_frames",
    "safe_get_memory_map_data_from",
    "append_vis_frame_unified",
    "compute_mse",
]


# --- Minimal writer with old API (append_data / close) ---------------------
class VideoGridmap:
    """
    Browser-safe video writer.
    Prefers FFmpeg (H.264 baseline, yuv420p, faststart), falls back to OpenCV avc1/mp4v or MJPG/AVI.
    API: append_data(frame: HxWx{3,4} uint8 RGB), close()
    """
    def __init__(self, out_path: str, fps: int = 10):
        self.out_path = str(out_path)
        self.fps = int(fps)
        self._method = None          # 'ffmpeg' | 'opencv'
        self._proc = None            # FFmpeg subprocess
        self._writer = None          # cv2.VideoWriter
        self._shape = None           # (H, W)
        self._count = 0
        os.makedirs(os.path.dirname(self.out_path) or ".", exist_ok=True)

        # detect ffmpeg
        self._ffmpeg = shutil.which("ffmpeg")

    # ---------- FFmpeg path ----------
    def _start_ffmpeg(self, W: int, H: int):
        base, ext = os.path.splitext(self.out_path)
        # force .mp4 for browser playback
        if ext.lower() != ".mp4":
            self.out_path = base + ".mp4"

        cmd = [
            self._ffmpeg, "-loglevel", "error", "-y",
            # raw RGB input via stdin
            "-f", "rawvideo", "-vcodec", "rawvideo",
            "-pix_fmt", "rgb24",
            "-s", f"{W}x{H}",
            "-r", str(self.fps),
            "-i", "-",
            # no audio
            "-an",
            # browser-safe H.264
            "-vcodec", "libx264",
            "-pix_fmt", "yuv420p",
            "-profile:v", "baseline",
            "-level", "3.0",
            "-movflags", "+faststart",
            # CFR at the same fps on the output side too
            "-r", str(self.fps),
            self.out_path,
        ]
        try:
            self._proc = subprocess.Popen(cmd, stdin=subprocess.PIPE)
            self._method = "ffmpeg"
            print(f"[VideoGridmap] Using FFmpeg → {self.out_path}  {W}x{H}@{self.fps}fps")
        except Exception as e:
            self._proc = None
            print("[VideoGridmap] FFmpeg failed to start:", e)

    def _ffmpeg_write(self, frame_rgb: np.ndarray):
        try:
            self._proc.stdin.write(frame_rgb.tobytes())
        except BrokenPipeError:
            raise RuntimeError("[VideoGridmap] FFmpeg pipe broken (check codec support).")

    def _ffmpeg_close(self):
        if self._proc is not None:
            try:
                self._proc.stdin.close()
            except Exception:
                pass
            self._proc.wait()
            self._proc = None

    # ---------- OpenCV fallback ----------
    def _opencv_init(self, W: int, H: int):
        import cv2
        tried = []
        base, _ = os.path.splitext(self.out_path)

        # best → avc1 (if your OpenCV build can encode H.264)
        for fourcc_str, ext in [("avc1", "mp4"), ("mp4v", "mp4"), ("MJPG", "avi")]:
            out = f"{base}.{ext}"
            fourcc = cv2.VideoWriter_fourcc(*fourcc_str)
            writer = cv2.VideoWriter(out, fourcc, self.fps, (W, H))
            ok = bool(writer is not None and writer.isOpened())
            tried.append((fourcc_str, out, ok))
            if ok:
                self.out_path = out
                self._writer = writer
                self._shape = (H, W)
                self._method = "opencv"
                print(f"[VideoGridmap] OpenCV writer fourcc={fourcc_str} → {out} {W}x{H}@{self.fps}fps")
                if ext != "mp4":
                    print("[VideoGridmap] NOTE: AVI/MJPG won’t play in browsers. Use FFmpeg path for browser-safe MP4.")
                return

        raise RuntimeError(f"[VideoGridmap] Could not open any OpenCV VideoWriter. Tried={tried}")

    def append_data(self, frame: np.ndarray) -> None:
        if frame is None:
            print("[VideoGridmap] WARNING: None frame; skipping")
            return

        # ensure HxWx3 uint8 RGB
        if frame.ndim == 2:
            frame = np.repeat(frame[..., None], 3, axis=2)
        if frame.shape[-1] == 4:
            frame = frame[..., :3]
        if frame.dtype != np.uint8:
            frame = np.clip(frame, 0, 255).astype(np.uint8)

        H, W, _ = frame.shape

        # lazy-init on first frame, prefer FFmpeg
        if self._method is None:
            if self._ffmpeg is not None:
                self._start_ffmpeg(W, H)
            if self._method is None:
                # FFmpeg not available or failed; try OpenCV
                self._opencv_init(W, H)

            # dump first frame for sanity
            try:
                import cv2
                dbg_png = os.path.join(os.path.dirname(self.out_path), "_first_frame_rgb.png")
                cv2.imwrite(dbg_png, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
                print(f"[VideoGridmap] First-frame debug → {dbg_png} (shape={frame.shape})")
            except Exception as e:
                print("[VideoGridmap] Could not write first-frame PNG:", e)

            self._shape = (H, W)

        # size consistency (resize to the first frame’s shape)
        if (H, W) != self._shape:
            import cv2
            frame = cv2.resize(frame, (self._shape[1], self._shape[0]), interpolation=cv2.INTER_AREA)

        # write
        if self._method == "ffmpeg":
            self._ffmpeg_write(frame)  # rgb24 piped in
        elif self._method == "opencv":
            import cv2
            self._writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
        else:
            raise RuntimeError("[VideoGridmap] No writer initialized")

        self._count += 1

    def close(self) -> None:
        # close writers and report
        try:
            if self._method == "ffmpeg":
                self._ffmpeg_close()
            elif self._method == "opencv":
                if self._writer is not None:
                    self._writer.release()
        finally:
            self._writer = None
            self._proc = None

        try:
            sz = os.path.getsize(self.out_path)
            print(f"[VideoGridmap] Closed → {self.out_path}  frames={self._count}  size={sz/1024:.1f} KiB  fps={self.fps}  method={self._method}")
        except Exception as e:
            print("[VideoGridmap] Could not stat output:", e)

# --- Small debug printer used by the figure builder ------------------------
def dbg_stats(name: str, obj: Any) -> None:
    try:
        import torch
    except Exception:
        torch = None
    shp = None
    if isinstance(obj, np.ndarray):
        shp = obj.shape, obj.dtype
    elif torch is not None and hasattr(torch, "is_tensor") and torch.is_tensor(obj):
        shp = tuple(obj.shape), str(obj.dtype)
    elif isinstance(obj, (list, tuple)) and obj and isinstance(obj[0], np.ndarray):
        shp = [x.shape for x in obj[:3]] + (["..."] if len(obj) > 3 else [])
    print(f"[DBG] {name}: {shp}")


def prepare_for_imshow(x: Any) -> np.ndarray:
    """
    Accepts: numpy HxWx3 uint8, torch (C,H,W)/(H,W,3) in [-0.5,0.5] or [0,1], or list of such.
    Returns: numpy HxWx3 uint8.
    """
    try:
        import torch
    except Exception:
        torch = None

    def _to_uint8(arr: np.ndarray) -> np.ndarray:
        arr = arr.astype(np.float32)
        a, b = float(np.nanmin(arr)), float(np.nanmax(arr))
        if b <= 1.0 and a >= 0.0:
            # already [0,1] → just scale
            arr = np.clip(arr * 255.0, 0, 255)
        elif b <= 0.5 and a >= -0.5:
            # Dreamer-style [-0.5,0.5] → shift then scale
            arr = np.clip((arr + 0.5) * 255.0, 0, 255)
        else:
            # assume already [0,255]ish
            arr = np.clip(arr, 0, 255)
        return arr.astype(np.uint8)

    if x is None:
        return np.zeros((64, 64, 3), dtype=np.uint8)

    if isinstance(x, (list, tuple)) and len(x) > 0:
        return prepare_for_imshow(x[0])

    if isinstance(x, np.ndarray):
        arr = x
        # CHW -> HWC
        if arr.ndim == 3 and arr.shape[0] in (1, 3) and arr.shape[-1] not in (1, 3):
            arr = np.transpose(arr, (1, 2, 0))
        if arr.ndim == 2:
            arr = np.repeat(arr[..., None], 3, axis=2)
        if arr.dtype != np.uint8:
            arr = _to_uint8(arr)
        return arr

    if torch is not None and hasattr(torch, "is_tensor") and torch.is_tensor(x):
        t = x.detach().float()
        if t.ndim == 4 and t.shape[0] == 1:
            t = t[0]
        if t.ndim == 3 and t.shape[0] in (1, 3):
            t = t.permute(1, 2, 0).contiguous()
        if t.ndim == 2:
            t = t.unsqueeze(-1).repeat(1, 1, 3)
        arr = t.cpu().numpy()
        return _to_uint8(arr)

    return np.zeros((64, 64, 3), dtype=np.uint8)


# --- Tiny helpers used by record_video_frames --------------------------------
def get_image_plot(ax, data, title):
    # Guard: if data is None, draw a placeholder so figure still rasterizes
    if data is None:
        ax.text(0.5, 0.5, f"{title}: None", ha="center", va="center", fontsize=10)
        ax.set_title(title)
        ax.axis("off")
        return ax
    ax.imshow(data)
    ax.set_title(title)
    ax.axis("off")
    return ax


def plot_MSE_bar(ax, mse_err: float, agent_lost: bool):
    try:
        color = "blue" if (mse_err is not None and mse_err < 0.5) else "red"
        ax.bar("MSE error", mse_err if mse_err is not None else 0.0, color=color, width=0.1)
    except Exception as e:
        ax.text(-0.3, 0.9, f"{e}", fontsize=10, color="black")
    ax.set_ylim(0, 1)
    ax.set_facecolor("#ff8970" if agent_lost else "0.8")
    ax.grid(axis="y", alpha=0.5)
    ax.set_title("mse ob/expectation")


def plot_visited_rooms(ax, visited_rooms: List[Tuple[int, int]], env_definition: Dict[str, Any]):
    """
    Robust version: renders a simple grid heatmap if n_row/n_col exist,
    otherwise shows a small 'no grid' note.
    """
    try:
        n_row = int(env_definition.get("n_row", 0))
        n_col = int(env_definition.get("n_col", 0))
    except Exception:
        n_row = n_col = 0

    if n_row > 0 and n_col > 0:
        grid = np.ones((n_row, n_col), dtype=float)
        for i in range(n_col):
            for j in range(n_row):
                if (i, j) in visited_rooms:
                    pos = visited_rooms.index((i, j))
                    grid[j, i] = pos / max(1, len(visited_rooms))
        im = ax.imshow(grid, cmap="viridis", origin="lower")
        ax.set_title("Rooms ordered by discovery")
        ax.set_xticks(range(n_col)), ax.set_yticks(range(n_row))
        ax.set_xticklabels(range(n_col)), ax.set_yticklabels(range(n_row))
        # shrunk colorbar
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        div = make_axes_locatable(ax)
        cax = div.append_axes("right", size="5%", pad=0.05)
        cbar = plt.colorbar(im, cax=cax, orientation="vertical", label="Visited Rooms")
        cbar.set_ticks([0, 0.9, 1])
        cbar.set_ticklabels(["oldest", "newest", "unknown"])
    else:
        ax.text(0.5, 0.5, "No room grid available", ha="center", va="center")
        ax.set_title("Rooms")
    ax.set_aspect("equal")
    ax.grid(False)
    return ax


def plot_memory_map(ax, memory_map_data: Dict[str, Any], *, dbg: bool = False):
    """
    Experience-map visual with guards and dedup. (Enhanced version.)
    """
    ax.cla()
    ax.set_title("Experience Map")
    ax.grid(True)
    ax.set_aspect("equal", "datalim")

    curr_id = memory_map_data.get("current_exp_id", -1)
    if curr_id < 0:
        if dbg:
            print("[VIS] No current exp → skipping map.")
        return ax

    exps_gp    = memory_map_data.get("exps_GP", [])
    exps_decay = memory_map_data.get("exps_decay", [])
    if exps_gp:
        pts = np.array(exps_gp)
        ax.scatter(
            pts[:, 0], pts[:, 1],
            c=exps_decay if len(exps_decay) == len(pts) else None,
            cmap="viridis",
            s=60, edgecolor="k", linewidth=0.5, label="Places"
        )
        # annotate w/ index when we can
        for idx, (x, y) in enumerate(pts):
            ax.text(x, y, str(idx), fontsize=7, color="white",
                    ha="center", va="center",
                    bbox=dict(facecolor="black", alpha=0.35, pad=0.5))

    # Ghost links
    gl = memory_map_data.get("ghost_exps_link", [])
    for i in range(0, len(gl) // 2 * 2, 2):
        p0 = tuple(gl[i]); p1 = tuple(gl[i+1])
        ax.plot([p0[0], p1[0]], [p0[1], p1[1]],
                ls="--", color="magenta", alpha=0.3, linewidth=2)

    # Real links (dedup)
    rl, seen = [], set()
    raw = memory_map_data.get("exps_links", [])
    for i in range(0, len(raw) // 2 * 2, 2):
        p0 = tuple(raw[i]); p1 = tuple(raw[i+1])
        if (p0, p1) not in seen:
            seen.add((p0, p1))
            rl.append((p0, p1))
    for p0, p1 in rl:
        ax.plot([p0[0], p1[0]], [p0[1], p1[1]], color="gray", linewidth=2)

    # current exp
    cx, cy = memory_map_data.get("current_exp_GP", [0, 0])[:2]
    ax.plot(cx, cy, marker="X", color="red", markersize=10, label="Current")
    ax.text(cx, cy, str(curr_id), color="white", fontsize=9, weight="bold",
            ha="center", va="center", bbox=dict(facecolor="red", alpha=0.5, pad=1))

    if dbg:
        ax.legend(loc="upper right", fontsize=8)
    return ax


def print_positions(ax, GP, pose, step_count: int, mode_str: Optional[str] = None):
    current_gp = [float(x) for x in np.around(np.array(GP), 2)] if GP is not None else ["?", "?", "?"]
    lines = [
        f"Step: {step_count}",
        "Global Position [x,y,th]:",
        str(current_gp),
        "Local Position  [x y th]:",
        str(pose if pose is not None else ["?", "?", "?"]),
    ]
    if mode_str:
        lines += ["", mode_str]
    ax.text(0.0, 1.0, "\n".join(lines), fontsize=10, va="top", ha="left", family="monospace")
    ax.set_axis_off()
    return ax


# --- The main frame builder you already use (ported + safer reshape) --------
def record_video_frames(data: Dict[str, Any],
                        env_definition: Dict[str, Any],
                        agent_lost: bool,
                        visited_rooms: List[Tuple[int, int]],
                        memory_map_data: Dict[str, Any],
                        step_count: int) -> np.ndarray:
    """
    Creates a full HxWx3 uint8 image for the video.
    """
    # IMPORTANT: don't reuse a fixed figure number; stale canvases can corrupt buffers.
    fig = plt.figure(figsize=(12, 6), dpi=160)
    s = fig.add_gridspec(
        4, 6,
        width_ratios=[3, 3, 3, 3, 3, 3],
        height_ratios=[3, 3, 3, 3],
        wspace=0.8, hspace=0.4
    )

    # WORLD
    ax1 = fig.add_subplot(s[:2, :2])
    dbg_stats("env_image", data.get('env_image'))
    ax1 = get_image_plot(ax1, prepare_for_imshow(data.get('env_image')), 'World')

    # GT OBS
    ax2 = fig.add_subplot(s[:1, 2:3])
    dbg_stats("ground_truth_ob", data.get('ground_truth_ob'))
    ax2 = get_image_plot(ax2, prepare_for_imshow(data.get('ground_truth_ob')), 'GT OB')

    # PREDICTED
    if data.get('image_predicted') is not None:
        pred_img = prepare_for_imshow(data['image_predicted'])
        ax3 = fig.add_subplot(s[:2, 3])
        ax3 = get_image_plot(ax3, pred_img, 'Predicted ob')

    # DECODED
    if data.get('decoded') is not None:
        dec_img = prepare_for_imshow(data['decoded'])
        ax_dec = fig.add_subplot(s[1:2, 2:3])
        ax_dec = get_image_plot(ax_dec, dec_img, 'Decoded ob')

    # VISITED ROOMS
    ax4 = fig.add_subplot(s[2:4, :2])
    ax4 = plot_visited_rooms(ax4, visited_rooms, env_definition)

    # MODE / SUBMODE (HMM)
    mode_line = ""
    if 'recommended_mode' in data:
        mode_line = f"mode={data['recommended_mode']}   submode={data.get('recommended_submode')}"
        ax_state = fig.add_subplot(s[2:4, 2:4])
        txt = []
        txt.append(f"► MODE :  {data['recommended_mode']} ({data.get('mode_confidence', 0):.2f})")
        txt.append(f"► SUB  :  {data.get('recommended_submode')} ({data.get('submode_confidence', 0) or 0:.2f})")
        if 'uncertainty' in data:
            txt.append(f"entropy={data['uncertainty']:.2f}   changepoint P={data.get('changepoint_mass', 0):.2f}")
        ax_state.text(0.0, 1.0, "\n".join(txt), fontsize=10, va="top", ha="left", family="monospace")
        ax_state.set_axis_off()

    # MSE
    ax6 = fig.add_subplot(s[:2, 4])
    ax6 = plot_MSE_bar(ax6, data.get('mse', 0.0), agent_lost)

    # COGNITIVE MAP
    ax7 = fig.add_subplot(s[2:4, 4:])
    ax7 = plot_memory_map(ax7, memory_map_data)

    # POSES
    ax8 = fig.add_subplot(s[:2, 5])
    ax8 = print_positions(ax8, data.get('GP'), data.get('pose'), step_count, mode_str=mode_line)

    # ---- Ensure Agg canvas and explicit draw before grabbing buffer ----
    fig.set_canvas(FigureCanvas(fig))
    fig.canvas.draw()

    s_bytes, (width, height) = fig.canvas.print_to_buffer()
    if not s_bytes:
        print("[RECORDER] WARNING: print_to_buffer() returned empty bytes")
    buf = np.frombuffer(s_bytes, np.uint8).reshape((height, width, 4))[:, :, :3]
    plt.close(fig)
    return buf



# --- Adapter that mirrors your old record_data() but for unified runner ----
def safe_get_memory_map_data_from(planner_like: Any) -> Dict[str, Any]:
    """
    Tries several ways to fetch memory_map_data with fields expected by plot_memory_map.
    Falls back to {'current_exp_id': -1} if we can’t access the map yet.
    """
    # 1) Dedicated method present?
    if hasattr(planner_like, "get_memory_map_data"):
        try:
            return planner_like.get_memory_map_data(dbg=False)  # type: ignore[arg-type]
        except Exception:
            pass

    # 2) Try to reconstruct from planner.emap / planner.cog (best-effort)
    try:
        mg = getattr(planner_like, "cog", None).mg if hasattr(planner_like, "cog") else None
        if mg is None:
            mg = getattr(planner_like, "emap", None)  # if you exposed it
        if mg is None:
            return {'exps_GP': [], 'exps_decay': [], 'ghost_exps_GP': [],
                    'ghost_exps_link': [], 'exps_links': [], 'current_exp_id': -1}
        emap = mg.experience_map
        data = {'exps_GP': [], 'exps_decay': [], 'ghost_exps_GP': [],
                'ghost_exps_link': [], 'exps_links': []}
        data['current_exp_id'] = mg.get_current_exp_id()
        data['current_GP']     = mg.get_global_position()
        if data['current_exp_id'] < 0:
            return data
        data['current_exp_GP'] = mg.get_exp_global_position()
        for vc in mg.view_cells.cells:
            for exp in vc.exps:
                data['exps_GP'].append([exp.x_m, exp.y_m])
                data['exps_decay'].append(vc.decay)
        for ghost in emap.ghost_exps:
            data['ghost_exps_GP'].append([ghost.x_m, ghost.y_m])
            for link in ghost.links:
                data['ghost_exps_link'] += [[ghost.x_m, ghost.y_m], [link.target.x_m, link.target.y_m]]
        for exp in emap.exps:
            for link in exp.links:
                if not getattr(link.target, 'ghost_exp', False):
                    data['exps_links'] += [[link.target.x_m, link.target.y_m], [exp.x_m, exp.y_m]]
        # dedup
        clean, seen = [], set()
        pts = data['exps_links']
        for i in range(0, len(pts) // 2 * 2, 2):
            p0, p1 = tuple(pts[i]), tuple(pts[i+1])
            if (p0, p1) not in seen:
                seen.add((p0, p1))
                clean += [list(p0), list(p1)]
        data['exps_links'] = clean
        return data
    except Exception:
        return {'exps_GP': [], 'exps_decay': [], 'ghost_exps_GP': [],
                'ghost_exps_link': [], 'exps_links': [], 'current_exp_id': -1}


def append_vis_frame_unified(*,
                             planner_like: Any,
                             video_gridmap: VideoGridmap,
                             data: Dict[str, Any],
                             env_definition: Dict[str, Any],
                             visited_rooms: List[Tuple[int, int]],
                             step_count: int,
                             seconds_per_step: float | None = None) -> None:
    """
    Builds memory_map_data and pushes a frame to the writer.
    If seconds_per_step is provided, duplicates the frame to hold on screen.
    """
    memory_map_data = safe_get_memory_map_data_from(planner_like)
    frame = record_video_frames(
        data, env_definition, agent_lost=False,
        visited_rooms=visited_rooms, memory_map_data=memory_map_data,
        step_count=step_count
    )

    # hold each step for N frames (CFR ensures duration = N / fps seconds)
    if seconds_per_step and video_gridmap.fps > 0:
        repeats = max(1, int(round(video_gridmap.fps * float(seconds_per_step))))
    else:
        repeats = 1

    for _ in range(repeats):
        video_gridmap.append_data(frame)


def compute_mse(a: np.ndarray, b: np.ndarray) -> float:
    try:
        a = prepare_for_imshow(a).astype(np.float32) / 255.0
        b = prepare_for_imshow(b).astype(np.float32) / 255.0
        return float(np.mean((a - b) ** 2))
    except Exception:
        return 0.0


# (Optional) tiny smoke test when run directly
if __name__ == "__main__":
    # Creates a 2-frame demo MP4 in ./dbg/ just to verify dependencies
    os.makedirs("dbg", exist_ok=True)
    writer = VideoGridmap("dbg/_smoketest.mp4", fps=2)
    dummy_env_def = {"n_row": 2, "n_col": 3, "max_steps": 2}
    mem = {'current_exp_id': -1}
    for t in range(2):
        img = (np.random.rand(240, 480, 3) * 255).astype(np.uint8)
        data = {
            "env_image": img,
            "ground_truth_ob": img,
            "decoded": img[::-1],  # just to exercise the slots
            "mse": 0.2,
            "GP": [t, t, 0.0],
            "pose": [t, t, 0.0],
            "recommended_mode": "EXPLORE",
            "recommended_submode": "ego",
            "mode_confidence": 0.9,
            "submode_confidence": 0.6,
            "uncertainty": 0.3,
            "changepoint_mass": 0.1,
        }
        frame = record_video_frames(data, dummy_env_def, agent_lost=False,
                                    visited_rooms=[(0,0), (1,0)], memory_map_data=mem, step_count=t)
        writer.append_data(frame)
    writer.close()
    print("✓ smoketest video → dbg/_smoketest.mp4")

```

`hierarchical-nav/dreamer_mg/world_model_utils.py`:

```py
# dreamer_mg/world_model_utils.py
# ----------------------------------------------------------------------
#  ✧  World-model utilities for MiniGrid collision prediction & planning
# ----------------------------------------------------------------------
import importlib, sys, pathlib
from dataclasses import dataclass, field
from typing import List, Optional, Tuple, Dict, Any
from importlib import util
# or
from importlib.util import spec_from_file_location, module_from_spec

# point "dreamer" to dreamer_mg.dreamer
pkg_path = pathlib.Path(__file__).parent / "dreamer"
spec = importlib.util.spec_from_file_location("dreamer", pkg_path / "__init__.py")
dreamer_pkg = importlib.util.module_from_spec(spec)
spec.loader.exec_module(dreamer_pkg)
sys.modules["dreamer"] = dreamer_pkg
import yaml, torch, pathlib, heapq, random, numpy as np
from collections import namedtuple
from types import SimpleNamespace  
from dreamer.modules.encoder import Encoder
from dreamer.modules.model   import RSSM
from dreamer.modules.decoder import Decoder 
from dreamer.modules.model import RewardModel
from attrdict import AttrDict 
import gym_minigrid
from gym_minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper
from gym.wrappers import ResizeObservation
from dreamer.envs.wrappers import ChannelFirstEnv
import matplotlib.pyplot as plt
import torch.nn.functional as F 
import textwrap, pprint, itertools
import math
from PIL import Image, ImageDraw, ImageFont   # Pillow
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import torchvision.utils as vutils
import torchvision.transforms.functional as TF 
from torchvision.utils import save_image
from pathlib import Path
from itertools import islice
from torchvision.utils import save_image
 # --- Cognitive graph harness ---------------------------------------------------
from dataclasses import dataclass
from typing import Optional, List, Tuple, Any, Dict
import numpy as np
import sys as _sys


_REPO_ROOT = str(Path(__file__).resolve().parents[1])  # one level up from dreamer_mg/
if _REPO_ROOT not in _sys.path:
    _sys.path.insert(0, _REPO_ROOT)

# Optional deps used in the template encoder; all guarded.
try:
    import cv2  # for resize / gradients if available
except Exception:
    cv2 = None
try:
    import yaml
except Exception:
    yaml = None

# Robust imports for your MemoryGraph + config loader
MemoryGraph = None
setup_memory_config = None
_import_errs = []
for modpath, name in [
    ("navigation_model.Services.memory_service.memory_graph", "MemoryGraph"),
    ("navigation_model.Services.memory_service.memory_graph.memory_graph", "MemoryGraph"),
    ("navigation_model.Services.memory_service", "memory_graph"),
]:
    try:
        mod = __import__(modpath, fromlist=[name])
        MemoryGraph = getattr(mod, name) if hasattr(mod, name) else getattr(mod, "MemoryGraph")
        break
    except Exception as e:
        _import_errs.append((modpath, str(e)))

if setup_memory_config is None:
    for modpath in [
        "control_eval.input_output",
        "navigation_model.control_eval.input_output",
        "control_eval.io",
    ]:
        try:
            mod = __import__(modpath, fromlist=["setup_memory_config"])
            setup_memory_config = getattr(mod, "setup_memory_config")
            break
        except Exception as e:
            _import_errs.append((modpath, str(e)))

@dataclass
class CognitiveGraphHarness:
    """
    Minimal helper around MemoryGraph so the planner can update/read the 'cognitive graph'
    without importing your old Manager.
    """
    cfg_path: Optional[str] = None
    cfg_obj: Optional[Dict[str, Any]] = None
    mg: Any = None  # MemoryGraph instance
    _use_manager_exact_template: bool = False  # if True, plug your original rgb56_to_template64

    def __post_init__(self):
        if MemoryGraph is None:
            raise ImportError(f"Could not import MemoryGraph. Tried: {_import_errs}")
        if self.cfg_obj is None:
            if setup_memory_config is not None and self.cfg_path:
                self.cfg_obj = setup_memory_config(self.cfg_path)
            else:
                if not (self.cfg_path and yaml):
                    raise ValueError("Provide cfg_path (and have PyYAML available) or pass cfg_obj directly.")
                with open(self.cfg_path, "r") as f:
                    self.cfg_obj = yaml.safe_load(f)
        print("THIS IS THE CFG OBJ",self.cfg_obj)
        self.mg = MemoryGraph(**self.cfg_obj)

    # ---------- image → 64D template ----------
    def _to_template64(self, image_rgb: np.ndarray) -> "np.ndarray | torch.Tensor":
        """
        Safe, dependency-light 64D descriptor:
        - resize to 56x56
        - grayscale
        - average-pool to 8x8 → 64 dims
        Replace this with your exact rgb56_to_template64 for full fidelity.
        """
        arr = np.asarray(image_rgb)
        if arr.ndim == 3 and arr.shape[-1] == 3:
            if cv2 is not None:
                img = cv2.resize(arr, (56, 56), interpolation=cv2.INTER_AREA)
                gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY).astype(np.float32)
            else:
                # naive fallback
                from math import floor
                # quick nearest-neighbor shrink to 56 if needed
                if arr.shape[0] != 56 or arr.shape[1] != 56:
                    # crude resize with slicing
                    step0 = max(1, arr.shape[0] // 56)
                    step1 = max(1, arr.shape[1] // 56)
                    img = arr[::step0, ::step1, :]
                    img = img[:56, :56, :]
                else:
                    img = arr
                gray = (0.2989*img[...,0] + 0.5870*img[...,1] + 0.1140*img[...,2]).astype(np.float32)
            # 8x8 average pooling
            pool = gray.reshape(8, 7, 8, 7).mean(axis=(1,3))  # (8,8)
            vec64 = (pool / (pool.sum() + 1e-8)).reshape(-1).astype(np.float32)
            return vec64
        raise ValueError("Expected HxWx3 RGB image")

    # ---------- public API ----------
    def digest(
        self,
        image_rgb: np.ndarray,
        action_onehot: np.ndarray,        # [F,R,L] one-hot or similar
        pose_xyz: Tuple[int, int, int],   # (x,y,th) with th∈{0,1,2,3}
        *,
        place_post: Optional["np.ndarray | Any"] = None,  # optional (allocentric posterior)
        place_std: Optional[float] = None,
        std_th: Optional[float] = None,
        agent_lost: bool = False,
    ):
        obs = {
            "image": self._to_template64(image_rgb),
            "HEaction": np.asarray(action_onehot, dtype=np.float32),
            "pose": np.asarray(pose_xyz, dtype=np.int32),
            "place": None,
        }
        if not agent_lost and place_post is not None and (place_std is None or (std_th is not None and place_std < std_th)):
            # Mirror your manager: mean over posterior samples along dim 0
            try:
                import torch
                if isinstance(place_post, torch.Tensor):
                    obs["place"] = place_post.mean(dim=0).squeeze(0)
                else:
                    # assume numpy: mean over axis 0
                    obs["place"] = np.mean(place_post, axis=0)
            except Exception:
                obs["place"] = None
        else:
            # reset 'door memories' if lost (no-op if your MemoryGraph ignores this)
            if hasattr(self.mg, "memorise_poses"):
                try:
                    self.mg.memorise_poses([])
                except Exception:
                    pass
        self.mg.digest(obs, dt=1, adjust_map=False)

    def nodes(self) -> List[Dict[str, Any]]:
        """Return a lightweight summary of experience nodes & links."""
        out = []
        emap = getattr(self.mg, "experience_map", None)
        if emap is None:
            return out
        exps = getattr(emap, "exps", [])
        for e in exps:
            # try to be permissive about field names
            x = getattr(e, "x_m", getattr(e, "x", 0.0))
            y = getattr(e, "y_m", getattr(e, "y", 0.0))
            eid = getattr(e, "id", None)
            links = []
            for lk in getattr(e, "links", []):
                tid = getattr(getattr(lk, "target", None), "id", None)
                if tid is not None:
                    links.append(tid)
            out.append({"id": eid, "x": float(x), "y": float(y), "links": links})
        return out

    def node_positions(self) -> List[Tuple[float, float]]:
        return [(n["x"], n["y"]) for n in self.nodes()]


# ----------------------------------------------------------------------
#  ✧ DualPathCollage: build two separate collages (REAL vs IMAGINED)
#     Call .add_step(...) every step; call .finalize() at the end.
# ----------------------------------------------------------------------
from dataclasses import dataclass, field
from typing import Optional, List, Tuple, Any
import math
import numpy as np

try:
    import torch
    import torchvision.transforms.functional as TF
    import torchvision.utils as vutils
except Exception as _e:
    torch = None
    TF = None
    vutils = None

@dataclass
class DualPathCollage:
    out_dir: str = "dbg/collages"
    tag: str = "run"
    tile: int = 64                   # tile edge in pixels (images are resized to tile x tile)
    cols: int = 20                   # columns in the collage grid
    live_write: bool = False         # if True, re-save collages after every add_step
    live_every: int = 1              # save every N steps when live_write=True
    pad: int = 1                     # padding between tiles (in pixels)
    pad_value: float = 1.0           # 1.0 -> white padding
    annotate_idx: bool = False       # draw step indices on each tile (requires PIL)
    _reals: List[Any] = field(default_factory=list, init=False, repr=False)
    _imags: List[Any] = field(default_factory=list, init=False, repr=False)
    _step: int = field(default=0, init=False, repr=False)

    # Optional hooks if you want the class to decode for you
    planner: Any = None
    wm: Any = None

    def _ensure_deps(self):
        if torch is None or vutils is None or TF is None:
            raise RuntimeError(
                "DualPathCollage requires torch/torchvision. "
                "Please ensure they are available in this environment."
            )

    @staticmethod
    def _to_chw01(x) -> "torch.Tensor":
        """Accept np.uint8 HWC, torch CHW or HWC, ranges [0,1] or [-.5,.5] → CHW float in [0,1]."""
        # torch already?
        if 'torch' in str(type(x)):
            t = x.detach().float().cpu()
            if t.ndim == 4 and t.shape[0] == 1:
                t = t[0]
            # HWC → CHW
            if t.ndim == 3 and t.shape[-1] in (1,3) and t.shape[0] not in (1,3):
                t = t.permute(2,0,1).contiguous()
            # single-channel → 3-channel
            if t.ndim == 3 and t.shape[0] == 1:
                t = t.repeat(3,1,1)
            # normalize to [0,1]
            if t.min() < 0.0 or t.max() <= 1.0 and t.min() >= -0.6:
                # assume Dreamer-style [-.5,.5] or [0,1]
                t = (t + 0.5).clamp(0.0, 1.0)
            else:
                t = t.clamp(0.0, 1.0)
            return t

        # numpy path
        if isinstance(x, np.ndarray):
            arr = x
            if arr.ndim == 3 and arr.dtype == np.uint8:
                # HWC uint8 [0..255]
                t = torch.as_tensor(arr).permute(2,0,1).float().cpu() / 255.0
            elif arr.ndim == 3:  # float?
                t = torch.as_tensor(arr).permute(2,0,1).float().cpu()
                if t.min() < 0.0 or (t.max() <= 1.0 and t.min() >= -0.6):
                    t = (t + 0.5).clamp(0.0, 1.0)
                else:
                    t = t.clamp(0.0, 1.0)
            else:
                raise ValueError(f"Unsupported numpy array shape: {arr.shape}")
            if t.shape[0] == 1:
                t = t.repeat(3,1,1)
            return t

        raise ValueError(f"Unsupported image type: {type(x)}")

    @staticmethod
    def _resize_to_tile(chw01: "torch.Tensor", tile: int) -> "torch.Tensor":
        """Resize CHW [0,1] to tile×tile using nearest to keep MiniGrid pixels crisp."""
        if TF is None:
            return chw01
        return TF.resize(chw01, [tile, tile], interpolation=TF.InterpolationMode.NEAREST)

    def _maybe_annotate(self, chw01: "torch.Tensor", idx: int) -> "torch.Tensor":
        if not self.annotate_idx:
            return chw01
        try:
            from PIL import Image, ImageDraw, ImageFont
            pil = TF.to_pil_image(chw01)
            draw = ImageDraw.Draw(pil)
            # tiny label in the corner; default font to avoid platform issues
            draw.rectangle([0,0,16,12], fill=(255,255,255))
            draw.text((2,0), str(idx), fill=(0,0,0))
            return TF.to_tensor(pil)
        except Exception:
            return chw01

    def add_step(
        self,
        real_img: Any = None,
        imagined_img: Any = None,
        belief_zd: Optional[Tuple[Any,Any]] = None
    ):
        """
        Add one step to the collages.
        - Pass `real_img` from env (e.g., obs["image"])
        - EITHER pass `imagined_img`, OR pass `belief_zd` and set planner/wm in the constructor.
        """
        self._ensure_deps()
        self._step += 1

        if real_img is None:
            raise ValueError("add_step requires real_img")

        # Prepare REAL
        r = self._to_chw01(real_img)
        r = self._resize_to_tile(r, self.tile)
        r = self._maybe_annotate(r, self._step)
        self._reals.append(r)

        # Prepare IMAGINED
        im = imagined_img
        if im is None and belief_zd is not None and self.planner is not None and self.wm is not None:
            # Reuse your existing utility if available in this module:
            try:
                # dreamer_decode_from_belief is defined elsewhere in this file
                im = dreamer_decode_from_belief(self.planner, self.wm, belief_zd, to_01=True)
            except Exception:
                im = None
        if im is None:
            # Fallback: if we can't decode, mirror the real image so can still visualize layout
            im = r
        else:
            im = self._to_chw01(im)
            im = self._resize_to_tile(im, self.tile)
            im = self._maybe_annotate(im, self._step)
        self._imags.append(im)

        if self.live_write and (self._step % self.live_every == 0):
            self._save_collage(pair=("real", "imagined"))

    def _save_collage(self, pair=("real","imagined")):
        """Save current collages to out_dir as two separate PNGs."""
        import os
        from pathlib import Path
        Path(self.out_dir).mkdir(parents=True, exist_ok=True)

        def write_one(stack_list: List["torch.Tensor"], suffix: str):
            if not stack_list:
                return
            grid = vutils.make_grid(
                torch.stack(stack_list, dim=0),
                nrow=self.cols,
                padding=self.pad,
                pad_value=self.pad_value
            )
            out_path = os.path.join(self.out_dir, f"{self.tag}_{suffix}.png")
            vutils.save_image(grid, out_path)
            return out_path

        real_path = write_one(self._reals, "real")
        imag_path = write_one(self._imags, "imagined")
        return real_path, imag_path

    def finalize(self):
        """Write the final collages for REAL and IMAGINED."""
        return self._save_collage(pair=("real","imagined"))


# ─────────────────────────────── constants ────────────────────────────
State    = namedtuple("State", ["x", "y", "d"])          # planner state
DIR_VECS = [(1,0), (0,1), (-1,0), (0,-1)]                # 0:right 1:down …

DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"

class WMPlanner:
    """High-level wrapper: loading, belief update, collision check, A* and render."""
    # ──────────────────────────────────────────────────────────────
    def __init__(self, ckpt: str, cfg: str | None = None, device="cpu",
                 memory_config: str | None = None, memory_cfg_obj: Dict[str, Any] | None = None):
        self.wm = self.load_world_model2(ckpt_path=ckpt)
        self.wm.device = device
        self.device = device
        self.cog: Optional[CognitiveGraphHarness] = None
        self.env=None
        if memory_config or memory_cfg_obj:
            self.cog = CognitiveGraphHarness(cfg_path=memory_config, cfg_obj=memory_cfg_obj)
        
        emap = self.cog.mg.experience_map
        self.emap=emap
    def load_world_model2(self,ckpt_path: str,
                        config_yaml: str = "/Users/lab25/hierarchical-nav/dreamer_mg/runs/mg_collision/20250704-220917/config.yml"):
        """
        Returns an object with .encoder and .rssm that exactly match *any*
        Dreamer checkpoint – no manual YAML tweaks required.

        • Reads sizes directly from the ckpt:
            in_dim  = W.shape[1]  of recurrent_model.linear.weight
            stoch   = (W₂.shape[0]) // 2      (# outputs / 2)
            act_sz  = in_dim - stoch
        • Injects the discovered numbers into a *copy* of the yaml dict.
        """
        import copy, yaml, torch
        ckpt = torch.load(ckpt_path, map_location=DEVICE)

            # --- 1. discover sizes ------------------------------------------------
        Wrec   = ckpt["modules"]["rssm"]["recurrent_model.linear.weight"]
        H3, in_dim = Wrec.shape                 # (3H, input)
        hidden = H3 // 3                        # true deterministic_size
        Wtrans = ckpt["modules"]["rssm"]["transition_model.network.2.weight"]
        stoch  = Wtrans.shape[0] // 2
        act_size= in_dim - stoch

        # --- 2. patch the yaml ------------------------------------------------
        cfg = AttrDict(yaml.safe_load(open(config_yaml)))
        cfg.parameters.dreamer.deterministic_size = hidden
        cfg.parameters.dreamer.stochastic_size    = stoch
        #cfg['parameters']['dreamer']['deterministic_size'] = H
        #cfg['parameters']['dreamer']['stochastic_size']    = stoch_sz

        enc  = Encoder((3, 64, 64), cfg).to(DEVICE)
        dec = Decoder((3, 64, 64), cfg).to(DEVICE)
        rssm = RSSM(action_size=act_size, config=cfg).to(DEVICE)
        rew  = RewardModel(config=cfg).to(DEVICE)

        enc .load_state_dict(ckpt["modules"]["encoder"]);  enc .eval()
        rssm.load_state_dict(ckpt["modules"]["rssm"   ]);  rssm.eval()
        dec.load_state_dict(ckpt["modules"]["decoder"]); dec.eval()
        rew.load_state_dict(ckpt["modules"]["reward"]); rew.eval()

        wm = lambda: None
        wm.encoder = enc
        wm.rssm    = rssm
        wm.decoder = dec
        wm.action_size = act_size                        # handy later
        wm.reward_predictor = rew
        wm.device=DEVICE
        if act_size == 3:                               # NEW model
            wm.idx = {"left": 0, "right": 1, "forward": 2}
        elif act_size == 7:                             # old checkpoints
            wm.idx = {
                "left":0,"right":1,"forward":2,
                "pickup":3,"drop":4,"toggle":5,"done":6}
        else:
            raise ValueError(f"Unexpected action dim {act_size}")
        print("rssm expects action dim:", wm.action_size)
        return wm
    def onehot(self,a_name: str,wm):
        if a_name not in wm.idx:
            raise KeyError(f"action '{a_name}' not in mapping {wm.idx}")
        v = torch.zeros(wm.action_size, device=DEVICE)
        v[wm.idx[a_name]] = 1.0
        return v

    # ══════════════════════════════════════════════════════════════════════
    # 2.  COLLISION PREDICTION  (wm_predict_collision)
    # ══════════════════════════════════════════════════════════════════════
    @torch.no_grad()
    @torch.no_grad()
    def wm_update_belief(self,wm,
                        prev_z_d: tuple[torch.Tensor, torch.Tensor],
                        frame_rgb: np.ndarray,
                        prev_action_onehot: torch.Tensor | None):
        """
        Update (z,d) belief given *one* observation & the *previous* action.

        Parameters
        ----------
        prev_z_d          tuple(z,d) from the previous step OR  None on first step
        frame_rgb         np.ndarray  (3,64,64)   current observation
        prev_action_onehot  torch.Tensor shape (1,3)  one-hot of action_t-1
                            →  pass None right after reset()

        Returns
        -------
        z_t, d_t  :  the new posterior latent tensors (no grads)
        """
        if prev_z_d is None:                               # first frame
            # dummy recurrent input (batch=1)
            prior, det = wm.rssm.recurrent_model_input_init(1)
        else:
            prior, det = prev_z_d

        # recurrent model if we have an *action that already happened*
        if prior is not None and prev_action_onehot is not None:
            det = wm.rssm.recurrent_model(
                prior,
                prev_action_onehot.unsqueeze(0),  # (1, action_size)
                det)
        prior_dist, prior = wm.rssm.transition_model(det)
        # encode observation & run representation model
        img = torch.tensor(frame_rgb, dtype=torch.float32,
                        device=DEVICE).unsqueeze(0)
        emb = wm.encoder(img).view(1, -1)
        _, post = wm.rssm.representation_model(emb, det)   # posterior 

        return post.detach(), det.detach()
   

    def _save_rollout_grid(self,frames, rewards, tag, upscale=256):
        """
        frames  : list[Tensor] each 3×64×64, values in [-.5,.5] or [0,1]
        rewards : list[float]  same length (use float('nan') for “no reward”)
        tag     : str          filename suffix, e.g. '00', '01', ...
        """

        assert len(frames) == len(rewards), "frames and rewards length mismatch"
        Path("dbg").mkdir(exist_ok=True)

        pil_frames = []
        for fr in frames:
            # (3,64,64) → (64,64,3) uint8 in [0,255]
            fr = fr.clone().cpu()
            if fr.min() < 0:
                fr = fr + 0.5
            fr = (fr.clamp(0, 1) * 255).byte().permute(1, 2, 0).numpy()
            img = Image.fromarray(fr)
            img = img.resize((upscale, upscale), resample=Image.NEAREST)

            pil_frames.append(img)

        # build a wide canvas: W = T*upscale, H = upscale+20
        T = len(pil_frames)
        canvas = Image.new("RGB", (T * upscale, upscale + 20), color="white")

        # optional: nicer mono-space font; fall back to default if not found
        try:
            font = ImageFont.truetype("DejaVuSansMono.ttf", size=14)
        except IOError:
            font = ImageFont.load_default()

        draw = ImageDraw.Draw(canvas)
        for i, (img, r) in enumerate(zip(pil_frames, rewards)):
            # paste frame
            canvas.paste(img, (i * upscale, 0))

            # reward text centred under the frame
            txt = "nan" if np.isnan(r) else f"{r:+.2f}"
            if hasattr(draw, "textbbox"):                 # Pillow ≥ 10
                x0, y0, x1, y1 = draw.textbbox((0, 0), txt, font=font)
                w, h = x1 - x0, y1 - y0
            else:                                         # Pillow < 10
                w, h = draw.textsize(txt, font=font)
            x = i * upscale + (upscale - w) // 2
            y = upscale + (20 - h) // 2
            draw.text((x, y), txt, fill="black", font=font)

        fname = f"dbg/coll_chk_{tag}.png"
        canvas.save(fname)
        print(f"saved {fname}")
    @torch.no_grad()
    
    def save_front_patches(self,frame: torch.Tensor,
                        tag: str = "step",
                        agent_tile=(3, 6),
                        core_fraction: float = 2/3):
        """
        frame : 3×H×W tensor in [-.5,.5] or [0,1]
        Saves three PNGs under dbg/:
            dbg/<tag>_frame.png          – full decoded frame
            dbg/<tag>_patch_full.png     – 9×9 (or 18×18) full tile
            dbg/<tag>_patch_core.png     – centred sub-patch of that tile
        """
        Path("dbg").mkdir(exist_ok=True, parents=True)

        # put frame in [0,1] for saving
        vis = frame.detach().clone()
        if vis.min() < 0: vis += 0.5
        vis = vis.clamp(0, 1)

        # full 9×9 tile
        full_patch = self._front_patch(vis, agent_tile=agent_tile, core_fraction=1.0)

        # centre crop
        core_patch = self._front_patch(vis, agent_tile=agent_tile,
                                core_fraction=core_fraction)
        p = core_patch.detach().cpu().clamp(0,1)
        print("mean RGB :", p.mean(dim=(1,2)))
        print("variance :", p.var())
        up = vis.shape[-1]                 # 64 (or 128, …)
        full_up = F.interpolate(full_patch.unsqueeze(0), size=vis.shape[-1],
                                mode="nearest").squeeze(0)
        core_up = F.interpolate(core_patch.unsqueeze(0), size=vis.shape[-1],
                                mode="nearest").squeeze(0)

        imgs = torch.stack([vis, full_up, core_up], dim=0)   # (3, 3, 64, 64)

        save_image(imgs,
                f"dbg/{tag}_triplet.png",
                nrow=3,          # one row: frame | full | core
                normalize=True, scale_each=False)
        print(f"saved dbg/{tag}_triplet.png")
        print("bright :", (core_patch.mean()).item())
        print("sat    :", (core_patch[0]-core_patch[1]).abs().max().item())
        print("var    :", core_patch.var().item())
        save_image(vis,         f"dbg/{tag}_frame.png",  normalize=True)
        save_image(full_patch,  f"dbg/{tag}_patch_full.png", normalize=True)
        save_image(core_patch,  f"dbg/{tag}_patch_core.png", normalize=True)

        print(f"saved dbg/{tag}_frame.png, ..._patch_full.png, ..._patch_core.png")
    
    def _front_patch(self,frame: torch.Tensor,
                    agent_tile=(3, 6),
                    core_fraction: float = 2/3):
        H   = frame.shape[-1]            # assume square
        tile = H // 7                    # 9 for 64×64, 18 for 128×128 …
        margin = (H - tile * 7) // 2     # 1-px left/top margin at 64×64
        fx, fy = agent_tile[0], agent_tile[1] - 1   # tile straight ahead

        # full front-tile coordinates
        x0 = margin + fx * tile
        y0 = margin + fy * tile
        full_tile = frame[:, y0:y0+tile, x0:x0+tile]   # 3×tile×tile

        if not (0 < core_fraction < 1):
            return full_tile                           # return whole tile

        # size of the centred crop
        c = int(round(tile * core_fraction))           # e.g. 6
        core_tile = TF.center_crop(full_tile, (c, c)) 
        return core_tile

    def _is_wall_patch(self,patch: torch.Tensor,
                    bright_min: float = 0.35,
                    sat_max:   float = 0.04,
                    frac_min:  float = 0.60) -> bool:
        """
        Treat the patch as a wall if at least `frac_min` of its pixels are
        both bright AND grey (low saturation).

        bright := (R+G+B)/3 > bright_min
        grey   := max(|R-G|,|G-B|,|B-R|) < sat_max
        """
        p = patch.detach().cpu().clamp(0, 1)              # 3×T×T  in [0,1]
        R, G, B = p                                       # unpack channels
        brightness = (R + G + B) / 3                      # T×T
        saturation = torch.max(torch.stack([
                            (R-G).abs(),
                            (G-B).abs(),
                            (B-R).abs()]), dim=0).values  # T×T

        mask = (brightness > bright_min) & (saturation < sat_max)
        frac = mask.float().mean().item()                 # fraction of “wall-like” px
        return frac >= frac_min

    

    def save_tiled_overlay(self,frame: torch.Tensor,
                        tag: str = "latest",
                        upscale: int = 256):
        """
        frame  : 3×H×W tensor in [-.5,.5] or [0,1]
        Writes dbg/overlay_<tag>.png  +  overlay_<tag>.npy (7×7×3 RGB means)
        """
        Path("dbg").mkdir(exist_ok=True, parents=True)
        png_name = f"dbg/overlay_{tag}.png"
        npy_name = f"dbg/overlay_{tag}.npy"

        H = frame.shape[-1]                       # assume square
        tile = H // 7
        margin = 3

        # move to [0,1] and PIL
        img = frame.detach().clone()
        if img.min() < 0: img = img + 0.5
        img = img.clamp(0,1)
        pil = vutils.make_grid(img.unsqueeze(0)).permute(1,2,0).numpy()*255
        pil = Image.fromarray(pil.astype("uint8"))
        pil = pil.resize((upscale,upscale), resample=Image.NEAREST)
        draw = ImageDraw.Draw(pil)
        font = ImageFont.load_default()

        rgb_means = np.zeros((7,7,3), dtype=np.float32)

        # draw tiles
        up_tile = upscale // 7
        for y in range(7):
            for x in range(7):
                px0 = margin + x*tile
                py0 = margin + y*tile
                patch = img[:, py0:py0+tile, px0:px0+tile]
                rgb   = patch.mean(dim=(1,2)).cpu().numpy()
                rgb_means[y,x] = rgb

                # grid on big image
                gx0, gy0 = x*up_tile, y*up_tile
                draw.rectangle([gx0, gy0, gx0+up_tile, gy0+up_tile],
                            outline="white", width=1)

                # two-line label
                txt1 = f"{x},{y}"
                txt2 = f"{rgb[0]:+.2f},{rgb[1]:.2f},{rgb[2]:.2f}"
                draw.text((gx0+2, gy0+2), txt1, fill="yellow", font=font)
                draw.text((gx0+2, gy0+12), txt2, fill="white",  font=font)

        pil.save(png_name)
        np.save(npy_name, rgb_means)
        print(f"saved {png_name}  &  {npy_name}")

        # console table for quick glance
        for y in range(7):
            row = " | ".join(f"{rgb_means[y,x,0]:.2f},{rgb_means[y,x,1]:.2f},{rgb_means[y,x,2]:.2f}"
                            for x in range(7))
            print(f"y={y}  {row}")
    def wm_collision_by_decoder(
            self,
            wm,
            belief_zd: tuple[torch.Tensor, torch.Tensor],
            seq: list[str],
            wall_gray_thresh: float = 0.65,
            num_rollouts: int = 2,
            debug: bool = False,
    ) -> bool:
        """
        Look at the decoded image after each imagined action and declare UNSAFE
        if the front-tile patch is brighter than `wall_gray_thresh`.
        """
        z0, d0 = (x.detach() for x in belief_zd)
        idx  = torch.tensor([wm.idx[a] for a in seq], device=wm.device)
        oneh = F.one_hot(idx, num_classes=wm.action_size).float()
        unsafe = False
        # static attribute lives across calls
        if not hasattr(self.wm_collision_by_decoder, "_wall_id"):
            self.wm_collision_by_decoder._wall_id = 0

        for r in range(num_rollouts):
            z, d = z0.clone(), d0.clone()

            if debug:
                frames_dbg, wall_dbg = [], []

            for t, a in enumerate(oneh, 1):
                d = wm.rssm.recurrent_model(z, a.unsqueeze(0), d)
                _, z = wm.rssm.transition_model(d)
                frame = wm.decoder(z, d).mean.squeeze(0)        # 3×64×64, in [-.5,.5]
                vis = (frame + 0.5).clamp(0, 1)                 # → [0,1]
                patch = self._front_patch(vis)                       # 3×tile×tile
                #save_front_patches(frame, tag="step011")
                #save_tiled_overlay(vis, tag="_".join(seq))
                
                is_wall = self._is_wall_patch(patch)

                if debug:
                    frames_dbg.append(vis.cpu())
                    wall_dbg.append(is_wall)

                if is_wall:
                    wall_tag = f"wall_{self.wm_collision_by_decoder._wall_id:05d}"
                    self.save_front_patches(frame, tag=wall_tag)   # writes triplet
                    self.wm_collision_by_decoder._wall_id += 1
                    unsafe = True
                    break

            if debug:
                self._save_rollout_grid(frames_dbg,
                                [1.0 if w else 0.0 for w in wall_dbg],
                                tag=f"pix{self.wm_collision_by_decoder._counter:05d}")
                self.wm_collision_by_decoder._counter += 1

            if unsafe:
                break

        return unsafe

    # initialise the static counter
    wm_collision_by_decoder._counter = 0


    def is_wall_ahead_now(self,wm, belief_zd, debug=False, tag=None) -> bool:
        """
        Decode the *current* latent, grab the front core patch, and test it.
        No rollout at all.
        """
        z, d = belief_zd
        frame = wm.decoder(z, d).mean.squeeze(0)   # [0,1]
        vis = (frame + 0.5).clamp(0, 1) 
        patch = self._front_patch(vis)                      # 6×6 by default
        wall  = self._is_wall_patch(patch)

        if debug and wall and tag is not None:
            self.save_front_patches(frame, tag=tag)           # optional snapshot
        return wall
    # ─────────────────────────────────────────────────────────────────────
    # Decision tree node to expose per-node metrics and children
    # ─────────────────────────────────────────────────────────────────────
        # ─────────────────────────────────────────────────────────────────────
    # Decision tree node to expose per-node metrics and children
    # ─────────────────────────────────────────────────────────────────────
    @dataclass
    class TreeNode:
        node_id: int
        depth: int
        pose: Tuple[int, int, int]                  # (x,y,d)
        seq: List[str]                              # actions from root to this node
        belief_zd: Optional[Tuple[torch.Tensor, torch.Tensor]] = None  # (z,h) DETACHED, on CPU
        # per-step metrics for edge (parent → this)
        step_act: Optional[str] = None
        step_novelty: float = 0.0
        step_penalty: float = 0.0
        step_p2e: float = 0.0
        step_graph: float = 0.0
        step_reward: float = 0.0                    # combined reward for that edge
        cum_gain: float = 0.0                       # discounted sum from root to this node
        children: List["WMPlanner.TreeNode"] = field(default_factory=list)
        is_leaf: bool = False
        p2e_leaf_value: Optional[float] = None      # cached K-step value at the node

    # ─────────────────────────────────────────────────────────────────────
    # Small helpers (latent stepping + self-disagreement)
    # ─────────────────────────────────────────────────────────────────────
    def rssm_step(self, wm, belief, act_name: str):
        """One Dreamer PRIOR step. Returns NEXT (z,h) with no grads."""
        import torch.nn.functional as F
        z, h = belief
        onehot = F.one_hot(
            torch.tensor([wm.idx[act_name]], device=wm.device),
            num_classes=wm.action_size
        ).float()
        with torch.no_grad():
            h_next = wm.rssm.recurrent_model(z, onehot, h)
            _, z_next = wm.rssm.transition_model(h_next)
        return (z_next.detach(), h_next.detach())
    def _prior_mean(self, dist) -> torch.Tensor:
        mu = getattr(dist, "mean", None)
        if mu is None:
            # last-resort: try loc (Normal(loc, scale))
            mu = getattr(dist, "loc", None)
        return torch.as_tensor(mu).float()

    def _pairwise_dispersion(self, vecs, metric: str = "l2") -> float:
        """Average pairwise distance across a list of 1D tensors (CPU)."""
        import torch
        V = [torch.as_tensor(v).view(-1).detach().cpu().float() for v in vecs]
        n = len(V)
        if n < 2:
            return 0.0
        total = 0.0
        cnt = 0
        for i in range(n):
            for j in range(i + 1, n):
                if metric == "cos":
                    a, b = V[i], V[j]
                    denom = (a.norm() * b.norm()).clamp_min(1e-8)
                    d = 1.0 - float((a @ b) / denom)
                else:  # "l2"
                    d = float((V[i] - V[j]).pow(2).mean().sqrt())
                total += d
                cnt += 1
        return total / cnt
    def _roll_same_action_collect_z(self, wm, belief, act_name: str, K: int):
        """Roll K steps applying the same action; return list of z (flattened)."""
        zs = []
        cur = belief
        for _ in range(max(0, K)):
            cur = self.rssm_step(wm, cur, act_name)
            z, _h = cur
            zs.append(z.view(-1).detach().cpu().float())
        return zs
    def _roll_same_action_collect_means(self, wm, belief, act_name: str, K: int) -> List[torch.Tensor]:
        """
        Deterministic rollout K steps applying the same action; return list of prior means.
        """
        means = []
        cur = belief
        for _ in range(K):
            (cur, _prior) = self.rssm_step(wm, cur, act_name)
            # Note: rssm_step already sampled z_next; we also want the *mean* of that prior:
            # re-compute prior on the new (z,h) to get its mean reproducibly:
            _, prior_dist = wm.rssm.transition_model(cur[1])  # prior over current h
            mu = self._prior_mean(prior_dist).view(-1).cpu()
            means.append(mu)
        return means

    def _mc_dispersion_same_action(self, wm, belief, act_name: str, K: int, S: int, metric: str) -> float:
        """
        Monte Carlo self-disagreement:
        - For each future step t=1..K, sample S latents from the PRIOR over z_t
        under repeated action 'act_name'. Measure dispersion across samples.
        - Return the average dispersion across t.
        """
        if K <= 0 or S <= 1:
            return 0.0
        cur = belief
        step_vals = []
        with torch.no_grad():
            for _ in range(K):
                # one recurrent step to get the *next* prior
                (next_belief, prior_dist) = self.rssm_step(wm, cur, act_name)
                # sample S latents from that prior
                samples = []
                for _s in range(S):
                    try:
                        z_s = prior_dist.rsample()  # (1, Z)
                    except Exception:
                        z_s = prior_dist.sample()
                    samples.append(z_s.view(-1).cpu().float())
                # dispersion across samples at current step
                step_vals.append(self._pairwise_dispersion(samples, metric=metric))
                cur = next_belief
        return float(np.mean(step_vals)) if step_vals else 0.0

    def p2e_leaf_value(
        self,
        wm,
        belief,
        act_name: str,
        *,
        K: int = 4,
        metric: str = "l2",
        roll_from: str = "parent"  # "parent": start at node belief; "child": start after 1 step
    ) -> float:
        """
        K-rollout self-disagreement for a single action from a node:
        - If roll_from="parent": start at the node's belief, apply 'act_name' K times.
        - If roll_from="child" : first step to the child, then apply 'act_name' K times.
        - Compare the K predicted z's to each other (pairwise dispersion).
        """
        if K <= 0:
            return 0.0
        start_belief = belief
        if roll_from == "child":
            start_belief = self.rssm_step(wm, belief, act_name)
        zs = self._roll_same_action_collect_z(wm, start_belief, act_name, K)
        return self._pairwise_dispersion(zs, metric=metric)

    def open_graph_bonus(self, predicted_pose, memory_graph=None) -> float:
        """
        Hook for cognitive-graph term. For now this is stubbed to 0.0.
        Later: e.g., +1 if pose would create a new node or reduce distance to frontier.
        """
        return 0.0


    # ══════════════════════════════════════════════════════════════════════
    # 3.  A*  PLANNER  (astar_prims)  –  uses wm_predict_collision
    # ══════════════════════════════════════════════════════════════════════
    def heuristic(self,s: State, g: State) -> int:
        manh = abs(s.x - g.x) + abs(s.y - g.y)
        turn = min((s.d - g.d) % 4, (g.d - s.d) % 4)
        return manh + turn
    def astar_prims(self,
                wm,
                belief_zd,                  # (z,h) (latent + RNN state) at the *start* frame
                start: State,
                goal:  State,
                max_actions: int = 50,      # hard budget
                num_rollouts: int = 8,
                verbose: bool = False,
                allow_partial: bool = False):
        import torch
        import torch.nn.functional as F
        import math, heapq

        def to_onehot_tensor(seq: list[str]) -> torch.Tensor:
            mapping = {'forward': [1, 0, 0],
                    'right'  : [0, 1, 0],
                    'left'   : [0, 0, 1]}
            # (T,3) on CPU; NavigationSystem is happy with CPU tensors
            return torch.tensor([mapping[a] for a in seq], dtype=torch.float32)

        # priority-queue items: (f, g, state, seq, belief)
        pq      = [(self.heuristic(start, goal), 0, start, [], belief_zd)]
        g_score = {start: 0}
        closed  = set()

        # nearest fallback bookkeeping (only used if allow_partial=True)
        best_dist = float("inf")
        best_seq  = []

        while pq:
            f, g, (x, y, d), seq, belief = heapq.heappop(pq)
            if (x, y, d) in closed:
                continue
            closed.add((x, y, d))

            # update nearest
            dist_xy = math.hypot(x - goal.x, y - goal.y)
            if dist_xy < best_dist:
                best_dist, best_seq = dist_xy, seq

            if verbose:
                print(f"[A*] pop {State(x,y,d)}  g={g}  f={f}  seq={seq}")

            # ───── success ─────────────────────────────────────────────
            if (x, y, d) == (goal.x, goal.y, goal.d):
                return to_onehot_tensor(seq)  # ✔ exact plan

            # budget check
            if g >= max_actions:
                continue

            # ───── expand successors ───────────────────────────────────
            for act in ("left", "right", "forward"):
                # pose after action
                if act == "forward":
                    dx, dy = DIR_VECS[d]
                    nx, ny, nd = x + dx, y + dy, d
                elif act == "left":
                    nx, ny, nd = x, y, (d - 1) % 4
                else:  # right
                    nx, ny, nd = x, y, (d + 1) % 4

                ns = State(nx, ny, nd)
                if ns in closed:
                    continue

                # belief one step ahead (no-grad)
                with torch.no_grad():
                    z, h = belief
                    onehot = F.one_hot(
                        torch.tensor([wm.idx[act]], device=wm.device),
                        num_classes=wm.action_size
                    ).float()  # (1,3)
                    h_next = wm.rssm.recurrent_model(z, onehot, h)
                    _, z_next = wm.rssm.transition_model(h_next)
                belief_next = (z_next.detach(), h_next.detach())

                # collision prune only for forward
                if act == "forward":
                    if hasattr(self, "is_wall_ahead_now") and self.is_wall_ahead_now(wm, belief, debug=False):
                        if verbose:
                            print("   prune (wall ahead now)", seq)
                        continue

                # push successor if still within action budget
                new_seq = seq + [act]
                g2 = g + 1
                if g2 > max_actions:
                    continue

                h2 = self.heuristic(ns, goal)
                f2 = g2 + h2
                if g2 < g_score.get(ns, float("inf")):
                    g_score[ns] = g2
                    heapq.heappush(pq, (f2, g2, ns, new_seq, belief_next))

        # ───── failure to reach goal ─────────────────────────────────────
        if allow_partial and best_seq:
            if verbose:
                print(f"[A*] no path; returning NEAREST (len={len(best_seq)}, dist={best_dist:.2f})")
            return to_onehot_tensor(best_seq)

        if verbose:
            print("[A*] no path; returning EMPTY")
        # Explicit (0,3) tensor → NavigationSystem sees numel()==0
        return torch.empty((0, 3), dtype=torch.float32)
    def render_plan(self,wm, belief_zd, actions, include_last=True):
        """
        Return list[Tensor] of decoded RGB frames.
            If include_last=True :  N actions → N+1 frames  (arrival included)
            If include_last=False:  N actions → N   frames  (default behaviour)
        """
        frames = []
        z, h = belief_zd
        with torch.no_grad():
            # 1. current state before the first action
            if include_last:                 # also useful as initial observation
                frame = wm.decoder(z, h).mean.squeeze(0) + 0.5
                frames.append(frame.cpu())

            for act in actions:
                # advance latent one step
                onehot = F.one_hot(torch.tensor([wm.idx[act]], device=wm.device),
                                num_classes=wm.action_size).float()
                h = wm.rssm.recurrent_model(z, onehot, h)
                _, z = wm.rssm.transition_model(h)

                frame = wm.decoder(z, h).mean.squeeze(0) + 0.5
                frames.append(frame.cpu())

        return frames
    def _prepare_frame(self,obs: dict, out_hw=(64, 64)):
        assert "image" in obs, "observation lacks 'image' key"
        img = obs["image"]                                   # H×W×3, uint8 or float
        if img.dtype != np.float32:                          # uint8 → float 0-1
            img = img.astype(np.float32) / 255.0
        if img.shape[:2] != out_hw:                          # resize if needed
            img = cv2.resize(img, out_hw[::-1], interpolation=cv2.INTER_AREA)
        img = img.transpose(2, 0, 1)                         # to CHW
        img = img - 0.5                                      # centre at 0
        return img                                           # (3,64,64) float32
    def update_belief_from_obs(self,obs_dict,belief_zd, prev_onehot):
            
        frame = self._prepare_frame(obs_dict)       # ★ (3,64,64) float
        return self.wm_update_belief(self.wm, belief_zd, frame, prev_onehot)
    def probe_decoder(self,wm, belief_zd, obs, action_plan, outdir="recon_demo"):
        outdir = Path(outdir); outdir.mkdir(exist_ok=True)
        # ---------- 1. current posterior ------------------------
        rgb = obs["image"].astype(np.float32).transpose(2,0,1)/255.0
        z, d = belief_zd            # posterior of current frame (already updated)
        recon0 = wm.decoder(z, d).mean.squeeze(0).cpu()

        # ---------- 2. imagine K steps (priors) -----------------
        zs, ds, frames = [z], [d], [recon0]
        onehots = torch.stack([onehot(a,wm) for a in action_plan]).unsqueeze(0)

        with torch.no_grad():
            for t in range(len(action_plan)):
                d = wm.rssm.recurrent_model(zs[-1], onehots[:,t], ds[-1])
                _, z = wm.rssm.transition_model(d)   # PRIOR
                zs.append(z);  ds.append(d)
                frames.append( wm.decoder(z, d).mean.squeeze(0).cpu() )

        # ---------- 3. build & save grid ------------------------
        frames = [fr.unsqueeze(0) for fr in frames]          #  → list of 1×3×64×64
        grid   = torch.cat([torch.tensor(rgb).unsqueeze(0)]  # ground-truth 1×3×64×64
                        + frames,
                        dim=0)          
        grid = F.interpolate(grid, size=256,
                                                mode="nearest")  # pixel-art
        fname = outdir / f"probe_{len(list(outdir.glob('probe_*.png'))):03d}.png"
        save_image(grid, fname, nrow=len(frames)+1, normalize=True)
        print("saved", fname)
        
    def save_decoder(self,belief_zd,obs):
                # --- grab one RGB frame from the env -------------------------------
            OUTDIR   = Path("recon_demo")         # ./recon_demo/…
            OUTDIR.mkdir(exist_ok=True)

            #z0,d0= wm.rssm.recurrent_model_input_init(1)                         # posterior (current belief)
            z0,d0=belief_zd
            # ---- choose any action pattern you want the model to fantasise ----
            plan      = ["forward","left", "right", "forward", "forward"]   # length = K
            onehots   = torch.stack([onehot(a, self.wm) for a in plan]).unsqueeze(0)
            zs, ds    = [z0], [d0]

            # ---- latent roll-out (PRIOR predictions!) -------------------------
            for t in range(len(plan)):
                d_next  = self.wm.rssm.recurrent_model(zs[-1], onehots[:, t], ds[-1])
                _, z_next = self.wm.rssm.transition_model(d_next)      # sample from p(zₜ₊₁)
                zs.append(z_next);  ds.append(d_next)

            # ---- decode all latents ------------------------------------------
            with torch.no_grad():
                recons = torch.stack([self.wm.decoder(z, d).mean.squeeze(0).cpu()
                                    for z, d in zip(zs, ds)])   # (K+1,3,64,64)

            # ---- build a pretty  grid  (1×truth  +  K+1×pred) -----------------
            truth = obs["image"].astype(np.float32).transpose(2,0,1)/255.0
            grid  = torch.cat([torch.tensor(truth).unsqueeze(0), recons], dim=0)

            # upscale to 256 px per tile for readability
            grid_big = F.interpolate(grid, size=256,
                                                    mode="bilinear", align_corners=False)

            fname = OUTDIR / f"recon_{len(list(OUTDIR.glob('recon_*.png'))):03d}.png"
            save_image(grid_big, fname, nrow=len(plan)+1, normalize=True)
            print(f"[demo] saved  →  {fname}")

    
    def imagine_decode_embed(self, wm, start_belief, action_seq, *, flatten=True):
        """
        Step Dreamer prior using rssm_step along action_seq, then decode+embed.
        Returns: embedding vector (P,) on CPU (matches dreamer_embed_fn flatten=True).
        """
        belief = start_belief
        for a in action_seq:
            belief = self.rssm_step(wm, belief, a)
        # Reuse your embed path (it already decodes internally in your setup)
        emb = self.dreamer_embed_fn(belief, flatten=flatten)  # (P,) on CPU
        return torch.as_tensor(emb).view(-1).detach().cpu().float()

    def build_enhanced_perception(self, wm, belief_zd, *, combos=None):
        """
        Produce a small set of imagined-view embeddings from the *current* belief.
        combos: list of tuples of actions, default = [('left',), ('right',), ('left','left'), ('right','right')]
        Returns: list[{'seq': tuple[str], 'embed': Tensor(P,)}]
        """
        if combos is None:
            combos = [('left',), ('right',), ('left','left'), ('right','right')]
        out = []
        for seq in combos:
            try:
                emb = self.imagine_decode_embed(wm, belief_zd, seq, flatten=True)
                out.append({'seq': tuple(seq), 'embed': emb})
            except Exception as e:
                # Stay robust; skip bad predictions rather than crashing
                if getattr(self, "_ep_verbose", False):
                    print(f"[EP] warn: failed for seq={seq}: {e}")
                continue
        return out

    def get_recent_enhanced_embeds(self, replay_buffer, K_recent: int, *, skip_last_n: int = 0):
        """
        Collect enhanced predicted embeddings from the last K entries.
        skip_last_n: how many *most recent* entries to skip (e.g., 1 to avoid depth-1 exact match).
        Returns: Tensor (N_pred, P) on CPU, or None if empty.
        """
        preds = []
        # Take last K, optionally skipping the last 'skip_last_n' entries
        items = list(replay_buffer)[-K_recent:] if K_recent > 0 else list(replay_buffer)
        if skip_last_n > 0:
            items = items[:-skip_last_n] if len(items) > skip_last_n else []
        for st in items:
            eps = st.get('enhanced_preds', None)
            if not eps: 
                continue
            for e in eps:
                v = e.get('embed', None)
                if v is not None:
                    preds.append(torch.as_tensor(v).view(-1).detach().cpu().float())
        if len(preds) == 0:
            return None
        return torch.stack(preds, dim=0)
    def hybrid_novelty(
        self,
        cand_vec: torch.Tensor,                # (P,)
        real_bank: Optional[torch.Tensor],     # (N_real, P) or None
        pred_bank: Optional[torch.Tensor],     # (N_pred, P) or None
        *,
        metric: str = "kl",
        mode: str = "weighted_min",            # "weighted_min" | "blend"
        pred_weight: float = 1.25,             # >1.0 reduces pred influence in weighted_min
        blend_beta: float = 0.4                # 0..1, lower => pred contributes less
    ) -> float:
        """
        Returns a single novelty scalar like your old novelty_score.
        - weighted_min: novelty = min(nov_real, pred_weight * nov_pred)   (distance-like scores)
        - blend       : novelty = (1-beta) * nov_real + beta * nov_pred
        If one bank is None, falls back to the other.
        """
        # Helper: reuse existing novelty_score for each bank
        def _nov(bank):
            if bank is None or (hasattr(bank, "numel") and bank.numel() == 0):
                return None
            return float(self.novelty_score(cand_vec, bank, extra_embeds=None, metric=metric))

        nov_r = _nov(real_bank)
        nov_p = _nov(pred_bank)

        # Fallbacks
        if nov_r is None and nov_p is None:
            return 0.0
        if nov_r is None:
            # Only predicted memory available
            if mode == "weighted_min":
                return float(pred_weight * nov_p)
            else:
                return float(nov_p)
        if nov_p is None:
            return float(nov_r)

        # Combine
        if mode == "weighted_min":
            return float(min(nov_r, pred_weight * nov_p))
        else:  # "blend"
            return float((1.0 - blend_beta) * nov_r + blend_beta * nov_p)
    def _dreamer_to_graph_1h(self, a):
        # a: torch or np, 3-d in Dreamer order [L,R,F] → return [F,R,L]
        import numpy as np, torch
        if isinstance(a, torch.Tensor):
            a = a.detach().to("cpu").float().numpy()
        return [float(a[2]), float(a[1]), float(a[0])]

    def update_cog(self, obs_or_img, action_onehot_dreamer, pose_xyz, *,
                place_post=None, place_std=None, std_th=None, agent_lost=False):
        

        if self.cog is None:
            print("[update_cog] cognitive graph not initialized; pass memory_config=... to WMPlanner(...)")
            return

        # 0) action [L,R,F] → [F,R,L] for HE odometry expected by MemoryGraph
        he = self._dreamer_to_graph_1h(action_onehot_dreamer)  # list[3] float, order [F,R,L]

        # 1) pose (we keep it around for debugging / future hooks; MG uses HE odom)
        x, y, th = pose_xyz
        pose_arr = np.asarray([float(x), float(y), float(th)], dtype=np.float32)

        # 2) ExperienceMap context (so first experience doesn’t deref None)
        emap = self.cog.mg.experience_map
        self.emap=emap
        emap.env = self.env
        if getattr(emap, "last_real_pose", None) is None:
            emap.last_real_pose = (float(x), float(y), float(th))
        if getattr(emap, "spawn_pose_real", None) is None:
            emap.spawn_pose_real = (float(x), float(y), float(th))
        # bookkeeping for link creation (optional, safe if these attrs exist)
        if isinstance(action_onehot_dreamer, torch.Tensor):
            act_idx = int(action_onehot_dreamer.argmax().item())
        else:
            act_idx = int(np.argmax(action_onehot_dreamer))
        idx2name = {0: "left", 1: "right", 2: "forward"}
        act_name = idx2name.get(act_idx, "forward")
        emap.last_link_action = act_name
        try:
            emap._recent_prims.append(emap._map_raw_action(act_name))
        except Exception:
            pass

        # 3) image/template
        raw = obs_or_img

        # Lost → do NOT provide an observation (lets MG digest odom, but no new view cell)
        if agent_lost:
            tmpl = None
        else:
            # If we already have a 64-D template, pass it; otherwise, compute it from RGB
            if torch.is_tensor(raw) and raw.ndim == 1 and raw.numel() == 64:
                tmpl = raw.detach().cpu().float()
            elif isinstance(raw, np.ndarray) and raw.ndim == 1 and raw.size == 64:
                import torch as _torch
                tmpl = _torch.from_numpy(raw.astype(np.float32))
            else:
                # Expect HxWx3 (or CHW) → 64-D template
                tmpl = emap.rgb_to_template64(raw, device="cpu")

        # 4) build the observation dict the graph expects
        obs = {
            "image": tmpl,                 # None when lost, else torch.float32(64,)
            "pose":  pose_arr,             # harmless; MG mainly uses HEaction
            "HEaction": he,                # [F,R,L]; HotEncodedActionOdometry will parse this
        }
        # optional: keep raw for debugging
        obs["image_rgb"] = raw

        # 5) update the cognitive graph (internally handles pure-rotation frames)
        self.cog.mg.digest(obs, dt=1, adjust_map=True)

    def get_cog_nodes(self) -> List[Dict[str, Any]]:
        return self.cog.nodes() if self.cog else []
    
    def novelty_astar_tree(
        self,
        wm,
        belief_zd,                  # tuple (z, h) at the *current* frame; each (1, dim)
        start_state,                # State(x,y,d)
        lookahead: int,             # horizon T (e.g., 4)
        replay_buffer,              # deque/list of recent steps
        *,
        K_recent: int = 30,
        metric: str = "kl",         # novelty metric: "kl" | "cos" | "l2"
        lambda_depth: float = 1e-3,
        verbose: bool = False,
        topk: int = 0,              # kept for API compat; unused in this builder
        # viz controls
        viz_tree: bool = False,
        viz_outdir: str = "dbg",
        viz_tag: str = None,
        # weights and hooks
        w_novelty: float = 0.3,
        w_penalty: float = 1.5,     # explicit weight on penalty
        w_p2e: float = 0.0,         # treated as a COST (smaller is better)
        w_graph: float = 0.0,
        gamma: float = 1.0,
        # K-rollout configuration for the uncertainty term (treated as cost)
        p2e_K: int = 4,
        p2e_mode: str = "mc",          # present for API compat; p2e_leaf_value may ignore it
        p2e_S: int = 5,                # if your p2e_leaf_value uses MC internally
        p2e_roll_from: str = "child",  # "child" aligns the probe with edge being evaluated
        p2e_metric: str = "l2",
        # novelty gate
        prune_step_nov_below: float = 0.00,  # set to None to keep all branches
        # graph hook
        memory_graph: Any = None,
        # predicted-memory knobs
        use_pred_memory: bool = True,
        pred_mode: str = "weighted_min",
        pred_weight: float = 1.2,
        pred_blend_beta: float = 0.4,
        skip_pred_for_depth1: bool = True,
    ):
        """
        Build a decision tree over imagined actions with per-node metrics.

        Edge reward at each expansion:
            step_reward = w_novelty * raw_nov
                        - w_penalty * pen
                        - w_p2e     * P2E_Krollout(child_as_cost)
                        + w_graph   * graph_bonus

        Notes:
        - We still apply cheap gates (forward-wall check and optional novelty floor).
        When a child is novelty-pruned, we still create a child node (not enqueued),
        tag it with pruned_reason, and log its metrics.
        - "skip closed" means we've already processed this (pose, depth) once in this pass;
        we avoid duplicate expansions from identical (pose,depth).
        """
        import heapq, time, os
        import numpy as np
        import torch
        F = torch.nn.functional

        # ---------- helpers ----------
        def successor_pose(x, y, d, act):
            if act == "forward":
                dx, dy = DIR_VECS[d]
                return (x + dx, y + dy, d)
            elif act == "left":
                return (x, y, (d - 1) % 4)
            else:  # right
                return (x, y, (d + 1) % 4)

        def step_belief(belief, act: str):
            import torch.nn.functional as F
            z, h = belief  # each (1, dim)
            onehot = F.one_hot(
                torch.tensor([wm.idx[act]], device=wm.device),
                num_classes=wm.action_size
            ).float()
            with torch.no_grad():
                h_next = wm.rssm.recurrent_model(z, onehot, h)
                _, z_next = wm.rssm.transition_model(h_next)
            return (z_next.detach(), h_next.detach())

        # ---------- viz helpers (unchanged) ----------
        if viz_tree:
            import math
            from pathlib import Path
            Path(viz_outdir).mkdir(parents=True, exist_ok=True)

            # We keep these around so old _record() calls don't break, and
            # so we can still fall back to the old collage if PIL is missing.
            _viz_rows = {d: [] for d in range(1, lookahead + 1)}
            _viz_first_hw = None

            # --- Try PIL for pretty tree; if not available, we fall back to collage ---
            try:
                from PIL import Image, ImageDraw, ImageFont
                _pil_ok = True
            except Exception:
                _pil_ok = False

            # no-op-ish annotate (we keep it so existing calls work; used only for fallback)
            def _annotate_tile(img_chw: torch.Tensor, text: str) -> torch.Tensor:
                return img_chw.detach().cpu()

            def _record(depth: int, img_chw: torch.Tensor, label: str):
                # keep minimal work; only used by fallback collage if PIL isn't available
                nonlocal _viz_first_hw
                if depth < 1 or depth > lookahead:
                    return
                img_chw = img_chw.detach().cpu().float()
                if _viz_first_hw is None:
                    _viz_first_hw = (int(img_chw.shape[1]), int(img_chw.shape[2]))
                _viz_rows[depth].append(_annotate_tile(img_chw, label))

            def _finalize_pretty_tree(root_node) -> str:
                """
                Render a tidy layered tree with circular nodes and connecting edges.
                Adds per-node imagined images if available (n.viz_img_chw).
                Also saves a 'plain' version without images for quick scanning.

                Returns path to the image-embedded version.
                """
                if not _pil_ok:
                    # --- Fallback to the previous collage behavior ---
                    if _viz_first_hw is None:
                        return ""
                    from torchvision.utils import make_grid, save_image
                    H, W = _viz_first_hw
                    max_cols = max(len(v) for v in _viz_rows.values()) if _viz_rows else 0
                    if max_cols == 0:
                        return ""
                    gray = torch.ones(3, H, W) * 0.5
                    tiles_all = []
                    for depth in range(1, lookahead + 1):
                        row = _viz_rows.get(depth, [])
                        if len(row) < max_cols:
                            row = row + [gray.clone() for _ in range(max_cols - len(row))]
                        tiles_all.extend(row)
                    grid = make_grid(tiles_all, nrow=max_cols, padding=2)
                    tag = viz_tag if viz_tag else f"{int(time.time()*1000)}"
                    out_path = os.path.join(viz_outdir, f"novastar_tree_{tag}.png")
                    save_image(grid, out_path)
                    return out_path

                import math
                import numpy as np
                from PIL import Image, ImageDraw, ImageFont

                # ---- Collect nodes/edges ----
                nodes, edges = [], []
                def dfs(n):
                    nodes.append(n)
                    for c in n.children:
                        edges.append((n, c))
                        dfs(c)
                dfs(root_node)
                # --- Visible set: only nodes that are NOT pruned and DO have an image tile cached ---
                # --- Visible set: only nodes that are NOT pruned and DO have an image tile cached ---
                eligible = [
                    n for n in nodes
                    if (n.depth >= 0)
                    and (getattr(n, "viz_img_chw", None) is not None)
                    and (not hasattr(n, "pruned_reason"))
                ]
                if not eligible:
                    return ""

                # Use ids for O(1) membership (TreeNode is unhashable)
                eligible_ids = {id(n) for n in eligible}

                # --- Position using ONLY visible children, so hidden/pruned nodes don't affect layout ---
                leaf_counter = 0
                def assign_x_visible(n):
                    nonlocal leaf_counter
                    # Always recurse; only place visible nodes.
                    vis_children = [ch for ch in n.children if id(ch) in eligible_ids]
                    # If this node is not visible, still push into children so visible descendants can be placed.
                    if id(n) not in eligible_ids:
                        for ch in n.children:
                            assign_x_visible(ch)
                        return
                    if not vis_children:
                        setattr(n, "_x", float(leaf_counter))
                        leaf_counter += 1
                    else:
                        for ch in vis_children:
                            assign_x_visible(ch)
                        setattr(n, "_x", sum(getattr(ch, "_x") for ch in vis_children) / len(vis_children))

                assign_x_visible(root_node)

                draw_nodes = eligible
                edges_vis = [(p, c) for (p, c) in edges if (id(p) in eligible_ids and id(c) in eligible_ids)]

                # Bounds from visible nodes only
                xs = [getattr(n, "_x") for n in draw_nodes]
                min_x, max_x = min(xs), max(xs)
                min_depth = min(n.depth for n in draw_nodes)
                max_depth = max(n.depth for n in draw_nodes)


                # ---- Layout constants ----
                HSPACE = 580   # was 200  → more horizontal room between siblings
                VSPACE = 480   # was 140  → more vertical room between levels
                R      = 188    # was 18   → bigger node radius (bigger thumbnails)
                SCALE  = 1    
                MARGIN = 60    # canvas margin
            

                width  = int(MARGIN * 2 + (max_x - min_x + 1) * HSPACE)
                height = int(MARGIN * 2 + (max_depth - min_depth + 1) * VSPACE + 2 * R)

                def render(draw_images: bool) -> Image.Image:
                    W2, H2 = width * SCALE, height * SCALE
                    img = Image.new("RGBA", (W2, H2), (255, 255, 255, 255))
                    draw = ImageDraw.Draw(img)
                    try:
                        font = ImageFont.truetype("DejaVuSansMono.ttf", size=18 * SCALE*32)
                        font_small = ImageFont.truetype("DejaVuSansMono.ttf", size=12 * SCALE*32)
                    except Exception:
                        font = ImageFont.load_default()
                        font_small = ImageFont.load_default()

                    def npx(xu):  # x unit → px
                        return int((MARGIN + (xu - min_x) * HSPACE) * SCALE)

                    def dpy(depth):  # depth → px (depth=1 is top row)
                        return int((MARGIN + (depth - min_depth) * VSPACE + R) * SCALE)

                    def draw_dashed(p1, p2, dash=10*SCALE, gap=8*SCALE, width=2*SCALE, fill=(150,150,150,255)):
                        x1, y1 = p1; x2, y2 = p2
                        Dx, Dy = x2 - x1, y2 - y1
                        dist = math.hypot(Dx, Dy)
                        if dist == 0:
                            return
                        vx, vy = Dx / dist, Dy / dist
                        t = 0.0
                        while t < dist:
                            seg = min(dash, dist - t)
                            xA = int(x1 + vx * t)
                            yA = int(y1 + vy * t)
                            xB = int(x1 + vx * (t + seg))
                            yB = int(y1 + vy * (t + seg))
                            draw.line([(xA, yA), (xB, yB)], fill=fill, width=width)
                            t += dash + gap

                    # tiny root marker above first layer


                    # ---- Edges first (behind nodes) ----
                    for p, c in edges_vis:
                        x1, y1 = npx(getattr(p, "_x")), dpy(p.depth)
                        x2, y2 = npx(getattr(c, "_x")), dpy(c.depth)
                        draw.line([(x1, y1), (x2, y2)], fill=(0,0,0,255), width=2*SCALE)
                    # ---- Helpers for images ----
                    def chw_to_rgba(img_chw: torch.Tensor) -> Image.Image:
                        t = img_chw
                        if t.dim() == 4:
                            t = t[0]
                        t = t.detach().cpu().float().clamp(0.0, 1.0)
                        if t.shape[0] == 1:
                            t = t.repeat(3, 1, 1)
                        arr = (t.numpy().transpose(1, 2, 0) * 255.0 + 0.5).astype(np.uint8)
                        return Image.fromarray(arr, mode="RGB").convert("RGBA")

                    ACT_ABBR = {"left": "L", "right": "R", "forward": "F"}

                    # ---- Nodes (with optional images) ----
                                        # ---- Nodes (visible only; each must have viz_img_chw) ----
                    for n in draw_nodes:
                        cx, cy = npx(getattr(n, "_x")), dpy(n.depth)
                        r = R * SCALE
                        bb = [cx - r, cy - r, cx + r, cy + r]

                        tile_src = getattr(n, "viz_img_chw", None)
                        if tile_src is not None:
                            try:
                                tile = chw_to_rgba(tile_src)
                                tile = tile.resize((2*r, 2*r), Image.LANCZOS)
                                # circular clip
                                mask = Image.new("L", (2*r, 2*r), 0)
                                mdraw = ImageDraw.Draw(mask)
                                mdraw.ellipse([0, 0, 2*r-1, 2*r-1], fill=255)
                                img.paste(tile, (cx - r, cy - r), mask)
                            except Exception:
                                # extremely unlikely given our eligible filter; draw a white circle fallback
                                draw.ellipse(bb, fill=(255,255,255,255))

                        # crisp outline
                        draw.ellipse(bb, outline=(0,0,0,255), width=2*SCALE)

                        # label inside node
                        ACT_ABBR = {"left": "L", "right": "R", "forward": "F"}
                        act = getattr(n, "step_act", None)
                        if act is None and hasattr(n, "seq") and n.seq:
                            act = n.seq[-1]
                        abbr = ACT_ABBR.get(act, "?")

                        try:
                            tw, th = draw.textbbox((0,0), abbr, font=font)[2:]
                        except Exception:
                            tw, th = draw.textlength(abbr, font=font), 10 * SCALE
                        draw.text(
                            (cx - tw/2, cy - th/2),
                            abbr,
                            fill=(255,255,255,255),
                            font=font,
                            stroke_width=2*SCALE,
                            stroke_fill=(0,0,0,255),
                        )

                        # small metric below (optional; keep if you like)
                        gtext = f"G={getattr(n, 'cum_gain', 0.0):.2f}"
                        try:
                            tw2, th2 = draw.textbbox((0,0), gtext, font=font_small)[2:]
                        except Exception:
                            tw2, th2 = draw.textlength(gtext, font=font_small), 10 * SCALE
                        draw.text((cx - tw2/2, cy + r + 6*SCALE), gtext, fill=(0,0,0,255), font=font_small)


                    
                    return img

                tag = viz_tag if viz_tag else f"{int(time.time()*1000)}"

                # Render with images
                img = render(draw_images=True)
                img_small = img.resize((width, height), Image.LANCZOS).convert("RGB")
                out_img = os.path.join(viz_outdir, f"novastar_tree_{tag}_img.png")
                img_small.save(out_img)


                return out_img

        # ---------- memory banks ----------
        recent_real = self.get_recent_dreamer_embeddings_from_replay(replay_buffer, K_recent)

        recent_pred_all = None
        if use_pred_memory:
            # This expects you to have populated replay entries with 'enhanced_preds'
            # (see build_enhanced_perception / imagine_decode_embed in your codebase).
            recent_pred_all = self.get_recent_enhanced_embeds(replay_buffer, K_recent, skip_last_n=0)

        if verbose:
            print("\n[NOV-A*] ---- init ----")
            print(f"[NOV-A*] metric={metric}  lookahead={lookahead}  K_recent={K_recent}  lambda_depth={lambda_depth}")
            try:
                if hasattr(recent_real, "shape"):
                    print(f"[NOV-A*] recent_real shape={tuple(recent_real.shape)}")
            except Exception as e:
                print(f"[NOV-A*] recent_real introspection failed: {e}")
            if use_pred_memory:
                if recent_pred_all is None:
                    print("[NOV-A*] predicted-memory bank: none (enhanced_preds missing)")
                else:
                    print(f"[NOV-A*] predicted-memory bank: {recent_pred_all.shape[0]} embeddings")
                    print(f"[NOV-A*] pred combine mode={pred_mode}  weight={pred_weight:.2f}  beta={pred_blend_beta:.2f}  skip_depth1={skip_pred_for_depth1}")

        # ---------- PQ item = (key, depth, (x,y,d), seq, belief, gain, local_embeds, local_cells, last_act, node_ref)
        start = start_state
        local_embeds_root = []
        local_cells_root  = {(start.x, start.y)}
        last_act_root     = None

        if not hasattr(self, "TreeNode"):
            raise RuntimeError("WMPlanner.TreeNode not found — ensure dataclass is defined above.")

        _node_id = 0
        root_node = self.TreeNode(
            node_id=_node_id, depth=0, pose=(start.x, start.y, start.d), seq=[],
            belief_zd=(belief_zd[0].detach().cpu(), belief_zd[1].detach().cpu()),
            step_act=None, step_novelty=0.0, step_penalty=0.0, step_p2e=0.0, step_graph=0.0,
            step_reward=0.0, cum_gain=0.0, children=[], is_leaf=False
        )
        if viz_tree:
            try:
                _root_img = self.dreamer_embed_fn(belief_zd, flatten=False)
                setattr(root_node, "viz_img_chw", _root_img.detach().cpu())
            except Exception:
                pass

        pq = [(0.0, 0, (start.x, start.y, start.d), [], belief_zd, 0.0,
            local_embeds_root, local_cells_root, last_act_root, root_node)]
        closed = set()
        import heapq as _heap

        while pq:
            key, g, (x, y, d), seq, belief, gain_so_far, local_embeds, local_cells, last_act, parent_node = _heap.heappop(pq)

            closed_key = (x, y, d, g)
            if closed_key in closed:
                if verbose:
                    print(f"[NOV-A*] skip closed depth={g} pose=({x},{y},{d}) seq={seq} → already expanded this (pose,depth)")
                continue
            closed.add(closed_key)

            if verbose:
                print(f"[NOV-A*] POP depth={g} pose=({x},{y},{d}) G={gain_so_far:.3f} seq={seq} pq={len(pq)}")

            # leaf
            if g >= lookahead:
                parent_node.is_leaf = True
                if parent_node.step_act is not None:
                    parent_node.p2e_leaf_value = self.p2e_leaf_value(
                        wm,
                        (belief[0].detach(), belief[1].detach()),
                        act_name=parent_node.step_act,
                        K=p2e_K,
                        metric=p2e_metric,
                        roll_from=p2e_roll_from
                    )
                    if verbose:
                        print(f"[NOV-A*] leaf p2e({parent_node.step_act})={parent_node.p2e_leaf_value:.3f}")
                else:
                    parent_node.p2e_leaf_value = 0.0
                continue

            for act in ("left", "right", "forward"):
                nx, ny, nd = successor_pose(x, y, d, act)

                # forward collision prune (kept)
                if act == "forward" and self.is_wall_ahead_now(wm, belief, debug=False):
                    if verbose:
                        print(f"   [NOV-A*] PRUNE forward: wall ahead at ({x},{y},{d}) → ({nx},{ny},{nd})")

                    # NEW: compute the "next belief" once, up front
                    with torch.no_grad():
                        _viz_next = step_belief(belief, act)  # (z_next, h_next) on device

                    _node_id += 1
                    pruned_node = self.TreeNode(
                        node_id=_node_id,
                        depth=g+1,
                        pose=(nx, ny, nd),
                        seq=seq + [act],
                        # NEW: store a detached CPU copy so it's safe to keep in the tree
                        belief_zd=(_viz_next[0].detach().cpu(), _viz_next[1].detach().cpu()),
                        step_act=act,
                        step_novelty=float("-inf"),
                        step_penalty=0.0,
                        step_p2e=0.0,
                        step_graph=0.0,
                        step_reward=float("-inf"),
                        cum_gain=float(gain_so_far),
                        children=[],
                        is_leaf=False
                    )
                    setattr(pruned_node, "pruned_reason", "wall_ahead")

                    # NEW: cache a tile now (on CPU) so the renderer doesn't need to re-decode
                    if viz_tree:
                        try:
                            _viz_img = self.dreamer_embed_fn(_viz_next, flatten=False)
                            setattr(pruned_node, "viz_img_chw", _viz_img.detach().cpu())
                        except Exception:
                            pass

                    parent_node.children.append(pruned_node)
                    continue

                # one imagined step
                next_belief = step_belief(belief, act)

                # --- keep this as-is above ---
                img_vec = self.dreamer_embed_fn(next_belief, flatten=True)  # (P,)
                img_chw = None

                if viz_tree:
                    img_chw = self.dreamer_embed_fn(next_belief, flatten=False)
                local_E = torch.stack(local_embeds, dim=0) if local_embeds else None

                # predicted-memory bank selection (unchanged)
                pred_bank = None
                if use_pred_memory and (recent_pred_all is not None):
                    if skip_pred_for_depth1 and g == 0:
                        pred_bank = self.get_recent_enhanced_embeds(replay_buffer, K_recent, skip_last_n=1)
                    else:
                        pred_bank = recent_pred_all

                # ====== NEW: augment the "real" bank with branch-local embeds so hybrid sees them ======
                real_bank_for_hybrid = recent_real
                if (local_E is not None) and (real_bank_for_hybrid is not None):
                    try:
                        rb = torch.as_tensor(real_bank_for_hybrid).float().cpu()
                        real_bank_for_hybrid = torch.cat([rb, local_E.float().cpu()], dim=0)
                    except Exception:
                        # If shape/type mismatch, fall back to original real bank (still safe)
                        real_bank_for_hybrid = recent_real

                # Compute novelty. IMPORTANT: we no longer pass extra_embeds here because we've
                # already concatenated local_E into the real bank to avoid double-counting.
                nov_real = float(self.novelty_score(img_vec, real_bank_for_hybrid, extra_embeds=None, metric=metric))
                if pred_bank is None:
                    raw_nov = nov_real
                else:
                    raw_nov = self.hybrid_novelty(
                        img_vec, real_bank_for_hybrid, pred_bank,
                        metric=metric, mode=pred_mode, pred_weight=pred_weight, blend_beta=pred_blend_beta
                    )
                pen = float(self.pose_recency_penalty(
                    (nx, ny, nd), replay_buffer,
                    local_cells=local_cells,
                    local_last_pose=(x, y, d),
                    last_action=last_act,
                ))

                # K-rollout uncertainty for this child (treated as COST → smaller is better)
                step_p2e = self.p2e_leaf_value(
                    wm,
                    belief if p2e_roll_from == "parent" else next_belief,
                    act_name=act,
                    K=p2e_K,
                    metric=p2e_metric,
                    roll_from=p2e_roll_from
                )

                # graph hook (stub = 0.0 unless you wired it)
                step_graph = self.open_graph_bonus((nx, ny, nd), memory_graph)

                # combine into per-edge reward (explicit penalty + uncertainty as a cost)
                step_reward = (w_novelty * raw_nov) - (w_penalty * pen) - (w_p2e * step_p2e) + (w_graph * step_graph)
                g2 = g + 1
                gain2 = gain_so_far + (gamma ** g) * step_reward

                # novelty floor gate (AFTER computing metrics so we can print/log them)
                if (prune_step_nov_below is not None) and ((raw_nov - pen) < prune_step_nov_below):
                    if verbose:
                        print(f"             PRUNE metric: nov-pen={(raw_nov-pen):.3f} < {prune_step_nov_below:.3f} | "
                            f"raw_nov={raw_nov:.3f} pen={pen:.3f} p2eK={step_p2e:.3f} graph={step_graph:.3f} "
                            f"r={step_reward:.3f} Gwould→{gain2:.3f}")
                    if viz_tree:
                        lab = (f"d={g2} {''.join([s[0].upper() for s in (seq+[act])])} | "
                            f"nov={raw_nov-pen:.2f} p2eK={step_p2e:.2f} gph={step_graph:.2f} PRUNE")
                        _record(g2, img_chw, lab)
                    # attach a PRUNED child with full metrics for visibility
                    _node_id += 1
                    pruned_node = self.TreeNode(
                        node_id=_node_id, depth=g2, pose=(nx, ny, nd), seq=seq + [act],
                        belief_zd=(next_belief[0].detach().cpu(), next_belief[1].detach().cpu()),
                        step_act=act, step_novelty=float(raw_nov - pen), step_penalty=float(pen),
                        step_p2e=float(step_p2e), step_graph=float(step_graph),
                        step_reward=float(step_reward), cum_gain=float(gain_so_far), children=[], is_leaf=False
                    )
                    setattr(pruned_node, "pruned_reason", "novelty_floor")
                    parent_node.children.append(pruned_node)
                    if viz_tree and (img_chw is not None):
                        setattr(pruned_node, "viz_img_chw", img_chw.detach().cpu())
                    continue

                if verbose:
                    print(f"   [NOV-A*] a={act:<6} → ({nx},{ny},{nd})  "
                        f"raw_nov={raw_nov:.3f}  pen={pen:.3f}  p2eK={step_p2e:.3f}  graph={step_graph:.3f}  "
                        f"r={step_reward:.3f}  G→{gain2:.3f}")

                if viz_tree:
                    lab = (f"d={g2} {''.join([s[0].upper() for s in (seq+[act])])} | "
                        f"nov={raw_nov-pen:.2f} p2eK={step_p2e:.2f} gph={step_graph:.2f} r={step_reward:.2f} G={gain2:.2f}")
                    _record(g2, img_chw, lab)

                # carry local state for novelty
                new_local_embeds = local_embeds + [torch.as_tensor(img_vec).float().cpu()]
                new_local_cells = set(local_cells); new_local_cells.add((nx, ny))

                # A*-like key controls expansion order (not pruning)
                new_key = -(gain2) + lambda_depth * g2

                # build child node
                _node_id += 1
                child_node = self.TreeNode(
                    node_id=_node_id, depth=g2, pose=(nx, ny, nd), seq=seq + [act],
                    belief_zd=(next_belief[0].detach().cpu(), next_belief[1].detach().cpu()),
                    step_act=act, step_novelty=float(raw_nov - pen), step_penalty=float(pen),
                    step_p2e=float(step_p2e), step_graph=float(step_graph),
                    step_reward=float(step_reward), cum_gain=float(gain2), children=[], is_leaf=False
                )
                parent_node.children.append(child_node)
                if viz_tree and (img_chw is not None):
                    setattr(child_node, "viz_img_chw", img_chw.detach().cpu())

                heapq.heappush(
                    pq,
                    (new_key, g2, (nx, ny, nd), seq + [act], next_belief, gain2,
                    new_local_embeds, new_local_cells, act, child_node)
                )

        if viz_tree:
            path = _finalize_pretty_tree(root_node)
            self._last_novastar_tree_path = path
            if verbose and path:
                print(f"[NOV-A*] saved decision tree → {path}")

        root_node.is_leaf = (lookahead == 0)
        return root_node


    # ─────────────────────────────────────────────────────────────────────
    # SHIM: keep the old API, but drive the new tree builder under the hood
    # ─────────────────────────────────────────────────────────────────────
    def novelty_astar_plan(
        self, wm, belief_zd, start_state, lookahead, replay_buffer, *,
        K_recent: int = 30, metric: str = "kl", lambda_depth: float = 1e-3,
        verbose: bool = False, topk: int = 0,
        viz_tree: bool = False, viz_outdir: str = "dbg", viz_tag: str = None,
        # weights
        w_p2e: float = 0.0, w_novelty: float = 1.0, w_graph: float = 0.0,
        gamma: float = 1.0,
        # K-rollout config
        p2e_K: int = 4, p2e_mode: str = "mc", p2e_S: int = 5,
        p2e_roll_from: str = "child", p2e_metric: str = "l2",
        memory_graph: Any = None,
    ):
        """
        Wrapper to keep old API ((T,3) one-hot). We first build the *full tree*,
        then pick an action by scanning the root’s children for max cum_gain.
        """
        def to_onehot(seq: List[str]) -> torch.Tensor:
            mapping = {'forward': [1, 0, 0], 'right': [0, 1, 0], 'left': [0, 0, 1]}
            return torch.tensor([mapping[a] for a in seq], dtype=torch.float32)
        self._last_memory_graph = memory_graph
        tree = self.novelty_astar_tree(
            wm, belief_zd, start_state, lookahead, replay_buffer,
            K_recent=K_recent, metric=metric, lambda_depth=lambda_depth,
            verbose=verbose, topk=0, viz_tree=viz_tree, viz_outdir=viz_outdir, viz_tag=viz_tag,
            w_p2e=w_p2e, w_novelty=w_novelty, w_graph=w_graph, gamma=gamma,
            p2e_K=p2e_K, p2e_roll_from=p2e_roll_from, p2e_metric=p2e_metric,
            memory_graph=memory_graph,
        )
        self._last_decision_tree = tree

        best_action, action_scores, top_paths = self.first_action_from_topk_paths(
            tree,
            k=5,                       # change if you want a different K
            include_pruned=True,
            weight="linear",            # "linear" | "harmonic" | "exp"
            alpha=0.85,                 # only used for weight="exp"
            require_min_depth=1,
            verbose=verbose,
        )


        if best_action is None:
            return to_onehot([])
        return to_onehot([best_action])
        # ─────────────────────────────────────────────────────────────────────
    # Dynamic Programming backup over the decision tree
    # ─────────────────────────────────────────────────────────────────────
    def _p2e_leaf_bonus(self, node, *, coef: float, lower_is_better: bool) -> float:
        """Scalar terminal bonus from p2e_leaf_value (None -> 0)."""
        v = 0.0 if node.p2e_leaf_value is None else float(node.p2e_leaf_value)
        # You asked: "less p2e_leaf_value is BETTER (more uncertainty)" → invert.
        signed = -v if lower_is_better else +v
        return coef * signed

    
    
    def _graph_nodes_for_ranking(self, *, dbg=False):
        """
        Return (nodes_xy:list[(x,y)], weights:list[float]) using EXACTLY the same
        sources your visualizer uses:
        1) get_memory_map_data(dbg=False): uses self.cog.mg + experience_map
        2) fallback: self.cog.mg.view_cells.cells[*].exps and mg.experience_map.exps
        """
        def _norm_pairs(pairs):
            xy = []
            for p in pairs:
                if isinstance(p, (list, tuple)) and len(p) >= 2:
                    x, y = p[0], p[1]
                    if x is None or y is None:
                        continue
                    xy.append((float(x), float(y)))
            return xy

        # --- 1) Preferred: the exact data structure your viz builds ---
        try:
            if hasattr(self, "get_memory_map_data") and callable(self.get_memory_map_data):
                data = self.get_memory_map_data(dbg=False)
                exps = data.get("exps_GP", []) or []
                decays = data.get("exps_decay", []) or []
                xy = _norm_pairs(exps)
                if xy:
                    use_decay = bool(getattr(self, "rank_graph_use_decay_weights", False))
                    if use_decay and decays and len(decays) == len(xy):
                        w = [float(d) for d in decays]
                    else:
                        w = [1.0] * len(xy)
                    if dbg:
                        print(f"[RANK:cog] nodes from get_memory_map_data: {len(xy)} (weights={'decay' if use_decay else '1s'})")
                    return xy, w
        except Exception as e:
            if dbg:
                print(f"[RANK:cog] get_memory_map_data failed: {e}")

        # --- 2) Fallback: mirror get_memory_map_data internals directly ---
        try:
            cog = getattr(self, "cog", None)
            if cog is None:
                if dbg: print("[RANK:cog] self.cog is None")
                return [], []
            mg = getattr(cog, "mg", None)
            if mg is None:
                if dbg: print("[RANK:cog] self.cog.mg is None")
                return [], []

            xy, w = [], []
            use_decay = bool(getattr(self, "rank_graph_use_decay_weights", False))

            # Experiences via view_cells (nodes + decay)
            try:
                vcc = getattr(mg, "view_cells", None)
                cells = getattr(vcc, "cells", []) if vcc is not None else []
                for vc in cells:
                    dec = float(getattr(vc, "decay", 1.0))
                    for exp in getattr(vc, "exps", []):
                        x = getattr(exp, "x_m", None)
                        y = getattr(exp, "y_m", None)
                        if x is None or y is None:
                            continue
                        xy.append((float(x), float(y)))
                        w.append(dec if use_decay else 1.0)
            except Exception:
                pass

            # Experiences via experience_map (nodes)
            try:
                emap = getattr(mg, "experience_map", None)
                if emap is not None:
                    for exp in getattr(emap, "exps", []):
                        x = getattr(exp, "x_m", None)
                        y = getattr(exp, "y_m", None)
                        if x is None or y is None:
                            continue
                        xy.append((float(x), float(y)))
                        w.append(1.0)
            except Exception:
                pass

            # Deduplicate (tolerant to float fuzz)
            if xy:
                seen, xy_d, w_d = set(), [], []
                for (x, y), ww in zip(xy, w):
                    key = (round(x, 4), round(y, 4))
                    if key in seen:
                        continue
                    seen.add(key)
                    xy_d.append((x, y)); w_d.append(float(ww))
                if dbg:
                    print(f"[RANK:cog] nodes from self.cog.mg: {len(xy_d)}")
                return xy_d, w_d

            if dbg:
                print("[RANK:cog] no nodes found in cog.mg")
            return [], []
        except Exception as e:
            if dbg:
                print(f"[RANK:cog] fallback via cog.mg failed: {e}")
            return [], []


    def first_action_from_topk_paths(
        self,
        root,
        *,
        k: int = 10,
        include_pruned: bool = True,
        weight: str = "linear",     # "linear" | "harmonic" | "exp"
        alpha: float = 0.85,        # only used when weight="exp"
        require_min_depth: int = 1, # ignore empty sequences (no first action)
        verbose: bool = False,
    ):
        """
        Rank root→terminal paths by a NEW score that ignores the builder's 'gain':
            score(path) = new_gain_from_steps
                        + B * dist_from_start
                        + C * dist_from_graph_nodes   (now computed against ALL graph nodes)

        - new_gain_from_steps uses only per-step metrics stored in TreeNode.
        - dist_from_graph_nodes uses a robust aggregator over ALL graph nodes (default: RBF separation).

        Everything else (top-k, rank-weighted first-action voting) is unchanged.
        Returns:
            best_action : str   in {"left","right","forward"}
            action_scores : dict[str, float]
            top_paths : list[dict] enriched with 'score' and 'score_terms'
        """

        # =========================
        # Tunables (override on planner after init)
        # =========================
        Wp2e        = getattr(self, "rank_Wp2e", 1.0)       # weight for sibling-normalized p2e in step value
        gammaR      = getattr(self, "rank_gamma", 1.0)      # per-step discount in ranking sum (1.0 = none)
        Bdist       = getattr(self, "rank_B", 0.8)         # weight for distance from start
        Cgraph      = getattr(self, "rank_C", 1.2)         # weight for graph separation term
        dist_metric = getattr(self, "rank_dist_metric", "manhattan")  # or "euclidean"

        # Graph aggregation controls
        graph_mode  = getattr(self, "rank_graph_mode", "rbf_softmin")  # "rbf_sep" | "knn_mean" | "power_mean" | "min" | "mean"
        graph_sigma = float(getattr(self, "rank_graph_sigma", 2.5))# for rbf_sep: larger = broader influence (tiles)
        graph_k     = int(getattr(self, "rank_graph_k", 3))        # for knn_mean
        graph_p     = float(getattr(self, "rank_graph_p", 2.0))    # for power_mean

        # Optional:
        self.rank_dist_metric = "manhattan"  # or "euclidean"
        # =========================
        # Helpers
        # =========================
        def _dist_xy(pose, xy):
            x0, y0, _ = pose
            x1, y1    = xy
            dx = abs(float(x1) - float(x0))
            dy = abs(float(y1) - float(y0))
            if dist_metric == "euclidean":
                return (dx*dx + dy*dy) ** 0.5
            return dx + dy  # manhattan


        def _path_nodes_by_seq(root_node, seq):
            """Return list of nodes along seq (excluding root), or [] if not found."""
            nodes = []
            cur = root_node
            prefix = []
            for a in seq:
                prefix.append(a)
                nxt = None
                # prefer exact-seq match
                for ch in cur.children:
                    if getattr(ch, "seq", None) == prefix:
                        nxt = ch
                        break
                if nxt is None:
                    # fallback: first child with action
                    for ch in cur.children:
                        if getattr(ch, "step_act", None) == a:
                            nxt = ch
                            break
                if nxt is None:
                    return []
                nodes.append(nxt)
                cur = nxt
            return nodes

        def _sibling_p2e_norm_inverted(child_node, parent_node):
            """Normalize this child's step_p2e among siblings and invert ⇒ smaller p2e → larger bonus ∈ [0,1]."""
            try:
                sibs = [c for c in getattr(parent_node, "children", []) if hasattr(c, "step_p2e")]
                vals = [float(c.step_p2e) for c in sibs if (c.step_p2e is not None) and (float(c.step_p2e) >= 0.0)]
                v = float(child_node.step_p2e)
                if not vals or len(vals) == 1:
                    return 0.5
                vmin, vmax = min(vals), max(vals)
                if vmax <= vmin:
                    return 0.5
                return (vmax - v) / (vmax - vmin + 1e-8)
            except Exception:
                return 0.5

        # ---- graph nodes collection (robust to several backends) ---


        def _graph_separation(end_pose, nodes_xy, nodes_w):
                """
                Aggregates distances to ALL nodes into a single scalar.
                Modes:
                - rbf_sep (default): sep = 1 - (Σ w_i * exp(-d_i/σ)) / (Σ w_i)
                - knn_mean:          mean of k nearest distances
                - power_mean:        (Σ w_i * d_i^p / Σ w_i)^(1/p)
                - min / mean:        obvious
                Returns a value where LARGER means "farther from graph overall".
                """
                if not nodes_xy:
                    return 0.0

                # distances
                D = [_dist_xy(end_pose, xy) for xy in nodes_xy]

                if graph_mode == "rbf_sep":
                    import math
                    sigma = max(1e-6, graph_sigma)
                    wsum = sum(nodes_w) if nodes_w else float(len(D))
                    dens = 0.0
                    for d, w in zip(D, nodes_w if nodes_w else [1.0]*len(D)):
                        dens += w * math.exp(-d / sigma)
                    dens /= max(wsum, 1e-6)
                    sep = 1.0 - dens
                    return float(sep)
                elif graph_mode == "rbf_softmin":
                    import math

                    sigma = max(1e-6, graph_sigma)
                    W = nodes_w if nodes_w else [1.0] * len(D)

                    # Stable soft-min: shift distances by the minimum to avoid tiny exponent issues.
                    d0 = min(D)

                    # s = Σ w_i * exp(-(d_i - d0)/σ), so that softmin = d0 - σ * log(s)
                    s = 0.0
                    for d, w in zip(D, W):
                        s += w * math.exp(-(d - d0) / sigma)

                    # Prevent log(0); then compute softmin.
                    d_softmin = d0 - sigma * math.log(max(s, 1e-12))

                    # Multiple nearly-equal nearest nodes can make this slightly negative; clamp to 0.
                    if d_softmin < 0.0:
                        d_softmin = 0.0

                    return float(d_softmin)

                elif graph_mode == "knn_mean":
                    k = max(1, min(graph_k, len(D)))
                    return float(sum(sorted(D)[:k]) / k)

                elif graph_mode == "power_mean":
                    p = float(graph_p)
                    if abs(p) < 1e-9:
                        # p ~ 0 → geometric mean; avoid log for simplicity: fall back to mean
                        return float(sum(D) / len(D))
                    wsum = sum(nodes_w) if nodes_w else float(len(D))
                    num = 0.0
                    for d, w in zip(D, nodes_w if nodes_w else [1.0]*len(D)):
                        num += w * (d ** p)
                    return float((num / max(wsum, 1e-6)) ** (1.0 / p))

                elif graph_mode == "min":
                    return float(min(D))

                else:  # "mean"
                    return float(sum(D) / len(D))

        # =========================
        # 1) Enumerate all paths from the tree (we ignore its stored 'gain')
        # =========================
        ranked_all = self.rank_paths(root, include_pruned=include_pruned)

        # Precollect graph nodes once
        graph_nodes_xy, graph_nodes_w = self._graph_nodes_for_ranking(dbg=verbose)
        if verbose:
            print(f"[DEBUG cog] nodes={len(graph_nodes_xy)}")
        # =========================
        # 2) Compute NEW score per path
        # =========================
        start_pose = getattr(root, "pose", None) or (None, None, None)
        enriched = []
        for rec in ranked_all:
            seq   = rec.get("seq", [])
            depth = int(rec.get("depth", len(seq) or 0))
            if depth < require_min_depth:
                continue

            # Traverse to get actual nodes (to access per-step metrics w/ sibling context)
            step_nodes = _path_nodes_by_seq(root, seq)
            if not step_nodes:
                continue

            # Accumulate per-step contributions
            new_gain = 0.0
            parent = root
            for t, node in enumerate(step_nodes, start=1):
                step_pen = float(getattr(node, "step_penalty", 0.0))
                step_nov_minus_pen = float(getattr(node, "step_novelty", 0.0))
                raw_nov = step_nov_minus_pen + step_pen  # since step_novelty = raw_nov - pen

                p2e_bonus = _sibling_p2e_norm_inverted(node, parent)  # ∈ [0,1]
                step_value = (raw_nov - step_pen) + (Wp2e * p2e_bonus)

                new_gain += (gammaR ** (t - 1)) * step_value
                parent = node

            # Distance terms
            dist_start = 0.0
            end_pose = rec.get("end_pose", getattr(step_nodes[-1], "pose", None))
            if start_pose[0] is not None and end_pose is not None:
                dist_start = _dist_xy(start_pose, (end_pose[0], end_pose[1]))

            dist_graph_all = 0.0
            if graph_nodes_xy:
                dist_graph_all = _graph_separation(end_pose, graph_nodes_xy, graph_nodes_w)
                print("[DEBUG cog2]",dist_graph_all)

            score = float(new_gain + Bdist * dist_start + Cgraph * dist_graph_all)

            rec2 = dict(rec)
            rec2["score"] = score
            rec2["score_terms"] = {
                "new_gain": float(new_gain),
                "dist_from_start": float(dist_start),
                "dist_from_graph_nodes": float(dist_graph_all),
                "B": float(Bdist),
                "C": float(Cgraph),
                "Wp2e": float(Wp2e),
                "gamma_rank": float(gammaR),
                "graph_mode": str(graph_mode),
            }
            enriched.append(rec2)

        # =========================
        # 3) Sort by NEW score (DESC) and take top-k
        # =========================
        ranked_desc = sorted(enriched, key=lambda r: r["score"], reverse=True)
        print(ranked_desc)
        top_paths = ranked_desc[:k]

        # =========================
        # 4) Rank-weighted FIRST-action voting (unchanged)
        # =========================
        actions = ("left", "right", "forward")
        scores = {a: 0.0 for a in actions}
        contrib = {a: [] for a in actions}

        def rank_weight(i: int) -> float:
            if weight == "harmonic":
                return 1.0 / (i + 1)
            if weight == "exp":
                return float(alpha ** i)
            return float(max(k - i, 1))  # linear

        for i, rec in enumerate(top_paths):
            seq = rec.get("seq", [])
            if len(seq) < require_min_depth:
                continue
            a0 = seq[0]
            if a0 not in scores:
                continue
            w = rank_weight(i)
            scores[a0] += w
            contrib[a0].append((i + 1, w, rec.get("score", 0.0), list(seq)))

        def tiebreak(kv):
            order = {"forward": 2, "left": 1, "right": 0}
            return (kv[1], order.get(kv[0], -1))

        best_action = max(scores.items(), key=tiebreak)[0]

        # =========================
        # 5) Verbose debug
        # =========================
        if verbose:
            print("\n[TOP-K FIRST-ACTION VOTING]")
            print(f"  k={k} weight={weight} alpha={alpha} include_pruned={include_pruned}")
            for i, rec in enumerate(ranked_desc, 1):
                a0 = rec["seq"][0] if rec["seq"] else "∅"
                s_terms = rec.get("score_terms", {})
                print(f"  #{i:02d} S={rec['score']:.3f}  first={a0:<7} depth={rec.get('depth')} seq={rec.get('seq')} "
                    f"pruned={rec.get('pruned_reason')}")
                print(f"        └ new_gain={s_terms.get('new_gain', 0.0):.3f}  "
                    f"B*dist_start={s_terms.get('B',0.0)*s_terms.get('dist_from_start',0.0):.3f}  "
                    f"C*graph={s_terms.get('C',0.0)*s_terms.get('dist_from_graph_nodes',0.0):.3f}  "
                    f"[graph_mode={s_terms.get('graph_mode','?')}]")

            print("  Action scores:")
            for a in actions:
                print(f"    {a:<7} → {scores[a]:.3f}  via {len(contrib[a])} hits")
                for rank, w, S, s in contrib[a]:
                    print(f"       - rank#{rank:02d} w={w:.3f} S={S:.3f} seq={s}")

        if verbose:
            print("\n[TOP-K FIRST-ACTION VOTING]")
            print(f"  k={k} weight={weight} alpha={alpha} include_pruned={include_pruned}")
            for i, rec in enumerate(top_paths, 1):
                a0 = rec["seq"][0] if rec["seq"] else "∅"
                s_terms = rec.get("score_terms", {})
                print(f"  #{i:02d} S={rec['score']:.3f}  first={a0:<7} depth={rec.get('depth')} seq={rec.get('seq')} "
                    f"pruned={rec.get('pruned_reason')}")
                print(f"        └ new_gain={s_terms.get('new_gain', 0.0):.3f}  "
                    f"B*dist_start={s_terms.get('B',0.0)*s_terms.get('dist_from_start',0.0):.3f}  "
                    f"C*graph={s_terms.get('C',0.0)*s_terms.get('dist_from_graph_nodes',0.0):.3f}  "
                    f"[graph_mode={s_terms.get('graph_mode','?')}]")

            print("  Action scores:")
            for a in actions:
                print(f"    {a:<7} → {scores[a]:.3f}  via {len(contrib[a])} hits")
                for rank, w, S, s in contrib[a]:
                    print(f"       - rank#{rank:02d} w={w:.3f} S={S:.3f} seq={s}")

        self._last_top_paths = top_paths
        self._last_action_scores = scores
        return best_action, scores, top_paths

    def rank_paths(self, root, *, include_pruned: bool = True):
        """
        Enumerate and rank all root→terminal paths by cumulative gain (desc).
        'Terminal' = (is_leaf==True) OR (pruned child) OR (no children).
        Returns: list of dicts {gain, seq, depth, end_pose, leaf_p2e, pruned_reason}
        """
        out = []

        def is_terminal(node):
            # terminal if explicitly leaf OR all children are pruned OR no children
            if getattr(node, "is_leaf", False):
                return True
            if not node.children:
                return True
            if all(getattr(c, "pruned_reason", None) is not None for c in node.children):
                return True
            return False

        def dfs(node):
            if is_terminal(node):
                out.append({
                    "gain": float(node.cum_gain),
                    "seq": list(node.seq),
                    "depth": int(node.depth),
                    "end_pose": tuple(node.pose),
                    "leaf_p2e": float(node.p2e_leaf_value) if getattr(node, "p2e_leaf_value", None) is not None else None,
                    "pruned_reason": getattr(node, "pruned_reason", None),
                })
                return
            for c in node.children:
                if (getattr(c, "pruned_reason", None) is not None) and not include_pruned:
                    continue
                dfs(c)

        dfs(root)
        out.sort(key=lambda d: d["gain"], reverse=False)
        
        return out


    
    def dreamer_embed_fn(self, belief, *, flatten: bool = True, scale01: bool = True):
        """
        Decode a Dreamer belief (z,h) to an image and (optionally) flatten it for similarity.
        Returns a CPU tensor:
        - if flatten=True: (C*H*W,) for cosine/L2 comparisons
        - else: (C,H,W)
        """
        import torch

        if belief is None:
            return torch.empty(0)

        z, h = belief  # (1,Z), (1,H)
        with torch.no_grad():
            # Use the simple, native decoder call pattern you’re already using elsewhere.
            # Expect outputs in Dreamer scale ~[-0.5, 0.5]; shift to [0,1] if requested.
            dec = None
            if hasattr(self.wm, "decoder"):
                dec = self.wm.decoder(z, h)
            elif hasattr(self.wm, "rssm") and hasattr(self.wm.rssm, "decoder"):
                dec = self.wm.rssm.decoder(z, h)
            else:
                raise AttributeError("No decoder found on wm (expected wm.decoder or wm.rssm.decoder)")

            # Distribution or tensor
            x = getattr(dec, "mean", dec)
            x = x.squeeze(0)  # (C,H,W)

            if scale01:
                x = (x + 0.5).clamp(0.0, 1.0)  # to [0,1] for image cosine/L2 stability

            x = x.detach().to("cpu")
            return x.reshape(-1) if flatten else x

    def novelty_score(
        self,
        z_pred,
        recent_embeds,
        *,
        extra_embeds=None,      # OPTIONAL: (G,P) tensor of path-local flattened images
        metric: str = "kl",
        age_tau: float = 8.0,   # NEW: larger = slower decay ⇒ older items still matter; smaller = emphasize recency
    ) -> torch.Tensor:
        """
        Compute novelty of z_pred (flattened image) vs union of:
        - recent_embeds: (K,P) from replay (assumed chronological oldest→newest)
        - extra_embeds:  (G,P) from current path (assumed earliest→latest)

        Recency weighting:
        - Each comparison gets a weight w(age) = exp(-age / age_tau), where age=0 is the MOST RECENT item.
        - Recent items (age≈0) get w≈1 → they dominate; old items get w→0 → they barely penalize novelty.
        - This prevents pruning just because something looked similar 20–40 steps ago.

        Metrics:
        - 'cos' (recommended for images): novelty = 1 - max_i( w_i * cos_sim_i )
        - 'l2' : novelty = min_i( ||x_i - z|| / max(w_i, eps) )
        - 'kl' : novelty = min_i( 0.5 * ||x_i - z||^2 / max(w_i, eps) )

        Returns: scalar 0-D torch.Tensor (CPU).
        """
        import torch
        import torch.nn.functional as F

        # ---- Collect comparison sets ----
        parts = []
        weights = []

        # Helper to append a block and its recency weights from oldest→newest rows
        def _append_block(block):
            B = torch.as_tensor(block).float()
            if B.ndim == 1:
                B = B.unsqueeze(0)
            if B.numel() == 0:
                return None
            # ages: newest row has age=0
            n = B.shape[0]
            ages = torch.arange(n - 1, -1, -1, dtype=torch.float32)  # [n-1, ..., 1, 0]
            w = torch.exp(-ages / float(age_tau))                    # exp decay; newest→1.0
            parts.append(B)
            weights.append(w)
            return B

        if recent_embeds is not None:
            _append_block(recent_embeds)
        if extra_embeds is not None:
            _append_block(extra_embeds)

        if not parts:
            return torch.tensor(1.0)

        # Concatenate comparisons and their weights
        Eall = torch.cat(parts, dim=0)                 # (M, P)
        Wall = torch.cat(weights, dim=0)               # (M,)
        z = torch.as_tensor(z_pred).float().view(-1)   # (P,)
        eps = 1e-6

        with torch.no_grad():
            if metric == "cos":
                # Cosine similarity with recency weighting on the similarities
                z_n = F.normalize(z, dim=0)
                E_n = F.normalize(Eall, dim=1)
                sims = E_n @ z_n                        # (M,)
                wsims = Wall * sims                     # weight recent matches more
                novelty = 1.0 - torch.clamp(wsims.max(), -1.0, 1.0)

            elif metric == "l2":
                # Make recent near-neighbors *more influential* by dividing distance by weight
                dists = torch.linalg.norm(Eall - z, dim=1)          # (M,)
                eff = dists / torch.clamp(Wall, min=eps)
                novelty = eff.min()

            elif metric == "kl":
                diffs = Eall - z
                sq = 0.5 * torch.sum(diffs * diffs, dim=1)          # (M,)
                eff = sq / torch.clamp(Wall, min=eps)
                novelty = eff.min()

            else:
                raise ValueError(f"Unknown metric '{metric}'")

            if torch.isnan(novelty):
                novelty = torch.tensor(0.0)

            return novelty

    def get_recent_dreamer_embeddings_from_replay(self, replay_buffer, K_recent: int):
        """
        Return up to K_recent *decoded images* from the replay buffer, flattened to 1D.
        Shape: (N<=K_recent, C*H*W) on CPU.
        Tries keys in each state:
        - 'dreamer_decoded' (preferred, (C,H,W) or (1,C,H,W))
        - else decodes from 'belief_zd' or 'belief' if present
        """
        import torch

        if not replay_buffer or K_recent <= 0:
            return torch.empty(0, dtype=torch.float32)

        outs = []
        count = 0
        # newest → oldest
        for st in reversed(replay_buffer):
            x = st.get("dreamer_decoded", None)

            if x is None:
                belief = st.get("belief_zd", None) or st.get("belief", None)
                if belief is not None:
                    try:
                        x = self.dreamer_embed_fn(belief, flatten=False, scale01=True)  # (C,H,W)
                    except Exception:
                        x = None

            if x is None:
                continue

            x = torch.as_tensor(x, dtype=torch.float32)
            if x.ndim == 4 and x.shape[0] == 1:  # (1,C,H,W) → (C,H,W)
                x = x[0]
            if x.ndim == 3 and x.shape[0] in (1, 3):
                pass
            elif x.ndim == 3 and x.shape[-1] in (1, 3):  # HWC → CHW
                x = x.permute(2, 0, 1).contiguous()
            elif x.ndim == 2:
                x = x.unsqueeze(0)  # make (1,H,W)

            outs.append(x.reshape(-1).cpu())
            count += 1
            if count >= K_recent:
                break

        if not outs:
            return torch.empty(0, dtype=torch.float32)

        # chronological (oldest→newest) for sanity; order doesn't affect max/nn ops
        return torch.stack(list(reversed(outs)), dim=0)


    def pose_recency_penalty(
        self,
        candidate_pose,
        replay_buffer,
        *,
        local_cells=None,               # set of (x,y) from the CURRENT PATH (may be empty set)
        local_last_pose=None,           # (x,y,d) of parent node to detect turn-in-place
        window: int = 40,               # how many recent replay states to consider
        spin_small: float = 0.05,       # legacy small spin penalty (used if last_action=None or first_turn_free=False)
        turn_again_penalty: float = 0.35,  # stronger penalty for turn-after-turn
        revisit_large: float = 0.35,    # big penalty for returning to a known cell
        tau: float = 6.0,               # recency decay for replay-based cell revisits
        last_action: str | None = None, # previous action in the branch ("left"|"right"|"forward"|None)
        first_turn_free: bool = True,   # if True, first turn after a forward is unpenalized
    ) -> float:
        """
        Adaptive penalties:

        (A) TURN-IN-PLACE (same cell, orientation changes):
            - If last_action was 'forward' (or None) and first_turn_free=True → 0 penalty.
            - If last_action was a turn ('left' or 'right') → apply `turn_again_penalty`.
            - If last_action is None and you want legacy behavior → set first_turn_free=False to use `spin_small`.

        (B) MOVE INTO A PREVIOUSLY SEEN CELL:
            - If the next cell is already in THIS path → immediate `revisit_large`.
            - Else if that cell appears in recent replay → `revisit_large * exp(-age/tau)` (age=0 is most recent).

        IMPORTANT FIX:
            - The replay-based revisit penalty is applied ONLY if the step MOVES to a new cell.
            Turning in place (same (x,y)) will NOT trigger the heavy replay penalty.
        """
        import math
        from itertools import islice

        if candidate_pose is None:
            return 0.0

        x, y, d = map(int, candidate_pose)
        pen = 0.0

        # ----- (A) adaptive turn-in-place handling -----
        moved_to_new_cell = True  # default; will correct below if we know parent pose
        if local_last_pose is not None:
            lx, ly, ld = map(int, local_last_pose)
            same_cell = (x, y) == (lx, ly)
            moved_to_new_cell = not same_cell
            turned = same_cell and (d != ld)

            if turned:
                if last_action in ("left", "right"):
                    # consecutive turning → stronger penalty
                    pen += float(turn_again_penalty)
                else:
                    # previous was forward (or unknown)
                    if first_turn_free:
                        pen += 0.0
                    else:
                        pen += float(spin_small)
        else:
            # No parent pose provided (e.g., very first call): if we *know* the last action was a turn,
            # treat this as a turn-in-place for the purpose of replay penalty gating.
            if last_action in ("left", "right"):
                moved_to_new_cell = False  # prevents heavy replay penalty on first turn

        # ----- (B1) local-path revisit penalty (immediate) -----
        if local_cells is not None and local_last_pose is not None:
            lx, ly, _ = map(int, local_last_pose)
            if (x, y) != (lx, ly) and (x, y) in local_cells:
                pen = max(pen, float(revisit_large))

        # ----- (B2) replay-based revisit penalty with recency decay -----
        # APPLY ONLY IF WE MOVED TO A NEW CELL (prevents heavy penalty on first turn-in-place)
        if moved_to_new_cell and replay_buffer:
            for age, state in enumerate(islice(reversed(replay_buffer), 0, window)):
                pose = state.get("imagined_pose") or state.get("real_pose")
                if pose is None:
                    continue
                try:
                    px, py = int(pose[0]), int(pose[1])
                except Exception:
                    continue
                if (px, py) == (x, y):
                    pen = max(pen, float(revisit_large * math.exp(-age / tau)))
                    break  # most recent hit dominates

        return float(pen)
    def get_memory_map_data(self, dbg=True):
        if self.cog is None:
            return {'exps_GP': [], 'exps_decay': [], 'ghost_exps_GP': [],
                    'ghost_exps_link': [], 'exps_links': [], 'current_exp_id': -1}

        mg   = self.cog.mg
        emap = mg.experience_map

        memory_map_data = {
            'exps_GP': [], 'exps_decay': [],
            'ghost_exps_GP': [], 'ghost_exps_link': [],
            'exps_links': []
        }

        memory_map_data['current_exp_id'] = mg.get_current_exp_id()
        memory_map_data['current_GP']     = mg.get_global_position()
        if memory_map_data['current_exp_id'] < 0:
            if dbg: print("[DBG] No current experience → empty map")
            return memory_map_data
        memory_map_data['current_exp_GP'] = mg.get_exp_global_position()

        # Experiences as nodes (colored by view-cell decay)
        for vc in mg.view_cells.cells:
            for exp in vc.exps:
                memory_map_data['exps_GP'].append([exp.x_m, exp.y_m])
                memory_map_data['exps_decay'].append(vc.decay)

        # Ghost experiences
        for ghost in emap.ghost_exps:
            memory_map_data['ghost_exps_GP'].append([ghost.x_m, ghost.y_m])
            for link in ghost.links:
                memory_map_data['ghost_exps_link'] += [
                    [ghost.x_m, ghost.y_m], [link.target.x_m, link.target.y_m]
                ]

        # Real links (dedup happens after collection)
        for exp in emap.exps:
            for link in exp.links:
                if not getattr(link.target, 'ghost_exp', False):
                    memory_map_data['exps_links'] += [
                        [link.target.x_m, link.target.y_m],
                        [exp.x_m,          exp.y_m]
                    ]

        # Deduplicate consecutive link pairs
        clean, seen = [], set()
        pts = memory_map_data['exps_links']
        for i in range(0, len(pts), 2):
            p0, p1 = tuple(pts[i]), tuple(pts[i+1])
            if (p0, p1) not in seen:
                seen.add((p0, p1))
                clean.extend([list(p0), list(p1)])
        memory_map_data['exps_links'] = clean
        return memory_map_data
    def plot_cog_map(self, ax=None, dbg=False):
        """
        Build memory_map_data from the live CognitiveGraph
        and draw it using navigation_model.visualisation_tools.plot_memory_map.
        """
        import matplotlib.pyplot as plt
        from navigation_model.visualisation_tools import plot_memory_map

        data = self.get_memory_map_data(dbg=dbg)
        if ax is None:
            fig, ax = plt.subplots(figsize=(5, 5))
        plot_memory_map(ax, data, dbg=dbg)  # defensive & annotated
        return ax  # return for caller to show() or embed in their figure
    def save_cog_map_snapshot(self, t_step: int, out_dir: str = "dbg/cogmap",
                          pad: float = 1.25, min_span: float | None = None,
                          dpi: int = 160, annotate_ids: bool = True):
        """
        Save a PNG of the current cognitive map (nodes + links) at:
        {out_dir}/cogmap_t{t_step:03d}.png

        - Uses the project's plotter for consistency.
        - Centers the view and enforces a minimum span so early maps aren't a tiny dot.
        - Draws a dashed rectangle around the current view.
        """
        import os
        from pathlib import Path
        import numpy as np
        import matplotlib
        matplotlib.use("Agg")
        import matplotlib.pyplot as plt
        from matplotlib.patches import Rectangle

        # --- sanity ---
        if getattr(self, "cog", None) is None or getattr(self.cog, "mg", None) is None:
            print("[save_cog_map_snapshot] cognitive graph not initialized (planner.cog.mg is None).")
            return None

        # Build the structure your plotter expects (your own helper)
        data = self.get_memory_map_data(dbg=False)
        if data.get("current_exp_id", -1) < 0 and not data.get("exps_GP"):
            print("[save_cog_map_snapshot] nothing to plot yet; skipping.")
            return None

        # --- gather xy points robustly (drop theta if present) ---
        def _xy(p):
            try:
                # p could be (x,y), (x,y,th), numpy array, etc.
                if p is None:
                    return None
                if isinstance(p, (list, tuple, np.ndarray)):
                    if len(p) >= 2:
                        return float(p[0]), float(p[1])
            except Exception:
                pass
            return None

        xy = []

        for p in data.get("exps_GP", []):
            q = _xy(p);  xy.append(q) if q else None
        for p in data.get("ghost_exps_GP", []):
            q = _xy(p);  xy.append(q) if q else None

        # include link endpoints too (sometimes you have links before vc-decay-filled nodes)
        L = data.get("exps_links", [])
        for i in range(0, len(L), 2):
            q0 = _xy(L[i]);   q1 = _xy(L[i+1]) if i+1 < len(L) else None
            if q0: xy.append(q0)
            if q1: xy.append(q1)

        # include current markers
        q = _xy(data.get("current_exp_GP"));  xy.append(q) if q else None
        q = _xy(data.get("current_GP"));      xy.append(q) if q else None

        if not xy:
            print("[save_cog_map_snapshot] no xy points assembled; skipping.")
            return None

        P = np.asarray(xy, dtype=float)  # (N,2)

        # --- compute nice view bounds (center + margin) ---
        xmin, ymin = P.min(axis=0)
        xmax, ymax = P.max(axis=0)
        cx, cy     = (xmin + xmax) * 0.5, (ymin + ymax) * 0.5

        # set a sensible minimum span (use map size if available, else 6.0)
        emap      = self.cog.mg.experience_map
        default_min = max(6.0, float(getattr(emap, "DIM_XY", 10)) * 0.5)
        span_min  = default_min if min_span is None else float(min_span)

        span = max(xmax - xmin, ymax - ymin, span_min)
        half = 0.5 * span * float(pad)

        # --- draw with the project's plotter for consistent style ---
        try:
            from navigation_model.visualisation_tools import plot_memory_map
            fig, ax = plt.subplots(figsize=(5.0, 5.0), dpi=dpi)
            plot_memory_map(ax, data, dbg=False)
        except Exception as e:
            # Fallback (very simple but still useful)
            fig, ax = plt.subplots(figsize=(5.0, 5.0), dpi=dpi)
            ax.set_title("Experience Map")
            ax.set_aspect("equal", "box")
            ax.grid(True)
            xs = [p[0] for p in P]; ys = [p[1] for p in P]
            ax.scatter(xs, ys, s=50, edgecolor="k", linewidth=0.4)

            # draw links if present
            for i in range(0, len(L), 2):
                q0 = _xy(L[i]); q1 = _xy(L[i+1]) if i+1 < len(L) else None
                if q0 and q1:
                    ax.plot([q0[0], q1[0]], [q0[1], q1[1]], linewidth=1.0, alpha=0.8)

        # enforce our view window and frame rectangle
        ax.set_xlim(cx - half, cx + half)
        ax.set_ylim(cy - half, cy + half)
        rect = Rectangle((cx - half, cy - half), 2*half, 2*half,
                        fill=False, linewidth=1.0, linestyle="--", color="0.6")
        ax.add_patch(rect)

        # emphasize current experience
        cur_gp = _xy(data.get("current_exp_GP"))
        if cur_gp is not None:
            x0, y0 = cur_gp
            ax.plot([x0], [y0], marker="x", color="r", markersize=8, mew=2)
            if annotate_ids:
                ax.text(x0, y0, str(data.get("current_exp_id", "")),
                        color="red", fontsize=9, weight="bold", ha="left", va="bottom")

            # facing arrow if we can
            try:
                cur = getattr(emap, "current_exp", None)
                if cur is not None:
                    import math
                    dx, dy = math.cos(cur.facing_rad), math.sin(cur.facing_rad)
                    s = 0.8
                    ax.arrow(cur.x_m, cur.y_m, s*dx, s*dy,
                            width=0.02, head_width=0.25, head_length=0.25,
                            color="tab:red", length_includes_head=True, alpha=0.7)
            except Exception:
                pass

        # annotate *all* node ids if requested (helpful early on)
        if annotate_ids and getattr(emap, "exps", None):
            for e in emap.exps:
                ax.text(e.x_m, e.y_m, f"{e.id}", fontsize=7, color="0.25", ha="center", va="center")

        Path(out_dir).mkdir(parents=True, exist_ok=True)
        out_path = os.path.join(out_dir, f"cogmap_t{t_step:03d}.png")
        fig.tight_layout()
        fig.savefig(out_path, bbox_inches="tight")
        plt.close(fig)
        print(f"[save_cog_map_snapshot] saved {out_path}")
        return out_path
    def _resolve_memory_graph(self, root=None):
        """
        Try to find the same graph object your viz uses.
        Looks in the common places; returns the first non-None.
        """
        candidates = [
            getattr(self, "_last_memory_graph", None),
            getattr(self, "memory_graph", None),
            getattr(root, "memory_graph", None),

            # common containers
            getattr(getattr(self, "navigation_model", None), "memory_graph", None),
            getattr(getattr(self, "navigation_model", None), "graph", None),
            getattr(self, "experience_map", None),
            getattr(self, "emap", None),
            getattr(self, "exp_map", None),
            getattr(self, "cognitive_graph", None),
            getattr(self, "cog_graph", None),
        ]
        for g in candidates:
            if g is not None:
                return g
        return None


    def _collect_graph_nodes_and_weights(self, memory_graph):
        """
        Returns (node_xy_list, weight_list). If weights unavailable, returns ones.
        Mirrors the patterns viz code typically uses, and falls back to lots of
        common graph shapes (NetworkX, dicts, lists of objects, experience maps).
        Applies optional coord transform hook if provided.
        """
        out_xy, out_w = [], []

        # Optional coord transform hook to match your viz frame
        coord_tf = getattr(self, "graph_coord_transform", None)
        def _tf(xy):
            if callable(coord_tf):
                try:
                    return tuple(map(int, coord_tf(tuple(xy))))
                except Exception:
                    pass
            return (int(xy[0]), int(xy[1]))

        # Optional explicit provider (strongest signal)
        provider = getattr(self, "graph_nodes_provider", None)
        if callable(provider):
            try:
                items = list(provider())
                for it in items:
                    if isinstance(it, (tuple, list)):
                        if len(it) >= 3: x, y, w = it[0], it[1], float(it[2])
                        elif len(it) >= 2: x, y, w = it[0], it[1], 1.0
                        else: continue
                        out_xy.append(_tf((x, y))); out_w.append(w)
                if out_xy: return out_xy, out_w
            except Exception:
                pass

        G = memory_graph
        if G is None:
            return [], []

        # Unwrap one layer (.G or .graph) — matches how many viz funcs stash the nx graph
        for attr in ("G", "graph"):
            if hasattr(G, attr):
                try:
                    inner = getattr(G, attr)
                    if inner is not None:
                        G = inner
                        break
                except Exception:
                    pass

        # --- NetworkX-like ---
        try:
            # (a) nodes(data=True)
            try:
                got = False
                for _, data in G.nodes(data=True):
                    if not data: 
                        continue
                    xy = None
                    if "x" in data and "y" in data:               xy = (data["x"], data["y"])
                    elif "pose" in data and len(data["pose"])>=2: xy = (data["pose"][0], data["pose"][1])
                    elif "pos"  in data and len(data["pos"]) >=2: xy = (data["pos"][0],  data["pos"][1])
                    elif "xy"   in data and len(data["xy"])  >=2: xy = (data["xy"][0],   data["xy"][1])
                    elif "coord" in data and len(data["coord"])>=2: xy=(data["coord"][0], data["coord"][1])
                    elif "row" in data and "col" in data:         xy = (data["col"], data["row"])
                    if xy is None: 
                        continue
                    w = float(data.get("weight", 1.0))
                    out_xy.append(_tf(xy)); out_w.append(w); got = True
                if got: 
                    return out_xy, out_w
            except TypeError:
                pass

            # (b) pos maps commonly used in viz code
            try:
                # tolerant access without importing networkx
                pos_map = None
                for key in ("pos", "positions", "node_pos", "node_positions"):
                    pos_map = getattr(G, key, None)
                    if pos_map: break
                # networkx-style attribute dicts: G.nodes[n]['pos']
                if not pos_map:
                    # try reading attributes from nodes directly
                    # (if nodes() works but data=True path didn't give coords)
                    for nid in list(G.nodes()):
                        try:
                            data = G.nodes[nid]
                            if isinstance(data, dict) and "pos" in data and len(data["pos"])>=2:
                                if not pos_map: pos_map = {}
                                pos_map[nid] = data["pos"]
                        except Exception:
                            continue
                if pos_map:
                    for nid, p in pos_map.items():
                        if isinstance(p, (tuple, list)) and len(p) >= 2:
                            out_xy.append(_tf((p[0], p[1]))); out_w.append(1.0)
                    if out_xy: 
                        return out_xy, out_w
            except Exception:
                pass
        except Exception:
            pass

        # --- dict-like .nodes: dict[id] -> object/dict ---
        nodes_obj = getattr(G, "nodes", None)
        if isinstance(nodes_obj, dict):
            got = False
            for _, obj in nodes_obj.items():
                xy = None; w = 1.0
                if hasattr(obj, "x") and hasattr(obj, "y"):
                    xy = (getattr(obj, "x"), getattr(obj, "y"))
                elif hasattr(obj, "pose") and isinstance(getattr(obj, "pose"), (tuple, list)) and len(getattr(obj, "pose"))>=2:
                    p = getattr(obj, "pose"); xy = (p[0], p[1])
                elif isinstance(obj, dict):
                    if "x" in obj and "y" in obj:                   xy = (obj["x"], obj["y"])
                    elif "pose" in obj and len(obj["pose"]) >= 2:   xy = (obj["pose"][0], obj["pose"][1])
                    elif "pos" in obj and len(obj["pos"]) >= 2:     xy = (obj["pos"][0],  obj["pos"][1])
                    elif "xy" in obj and len(obj["xy"]) >= 2:       xy = (obj["xy"][0],   obj["xy"][1])
                    elif "row" in obj and "col" in obj:             xy = (obj["col"], obj["row"])
                if xy is not None:
                    out_xy.append(_tf(xy)); out_w.append(w); got = True
            if got: 
                return out_xy, out_w

        # --- arrays commonly used in viz overlays / experience maps ---
        for attr in ("experiences", "nodes", "anchors", "vertices", "points"):
            arr = getattr(G, attr, None)
            if isinstance(arr, (list, tuple)):
                got = False
                for obj in arr:
                    xy = None; w = 1.0
                    if hasattr(obj, "x") and hasattr(obj, "y"):
                        xy = (getattr(obj, "x"), getattr(obj, "y"))
                    elif hasattr(obj, "pose") and isinstance(getattr(obj, "pose"), (tuple, list)) and len(getattr(obj, "pose"))>=2:
                        p = getattr(obj, "pose"); xy = (p[0], p[1])
                    elif isinstance(obj, dict):
                        if "x" in obj and "y" in obj:                 xy = (obj["x"], obj["y"])
                        elif "pose" in obj and len(obj["pose"]) >= 2: xy = (obj["pose"][0], obj["pose"][1])
                        elif "pos" in obj and len(obj["pos"]) >= 2:   xy = (obj["pos"][0],  obj["pos"][1])
                        elif "xy" in obj and len(obj["xy"]) >= 2:     xy = (obj["xy"][0],   obj["xy"][1])
                        elif "row" in obj and "col" in obj:           xy = (obj["col"], obj["row"])
                    if xy is not None:
                        out_xy.append(_tf(xy)); out_w.append(w); got = True
                if got: 
                    return out_xy, out_w

        # --- raw iterable of tuples/lists (sometimes viz pipelines build these) ---
        if isinstance(G, (list, tuple)):
            try:
                for it in G:
                    if isinstance(it, (tuple, list)):
                        if len(it) >= 3: x, y, w = it[0], it[1], float(it[2])
                        elif len(it) >= 2: x, y, w = it[0], it[1], 1.0
                        else: continue
                        out_xy.append(_tf((x, y))); out_w.append(w)
                if out_xy:
                    return out_xy, out_w
            except Exception:
                pass

        return [], []

    # --- 2) Save an overlay of env + cognitive graph, SPAWN-anchored (translation only) ---
    def save_cog_map_vs_env(
        self,
        env,
        t_step: int,
        out_dir: str = "dbg/cogmap",
        tile_size: int = 32,
        annotate_ids: bool = True,
        dpi: int = 160,
    ):
        """
        Snapshot the environment image and the cognitive graph *separately* for
        later, flexible overlay in an external script. No in-loop alignment/overlay.

        Outputs (inside {out_dir}/t{t_step:04d}/):
        - env_tXXXX.png           : full environment render
        - cog_tXXXX.png           : transparent PNG of just the cognitive graph
        - snapshot_tXXXX.json     : metadata linking both + cog geometry/bbox

        The JSON contains:
        {
            "t_step": int,
            "timestamp": ISO8601,
            "env": {
            "img_path": str,
            "pixel_size": [H, W],
            "grid_size": [Wc, Hc],   # grid cells
            "tile_size": int
            },
            "cog": {
            "png_path": str,         # transparent graph image
            "nodes": [{"id": int|None, "x": float, "y": float,
+            "decay": float|None, "real_pose": [x,y,dir]|None,
+            "place_kind": "ROOM"|"CORRIDOR"|"UNKNOWN"|None,
+            "room_color": str|None,           # e.g., "purple"
+            "grid_xy": [gx,gy]|None}, ...],
            "links_xy": [[x1,y1],[x2,y2], ...],  # pairs, same order as drawn
            "current_exp_id": int|None,
            "bbox": [minx, maxx, miny, maxy]     # native cog coordinate bbox
            },
            "spawn": {"x": int, "y": int, "dir": int}|null
        }

        Later you can rotate/mirror/scale/translate the cog layer over the env image.
        """
        import os, json, math, datetime
        from pathlib import Path
        import numpy as np

        # --- Render env image ---
        try:
            env_img = env.render(mode="rgb_array")
        except TypeError:
            env_img = env.render("rgb_array", tile_size=tile_size)

        H, W = env_img.shape[:2]
        # Best-effort grid size from env if present; fallback to pixel/tile
        Wc = int(getattr(env, "width",  max(1, W // max(1, tile_size))))
        Hc = int(getattr(env, "height", max(1, H // max(1, tile_size))))

        # --- Access cognitive graph safely ---
        if not getattr(self, "cog", None) or not getattr(self.cog, "mg", None):
            print("[save_cog_map_vs_env] WARN: cognitive graph not initialized; saving env only.")
            cog_available = False
            mg = None
            emap = None
        else:
            cog_available = True
            mg = self.cog.mg
            emap = getattr(mg, "experience_map", None)
            if emap is None:
                cog_available = False
                print("[save_cog_map_vs_env] WARN: mg.experience_map missing; saving env only.")

        # --- Snapshot folder ---
        snap_dir = Path(out_dir) / f"t{t_step:04d}"
        snap_dir.mkdir(parents=True, exist_ok=True)

        # --- Save ENV image ---
        # Use matplotlib's Agg to avoid display backends
        import matplotlib
        matplotlib.use("Agg")
        import matplotlib.pyplot as plt

        env_png = snap_dir / f"env_t{t_step:04d}.png"
        fig_env, ax_env = plt.subplots(1, 1, figsize=(W/80, H/80), dpi=80)  # keep pixels 1:1-ish
        ax_env.imshow(env_img, origin="upper")
        ax_env.axis("off")
        fig_env.savefig(env_png, bbox_inches="tight", pad_inches=0)
        plt.close(fig_env)

        # --- Build cognitive graph geometry ---
        nodes = []
        links_xy = []
        current_exp_id = None
        spawn_pose = None
        bbox = None
        cog_png = None

        if cog_available:
            # Prefer your own helper if present
            try:
                data = self.get_memory_map_data(dbg=False)
            except Exception:
                data = {}

            # Nodes: try emap.exps (with ids & real_pose) as the authoritative source
            exps = list(getattr(emap, "exps", []) or [])
            have_exps_objects = len(exps) > 0

            # Fallback to data['exps_GP'] if no objects
            if have_exps_objects:
                # Build a decay lookup if available (align by id or by order)
                decays = data.get("exps_decay", None)
                # If decay length matches number of exps, align by index; else None
                decay_by_index = decays if isinstance(decays, (list, tuple)) and len(decays) == len(exps) else None

                for idx, e in enumerate(exps):
                    rp  = getattr(e, "real_pose", None)
                    gxy = getattr(e, "grid_xy", None)
                    if isinstance(gxy, (list, tuple)) and len(gxy) == 2:
                        try:
                            gxy_out = [int(gxy[0]), int(gxy[1])]
                        except Exception:
                            gxy_out = [float(gxy[0]), float(gxy[1])]
                    else:
                        gxy_out = None

                    nodes.append({
                        "id": int(getattr(e, "id", idx)) if getattr(e, "id", None) is not None else None,
                        "x": float(getattr(e, "x_m", 0.0)),
                        "y": float(getattr(e, "y_m", 0.0)),
                        "decay": float(decay_by_index[idx]) if decay_by_index is not None else None,
                        "real_pose": [int(rp[0]), int(rp[1]), int(rp[2])] if (isinstance(rp, (list, tuple)) and len(rp) >= 3) else None,
                        # --- NEW semantic fields ---
                        "place_kind": getattr(e, "place_kind", None),
                        "room_color": getattr(e, "room_color", None),
                        "grid_xy": gxy_out,
                    })
            else:
                # Use the raw GP list if that's what you maintain
                # Use the raw GP list if that's what you maintain
                gps = data.get("exps_GP", []) or []
                decays = data.get("exps_decay", [])
                for i, (x, y) in enumerate(gps):
                    dec = float(decays[i]) if i < len(decays) else None
                    nodes.append({
                        "id": None,
                        "x": float(x),
                        "y": float(y),
                        "decay": dec,
                        "real_pose": None,
                        # --- semantic fields unavailable in this fallback ---
                        "place_kind": None,
                        "room_color": None,
                        "grid_xy": None,
                    })
            # Links: use coords as saved by your helper
            raw_links = data.get("exps_links", []) or []
            # Ensure they are pairs of points [[x1,y1],[x2,y2], [x3,y3],[x4,y4], ...]
            if len(raw_links) >= 2:
                for i in range(0, len(raw_links) - 1, 2):
                    a = raw_links[i]
                    b = raw_links[i + 1]
                    if a is None or b is None or len(a) < 2 or len(b) < 2:
                        continue
                    links_xy.append([float(a[0]), float(a[1])])
                    links_xy.append([float(b[0]), float(b[1])])

            # Current node id if available
            current_exp_id = data.get("current_exp_id", None)
            try:
                current_exp_id = int(current_exp_id) if current_exp_id is not None else None
            except Exception:
                current_exp_id = None

            # Spawn (best-effort)
            spawn_pose = getattr(emap, "spawn_pose_real", None)
            if spawn_pose is None:
                # fallback to current agent pose if env provides it
                try:
                    spawn_pose = (int(env.agent_pos[0]), int(env.agent_pos[1]), int(getattr(env, "agent_dir", 0)))
                    print("[save_cog_map_vs_env] INFO: using env.agent_pos as spawn fallback.")
                except Exception:
                    spawn_pose = None

            # Compute bbox in *native cog coords*
            if nodes:
                xs = [n["x"] for n in nodes]
                ys = [n["y"] for n in nodes]
                minx, maxx = float(min(xs)), float(max(xs))
                miny, maxy = float(min(ys)), float(max(ys))
                # Add a small margin so the PNG isn't tight against the edges
                mx = (maxx - minx) * 0.05 if maxx > minx else 0.5
                my = (maxy - miny) * 0.05 if maxy > miny else 0.5
                bbox = [minx - mx, maxx + mx, miny - my, maxy + my]
            else:
                bbox = [0.0, 1.0, 0.0, 1.0]  # dummy, still usable

            # --- Save transparent COG PNG ---
            cog_png = snap_dir / f"cog_t{t_step:04d}.png"
            fig_c, ax_c = plt.subplots(1, 1, dpi=dpi)
            # Transparent everything
            fig_c.patch.set_alpha(0.0)
            ax_c.set_facecolor((1, 1, 1, 0.0))

            # Plot links first
            if links_xy:
                for i in range(0, len(links_xy) - 1, 2):
                    x1, y1 = links_xy[i]
                    x2, y2 = links_xy[i + 1]
                    ax_c.plot([x1, x2], [y1, y2], color="k", linewidth=1.2, alpha=0.9)

            # Plot nodes
            if nodes:
                nxy = np.array([[n["x"], n["y"]] for n in nodes], dtype=float)
                ax_c.scatter(nxy[:, 0], nxy[:, 1],
                            s=50, c="tab:blue", edgecolors="k", linewidths=0.5, alpha=0.95)
                if annotate_ids:
                    for n in nodes:
                        if n["id"] is not None:
                            ax_c.text(n["x"], n["y"], f"{n['id']}", fontsize=7,
                                    ha="center", va="center", color="0.2", alpha=0.9)

            # Highlight current node if we have it
            if current_exp_id is not None and nodes:
                # try to find coords for current id
                for n in nodes:
                    if n["id"] == current_exp_id:
                        ax_c.plot([n["x"]], [n["y"]], marker="x", color="r", mew=2, ms=9, alpha=0.95)
                        break

            # Use bbox so the image aligns with native cog units
            ax_c.set_xlim(bbox[0], bbox[1])
            ax_c.set_ylim(bbox[2], bbox[3])
            ax_c.set_aspect("equal", "box")
            ax_c.axis("off")
            fig_c.savefig(cog_png, transparent=True, bbox_inches="tight", pad_inches=0)
            plt.close(fig_c)

        # --- Write JSON snapshot manifest ---
        manifest = {
            "t_step": int(t_step),
            "timestamp": datetime.datetime.now().isoformat(timespec="seconds"),
            "env": {
                "img_path": str(env_png),
                "pixel_size": [int(H), int(W)],
                "grid_size": [int(Wc), int(Hc)],
                "tile_size": int(tile_size),
            },
            "cog": {
                "png_path": str(cog_png) if cog_png else None,
                "nodes": nodes,
                "links_xy": links_xy,           # drawn as pairs [A,B],[C,D],...
                "current_exp_id": current_exp_id,
                "bbox": bbox,
            },
            "spawn": {"x": int(spawn_pose[0]), "y": int(spawn_pose[1]), "dir": int(spawn_pose[2])} if spawn_pose else None,
        }

        man_path = snap_dir / f"snapshot_t{t_step:04d}.json"
        with open(man_path, "w", encoding="utf-8") as f:
            json.dump(manifest, f, indent=2)

        print(f"[save_cog_map_vs_env] Snapshot saved:\n  {env_png}\n  {cog_png if cog_png else '(no cog)'}\n  {man_path}")
        return str(man_path)

    




import gym
import cv2
class DictResizeObs(gym.ObservationWrapper):
    def __init__(self, env, out_hw=(64,64)):
        super().__init__(env)
        self.out_hw = out_hw              # (H,W)

    def observation(self, obs):
        assert isinstance(obs, dict) and "image" in obs, \
               "expect dict with 'image' key"
        img = obs["image"]                               # (H,W,3) uint8
        img = cv2.resize(img, self.out_hw[::-1],
                         interpolation=cv2.INTER_AREA)
        obs["image"] = img
        return obs

if __name__ == "__main__":
    import time, random, numpy as np, torch
    import gym, gym_minigrid
    from collections import deque
    from itertools import islice
    from gym_minigrid.minigrid import Wall
    from gym_minigrid.wrappers import RGBImgPartialObsWrapper, ImgActionObsWrapper
    from world_model_utils import DictResizeObs, WMPlanner, State

    # ---------- config ----------
    CKPT        = "runs/mg_collision/20250704-220917/ckpt/iter05000.pt"
    N_STEPS     = 1290            # run the novelty policy for this many env steps
    LOOKAHEAD   = 7             # A* novelty horizon
    K_RECENT    = 30            # how many recent embeddings to compare against
    METRIC      = "cos"         # "kl" | "cos" | "l2"
    DEVICE      = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Debug/vis knobs
    DEBUG_PRINT          = True   # keep tree/top-K/action scores logs
    VIZ_TREE             = False   # save decision tree collage from novelty_astar_plan
    VIZ_EVERY            = 15      # save imagined rollout strip and replay history every k steps (set 1 to save each step)
    VIZ_OUTDIR           = "dbg"  # where to write images
    SAVE_HISTORY_LENGTHS = (30, 60, 90, 120,150,180,210,240,270,300,330,360,390,410,440,470,500,530,560,590,620,650,685,710,740,760,790,890,990,1090,1190,1290)

    # ---------- boot world model & env --------
    from pathlib import Path
    ROOT = Path(__file__).resolve().parents[1]  # repo root (…/hierarchical-nav/)
    MEMCFG = ROOT / "navigation_model/Services/memory_service/memory_graph_config.yml"

    planner = WMPlanner(ckpt=CKPT,device=str(DEVICE),memory_config=str(MEMCFG))
    wm = planner.wm
    print("✓ world-model loaded")

    env = gym.make("MiniGrid-4-tiles-ad-rooms-v0", rooms_in_row=7, rooms_in_col=7, max_steps=None)
    env.seed(218)
    env = RGBImgPartialObsWrapper(env)
    env = ImgActionObsWrapper(env)
    env = DictResizeObs(env, (64, 64))
    planner.env=env
    
    # ---------- helpers ----------
    def decode_policy_onehot(onehot_tensor: torch.Tensor) -> list[str]:
        """
        Fallback decoder if novelty_astar_plan returns a (T,3) plan.
        """
        idx_to_act = {0: "forward", 1: "right", 2: "left"}
        if onehot_tensor is None or onehot_tensor.numel() == 0:
            return []
        acts = []
        for row in onehot_tensor.cpu():
            j = int(row.argmax().item())
            acts.append(idx_to_act[j])
        return acts

    def dreamer_decode_from_belief(planner, wm, belief_zd, *, to_01: bool = True):
        import torch
        from torch.distributions import Distribution

        if belief_zd is None:
            return None

        z, h = belief_zd
        with torch.no_grad():
            out = wm.decoder(z, h)
            x = out.mean if isinstance(out, Distribution) else out
            x = torch.as_tensor(x).detach().float()
            if x.ndim == 4 and x.shape[0] == 1:
                x = x[0]
            if x.ndim == 2:
                x = x.unsqueeze(0).repeat(3, 1, 1)
            elif x.ndim == 3 and x.shape[0] not in (1, 3) and x.shape[-1] in (1, 3):
                x = x.permute(2, 0, 1).contiguous()
            elif x.ndim == 3 and x.shape[0] == 1:
                x = x.repeat(3, 1, 1)
            if to_01:
                x = (x + 0.5).clamp(0.0, 1.0)
            return x.cpu()

    def append_to_replay_buffer(buf: deque, obs: dict, action_int: int, belief_zd):
        """
        Store Dreamer belief, embeddings, decoded prediction, and enhanced predictions.
        """
        z_embed = planner.dreamer_embed_fn(belief_zd)  # (Z,) flattened image embedding in [0,1]
        decoded = dreamer_decode_from_belief(planner, wm, belief_zd)  # (3,H,W) or None
        enhanced_preds = planner.build_enhanced_perception(
            wm, belief_zd,
            combos=[('left',), ('right',), ('left','left'), ('right','right')]
        )
        entry = {
            "node_id": None,
            "real_pose": obs.get("pose"),
            "imagined_pose": None,
            "real_image": obs.get("image"),
            "imagined_image": None,
            "action": int(action_int),
            "dreamer_z": z_embed,
            "belief_zd": belief_zd,
            "decoded_image": decoded,
            "enhanced_preds": enhanced_preds,
        }
        buf.append(entry)

    def visualize_policy_probe(planner, wm, belief_zd, obs, action_names, t_step,
                               out_dir="dbg", include_start=True):
        import torch
        from pathlib import Path
        from torchvision.utils import save_image

        if not action_names:
            return None

        real_frame = torch.as_tensor(obs["image"]).permute(2, 0, 1).float() / 255.0 - 0.5
        frames_pred = planner.render_plan(wm, belief_zd, action_names, include_last=True)
        frames = [real_frame.detach().cpu()] + [f.detach().cpu() for f in frames_pred]
        Path(out_dir).mkdir(parents=True, exist_ok=True)

        short = "".join(a[0].upper() for a in action_names)
        out_path = f"{out_dir}/probe_t{t_step:02d}_{short}.png"
        save_image(torch.stack(frames), out_path, nrow=len(frames), normalize=True, scale_each=False)
        print(f"saved {out_path}")
        return out_path

    def save_replay_image_history(replay_buffer, planner, wm, t_step, Ns=(5,10,15,20), out_dir="dbg/history"):
        import torch
        from pathlib import Path
        from torchvision.utils import save_image

        Path(out_dir).mkdir(parents=True, exist_ok=True)

        def to_chw_real(img_hwc):
            t = torch.as_tensor(img_hwc).permute(2,0,1).float() / 255.0 - 0.5
            return t

        for N in Ns:
            tail = list(islice(reversed(replay_buffer), 0, N))
            if not tail:
                continue
            tail = list(reversed(tail))

            reals, preds = [], []
            for st in tail:
                ri = st.get("real_image", None)
                bi = st.get("belief_zd", None)
                if ri is None:
                    continue
                reals.append(to_chw_real(ri))
                di = st.get("decoded_image", None)
                if di is None and bi is not None:
                    di = dreamer_decode_from_belief(planner, wm, bi)
                if di is None:
                    di = reals[-1]
                preds.append(di)

            if not reals:
                continue

            frames = reals + preds
            out_path = f"{out_dir}/history_t{t_step:02d}_N{len(reals)}.png"
            save_image(torch.stack(frames), out_path, nrow=len(reals), normalize=True, scale_each=False)
            print(f"saved {out_path}")

    
    # ---------- init replay & belief ----------
    replay_buffer = deque(maxlen=30)
    obs = env.reset()
    # --- at episode start
    episode_id=1
    collage = DualPathCollage(
        out_dir="dbg/collages",
        tag=f"ep{episode_id:03d}",
        tile=64,          # match your Dreamer decode size if 64x64
        cols=20,          # tweak to taste
        live_write=True,  # set False if you only want the final PNGs
        live_every=1,     # write after every step when live
        annotate_idx=True,
        planner=planner,  # so it can call dreamer_decode_from_belief
        wm=wm
    )


    # bootstrap belief from first observation
    frame = obs["image"].transpose(2,0,1) / 255.0 - 0.5
    belief = planner.wm_update_belief(wm, prev_z_d=None, frame_rgb=frame, prev_action_onehot=None)

    # take one forward step to match your current flow, then update belief and seed buffer
    obs, _, done, _ = env.step(env.actions.forward)
    prev_act_1h = planner.onehot("forward", wm)
    belief = planner.update_belief_from_obs(obs, belief, prev_act_1h)
    append_to_replay_buffer(replay_buffer, obs, env.actions.forward, belief)
    pose_xyz = tuple(obs["pose"])
    print(obs["image"].shape)
    collage.add_step(real_img=obs["image"], belief_zd=belief)

    planner.update_cog(obs["image"], prev_act_1h,pose_xyz, place_post=None)
    print(planner.get_cog_nodes())

    # maps between env ints and names
    act_to_name = {
        env.actions.left: "left",
        env.actions.right: "right",
        env.actions.forward: "forward",
    }
    name_to_act = {v: k for k, v in act_to_name.items()}

    def choose_action_from_novastar(result):
        """
        Accepts either:
          • (best_action: str, scores: dict, top_paths: list)     ← current utils
          • torch.Tensor one-hot plan (T,3), or list[str] plan    ← older utils
        Returns: (action_name: str, voted_scores: dict[str,float] | None)
        """
        import torch
        # Newer signature: tuple where first item is a string action name
        if isinstance(result, tuple) and len(result) >= 1 and isinstance(result[0], str):
            best_action = result[0]
            scores = result[1] if len(result) >= 2 and isinstance(result[1], dict) else None
            return best_action, scores

        # Older signature: full plan
        if isinstance(result, torch.Tensor):
            seq = decode_policy_onehot(result)
        elif isinstance(result, (list, tuple)) and all(isinstance(a, str) for a in result):
            seq = list(result)
        else:
            seq = []

        if seq:
            return seq[0], None
        return None, None

    print("\n=== Novelty-driven rollout (every step) ===")
    for t in range(1, N_STEPS + 1):
        start = State(*obs["pose"])

        # --- run exploration algorithm to pick ONE next action ---
        result = planner.novelty_astar_plan(
            wm=wm,
            belief_zd=belief,
            start_state=start,
            lookahead=LOOKAHEAD,
            replay_buffer=replay_buffer,
            K_recent=K_RECENT,
            metric=METRIC,
            verbose=DEBUG_PRINT,
            viz_tree=VIZ_TREE,
            viz_outdir=VIZ_OUTDIR,
            viz_tag=f"t{t:02d}",
            # weights / uncertainty-rollout knobs
            w_p2e=1.0,             # turn on K-rollout cost
            p2e_K=5,
            p2e_mode="temporal",   # your requested variant
            p2e_metric="l2",
            p2e_roll_from="parent" # start from node belief and apply 'act' K times
        )

        action_name, scores = choose_action_from_novastar(result)

        # Fallback if planner returns nothing:
        if action_name is None:
            # try the _last_action_scores if available
            scores_attr = getattr(planner, "_last_action_scores", None)
            if isinstance(scores_attr, dict) and scores_attr:
                action_name = max(scores_attr.items(), key=lambda kv: kv[1])[0]
            else:
                # last-resort, simple "don't collide" heuristic
                front_pos = env.front_pos
                front_cell = env.grid.get(*front_pos)
                action_name = "left" if isinstance(front_cell, Wall) else "forward"


        env_act = name_to_act[action_name]

        # --- take the step ---
        next_obs, _, done, _ = env.step(env_act)

        # --- update belief with observed frame and prev action ---
        prev_act_1h = planner.onehot(action_name, wm)
        belief = planner.update_belief_from_obs(next_obs, belief, prev_act_1h)
        pose_xyz = tuple(next_obs["pose"])
        collage.add_step(real_img=obs["image"], belief_zd=belief)
        planner.update_cog(next_obs["image"], prev_act_1h,pose_xyz, place_post=None)
        if planner.emap.current_exp is not None:
            e = planner.emap.current_exp
            print(f"[PLACE] Exp{e.id} at {e.grid_xy}: {e.place_kind}"
                + (f" ({e.room_color})" if e.room_color else ""))
        # --- push transition to replay buffer (includes decoded prediction) ---
        append_to_replay_buffer(replay_buffer, next_obs, env_act, belief)

        # --- debug prints / tree / top-K paths ---
        if DEBUG_PRINT:
            print(f"[t={t:02d}] chose action → {action_name}")
            # if internal records exist, echo them (kept from your probe block)
            tree = getattr(planner, "_last_decision_tree", None)
            if hasattr(planner, "_last_top_paths"):
                print("\n=== TOP-K PATHS USED FOR VOTING ===")
                for i, rec in enumerate(planner._last_top_paths, 1):
                    a0 = rec["seq"][0] if rec["seq"] else "∅"
                    print(f"#{i:02d} G={rec['gain']:.3f} first={a0:<7} depth={rec['depth']} "
                          f"seq={rec['seq']} pruned={rec.get('pruned_reason')}")
            if hasattr(planner, "_last_action_scores"):
                print("Action vote scores:", getattr(planner, "_last_action_scores"))

        # --- visualizations (policy rollout + history strips) ---
        if (t % max(1, VIZ_EVERY)) == 0:
            # visualize 1-step policy just for local context; if you want longer, call
            # planner.rank_paths(...) to grab the best sequence and pass it in.
            visualize_policy_probe(planner, wm, belief, next_obs, [action_name], t_step=t,
                                   out_dir=VIZ_OUTDIR)
            save_replay_image_history(replay_buffer, planner, wm, t_step=t,
                                      Ns=SAVE_HISTORY_LENGTHS, out_dir=f"{VIZ_OUTDIR}/history")
            planner.save_cog_map_snapshot(t_step=t, out_dir=f"{VIZ_OUTDIR}/cogmap")
            planner.save_cog_map_vs_env(env, t_step=t, out_dir="dbg/cogmap", tile_size=32, dpi=160)
            
            # advance obs pointer
        obs = next_obs

        # --- handle episode end robustly ---
        if done:
            obs = env.reset()
            frame = obs["image"].transpose(2,0,1) / 255.0 - 0.5
            belief = planner.wm_update_belief(wm, prev_z_d=None, frame_rgb=frame, prev_action_onehot=None)
            obs, _, done, _ = env.step(env.actions.forward)
            prev_act_1h = planner.onehot("forward", wm)
            belief = planner.update_belief_from_obs(obs, belief, prev_act_1h)
            replay_buffer.clear()
            collage.finalize()
            append_to_replay_buffer(replay_buffer, obs, env.actions.forward, belief)

    print("\nDone. (Executed novelty policy at every step.)")

""" def first_action_from_topk_paths(
        self,
        root,
        *,
        k: int = 10,
        include_pruned: bool = True,
        weight: str = "linear",     # "linear" | "harmonic" | "exp"
        alpha: float = 0.85,        # only used when weight="exp"
        require_min_depth: int = 1, # ignore empty sequences (no first action)
        verbose: bool = False,
    ):
        """ 
"""         Rank all root→terminal paths by a NEW score that ignores the builder's 'gain':
            score(path) = new_gain_from_steps
                        + B * dist_from_start
                        + C * dist_from_graph_nodes   (stub; 0.0 if not wired)

        new_gain_from_steps uses only per-step metrics stored in TreeNode:
            raw_nov      ≔ step_novelty + step_penalty
            p2e_bonus    ≔ inverted, sibling-normalized step_p2e in [0,1]
            step_value   ≔ (raw_nov - step_penalty) + Wp2e * p2e_bonus
            new_gain     ≔ Σ_t (γ_rank)^(t-1) * step_value_t

        Everything else (top-k, rank-weighted first-action voting) is unchanged.
        Returns:
            best_action : str   in {"left","right","forward"}
            action_scores : dict[str, float]
            top_paths : list[dict] enriched with 'score' and 'score_terms' """ """

        # -------------------------------
        # Tunables (readable from self.* if present; fall back to defaults)
        # -------------------------------
        Wp2e   = getattr(self, "rank_Wp2e", 1.0)         # weight for sibling-normalized p2e bonus in step_value
        gammaR = getattr(self, "rank_gamma", 1.0)        # per-step discount in new_gain_from_steps
        Bdist  = getattr(self, "rank_B", 0.10)           # B * distance from start
        Cgraph = getattr(self, "rank_C", 0.00)           # C * distance to graph nodes (stub—computed as 0.0 unless available)
        dist_metric = getattr(self, "rank_dist_metric", "manhattan")  # or "euclidean"

        # -------------------------------
        # Utilities: traverse the tree along a seq, compute sibling p2e stats, and distances
        # -------------------------------
        def _path_nodes_by_seq(root_node, seq):
            
            nodes = []
            cur = root_node
            prefix = []
            for a in seq:
                prefix.append(a)
                nxt = None
                # prefer exact-seq match for disambiguation
                for ch in cur.children:
                    if getattr(ch, "seq", None) == prefix:
                        nxt = ch
                        break
                if nxt is None:
                    # fallback: first child matching the action
                    for ch in cur.children:
                        if getattr(ch, "step_act", None) == a:
                            nxt = ch
                            break
                if nxt is None:
                    return []  # broken path
                nodes.append(nxt)
                cur = nxt
            return nodes

        def _sibling_p2e_norm_inverted(child_node, parent_node):
            
            try:
                sibs = [c for c in getattr(parent_node, "children", []) if hasattr(c, "step_p2e")]
                vals = [float(c.step_p2e) for c in sibs if (c.step_p2e is not None) and (float(c.step_p2e) >= 0.0)]
                v = float(child_node.step_p2e)
                if not vals or len(vals) == 1:
                    return 0.5
                vmin, vmax = min(vals), max(vals)
                if vmax <= vmin:
                    return 0.5
                # invert: best (smallest) p2e → 1.0 ; worst (largest) → 0.0
                return (vmax - v) / (vmax - vmin + 1e-8)
            except Exception:
                return 0.5

        def _dist(p0, p1):
            (x0, y0, _d0) = p0
            (x1, y1, _d1) = p1
            dx, dy = abs(x1 - x0), abs(y1 - y0)
            if dist_metric == "euclidean":
                return (dx * dx + dy * dy) ** 0.5
            return dx + dy  # manhattan default

        def _graph_distance_stub(node_pose):
            """""" Stub for future graph metrics. If you later wire a function like:
                self.distance_to_graph_nodes(pose) -> float
            we'll call it here. For now returns 0.0. """"""
            fn = getattr(self, "distance_to_graph_nodes", None)
            if callable(fn):
                try:
                    return float(fn(node_pose))
                except Exception:
                    return 0.0
            return 0.0

        # -------------------------------
        # 1) Get all paths (existing builder output)
        # -------------------------------
        ranked_all = self.rank_paths(root, include_pruned=include_pruned)  # DON'T trust 'gain' for ranking anymore

        # -------------------------------
        # 2) Compute NEW score per path (ignore 'gain')
        # -------------------------------
        start_pose = getattr(root, "pose", None) or (None, None, None)
        enriched = []
        for rec in ranked_all:
            seq   = rec.get("seq", [])
            depth = int(rec.get("depth", len(seq) or 0))
            if depth < require_min_depth:
                continue

            # follow the tree to get the actual nodes along this seq (need parent-child to do sibling norm)
            step_nodes = _path_nodes_by_seq(root, seq)
            if not step_nodes:
                # If we cannot reconstruct nodes, skip this path conservatively.
                continue

            # accumulate step contributions
            new_gain = 0.0
            parent = root
            for t, node in enumerate(step_nodes, start=1):
                # reconstruct raw_nov from stored parts
                step_pen = float(getattr(node, "step_penalty", 0.0))
                step_nov_minus_pen = float(getattr(node, "step_novelty", 0.0))
                raw_nov = step_nov_minus_pen + step_pen  # since step_novelty = raw_nov - pen

                # sibling-normalized, inverted p2e ∈ [0,1]
                p2e_bonus = _sibling_p2e_norm_inverted(node, parent)

                step_value = (raw_nov - step_pen) + (Wp2e * p2e_bonus)
                new_gain += (gammaR ** (t - 1)) * step_value
                parent = node

            # distance augments
            end_pose = rec.get("end_pose", getattr(step_nodes[-1], "pose", None))
            dist_start = _dist(start_pose, end_pose) if (start_pose[0] is not None and end_pose is not None) else 0.0
            dist_graph = _graph_distance_stub(end_pose)

            score = new_gain + Bdist * dist_start + Cgraph * dist_graph

            # stash for ranking + debugging
            rec2 = dict(rec)  # shallow copy original record
            rec2["score"] = float(score)
            rec2["score_terms"] = {
                "new_gain": float(new_gain),
                "dist_from_start": float(dist_start),
                "dist_from_graph_nodes": float(dist_graph),
                "B": float(Bdist),
                "C": float(Cgraph),
                "Wp2e": float(Wp2e),
                "gamma_rank": float(gammaR),
            }
            enriched.append(rec2)

        # -------------------------------
        # 3) Sort by NEW score (DESC best-first) and take top-k
        # -------------------------------
        ranked_desc = sorted(enriched, key=lambda r: r["score"], reverse=True)
        top_paths = ranked_desc[:k]

        # -------------------------------
        # 4) Rank-weighted FIRST-action voting (UNCHANGED)
        # -------------------------------
        actions = ("left", "right", "forward")
        scores = {a: 0.0 for a in actions}
        contrib = {a: [] for a in actions}

        def rank_weight(i: int) -> float:
            if weight == "harmonic":
                return 1.0 / (i + 1)        # 1, 1/2, 1/3, ...
            if weight == "exp":
                return float(alpha ** i)    # 1, α, α^2, ...
            return float(max(k - i, 1))     # linear: k, k-1, ..., 1

        for i, rec in enumerate(top_paths):
            seq = rec.get("seq", [])
            if len(seq) < require_min_depth:
                continue
            a0 = seq[0]
            if a0 not in scores:
                continue
            w = rank_weight(i)
            scores[a0] += w
            # keep debug consistent but show S=score instead of the builder's gain
            contrib[a0].append((i + 1, w, rec.get("score", 0.0), list(seq)))

        def tiebreak(key):
            # Prefer higher vote; then fixed order forward > left > right
            order = {"forward": 2, "left": 1, "right": 0}
            return (key[1], order.get(key[0], -1))

        best_action = max(scores.items(), key=tiebreak)[0]

        # -------------------------------
        # 5) Verbose debug
        # -------------------------------
        if verbose:
            print("\n[TOP-K FIRST-ACTION VOTING]")
            print(f"  k={k} weight={weight} alpha={alpha} include_pruned={include_pruned}")
            for i, rec in enumerate(top_paths, 1):
                a0 = rec["seq"][0] if rec["seq"] else "∅"
                s_terms = rec.get("score_terms", {})
                print(f"  #{i:02d} S={rec['score']:.3f}  first={a0:<7} depth={rec.get('depth')} seq={rec.get('seq')} "
                    f"pruned={rec.get('pruned_reason')}")
                print(f"        └ new_gain={s_terms.get('new_gain', 0.0):.3f}  "
                    f"B*dist_start={s_terms.get('B',0.0)*s_terms.get('dist_from_start',0.0):.3f}  "
                    f"C*dist_graph={s_terms.get('C',0.0)*s_terms.get('dist_from_graph_nodes',0.0):.3f}")

            print("  Action scores:")
            for a in actions:
                print(f"    {a:<7} → {scores[a]:.3f}  via {len(contrib[a])} hits")
                for rank, w, S, s in contrib[a]:
                    print(f"       - rank#{rank:02d} w={w:.3f} S={S:.3f} seq={s}")

        # External stash for inspection
        self._last_top_paths = top_paths
        self._last_action_scores = scores

    return best_action, scores, top_paths """    
```

`hierarchical-nav/env_specifics/env_calls.py`:

```py
import env_specifics.minigrid_maze_wt_aisles_doors.minigrid_maze_modules as minigrid_maze

def call_env_place_range(env:str, reduction:int=0) -> list :
    ''' Set the limits [x,y] of the place range (for hypothesis range)
    Minigrid_maze: considers the rooms number of tiles only
    '''
    if 'Minigrid_maze_aisle_wt_doors' in env:
        pose_options = minigrid_maze.minigrid_maze_aisles_doors_pose_range(env, reduction=reduction)
        return pose_options
    else:
        raise('call_env_place_range, '+ env + ' not implemented')

def call_env_entry_poses_assessment(env:str, entry_poses:list) -> list :
    ''' Determine which poses could possibly lead toward the place
    Minigrid_maze: considers the rooms number of tiles only
    '''
    if 'Minigrid_maze_aisle_wt_doors' in env:
        entry_poses = minigrid_maze.poses_leading_in_room(entry_poses)
        return entry_poses
    else:
        raise('call_env_place_range, '+ env + ' not implemented')

def call_env_remove_double_poses(env:str, poses:list) -> list:
    ''' 
    remove unecessary poses
    Minigrid_maze: consider only 1 pose by orientation, the 
    '''
    if 'Minigrid_maze_aisle_wt_doors' in env:
        door_poses = minigrid_maze.remove_double_orientation(poses)
        return door_poses
    else:
        raise('call_env_place_range, '+ env + ' not implemented')
    
def call_env_number_of_entry_points(env:str) -> int:
    ''' 
    how many entry points are expected by env
    '''
    if 'Minigrid_maze_aisle_wt_doors' in env:
        num_entry_points = 4
        return num_entry_points
    else:
        raise('call_env_place_range, '+ env + ' not implemented')
    
def call_process_limit_info(env:str, pose:list) -> dict:
    '''
    initiating a dictionnary containing the relative place position, place info,the direction 
    (axe + direction +/-)necessary to reach the pose, the next place info    
    '''
    if 'Minigrid_maze_aisle_wt_doors' in env:
        door_info = minigrid_maze.process_limit_info(pose)
        return door_info
    else:
        raise('call_env_place_range, '+ env + ' not implemented')
    
def call_from_door_view_to_door_pose(env:str,pose:list) -> list:
    '''
    ONLY MINIGRID_MAZE_AISLE_WT_DOOR
    When we have a pose associated to a door, 
    it is the pose of the view of the door, not the door pose
    '''
    if 'Minigrid_maze_aisle_wt_doors' in env:
        door_pose = minigrid_maze.from_door_view_to_door_pose(pose)
        return door_pose
    else:
        raise('call_from_door_view_to_door_pose is specific to Minigrid_maze_aisle_wt_doors, please refactor')
    
def call_get_place_behind_limit(env:str, manager:object, pose:list):
    """   get the eperience behind the given limit of current place"""
    if 'Minigrid_maze_aisle_wt_doors' in env:
        expected_exp_id, door_pose_from_new_place = minigrid_maze.get_place_behind_door(manager, pose)
        return expected_exp_id, door_pose_from_new_place
    else:
        raise('call_from_door_view_to_door_pose is specific to Minigrid_maze_aisle_wt_doors, please refactor')
    
def find_preferred_features_in_img(env:str, ob, agent_pose:list, goal_features:list) -> tuple[list, float]:
    """ search for goal feature matching in given ob as a Tensor, 
    return the pose of the goal in observation and how likely it is to be correct"""
    if 'Minigrid_maze_aisle_wt_doors' in env:
        goal_pose, colour_likelihood = minigrid_maze.find_tile_colour_in_img(ob, agent_pose, goal_features)
        return goal_pose, colour_likelihood
    else:
        raise('call_from_door_view_to_door_pose is specific to Minigrid_maze_aisle_wt_doors, please refactor')
```

`hierarchical-nav/env_specifics/minigrid_maze_wt_aisles_doors/__init__.py`:

```py
# from env_specifics.minigrid_maze_wt_aisles_doors.minigrid_maze_modules import *

```

`hierarchical-nav/env_specifics/minigrid_maze_wt_aisles_doors/minigrid_maze_modules.py`:

```py
from itertools import product
from control_eval.input_output import load_h5
import numpy as np
import torch
from navigation_model.Services.model_modules import torch_image

from navigation_model.Services.model_modules import torch_and_sample_observations,sample_ob
from navigation_model.Processes.AIF_modules import mse_elements
from navigation_model.Processes.motion_path_modules import action_to_pose
from navigation_model.visualisation_tools import convert_tensor_image_to_array


#TODO: add this call in env setup since it's particular to minigrid env      
def set_door_view_observation(manager: object)-> None:
    ''' Give the static info of what a door looks like to the model '''
    #Static Memory of a door view
    door_view_file = 'env_specifics/minigrid_maze_wt_aisles_doors/door_view.h5'
    door_view = load_h5(door_view_file)
    door_view = torch_and_sample_observations(door_view, manager.get_observations_keys(), manager.get_manager_sampling())
   
    manager.set_env_relevant_ob(door_view)


import datetime
import matplotlib.pyplot as plt
import torch

def debug_and_save_images(door_image, predicted_image, filename_prefix="debug_image"):
    # Helper function to print image statistics
    def print_stats(img, label):
        if torch.is_tensor(img):
            img_np = img.detach().cpu().numpy()
        else:
            img_np = img
        print(f"{label} - shape: {img_np.shape}, min: {img_np.min()}, max: {img_np.max()}, mean: {img_np.mean()}")

    print_stats(door_image, "Door image")
    print_stats(predicted_image, "Predicted image")
    
    # Convert to numpy arrays if necessary
    if torch.is_tensor(door_image):
        door_np = door_image.detach().cpu().numpy()
    else:
        door_np = door_image
        
    if torch.is_tensor(predicted_image):
        predicted_np = predicted_image.detach().cpu().numpy()
    else:
        predicted_np = predicted_image

    # If the images have a batch dimension, select the first image.
    if door_np.ndim == 4:
        door_np = door_np[0]
    if predicted_np.ndim == 4:
        predicted_np = predicted_np[0]

    # If the images are in (C, H, W) format, transpose them to (H, W, C)
    if door_np.ndim == 3 and door_np.shape[0] in [1, 3]:
        door_np = door_np.transpose(1, 2, 0)
    if predicted_np.ndim == 3 and predicted_np.shape[0] in [1, 3]:
        predicted_np = predicted_np.transpose(1, 2, 0)

    # Create a figure with two subplots for side-by-side comparison
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))
    
    axes[0].imshow(door_np)
    axes[0].set_title("Door Image")
    axes[0].axis('off')
    
    axes[1].imshow(predicted_np)
    axes[1].set_title("Predicted Image")
    axes[1].axis('off')
    
    # Append a timestamp to the filename to make it unique
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    filename = f"{filename_prefix}_{timestamp}.png"
    plt.savefig(filename, bbox_inches="tight")
    print(f"Saved debug image to {filename}")
    plt.close(fig)



#NOTE: sensitivity 0.18 for minigrid_aisle_wt_doors and current allocentric model
def is_agent_at_door(manager:object, real_obs , sensitivity:float= 1) -> bool:
    '''
    verify if the predicted observation matches a door view. if so, save in short-term memory 
    '''
    best_place_hypothesis = manager.get_best_place_hypothesis()
    #print("Real observation shape:", real_obs.shape)
    realob = torch_image(real_obs)
    realobs= sample_ob(realob,5,4)  


    if 'image_predicted' in best_place_hypothesis and best_place_hypothesis['image_predicted'] is not None:
        door_image = manager.get_env_relevant_ob()['image']
        predicted_image = best_place_hypothesis['image_predicted'].squeeze(1)
        if predicted_image.shape[0] > door_image.shape[0]:
            predicted_image = predicted_image[0]
        mse_door = mse_elements(predicted_image, door_image)
        mse_door2= mse_elements(realobs,door_image)
        print("mse_under_th",mse_door2, sensitivity,manager.mse_under_threshold(mse_door, sensitivity))
        if manager.mse_under_threshold(mse_door2, sensitivity):
            #debug_and_save_images(door_image, realobs)
            pose = list(best_place_hypothesis['pose'])
            print("save_pose_in_memory", pose)
            manager.save_pose_in_memory(pose)
            return True
    return False

def is_agent_at_door_given_ob(manager:object, pred_image, pose:list, sensitivity:float= 1) -> bool:
    '''
    verify if the predicted observation matches a door view. if so, save in short-term memory 
    '''
    door_image = manager.get_env_relevant_ob()['image']
    pred_image = pred_image.squeeze(1)
    if pred_image.shape[0] > door_image.shape[0]:
            pred_image = pred_image[0]
    mse_door = mse_elements(pred_image, door_image)
    if manager.mse_under_threshold(mse_door, sensitivity):
        pose = list(pose)
        manager.save_pose_in_memory(pose)
        return True
    return False



def minigrid_maze_aisles_doors_pose_range(env:str, reduction:int = 0)-> list :
        '''
        determine the squared range limits of the rooms deepending on its size
        '''
        #Poses options in room (+ a bit of aisle)
        if '4' in env:
            x_range = [0,7]
            y_range = [-3,3]
        elif '5' in env:
            x_range = [0,9]
            y_range = [-4,4]
        elif '6' in env:
            x_range = [0,10]
            y_range = [-5,5]
        elif '7' in env:
            x_range = [0,11]
            y_range = [-6,6]
        elif '8' in env:
            x_range = [0,12]
            y_range = [-7,7]
        else:
            raise(env +'length not accounted for place range')
        return pose_option_setup(x_range, y_range, reduction)

def pose_option_setup(x_range:list, y_range:list, reduction:int) -> list:
        '''  form a list of all possible poses in the range, 
        the reduction is used to restrain those options'''
        x = list(range(x_range[0]+reduction, x_range[1]+1-reduction)) 
        y = list(range(y_range[0]+reduction, y_range[1]+1-reduction)) 
        theta = list(range(0, 4))

        pose_options = list(product(*[x,y,theta]))
        return list(map(list, pose_options)) #all pose options as a list of list

def poses_leading_in_room(door_poses:list) -> list:
    """ in this env the entry point are the aisles, 
    thus we add positions in aisle around the door pose given
    """
    pose_options = []
    for p in door_poses:
        p = np.array(p)
        p[2] = (p[2]+2) % 4 #we want to face the room, not the door
        pose_options.append(list(p))
        pose_options.append(list(action_to_pose([1,0,0], p)))
        back_pose = action_to_pose([-1,0,0], p)
        pose_options.append(list(back_pose))
        pose_options.append(list(action_to_pose([-1,0,0], back_pose)))
    
    return pose_options

def remove_double_orientation(poses:list) -> list:
    '''
    we expect [[mse,p]...] as input sorted from best to worst
    erase all the duplicate poses with same orientation,
    keeping only the best one.
    '''   
    #TODO: DO A CHECK OF SHAPE TO AVOID ERROR

    for o in range(4):
        idx = [i for i, x in enumerate(poses[:,1]) if x[2] == o][::-1]
        if len(idx) > 1:
            poses = np.delete(poses, idx[:-1], axis=0)

    return poses
            

def process_limit_info(pose:list) -> dict:
    ''' we create a dictionnary containing the door pose info
    This i
    '''
    pose = np.array(pose)
    forward_pose_front_door = action_to_pose([1,0,0], pose)
    motion = forward_pose_front_door - pose 
    motion_index = np.nonzero(motion)

    door_info = {}
    door_info['door_pose'] = pose
    door_info['motion_axis'] = motion_index[0]
    door_info['direction'] = motion[motion_index[0]]
    door_info['connected_place'] = None
    door_info['origin_place'] = None
    door_info['exp_connected_place'] = None
    door_info['connected_place_door_pose'] = []
    return door_info

def from_door_view_to_door_pose(pose:list) -> list:
    for i in range(2):
        pose = action_to_pose([1,0,0], np.array(pose))
    return pose

def get_place_behind_door(manager:object, pose:list) ->tuple[int, list] :
        if pose is not None:
            expected_exp_id, door_pose_from_new_place = manager.memory_graph.linked_exp_behind_door(pose)
        else:
            expected_exp_id, door_pose_from_new_place = -1, []
        return expected_exp_id, door_pose_from_new_place 


def find_tile_colour_in_img(observation:torch.Tensor, agent_pose:list, colour_range:np.ndarray)-> tuple[list, float]:
    """ search for a tile having a colour in colour_range range
    extract this tile pose given the agent pose corresponding to ob
    This assumes an RGB observation of 56x56 representing 7 tiles and the agent pose is at the bottom middle
    return the pose of the tile in observation and how likely it is to be correct 
    (how much in the middle of range)
    """

    #Static with our Maze minigrid environment 
    # and observation as 56x56 with 7 tile observed and agent in bottom middle position
    tile_size = 8
    row_col_tile_nb = 7
    img_agent_pose = np.array([6,3]) #x,y in img ref frame

    if isinstance(agent_pose, torch.Tensor):
        if len(agent_pose.shape) > 1:
            agent_pose = torch.mean(agent_pose, dim=list(range(len(agent_pose.shape)-1)))
        agent_pose = agent_pose.cpu().detach().numpy()
        #Make sure to have only 1 image
    if len(observation.shape) > 3:
        observation= torch.mean(observation, dim=list(range(len(observation.shape)-3)))
    img = convert_tensor_image_to_array(observation)

    per_colour_likelihood = 0
    diff_img_pose = None
    tile_averaged_img = np.zeros(img.shape)
    for row_tile in range(row_col_tile_nb):
        for col_tile in range(row_col_tile_nb):
            tile_img = img[row_tile*tile_size: (row_tile+1)*tile_size,
                            col_tile*tile_size: (col_tile+1)*tile_size]
            tile_RGB_average = np.average(tile_img, axis = (0,1))
            # print('tile extracted', [row_tile*tile_size, (row_tile+1)*tile_size,
            # col_tile*tile_size, (col_tile+1)*tile_size])
            # print('tile RGB average',tile_RGB_average, type(tile_RGB_average), tile_RGB_average/255)
            tile_averaged_img[row_tile*tile_size: (row_tile+1)*tile_size,
                        col_tile*tile_size: (col_tile+1)*tile_size]= tuple(tile_RGB_average/255)
            if np.all(colour_range[0] <= tile_RGB_average) and np.all(tile_RGB_average <= colour_range[1]):
                #if we are above the colour range limit we save this pose
                colour_img_pose = np.array([row_tile, col_tile])
                diff_img_pose = abs(img_agent_pose - colour_img_pose)
                per_colour_likelihood = colour_likelihood(tile_RGB_average, colour_range)
                if colour_img_pose[1] < img_agent_pose[1]:
                    diff_img_pose[1] = colour_img_pose[1] - img_agent_pose[1]
                #print(' col and row contains WHITE TILE', col_tile, row_tile)
    wt_pose= None
    if diff_img_pose is not None:
        x = agent_pose[0] + (diff_img_pose[0] * np.cos(agent_pose[2]*np.pi/2) - diff_img_pose[1] * np.sin(agent_pose[2]*np.pi/2))
        y = agent_pose[1] + (diff_img_pose[0] * np.sin(agent_pose[2]*np.pi/2) + diff_img_pose[1] * np.cos(agent_pose[2]*np.pi/2))
        wt_pose = [round(x),round(y), int(agent_pose[2])]
        # plt.figure()
        # plt.title('agent p:' + str(agent_pose)+ ', wt pose:' +str(wt_pose))
        # plt.imshow(tile_averaged_img)
        # plt.show()
    return wt_pose, per_colour_likelihood

def RGB_euclidean_distance(rgb1:list, rgb2:list) -> float:
    r1, g1, b1 = rgb1
    r2, g2, b2 = rgb2
    return np.sqrt((r2 - r1) ** 2 + (g2 - g1) ** 2 + (b2 - b1) ** 2)

def colour_likelihood(rgb_colour:np.ndarray, colour_range:list) -> int:
    ''' 
    Define how well centered the value is 
    between the two ranges in percentage
    In the centre: 100%, on a limit colour range: 0%
    '''
    center_1 = (colour_range[0] + colour_range[1]) / 2
    max_colour_dist = RGB_euclidean_distance(center_1, colour_range[1])

    dist_to_center = np.linalg.norm(rgb_colour - center_1)
    per_dist_to_center = round(100- (dist_to_center/ max_colour_dist) * 100)
    return per_dist_to_center
```

`hierarchical-nav/experiments/GQN_v2/GQN.py`:

```py
from difflib import SequenceMatcher
import os
import torch
from torchvision.transforms import Compose
import glob
import numpy as np
from itertools import product
from dommel_library.datasets.dataset_factory import dataset_factory
from dommel_library.datasets import MemoryPool
from dommel_library.datasets.transforms import (
    Resize, ChannelFirst, RescaleShift, ToFloat,
    Squeeze, Subsample, Unsqueeze, Pad, Crop)
from dommel_library.datastructs import Dict, TensorDict, cat
from dommel_library.nn import module_factory
from dommel_library.nn.summary import summary
from dommel_library.train import (
    Trainer,
    loss_factory,
    optimizer_factory,
)
from dommel_library.modules.visualize import vis_images
from dommel_library.distributions.multivariate_normal import MultivariateNormal
from .models import GQNModel
from .train import ModelTrainer

# from test_benchmark import Benchmark

def get_model_parameters(log_dir, epoch=None):
    #check if we inputed a param file
    dir, ext = os.path.splitext(log_dir)
    if ext == '.pt':
        return log_dir

    model_dir = os.path.join(log_dir, "models")
    if epoch is None:
        model_files = glob.glob(os.path.join(model_dir, "*.pt"))
        model = max(model_files, key=os.path.getctime)
    else:
        model = os.path.join(model_dir, "model-{:04d}.pt".format(epoch))
    return model


class ToDist():
    """ Change tensor type to MultiVariateNormal """

    def __init__(self, keys):
        self.keys = keys

    def __call__(self, sequence):
        for key in self.keys:
            if key in sequence.keys():
                
                seq_dist = MultivariateNormal(sequence[key][:,:int(sequence[key].shape[-1] / 2)],sequence[key][:,int(sequence[key].shape[-1] / 2):])
                
                sequence[key] = seq_dist
        return sequence

def create_datasets(config, train=True):
    #
    # We wrap the file pools in a DictPool to cache the dataset
    # after transforms in memory, if the backend is not file.
    #
    # When sequence length is 1, we squeeze the time dimension
    #
    # We fetch sequences of double the length and subsample a timestep
    train_set = None
    transforms = [ToFloat()]

    if 'image' in config.dataset['keys']:
        try:
            size_cam = config.model.OZ.observations.image
        except AttributeError:
            if 'OZ' in config:
                size_cam = config.OZ.model.observations.image
            else:
                try:
                    size_cam = config.model.observations.image
                except AttributeError:
                    size_cam= [3,64,64]
        
        transforms.append(RescaleShift(1.0 / 255, 0, keys=["image"]))
        transforms.append(ChannelFirst(keys=["image"]))
        transforms.append(Resize(keys=["image"], size=size_cam))

    if 'posterior' in config.dataset['keys']:
        transforms.append(ToDist(keys=['posterior']))

    train_destination = config.dataset.get('train_destination',None)
    val_destination = config.dataset.get('val_destination',None)
        
    
    if train == True:
        # Wrap data in DictPool after transforming chunks of 300
        train_set = dataset_factory(config.dataset.train_set_location,
                                    train_destination,
                                    type=config.dataset.type,
                                    keys=config.dataset["keys"],
                                    sequence_length=config.dataset.sequence_length,
                                    sequence_stride=config.dataset.sequence_stride,
                                    transform=Compose(transforms), 
                                    cutoff = True)

        print('len train set', len(train_set))
    if config.dataset.val_set_location:
        val_set = dataset_factory(config.dataset.val_set_location,
                                  val_destination,
                                  type=config.dataset.type,
                                  keys=config.dataset["keys"],
                                  sequence_length=config.dataset.sequence_length,
                                  sequence_stride= config.dataset.sequence_stride,
                                  transform=Compose(transforms),
                                  cutoff = True)
        print('len val set', len(val_set))
        print('val set 0 shape', val_set[0].shape)

    else:
        val_set = None
    
    if train:
        if config.dataset.sequence_length == 1:
            # squeeze time dimension if sequence_length is 1
            train_set = MemoryPool(
                device=config.dataset.device,
                sequence_length=config.dataset.sequence_length,
                transform=Squeeze(keys=["image"], dim=0),).wrap(train_set)
            print('len train set', len(train_set))
            if val_set:
                val_set = MemoryPool(
                    device=config.dataset.device,
                    sequence_length=config.dataset.sequence_length,
                    transform=Squeeze(keys=["image"], dim=0)).wrap(val_set)
                print('len val set', len(val_set))
        else:
            train_set = MemoryPool(
                device=config.dataset.device).wrap(train_set)

            if val_set:
                val_set = MemoryPool(
                    device=config.dataset.device).wrap(val_set)
    
    return train_set, val_set


def create_model(config, train=True):
    # Construct model
    model_config = config.model
    model_type = model_config.get("type", None)
    if model_type == 'GQN':
        model = GQNModel(device = config.device, **model_config)
   
    else:
        #default option
        model = module_factory(**model_config)
        
    if not train:
        if "model_epoch" in config.keys():
            epoch = int(config.model_epoch)
        else:
            epoch = None
        if "params_dir" in model_config.keys():
            path = os.path.join(config.experiment_dir, model_config.params_dir)
            model.load(get_model_parameters(path, epoch = epoch))
        else:
            print('config log dir',config.log_dir)
            print('epoch,', epoch)
            try:
                model.load(get_model_parameters(config.log_dir, epoch= epoch), map_location='cpu')
            except TypeError:
                model.load(get_model_parameters(config.log_dir, epoch= epoch))
    
    model.to(config.device)
    #print(model)
    
    model.summary()


    return model 

def run(config):
    print("Loading data ...")
    train_set, val_set = create_datasets(config)

    print("Initializing model ...")
    model = create_model(config)

    loss = loss_factory(**config.loss)
    optimizer = optimizer_factory(loss.parameters() +
                                list(model.parameters()),
                                **config.optimizer)
    trainer = ModelTrainer(
        model=model,
        train_dataset=train_set,
        val_dataset=val_set,
        optimizer=optimizer,
        loss=loss,
        log_dir=config.log_dir,
        device=config.device,
        logging_platform="wandb",  # -> select the wandb platform
        experiment_config=config,  # -> store config in wandb as well
        ** config.trainer
    )

    # load from checkpoint
    print("Start training...")
    trainer.train(config.trainer.num_epochs, start_epoch=config.start_epoch)



import matplotlib.pyplot as plt

def evaluate(config):
    '''
    Call the benchmark to parse data, compute error and save data
    The rest is dependant on the model type

    n_sample (int): batch_size
    lookahead (int): predicted steps
    lookahead_ratio (bool): if True lookahead is interpreted as a %
    save_img_seq (bool): should we save the img pred/gt sequence
    unique_csv (bool): should the results of consecutive evaluation be saved in 1 file
    show_img (bool): do we want to display the evaluation img seq
    scenario (str): path of the scenario to run
    run_test (int): which test to run 
    1: run model with data than collect prediction and compare them -mse- to groundtruth
    '''
    #params for testbench, can be given as arguments to the main
    n_sample = config.get('n_sample', 3)
    lookahead = config.get('lookahead', 5)
    lookahead_ratio = config.get('lookahead_ratio', False)
    save_img_seq = config.get('save_img_seq', True)
    unique_csv = config.get('unique_csv', False)
    show_img = config.get('show_img', False)
    scenario = config.get('scenario', None)
    learn_steps = config.get('learn_steps', None)
    

    run_test = config.get('run_test', 1)

    

    #-- Create Model --
    gqn_oz_test_type = False #this is for the GQN trained with set Ozan model, currently we only have that (20/10/22)
    model_config = config.model.get("type", None)
    model = create_model(config, False)
    models =[model]

     #Check if we have Ozan model not in the trained model but as complemntary
    if ('OZ' in config.keys() or 'OZ' in config.model.keys()) and config.model['type']=='GQN':
        if 'OZ' in config.keys():
            #LOGDIR for PARAMS MUST BE SET IN OZ
            oz_model = create_model(config.OZ, False)
        elif 'OZ' in config.model.keys():
            oz_model = create_model(config.model.OZ, False)
        models.append(oz_model)
        gqn_oz_test_type = True


    #-- Collect Dataset --
    if 'scenario' in config.dataset and scenario== None:
        config.dataset.val_set_location = config.dataset.scenario
    elif scenario != None:
        config.dataset.val_set_location = scenario
    
    config.dataset.sequence_length = -1
    config.dataset.sequence_stride = -1
    _, eval_set = create_datasets(config, False) 


    test_setup = Benchmark(model_id = config.log_dir.split('/')[-1], data_file= config.dataset.val_set_location)
    model_set_data, pred_data = test_setup.parse_data(eval_set, n_sample=n_sample, learn_steps = learn_steps, lookahead=lookahead, lookahead_ratio=lookahead_ratio)
    
    if run_test ==1:
        
        #Run model from forward
        #m_output_seq, _ = run_model_step_by_step(models, model_set_data)
        #Run model from call
        m_output, models = run_model(models, model_set_data)
        

        #run the correct model type prediction process
        if model_config == 'GQN_OZ' or gqn_oz_test_type == True:
            pred_output = run_GQN_OZ_prediction_model(m_output, models, pred_data)
        elif model_config == 'GQN':
            pred_output = run_GQN_prediction_model(m_output, models, pred_data)
        elif model_config == 'Conv' :
            pred_output = run_OZ_prediction_model(models, pred_data)
        else :
            print('model %s not recognised', model_config)
    
        #Compute error in the benchmark and save data in a csv file
        keys = []
        if 'image_reconstructed' in pred_output:
            keys.append('image')
        if 'pose_reconstructed' in pred_output:
            keys.append('pose')

        mse_dict= test_setup.compute_mse_error(pred_output, key= keys, show_img = show_img)
        
        
        #Compute surprise between 2 steps
        # kl_dict = test_setup.compute_KL(state_dist_seq = m_output_seq['posterior'][1:], pref_dist_seq = m_output_seq['posterior'][:-1])
        # mse_dict.update(kl_dict)
        test_setup.save_test(run_test, mse_dict, save_img_seq= save_img_seq, unique_csv=unique_csv)

    elif run_test==2:
        query1 = pred_data[:,15:,...]
        # query2 = pred_data[:,8,...].unsqueeze(1)
        
        # queries = cat(query1, query2)
        # query3 = pred_data[:,13,...].unsqueeze(1)
        
        # queries = cat(queries, query3)

        
        pred_data = query1

        print('pred data', pred_data.shape)

        if model_config == 'GQN_OZ' or gqn_oz_test_type == True:
            pass
        elif model_config == 'GQN':
            
            pred_seq = TensorDict({})
            for k in pred_data.keys():
                pred_seq[k+'_query'] = pred_data[k]

            models[0].reset()
            standard_normal = MultivariateNormal(torch.zeros(n_sample,32),torch.ones(n_sample,32)).unsqueeze(1)
            future_step = models[0].forward(pred_seq, standard_normal, reconstruct=True)
            future_step['image_reconstructed'] = future_step['image_predicted']
           
            
            for step in range(future_step['image_predicted'].shape[1]):
                #for sample in range(images_comparison['image_predicted'].shape[0]):
                mse_by_step = torch.nn.functional.mse_loss(future_step['image_predicted'][:,step,...], pred_seq['image_query'][:,step,...], reduction='mean').cpu().detach().numpy().tolist()

                pred_img = vis_images(torch.mean(future_step['image_predicted'][:,step,...], dim=0).unsqueeze(0),show=False, fmt="torch").cpu().detach().numpy()
                query_img = vis_images(torch.mean(pred_seq['image_query'][:,step,...] , dim=0).unsqueeze(0),show=False, fmt="torch").cpu().detach().numpy()
                
                print('zese5utf',pred_img.shape)
                pred_img = np.transpose(pred_img, (1, 2, 0))
                query_img = np.transpose(query_img, (1, 2, 0))
                squared_error = (query_img- pred_img)**2
                

                squared_error = squared_error.sum(axis= 2 ) 
                plt.clf()
                plt.imshow( squared_error , vmin=0, vmax=3)
                plt.title(str(step) + '_mse:'+ str(mse_by_step))
                plt.colorbar()
                plt.savefig('test_results/'+ str(step))
            #mean mse of all images reconstructed vs groundtruth
            mse2 = test_setup.compute_mse_error(future_step, queries= pred_data,  show_img = False)
            
            
            #print('check', mse2)
            ##mean std of the posterior at each step##
            mean_std_place = test_setup.compute_std_dist(standard_normal)['mean_std_dist'][0]

            m_output_seq, model, result_dict = run_GQNmodel_step_by_step(models, model_set_data, pred_data, test_setup)
            
            result_dict['place_std_per_step'].insert(0,mean_std_place)
            result_dict['mse_step'].insert(0,mse2['mse_image']*10)

            print('result dict', result_dict['mse_step'])
        elif model_config == 'Conv' :
            pass
                
        if save_img_seq == True:
            steps=[]
            i=0
            n=1
            print(model_set_data.shape)
            while i < model_set_data.shape[1]:
                steps.append(i)
                i+=n
                #n+=1
            print(steps)
                
            test_setup.save_model_seq_steps(predicted_image_seq = m_output_seq['image_reconstructed'], img_query =pred_data,  steps=steps, save_model_steps = True) 
            test_setup.save_model_mean_steps_pred(predicted_image_seq = m_output_seq['image_reconstructed'], img_query =pred_data, steps=steps, mse_steps = result_dict['mse_step'])   

        test_setup.save_test(run_test, result_dict, save_img_seq= save_img_seq, unique_csv=unique_csv)
    
    elif run_test ==3 :
        #Poses options in gridworld (could go further in aisle, but well)
        x = list(range(-5, 6))
        y =  list(range(-5, 6))
        theta = list(range(0, 4))
        pose_options = list(product(*[x,y,theta]))
        pose_options = list(map(list, pose_options)) #all pose options as a list of list
        print('len(pose_options', len(pose_options))
        result_dict = {}
        reduced_seq = model_set_data[:,:4]
        reduced_seq_2 = model_set_data[:,4:5]
        pose_options.insert(0,reduced_seq_2['pose'][0].cpu().detach().numpy().tolist()[0]) #first value IS the correct one


        #print(model_set_data['pose'][0])
        if model_config == 'GQN_OZ' or gqn_oz_test_type == True:
            pass
        elif model_config == 'GQN':
            for i in range(0,len(pose_options)):
                pose_predicted = {'prev_pose': reduced_seq['pose'][0].cpu().detach().numpy().tolist(), 'real_pred_pose': pose_options[0] , 'pred_pose':pose_options[i]}
                result_dict.update(pose_predicted)
                pose = torch.Tensor(pose_options[i]).unsqueeze(0).repeat(reduced_seq_2['image'].shape[0],1,1)
                #print(reduced_seq_2.pose.shape, pose.shape, pose[0])
                reduced_seq_2['pose'] = pose
                #print(reduced_seq['pose'].shape)
                m_output, models = run_model(models,reduced_seq)
                pred_output = run_GQN_prediction_model(m_output, models, reduced_seq_2)
                #Compute error in the benchmark and save data in a csv file
                #print(pred_output['image_reconstructed'].shape, m_output.place.shape)
                mse_dict=test_setup.compute_mse_error_one_step(pred_output, reduced_seq_2, key=['image'], show_img = show_img)
                result_dict.update(mse_dict)
                            
                m_output_step2, models = run_model(models,reduced_seq_2, **{'reset': False})
                if 'place' in pred_output:
                    prev_post = m_output.place
                    post = m_output_step2.place
                else:
                    prev_post = m_output.posterior
                    post = m_output_step2.posterior

                #Compute surprise between 2 steps
                #print(post.shape, type(post), prev_post.shape)
                
                kl_dict = test_setup.compute_KL(state_dist_seq= post, pref_dist_seq=prev_post)
                kl_dict['kl_by_step'] = kl_dict['kl_by_step'][0]
                result_dict.update(kl_dict)
                result_dict.update(idx = i)
                test_setup.save_test(run_test, result_dict, save_img_seq= save_img_seq, unique_csv=unique_csv)

            
        elif model_config == 'Conv' :
            pass

    elif run_test == 6:
        standard_normal = MultivariateNormal(torch.zeros(n_sample,32),torch.ones(n_sample,32)).unsqueeze(1)

        poses_query = TensorDict({'pose_query': torch.tensor([[0.,0.,0.],[0.,0.,1.], [0.,0.,2.], [0.,0.,3.]]).unsqueeze(0).repeat(n_sample,1,1)})
        if model_config == 'GQN':
            step = models[0].forward(poses_query, place=standard_normal, reconstruct=True)
        
        vis_images(step['image_predicted'], show=True, fmt="torch", title="imagined views for p [4,4,0],[4,4,1], [4,4,2], [4,4,3] ")



        
def run_model(model, sequence, **kwargs):
    '''
    should work for all models as long as they authorise kwargs as fct call input and pass it to the forward fct 
    '''
    
    
    dict = {'train': False, 'reconstruct':False}
    output = model[0](sequence, **dict, **kwargs)
    
    return output, model


def run_GQNmodel_step_by_step(model,sequence, prediction_sequence, test_setup):
    '''
    should work for all models as long as they authorise kwargs as fct call input and pass it to the forward fct 
    '''
    res = None
    results_dict = {}
    #the summary created a prior in model
    model[0].reset()

    pred_seq = TensorDict({})
    for k in prediction_sequence.keys():
        pred_seq[k+'_query'] = prediction_sequence[k]
    #print(pred_seq['pose_query'].shape, sequence['pose'].shape)
    for step in range(sequence.shape[1]):
        
        data= sequence[:,step,...].unsqueeze(1)
        data.update(pred_seq) #Add the pred sequence to data to get the prediction with post at each step
        output = model[0].forward(data, reconstruct=True)
        output['image_reconstructed'] = output['image_predicted']
        if 'pose_predicted':
            output['pose_reconstructed'] = output['pose_predicted']
        #print('output rec', output['image_reconstructed'].shape, output['image'].shape)      
        
        if 'place' in output:
            place = output['place'] 
        elif 'posterior' in output:
            place = output['posterior']

        #mean mse of all images reconstructed vs groundtruth
        mse = test_setup.compute_mse_error(output, queries= prediction_sequence,  show_img = False)['mse_image']*10
        

        #mean std of the posterior at each step
        mean_std_place = test_setup.compute_std_dist(place)['mean_std_dist'][0]
               
        o = TensorDict(output).unsqueeze(1)       
        if res is None:
            res = o
            results_dict['place_std_per_step'] = [mean_std_place]
            results_dict['mse_step'] = [mse]
            
        else:
            res = cat(res, o) #OUTPUT : [batch, temporal_seq, number_of_elements_in_this_t, ... ]  #number_of_elements_in_this_t--> Linked to image_reconstructed
            results_dict['place_std_per_step'].append(mean_std_place)
            results_dict['mse_step'].append(mse)
        #print(res['state'].shape, res['image'].shape, res['image_reconstructed'].shape)
           
    return res, model, results_dict

        
def run_GQN_OZ_prediction_model(m_output, model, sequence):
    output = TensorDict({'place': m_output['place']}) #accumulated posterior
    res = None
    seq = TensorDict({})
    for k in sequence.keys():
        seq[k+'_query'] = sequence[k]
    #to have a prediction based on only present and past info, this shouldn't be necessary considering all archi, but just to be safe
    for timestep in range(seq.shape[1]):
        step = seq[:, timestep]
        step.unsqueeze(1)
        output = model[0].forward(step, place=output['place'], reconstruct=True)
        
        if len(model) == 2: #Ozan model is not in the forward function of model[0]
            output.update(model[1].likelihood(output['state_predicted'].sample(), 'image'))
    
        o = TensorDict(output.copy()).unsqueeze(1)
        if res is None:
            res = o
        else:
            res = cat(res, o)
        
    return res
        
def run_GQN_prediction_model(m_output, model, sequence):

    if 'place' not in m_output:
        old_GQN_archi = True
    else:
        old_GQN_archi= False

    if old_GQN_archi:
        output = TensorDict({'posterior': m_output['posterior']}) #accumulated posterior
    else:
        output = TensorDict({'place': m_output['place']}) #accumulated posterior
    
    seq = TensorDict({})
    for k in sequence.keys():
        seq[k+'_query'] = sequence[k]
        
    if old_GQN_archi:
        output = model[0].forward(seq, latent_place=output['posterior'], reconstruct=True)
    else:
        output = model[0].forward(seq, place=output['place'], reconstruct=True)
           
    output['image_reconstructed'] = output['image_predicted']
    if 'pose_predicted' in output:
        output['pose_reconstructed'] = torch.round(output['pose_predicted'])
    return output



def run_OZ_prediction_model(model, sequence):
    res = None
    
    for timestep in range(sequence.shape[1]):
        step = sequence[:, timestep,...]
        
        output = model[0].forward(action= step['action'], reconstruct=True)
        
        o = TensorDict(output.copy()).unsqueeze(1)
        if res is None:
            res = o
        else:
            res = cat(res, o)

    return output
        
```

`hierarchical-nav/experiments/GQN_v2/GQN_v2.yml`:

```yml

device: cuda
dataset:
  train_set_location: /home/idlab332/workspace/hierarchical_st_nav_aif/data/rssm_40s_10env
  val_set_location: /home/idlab332/workspace/hierarchical_st_nav_aif/data/rssm_40s_10env
  
  keys: [image, pose]
  type: FilePool
  device: cpu
  sequence_length: 40
  sequence_stride: 40

loss:
  kl:
    type: KL
    key: place 
    target: std_normal
    weight: 5
  reconstruct:
    type: MSE
    key: image_query 
    target: image_predicted 
    weight: 5
  pose_def:
    type: L1 #MSE
    key: pose_predicted
    target: pose_query  
optimizer:
  type: Adam
  lr: 0.0004
  amsgrad: True
trainer:
  log_epoch: 10
  save_epoch: 100
  batch_size: 64
  vis_batch_size: 3
  vis_epoch: {"before_rate": 10, "n": 100, "after_rate": 5}
  num_workers: 0
  num_epochs: 20000
  vis_args: 
    keys:
      - image_query
      - image_predicted
      - pose_query
      - pose_predicted
    vis_mapping:
      vector: ["pose_query","pose_predicted"]

model:
  type: 'GQN'
  observations: 
    image: [3,56,56]
    pose: 3
    z_size: 32
  min_context: 15
  min_query : 5
  max_query: 15

  SceneEncoder:
    expand: True
    channels: [16,32,64,128]
    #stride: 2
    MLP_channels: []
    activation: LeakyReLU
    aggregate_factor: 1
    clip_variance: 0.25
    ConvFilm: True

  SceneDecoder:
    channels: [256,128,64,32,32]
    interpolate_mode: 'bilinear'
    MLP_channels: []
    activation: LeakyReLU
    ConvFilm: True

  PositionalDecoder:
    activation: LeakyReLU
    activation_args:
      negative_slope: 1
    channels: [2048,512,128,64]
    pose_encoded_dim: 9

  #only 1 of the 2 can be used at once
  RandomSeqQuery: False
  RandomSelectQuery: True
```

`hierarchical-nav/experiments/GQN_v2/models/GQN_model.py`:

```py
import torch
import copy

from dommel_library.nn import summary
from dommel_library.datastructs import TensorDict
from dommel_library.distributions.multivariate_normal import MultivariateNormal
from experiments.GQN_v2.models import RandomSeqQuery,RandomSelectQuery, SelectContext, SceneEncoder as Posterior, SceneDecoder as Likelihood

from experiments.GQN_v2.models import PositionalDecoder



class GQNModel(torch.nn.Module):
    def __init__(
        self,
        SceneEncoder,
        SceneDecoder,
        observations = {'image':[3,64,64], 'pose':3},
        device = 'cpu',
        **kwargs,
    ):
        torch.nn.Module.__init__(self)
        self._observations= observations
        self._observation_shuffle = None
        
        if 'RandomSeqQuery' in kwargs and kwargs['RandomSeqQuery']==True:
            self._random_seq_query = RandomSeqQuery(kwargs.get('min_context', None))
        elif 'RandomSelectQuery' in kwargs and kwargs['RandomSelectQuery']==True:
            self._random_seq_query = RandomSelectQuery(kwargs.get('min_context', None),kwargs.get('max_query', 10), kwargs.get('min_query', 1))
        
        if 'PositionalDecoder' in kwargs:
            self._pose_decoder = PositionalDecoder(**{**kwargs.get('PositionalDecoder'), **observations, 'device': "cpu"})
            pose_encoded_dim = kwargs['PositionalDecoder'].get('pose_encoded_dim',9)
        else:
            self._pose_decoder = None
            pose_encoded_dim = 0
        
        self._posterior = Posterior(**{**SceneEncoder, **observations, 'device': device, 'pose_encoded_dim': pose_encoded_dim})
        try:
            input_length = self._posterior.output_length
        except:
            input_length = None
        self._scene_decoder = Likelihood(**{**SceneDecoder, **observations, 'input_length':input_length, 'pose_encoded_dim': pose_encoded_dim,'device': device})

        
        if 'SelectContext' in kwargs:
            self._observation_shuffle = SelectContext(**kwargs['SelectContext'])
        self._state, self._post = None,None

    @property
    def device(self):
        return next(self.parameters()).device

    def get_state(self):
        return self._state

    def state_size(self):
        return self._num_states

    def action_size(self):
        return self._num_actions

    def observations(self):
        return [*self._observations.keys()]

    def observation_size(self, key):
        return self._observations[key]

    def reset(self, state=None, post=None):
        self._state = state
        self._post = post
        

    def fork(self, batch_size=None):
        c = copy.copy(self)
        if batch_size is None:
            c._state = self._state.clone().detach()
        else:
            if len(self._state.shape) < 2:
                c._state = (self._state.expand(batch_size, self.state_size())
                        .clone().detach())
            else:
                cont = torch.mean(self._state, dim=0)
                c._state = (cont.repeat(batch_size, 1).clone().detach())

        
        return c


    def random_seq_query(self,obs,set_q_idx=None):
        return self._random_seq_query(obs, set_q_idx)


    def __call__(self, input_dict, reset=True, **kwargs):
        if reset:
            self.reset()
        #if batch but only 1 context ob

        
        if len(input_dict.shape) == 1:
            input_dict = input_dict.unsqueeze(1)
        obs = TensorDict({key: value for key, value in input_dict.items()
                            if key != 'place'})
        place = input_dict.get('place', None)

        train = kwargs.get('train', True)
        if train==True and self._random_seq_query:
            obs = self._random_seq_query(obs)
        
        
        # if self._observation_shuffle:
        #     obs = self._observation_shuffle(obs)[0]
              
        result = self.forward(obs, place, **kwargs)

        # print('in GQN call')
        # for k,v in result.items():
        #     print(k,v.shape)

        return result
        
    def forward(self, observations, place= None, reconstruct=True, **kwargs):
        result= TensorDict({})
        result.update(observations)
        
        if place is not None :
            if type(place) == torch.Tensor:
                if place.shape[-1] == self._observations['z_size']:
                    print("what the fuck is z size ",self._observations['z_size'] )
                    self._post =  place
                    self._state = self._post.sample()
                else:
                    print("what the fuck is z size 2222 ",self._observations['z_size'] )
                    self._post =  MultivariateNormal(place[:,:,:int(place.shape[-1] / 2)],place[:,:,int(place.shape[-1] / 2):])
                    self._state = place
            else:
                mu, sigma = place._mu_sigma()
                self._post  = MultivariateNormal(mu, sigma)
                self._state = self._post.sample()
            
            
        enc_post = None
        #if we have more info than just pose
        if 'image' in observations:
            
            #if torch version >=1.10 : self._post = self._posterior(observations['image'],observations['pose'], posterior=self._post)
          
            self._post, enc_post= self._posterior(observations['image'],observations['pose'], posterior=self._post)
            #else:
            self._post= MultivariateNormal(self._post.mean.unsqueeze(1), self._post.stdev.unsqueeze(1))
            
            self._state = self._post.sample()

            #TEMPORARY TEST
            
            # enc_post_state = enc_post.sample()  
            # pose_query = observations['pose']
            # for i in range(pose_query.shape[1]):
            #     enc_post_image_predicted = self._scene_decoder(pose_query[:,i], enc_post_state[:,0])
            #     enc_post_image_predicted =enc_post_image_predicted.unsqueeze(1)
            #     if 'mid_post_image_predicted' not in result:
            #         result['mid_post_image_predicted'] = enc_post_image_predicted
            #     else:
            #         result['mid_post_image_predicted'] = torch.cat((result['mid_post_image_predicted'], enc_post_image_predicted), dim=1)

        result.update(place = self._post)
        result.update(state= self._state)
        
        #result.update(pose_encoded= pose_encoded)
        if reconstruct == True :
            if 'pose_query' in observations:
                for i in range(observations['pose_query'].shape[1]):
                    image_predicted = self._scene_decoder(observations['pose_query'][:,i], self._state[:,0])
                    image_predicted =image_predicted.unsqueeze(1)
                    if 'image_predicted' not in result:
                        result['image_predicted'] = image_predicted
                    else:
                        result['image_predicted'] = torch.cat((result['image_predicted'], image_predicted), dim=1)

            if self._pose_decoder and 'image_query' in observations:
                for i in range(observations['image_query'].shape[1]):
                    pose_predicted = self._pose_decoder(observations['image_query'][:,i], self._state[:,0])
                    pose_predicted =pose_predicted.unsqueeze(1)
                    if 'pose_predicted' not in result:
                        result['pose_predicted'] = pose_predicted
                    else:
                        result['pose_predicted'] = torch.cat((result['pose_predicted'], pose_predicted), dim=1)
                   
        return result

        
    def summary(self):
        inputs = TensorDict({})
        for key, shape in self._observations.items():
            inputs[key] = torch.zeros(shape)
            inputs[key] = inputs[key].expand(2,*inputs[key].shape)
        inputs = inputs.unsqueeze(0).to(self.device)
        
        summary(self, inputs)

    def save(self, path):
        torch.save(self.state_dict(), path)
        new_path = path[:-2] + "full.pt"
        torch.save(self, new_path)

    def load(self, path, map_location= None):
        params = torch.load(path,
                            map_location=map_location)
        self.load_state_dict(params)

    @staticmethod
    def load_full(path):
        return torch.load(path)


    # def __str__(self):
    #     summary = ''
    #     # find all NNs in the model
    #     for (k, v) in self.__dict__.items():
    #         if isinstance(v, nn.Module):
    #             summary += k + " : " + str(v) + "\n"
    #         elif isinstance(v, dict):
    #             # sometimes we also store NNs in a dict
    #             for (kk, vv) in v.items():
    #                 if isinstance(vv, nn.Module):
    #                     summary += k + "." + kk + " : " + str(vv) + "\n"

    #     return summary
```

`hierarchical-nav/experiments/GQN_v2/models/__init__.py`:

```py
from experiments.GQN_v2.models.encoders import *
from experiments.GQN_v2.models.scene_decoder import *
from experiments.GQN_v2.models.modules import SelectContext, RandomSeqQuery, RandomSelectQuery
from experiments.GQN_v2.models.GQN_model import GQNModel

```

`hierarchical-nav/experiments/GQN_v2/models/encoders.py`:

```py
import logging

import numpy as np
import torch
import torch.nn as nn
from dommel_library.distributions.multivariate_normal import MultivariateNormal
from dommel_library.nn import ConvFiLM, ConvPipeline, VariationalMLP, get_activation

logger = logging.getLogger(__name__)

def no_process(x):
    return x

class PositionalEncoder(nn.Module):
    def __init__(self, input_size, embedding_size):
        nn.Module.__init__(self) 
        self.embed_input = nn.Linear(input_size, embedding_size, bias=False)
    
    def forward(self, x): 
        return torch.sin(self.embed_input(x))

class SceneEncoder(nn.Module):
    def __init__(
        self,
        channels,
        MLP_channels = None,
        z_size=256,
        expand=False,
        activation="ReLU",
        batch_norm=False,
        aggregation_method="kalman",
        aggregate_factor=2,
        clip_variance=0,
        dropout_prob=None,
        device="cpu",
        image=[3,64,64],
        pose = 3,
        ConvFilm= True,
        pose_encoded_dim = 0,
        **kwargs
    ):

        nn.Module.__init__(self)
        self.observations_keys = input
        self._aggregation_method = aggregation_method
        self._aggregate_factor = aggregate_factor

        self._batch_norm = batch_norm

        self.image= image.copy()
          
        if pose_encoded_dim > 0:
            self.pose_encoder = PositionalEncoder(pose, pose_encoded_dim)
            pose = pose_encoded_dim
        else:
            self.pose_encoder = no_process

        if expand:
            self._expand = nn.Conv2d(
                self.image[0], channels[0], kernel_size=1, stride=1
            )
            self.image[0] = channels[0]
            self.image[1] = int(np.ceil(image[1] / 1))
            self.image[2] = int(np.ceil(image[2] / 1))

            channels.pop(0)
        else:
           self._expand = no_process
        
        condition_size = pose if ConvFilm == True else 0
        # if ConvFilm != True: 
        #     condition_size = 0
        # else:
        #     condition_size = pose
            


        self._convs= ConvPipeline(input_shape=self.image,channels=channels, condition_size=condition_size, activation=activation, batch_norm=batch_norm, flatten=False, **kwargs)

        self.output_length= self._convs.output_length
        self.variational = VariationalMLP(
            self.output_length, z_size, MLP_channels, activation=activation
        )

        self.activation = get_activation(activation)

        self.dropout = nn.Dropout2d(p=dropout_prob) if dropout_prob is not None else no_process
        # if dropout_prob is not None:
        #     self.dropout = nn.Dropout2d(p=dropout_prob)
        # else:
        #     self.dropout = no_process

        self.device = device
        self.to(device)

        self._clip_variance = clip_variance

    

    def encode_single(self, x, p=None):
        """
        :param x: image
        :param p: pose. If the pose is abscent, the FiLM layer is not used
        :return: Gaussian distribution over the latent vector
        """
        p = self.pose_encoder(p)
        x = self._expand(x)
        
        x = self._convs(x, p) if p is not None else x
        x = self.dropout(x)
        x = x.view(x.shape[0], -1)
        return self.variational(x)

    def forward(self, cx, cp, posterior= None):
        """
        Implementation to compute the latent vector for all the context
        information in a batched manner
        :param cx: context views
        :param cp: context poses
        :param posterior: not None if we improve a previous latent place
        :return: tensor of means (mus), tensor of standard deviations (sigmas)
        """
        #TODOD: REDO IMP
  
    
        # Receives information of the shape
        # (batch_size, context_size, channels, im_width, im_height)
        # Transform to
        # (batch_size * context_size, channels, im_width, im_height)
        x_shape, p_shape = cx.shape, cp.shape

         # batch, ...  (channels, w, h) or other dims
        cx = cx.reshape(x_shape[0]*x_shape[1],*x_shape[2:])
        # batch, .... as poses
        cp = cp.reshape(p_shape[0]*p_shape[1],*p_shape[2:])

        post = self.encode_single(cx, cp)
        
        post = post.reshape(x_shape[0], x_shape[1], -1)
        aggregated_posterior = self.aggregate_method(post, posterior)

        return aggregated_posterior, post

    def aggregate_method(self, post, posterior=None):
        #if self._aggregation_method == "kalman":
        return self.aggregate(
            post, self._aggregate_factor, self._clip_variance, posterior
        )


    @staticmethod
    def aggregate(post, aggregate_factor=1, clip_variance=0, posterior = None):
        """
        Aggregate the posterior using hierarchical multivariate multiplication
        :param posterior: shape [B, L, MultivariateNormal]
        :return:
        """
        _, c, _ = post.shape
        
        if posterior != None:
            aggregated_posterior = posterior.squeeze(1)
            start_seq = 0
            
        else:
            aggregated_posterior = MultivariateNormal(post[:, 0, :])
            start_seq = 1

        #else the for is applied
        for i in range(start_seq, c):    
            
            aggregated_posterior = MultivariateNormal(
                aggregated_posterior.mean,
                aggregated_posterior.stdev * np.sqrt(aggregate_factor),
            )
            
            # multivariate gaussian multiplication!
            aggregated_posterior = aggregated_posterior * MultivariateNormal(
                post[:, i, :]
            )
            
            # Numerical stability clip variance
            aggregated_posterior_var = torch.max(
                aggregated_posterior.variance,
                clip_variance * torch.ones_like(aggregated_posterior.variance),
            )
            aggregated_posterior = MultivariateNormal(
                aggregated_posterior.mean, aggregated_posterior_var
            )

            
        #print('aggregated_posterior last step mean: ' + str(round(torch.mean(aggregated_posterior.mean).cpu().detach().numpy().tolist(),4)) +', std: ' +str(round(torch.mean(aggregated_posterior.stdev).cpu().detach().numpy().tolist(),4))\
             #+ ' var:'+ str(round(torch.mean(aggregated_posterior.variance).cpu().detach().numpy().tolist(),4)))
     
        return aggregated_posterior

    def combine_information(self, batch_mu, batch_sig):
        """
        Receives distributions as (Batch size, context_idx, mean/var)
        :param batch_mu: batch of means
        :param batch_sig: batch of variances
        :return: distributions (batch, mean/var)
        """
        _, c, _, = batch_mu.shape
        mu, sig = batch_mu[:, 0, :], batch_sig[:, 0, :]
        for i in range(1, c):
            mu, sig = self.add_information(
                (mu, sig), (batch_mu[:, i, :], batch_sig[:, i, :])
            )
            if mu.sum() != mu.sum():
                logger.info("nan detected in combine_information")
                logger.info(batch_mu[:, i, :])
                input()
            if sig.sum() != sig.sum():
                logger.info("nan detected in combine_information")
                logger.info(batch_sig[:, i, :])
                input()
        if self._aggregation_method == "mean":
            mu *= 1 / c

        return mu, sig

    def add_information(self, d0, d1):
        """
        combine two distributions
        :param d0: distribution 0 (mu, sig)
        :param d1: distribution 1 (mu, sig)
        :return: combined distribution mu, sig
        """
        if self._aggregation_method == "kalman":
            k = d1[1] / (d0[1] + d1[1])
            mu = k * d0[0] + (1 - k) * d1[0]
            sig = (1 - k) * d1[1]
        # elif self._aggregation_method == "kalman-fixed-noise":
        #     # Scale variance by multiplying by 2. To mitigate the reduced variance
        #     # Noise model is estimated by increasing variance over observations...
        #     scaling_factor = 2
        #     k = d1[1] / (scaling_factor * d0[1] + d1[1])
        #     mu = k * d0[0] + (1 - k) * d1[0]
        #     sig = (1 - k) * d1[1]

        # elif (
        #     self._aggregation_method == "addition"
        #     or self._aggregation_method == "mean"
        # ):
        #     mu = d0[0] + d1[0]
        #     sig = torch.ones_like(mu)
        # elif self._aggregation_method == "multiplication":
        #     mu = d0[0] * d1[0]
        #     sig = torch.ones_like(mu)
        # elif self._aggregation_method == "feature_wise_max_pool":
        #     mu = torch.max(d0[0], d1[0])
        #     sig = torch.ones_like(mu)

        return mu, sig + 1e-8


```

`hierarchical-nav/experiments/GQN_v2/models/modules.py`:

```py
import numpy as np
import random
import torch.nn
from dommel_library.distributions.multivariate_normal import MultivariateNormal
from dommel_library.nn import get_activation
from dommel_library.datastructs import TensorDict

class SelectContext(torch.nn.Module):
    """
    Module that randomly selects a context length, and extracts the
    relevant information from this
    """

    def __init__(self, context_size, min_context=3, random_length=False):
        torch.nn.Module.__init__(self)

        self._min_context = min_context
        self._context_size = context_size
        self._random_length = random_length

    def forward(self, *args):
        n = self._context_size
        if self._random_length:
            n = np.random.randint(self._min_context, self._context_size)
        indices = np.arange(args[0].size(1))
        np.random.shuffle(indices)
        return tuple(a[:, indices[:n]] for a in args)


class RandomSeqQuery(torch.nn.Module):
    """  create random sequences of representations and query
    """

    def __init__(self, min_context=None):
        torch.nn.Module.__init__(self)
        self.min_context=min_context

    def forward(self, sequence, set_q_idx=None):
        seq_size = sequence[list(sequence.keys())[0]].shape[1]
        min_context = self.min_context
        if set_q_idx==None or set_q_idx >= seq_size:
            if self.min_context == None or self.min_context >= seq_size:
                min_context = round(seq_size/6)
            if seq_size > 2:
                q_idx = np.random.randint(min_context,seq_size-1)
            else :
                q_idx = seq_size -1
        else:
            q_idx = set_q_idx  

        seq = TensorDict({})
        for key, value in sequence.items():
            seq[key] = value[:,:q_idx,...]
            seq[key+'_query'] = value[:,q_idx:,...]
        return seq

class RandomSelectQuery(torch.nn.Module):
    """  create random length sequences of representations and select queries randomly from anywhere on seq
    """

    def __init__(self, min_context=None, max_query=6, min_query=1):
        torch.nn.Module.__init__(self)
        self.min_context = min_context
        self.max_query = max_query
        self.min_query = min_query

    def forward(self, sequence, set_q_idx=None):
        # """  sequence: the full sequence of image and pose
        # set_q_idx: (optional) specific number of query we want
        # The sequence is divided in representation and query, with representation being a sequence of min_context starting from pose 0
        # """  
        seq_size = sequence[list(sequence.keys())[0]].shape[1]
        min_context = self.min_context
        max_query = self.max_query
        if set_q_idx==None or set_q_idx >= seq_size:
            if self.min_context == None or self.min_context >= seq_size:
                min_context = round(seq_size/6)
            if self.max_query >= seq_size: 
                max_query = round(seq_size/6)+2
            if seq_size > 2:
                #in representation seq, min amount of data: min context, max amount of data: up to the whole seq-x
                q_idx = np.random.randint(min_context,seq_size-self.min_query)
                set_q_idx = np.random.randint(self.min_query ,max_query)
                
            else :
                q_idx = seq_size -1
                set_q_idx = 1
                
        else:
            q_idx = seq_size - set_q_idx 
        #Generate set_q_idx random numbers between 1 and max_query
        randomquerylist = random.sample(range(0, seq_size), set_q_idx) 

        seq = TensorDict({})
        for key, value in sequence.items():
            seq[key] = value[:,:q_idx,...]
            seq[key+'_query'] =  torch.stack([value[:,i,...] for i in randomquerylist], dim=1)
            
        return seq

class LogDict(torch.nn.Module):
    def __init__(self):
        torch.nn.Module.__init__(self)

    def forward(self, *input):
        for i in input:
            try:
                print(i.shape)
            except:
                print(i)
        return tuple(input)


class ImageDistribution(torch.nn.Module):
    def __init__(self, variance, **kwargs):
        torch.nn.Module.__init__(self)
        self._var = variance

    def forward(self, x):
        return MultivariateNormal(x, self._var * torch.ones_like(x))


class LearnableMask(torch.nn.Module):
    def __init__(self, mask_shape=(3, 64, 64)):
        torch.nn.Module.__init__(self)
        self._mask = torch.nn.Parameter(
            torch.rand(mask_shape), requires_grad=True
        )
        self._act = get_activation("Sigmoid")

    def forward(self, x):
        return x * self._act(self._mask)


class Reshape(torch.nn.Module):
    """nn.Module for the PyTorch view method"""

    def __init__(self, out_shape, **kwargs):
        """
        :param shape: New shape to reshape the data into, should be a tuple
        """
        torch.nn.Module.__init__(self)
        self._shape = out_shape

    def forward(self, x):
        return x.reshape(x.data.size(0), *self._shape)


class Sum(torch.nn.Module):
    def __init__(self):
        torch.nn.Module.__init__(self)

    def forward(self, x, y):
        return x + y

import matplotlib.pyplot as plt
class VisualizeLayer(torch.nn.Module):

    def __init__(self, filename):
        torch.nn.Module.__init__(self)
        self.filename = filename
        self.calib = False

    def forward(self, x):
        if not self.calib:
            b, c, h, w = x.shape
            
            if (c > 2):
                fig, ax = plt.subplots(c//16, 16, figsize=(16 * 2, c//16 * 2))
                [a.axis('off') for a in ax.flatten()]
                if x.type() == 'torch.quantized.QUInt8Tensor':
                    x_p = torch.int_repr(x).numpy()
                    for bi in range(c//16):
                        for ci in range(16):
                            if c//16 == 1:
                                ax[ci].imshow(x_p[0, ci])
                                ax[ci].set_title(f"Ch: {ci}")
                            else:
                                ax[bi, ci].imshow(x_p[0, 16*bi + ci])
                                ax[bi, ci].set_title(f"Ch: {16*bi + ci}")
                    plt.savefig(self.filename, bbox_inches="tight")
                else:
                    for bi in range(c//16):
                        for ci in range(16):
                            if c//16 == 1:
                                ax[ci].imshow(x[0, ci])
                                ax[ci].set_title(f"Ch: {ci}")
                            else:
                                ax[bi, ci].imshow(x[0, 16*bi + ci])
                                ax[bi, ci].set_title(f"Ch: {16*bi + ci}")
                    plt.savefig(self.filename, bbox_inches="tight")
            else:
                fig, ax = plt.subplots(1, 2, figsize=(2 * 2, 1 * 2))
                [a.axis('off') for a in ax.flatten()]
                if x.type() == 'torch.quantized.QUInt8Tensor':
                    x_p = torch.int_repr(x).numpy()
                    for ci in range(2):
                        ax[ci].imshow(x_p[0, ci])
                        ax[ci].set_title(f"Ch: {ci}")
                else:
                    for ci in range(2):
                        ax[ci].imshow(x[0, ci])
                        ax[ci].set_title(f"Ch: {ci}")
                plt.savefig(self.filename, bbox_inches="tight")
            plt.show()
        return x

```

`hierarchical-nav/experiments/GQN_v2/models/scene_decoder.py`:

```py

import logging
import torch
import torch.nn as nn
import numpy as np

from dommel_library.nn import get_activation
from dommel_library.nn import MLP, UpConvPipeline


from experiments.GQN_v2.models import PositionalEncoder

logger = logging.getLogger(__name__)

def no_process(x):
    return x

class MiniConvBlock(nn.Module):
    def __init__(
        self,
        channels_in,
        channels_out,
        activation="LeakyReLU",
        residual=False,
    ):

        nn.Module.__init__(self)

        self.conv_1 = nn.Conv2d(
            channels_in, channels_out, kernel_size=3, stride=1, padding=1
        )
        self.conv_2 = nn.Conv2d(
            channels_out, channels_out, kernel_size=3, stride=1, padding=1
        )
        self.activation = get_activation(activation)

        self.residual = residual

    def forward(self, x):
        # Change number of channels
        r = self.activation(self.conv_1(x))
        # process
        x = self.conv_2(r)
        # Res connect
        if self.residual:
            return x + r
        return x


class SceneDecoder(nn.Module):
    """
    In contrast to the image decoder, this decoder
    starts from a viewpose. Using a linear layer it is
    able to reconstruct
    """

    def __init__(
        self,
        channels,
        z_size,
        input_length,
        MLP_channels = [],
        activation="ReLU",
        dropout_prob=None,
        up_first=None,
        image=[3,64,64],
        pose= 3,
        ConvFilm = True,
        pose_encoded_dim = 0,
        **kwargs
    ):
        nn.Module.__init__(self)
        self._upsample_factor = 2  # (im_size / 4) ** (1 / n_layers)
        # For models with more layers, the final convolution will downsample
        self.input_length = input_length
        self.channels = channels
        
        self._activation = get_activation(
            activation, **kwargs.get("activation_args", {})
        )
        self._conv_blocks = nn.ModuleList()
        self.observations_keys = input
    
        if pose_encoded_dim > 0:
            self.pose_encoder = PositionalEncoder(pose, pose_encoded_dim)
            pose = pose_encoded_dim
        else:
            self.pose_encoder = no_process

        # conv blocks
        condition_size = pose + z_size if ConvFilm == True else 0
        # if ConvFilm == True: 
        #     condition_size = z_size + pose
        # else:
        #     condition_size = 0

        
            
        self._conv_blocks = UpConvPipeline(output_shape=image,channels=channels, condition_size=condition_size, activation=activation, flatten=False, **kwargs)

        # shape pose into an image
        if len(MLP_channels) >0 :
            self._linear_to_image = MLP(z_size + pose, np.prod(self._conv_blocks.reshape_shape), MLP_channels)
            self.input_state_pos = True
        else:
            self._linear_to_image = nn.Linear(
            pose, np.prod(self._conv_blocks.reshape_shape)
            )
            self.input_state_pos = False
        
       
        self.dropout = nn.Dropout2d(p=dropout_prob) if dropout_prob is not None else no_process
        # if dropout_prob is not None:
        #     self._dropout = nn.Dropout2d(dropout_prob)
        # else:
        #     self._dropout = no_process

        self._upfirst = up_first
        self._im_size = image[2]
        self._compress = no_process
        self._sigmoid = get_activation("Sigmoid")

    #expect pose and state
    def forward(self, pose, state):
        pose = self.pose_encoder(pose)
        state = torch.cat([pose, state], dim=-1)

        # Learn a transformation of pose into a
        # base image
        x = self._linear_to_image(state) if self.input_state_pos == True else self._linear_to_image(pose)

        # if self.input_state_pos :
        #     x = self._linear_to_image(state)
        # else:
        #     x = self._linear_to_image(pose)
        x = self._activation(x)
        x = self._conv_blocks(x,state)
      
        return self._sigmoid(self._compress(x))


class PositionalDecoder(nn.Module):
    def __init__(self, z_size, 
                pose, image, 
                pose_encoded_dim= 9, 
                activation="ReLU", 
                channels=None, 
                device = 'cpu', 
                **kwargs):
        nn.Module.__init__(self) 
        """
        This decoder starts from a view. Using a linear layer it transform the info into pose
        """
        self._state_to_linear = MLP(np.prod(image) + z_size, pose_encoded_dim, hidden_layers= channels, activation= activation, bias=False)
        self.activation = get_activation(
            activation, **kwargs.get("activation_args", {}) )
        
        self.embed_output = nn.Linear(pose_encoded_dim, pose,  bias=False)
        self.device = device
        self.to(device)

    def forward(self, ob, state): 
        # Flatten ob
        ob = ob.reshape(-1, np.prod(ob.shape[1:]))
        #cat ob + state
        
        state = torch.cat([ob, state], dim=-1)

        x = torch.tanh(state) #-1 1
        x = self.activation(x) 
        #print('atan', x)
        x = torch.asin(x) 
        #print('arcsin x', x)
        #Reduce it to pose output
        x = self._state_to_linear(state)
        #print('self._state_to_linear',x)
        #x = self.activation(x)
        # print('activation', x)
        
        x = self.embed_output(x)
        #print('last linear to pose', x)
        #x= torch.round(x)
      
        return x

       



# =============================================================================
# Separate components needed for MultiStageDecoder
# =============================================================================
from dommel_library.nn.convolutions import Conv


class LinearToImage(nn.Module):
    def __init__(self, condition_size, channels):
        nn.Module.__init__(self)
        self._channels = channels
        self._linear_to_image = nn.Linear(
            condition_size, 4 * 4 * self._channels
        )

    def forward(self, x):
        x = self._linear_to_image(x)
        return x.reshape(-1, self._channels, 4, 4)


class FromRGB(nn.Module):
    def __init__(self, out_channels=16):
        nn.Module.__init__(self)
        self._expand = nn.Conv2d(3, out_channels, kernel_size=1, stride=1)

    def forward(self, x):
        return self._expand(x)


class ToRGB(nn.Module):
    def __init__(self, in_channels=16):
        nn.Module.__init__(self)
        self._compress = nn.Conv2d(in_channels, 3, kernel_size=1, stride=1)

    def forward(self, x):
        return self._compress(x)


class Resize(nn.Module):
    def __init__(self, out_shape):
        nn.Module.__init__(self)

        self._out_shape = out_shape

    def forward(self, x):
        return nn.functional.interpolate(x, self._out_shape)


```

`hierarchical-nav/experiments/GQN_v2/train/__init__.py`:

```py
from experiments.GQN_v2.train.model_trainer import ModelTrainer 

```

`hierarchical-nav/experiments/GQN_v2/train/model_trainer.py`:

```py
import torch
from tqdm import tqdm

from dommel_library.datastructs import TensorDict
from dommel_library.train import Trainer
from dommel_library.train.losses import Log



class ModelTrainer(Trainer):
    def __init__(self, model, train_dataset, loss, optimizer, log_dir,
                 warmup=1, **kwargs):
        # divide loss by the sequence length to have normalized loss value
        # sequence_length = train_dataset.sample(1).shape[1]
        # for _, loss_dict in loss._losses.items():
        #     weight = loss_dict.get("weight", 1.0)
        #     weight /= sequence_length
        #     loss_dict["weight"] = weight


        Trainer.__init__(self, model, train_dataset, loss,
                         optimizer, log_dir, **kwargs)
        self._warmup = warmup

    def _epoch(self):
        """
        Execute a single epoch of training
        :return: Log object containing information of the epoch step
        """
        logs = Log()
        self._model.train()
        for input_dict in tqdm(self._train_loader, disable=not self._verbose):
            input_dict = TensorDict(input_dict).to(self._device)
            self._optimizer.zero_grad()
            output_dict = self._model(input_dict)
            loss = self._loss(output_dict,output_dict) #this line is different from dommel trainer
            loss.backward()
            self._clip(self._model.parameters())
            self._optimizer.step()
            self._loss.post_backprop()
            logs += self._loss.logs
        return logs

    # def _visualize_prior(self, batch):
    #     with torch.no_grad():

    #         sequence = self._model(batch)
    #     prior_images = visualize_sequence(sequence, **self._vis_args)
    #     return prior_images

    # def _log_visualization(self, train_logs, val_logs, epoch):
    #     train_logs, val_logs = super()._log_visualization(train_logs, val_logs, epoch)

    #     # add prior visualizations
    #     prior_images = self._visualize_prior(self._visualize_batch["train"])
    #     for k, img in prior_images.items():
    #         train_logs.add("query/" + k, img, "image")
           

    #     if self._val_loader:
    #         prior_images = self._visualize_prior(self._visualize_batch["val"])
    #         for k, img in prior_images.items():
    #             train_logs.add("query/" + k, img, "image")
                

    #    return train_logs, val_logs


    def _initial_log(self, start_epoch):
        # compute initial batch and log this
        # also visualize ground truth
        #difference with dommel, output_dict is used for sequence visualisation
        with torch.no_grad():
            train_logs = Log()
            output_dict = self._model(self._visualize_batch["train"])
            _ = self._loss(output_dict, self._visualize_batch["train"])
            train_logs += self._loss.logs

            # visualize ground truth as well
            # dataset_images = visualize_sequence(
            #     output_dict, **self._vis_args, **self.vis_mapping
            # )
            # for k, img in dataset_images.items():
            #     train_logs.add("ground_truth/" + k, img, "image")

            val_logs = None
            if self._val_loader:
                self._model.eval()
                val_logs = Log()
                output_dict = self._model(self._visualize_batch["val"])
                _ = self._loss(output_dict, self._visualize_batch["val"])
                val_logs += self._loss.logs

                # dataset_images = visualize_sequence(
                #     output_dict, **self._vis_args, **self.vis_mapping
                # )
                # for k, img in dataset_images.items():
                #     val_logs.add("ground_truth/" + k, img, "image")

            self._log_callback(train_logs, val_logs, start_epoch)
```

`hierarchical-nav/experiments/OZ/OZ.py`:

```py
import os
import torch
from torchvision.transforms import Compose
import glob
import copy

from dommel_library.datasets.dataset_factory import dataset_factory
from dommel_library.datasets import MemoryPool
from dommel_library.datasets.transforms import (
    Resize, ChannelFirst, RescaleShift, ToFloat,
    Squeeze, Subsample, Unsqueeze, Pad, Crop)
from dommel_library.datastructs import Dict, TensorDict, cat as datastructs_cat
from dommel_library.nn import module_factory
from dommel_library.nn.summary import summary
from dommel_library.train import (
    Trainer,
    loss_factory,
    optimizer_factory,
)
from dommel_library.modules.visualize import visualize_sequence
from .models import ConvModel, BaseModel
from .train import ModelTrainer

# from test_benchmark import Benchmark

def get_model_parameters(log_dir, epoch=None):
    dir, ext = os.path.splitext(log_dir)
    if ext == '.pt':
        return log_dir

    model_dir = os.path.join(log_dir, "models")
    if epoch is None:
        model_files = glob.glob(os.path.join(model_dir, "*.pt"))
        model = max(model_files, key=os.path.getctime)
    else:
        model = os.path.join(model_dir, "model-{:04d}.pt".format(epoch))
    return model


class ConsecutiveDuplicateCheck():
    """ Check 2 consecutives poses to check if there is a collision """

    def __init__(self, keys, new_data_names):
        self.keys = keys
        self.new_data_names = new_data_names

    def __call__(self, sequence):
        new_data_names = self.new_data_names.copy()
        for key in self.keys:
            if key in sequence.keys():
                #print('sequence[key].shape for key pose', sequence[key].shape)
                duplicate_bool_list = [0.]
                
                for x in range(1,sequence[key].shape[0]):
                    #print(sequence[key][x-1],sequence[key][x], (sequence[key][x-1] == sequence[key][x]).all())
                    if (sequence[key][x-1] == sequence[key][x]).all():
                        duplicate = 1.
                    else:
                        duplicate = 0.
                    duplicate_bool_list.append(duplicate) 
                
                duplicate_tensor = torch.tensor(duplicate_bool_list).unsqueeze(1)
                #print('duplicate_tensor and sequence pose shape', duplicate_tensor.shape, sequence[key].shape)
                sequence[new_data_names[0]] = duplicate_tensor
                new_data_names.pop(0)
        return sequence

class DelKey():
    """ del input key"""

    def __init__(self, keys):
        self.keys = keys

    def __call__(self, sequence):
        for key in self.keys:
            if key in sequence.keys():
                del sequence[key]
        return sequence

def create_datasets(config, train=True):
    #
    # We wrap the file pools in a DictPool to cache the dataset
    # after transforms in memory, if the backend is not file.
    #
    # When sequence length is 1, we squeeze the time dimension
    #
    # We fetch sequences of double the length and subsample a timestep
    train_set = None
    
    if 'image' in config.dataset['keys']:
            size_cam = config.model.observations.image
            if isinstance(size_cam,dict):
                size_cam = size_cam['input_shape']
    else:
        size_cam = [3,64,64]
    
    #print('size_cam', size_cam)

    train_destination = config.dataset.get('train_destination',None)
    val_destination = config.dataset.get('val_destination',None)
        
    transforms = [ToFloat(),
                  #Unsqueeze(0, keys=['vel_ob']),
                  RescaleShift(1.0 / 255, 0, keys=["image"]),
                  ChannelFirst(keys=["image"]),
                  Resize(keys=["image"], size=size_cam),
                  ConsecutiveDuplicateCheck(['pose'], ['collision']),
                  DelKey(keys=['pose'])
                  ]
    if train == True:
        # Wrap data in DictPool after transforming chunks of 300
        train_set = dataset_factory(config.dataset.train_set_location,
                                    train_destination,
                                    type=config.dataset.type,
                                    keys=config.dataset["keys"],
                                    sequence_length=config.dataset.sequence_length,
                                    sequence_stride=config.dataset.sequence_stride,
                                    transform=Compose(transforms), 
                                    cutoff = True)

        print('len train set', len(train_set))
    if config.dataset.val_set_location:
        val_set = dataset_factory(config.dataset.val_set_location,
                                  val_destination,
                                  type=config.dataset.type,
                                  keys=config.dataset["keys"],
                                  sequence_length=config.dataset.sequence_length,
                                  sequence_stride= config.dataset.sequence_stride,
                                  transform=Compose(transforms),
                                  cutoff = True)
        print('len val set', len(val_set))
        print('val set 0 shape', val_set[0].shape)

    else:
        val_set = None
    
    if train:
        if config.dataset.sequence_length == 1:
            # squeeze time dimension if sequence_length is 1
            train_set = MemoryPool(
                device=config.dataset.device,
                sequence_length=config.dataset.sequence_length,
                transform=Squeeze(keys=["image"], dim=0),).wrap(train_set)
            print('len train set', len(train_set))
            if val_set:
                val_set = MemoryPool(
                    device=config.dataset.device,
                    sequence_length=config.dataset.sequence_length,
                    transform=Squeeze(keys=["image"], dim=0)).wrap(val_set)
                print('len val set', len(val_set))
        else:
            train_set = MemoryPool(
                device=config.dataset.device).wrap(train_set)

            if val_set:
                val_set = MemoryPool(
                    device=config.dataset.device).wrap(val_set)
    
    return train_set, val_set

def create_model(config, train=True):

    model_config = config.model
    
    model_type = model_config.get("type", None)
    if model_type == "Conv":
        model = ConvModel(device = config.device, **model_config)
    elif model_type == "Base":
        model = BaseModel(**model_config)
    else:
        #default option
        model = module_factory(**model_config)

    if not train:
        if "model_epoch" in config.keys():
            epoch = int(config.model_epoch)
        else:
            epoch = None
        if "params_dir" in model_config.keys():
            path = os.path.join(config.experiment_dir, model_config.params_dir)
            model.load(get_model_parameters(path, epoch = epoch), map_location='cpu')
        elif "params_dir" in config.keys():
            model.load(get_model_parameters(config.params_dir, epoch = epoch), map_location='cpu')
        else:
            print('config log dir',config.log_dir)
            print('epoch,', epoch)
            model.load(get_model_parameters(config.log_dir, epoch= epoch), map_location='cpu')
    
    model.to(config.device)
    print(model)
    
    model.summary()



    return model 

def run(config):
    print("Loading data ...")
    train_set, val_set = create_datasets(config)

    print("Initializing model ...")
    model = create_model(config)

    loss = loss_factory(**config.loss)
    optimizer = optimizer_factory(loss.parameters() +
                                list(model.parameters()),
                                **config.optimizer)
    trainer = ModelTrainer(
        model=model,
        train_dataset=train_set,
        val_dataset=val_set,
        optimizer=optimizer,
        loss=loss,
        log_dir=config.log_dir,
        device=config.device,
        logging_platform="wandb",  # -> select the wandb platform
        experiment_config=config,  # -> store config in wandb as well
        ** config.trainer
    )

    # load from checkpoint
    print("Start training...")
    trainer.train(config.trainer.num_epochs, start_epoch=config.start_epoch)


def evaluate(config):
    '''
    Call the benchmark to parse data, compute error and save data
    The rest is dependant on the model type

    n_sample (int): batch_size
    lookahead (int): predicted steps
    lookahead_ratio (bool): if True lookahead is interpreted as a %
    save_img_seq (bool): should we save the img pred/gt sequence
    unique_csv (bool): should the results of consecutive evaluation be saved in 1 file
    show_img (bool): do we want to display the evaluation img seq
    scenario (str): path of the scenario to run
    run_test (int): which test to run 
    1: run model with data than collect prediction and compare them -mse- to groundtruth
    '''
    #params for testbench, can be given as arguments to the main
    n_sample = config.get('n_sample', 3)
    lookahead = config.get('lookahead', 5)
    lookahead_ratio = config.get('lookahead_ratio', False)
    save_img_seq = config.get('save_img_seq', True)
    unique_csv = config.get('unique_csv', False)
    show_img = config.get('show_img', False)
    scenario = config.get('scenario', None)
    learn_steps = config.get('learn_steps', None)
    run_test = config.get('run_test', 1)


    #-- Create Model --
    model_config = config.model.get("type", None)
    
    model = create_model(Dict(copy.deepcopy(config)), False)

    models =[model]
    
    if 'scenario' in config.dataset and scenario== None:
        config.dataset.val_set_location = config.dataset.scenario
    elif scenario != None:
        config.dataset.val_set_location = scenario


    #-- Collect Dataset --
    config.dataset.sequence_length = -1
    config.dataset.sequence_stride = -1
    _, eval_set = create_datasets(config, False)
    print('config.log_dir',config.log_dir)


    test_setup = Benchmark(model_id = config.log_dir.split('/')[-1], data_file= config.dataset.val_set_location)
    
    if run_test ==1:
        model_set_data, pred_data = test_setup.parse_data(eval_set, n_sample=n_sample, learn_steps= learn_steps, lookahead=lookahead, lookahead_ratio=lookahead_ratio)

        #Run model from call
        m_output, models = run_model(models, model_set_data)

        
        if model_config == 'Conv' or model_config == 'RSSM':
            pred_output = run_OZ_prediction_model(models, pred_data)
        else :
            print('model %s not recognised', model_config)


        #Compute error in the benchmark and save data in a csv file
        mse_dict= test_setup.compute_mse_error(pred_output, key=['image', 'collision'], show_img = show_img)
        test_setup.save_test(run_test, mse_dict, save_img_seq= save_img_seq, unique_csv=unique_csv)

def run_model(model, sequence):
    '''
    should work for all models as long as they authorise kwargs as fct call input and pass it to the forward fct 
    '''
    model[0].reset()
    dict = {'train': False, 'reconstruct':False}
    #print(' sequence[state', sequence['state'].shape)
    # for step in range(sequence.shape[1]):
    #     images = model[0].likelihood(sequence['state'][:,step,...], 'image')
    #     images.update(image = sequence['image'][:,step,...])
    #     print('image shape', images['image'].shape)
    #     visualize_sequence(images.unsqueeze(1),
    #                         ["image", 'image_reconstructed'], 
    #                         show=True,
    #                         title="query and predicted sequence")

    output = model[0](sequence)
    
    return output, model


def run_OZ_prediction_model(model, sequence):
    res = None
    
    for timestep in range(sequence.shape[1]):
        step = sequence[:, timestep,...]
        
        #output = model[0].likelihood(step['state'], 'image')
        output = model[0].forward(action= step['action'], reconstruct=True)
        if not 'image_reconstructed' in output:
            output['image_reconstructed'] = output['image']

        o = TensorDict(output.copy()).unsqueeze(1)
        if res is None:
            res = o
        else:
            res = datastructs_cat(res, o)
        

    return res

#------ old ------

def old_evaluate(config):
    print("Loading data ...")
    if 'scenario' in config.dataset:
        config.dataset.val_set_location = config.dataset.scenario
    _, eval_set = create_datasets(config, False)
    model = create_model(config, False)

    sequence = eval_set.sample()
    evaluation_plots(model,sequence)


def evaluation_plots(model,sequence):
    ''' Display as image the observation seq, reconstruction seq and prior seq'''
    # visualize sequence
    visualize_sequence(sequence,
                       ["image"],
                       max_length=10,
                       vis_mapping={"image": ["image"]},
                       show=True,
                       title="Example sequence")


    # visualize posterior reconstruction
    sequence_posterior = model_rollout(model, sequence).squeeze(0)
    visualize_sequence(sequence_posterior,
                        ["image"],
                        max_length=10,
                        vis_mapping={"image": ["image"]},
                        show=True,
                        title="Posterior reconstructions")
    model.reset()
    model_copy = copy.deepcopy(model)
    #Visualise prior
    sequence_prior = imagine_test(model_copy, sequence.action)
    visualize_sequence(sequence_prior,
                    ["image"],
                    max_length=10,
                    vis_mapping={"image": ["image"]},
                    show=True,
                    title="Prior reconstructions")

def model_rollout(model, sequence,reset=True):
    ''' Calculate a sequence of prior and posterior distributions
        and likelihoods for a given sequence
        `model`               - the model with a prior and likelihood function
        `sequence`            - the sequence to process
    '''
    result = []
    sequence = sequence.to(model.device)
    if reset:
        model.reset()
    for j in range(sequence.action.shape[1]):
        action = sequence.action[:, j, :]
        observations = {k: sequence[k][:, j, :] for k in model.observations()}
        #vis_image(observations['image'].squeeze(0), show=True)
        
        step = model.forward(action, observations)
        #print(step.image.squeeze(0).shape)
        
        result.append(TensorDict(step).unsqueeze(1))

    return datastructs_cat(*result)


def imagine_test(model_copy, action_seq):
    imagined_sequence = []
    
    for step in range(action_seq.shape[1]):
        action = action_seq[:,step, :]
        step = model_copy.forward(action = action)
        step_storage = TensorDict({})
        step_storage.update(step)
        step_storage.action = action

        imagined_sequence.append(step_storage.unsqueeze(1))
    return datastructs_cat(*imagined_sequence)
```

`hierarchical-nav/experiments/OZ/OZ.yml`:

```yml
device: cuda
dataset:
  remark: data rooms exploration, 4t aisles, door in middle, 6white Tiles in rand rooms see aisle_door_6wFloor_3x3rooms_100_400s
  train_set_location: https://cloud.ilabt.imec.be/index.php/s/To58jREwsPYpaHr/download # /home/idlab332/workspace/hierarchical_st_nav_aif/data/train #corridor rooms
  val_set_location: https://cloud.ilabt.imec.be/index.php/s/Ln2ednMfxg3biGB/download # /home/idlab332/workspace/hierarchical_st_nav_aif/data/val

  # train_set_location: /home/idlab332/workspace/hierarchical_st_nav_aif/data/fake_AD_7t_train
  # val_set_location: /home/idlab332/workspace/hierarchical_st_nav_aif/data/fake_AD_7t_train
  keys: [image, action, pose]
  type: FilePool
  device: cpu
  sequence_length: 20
  sequence_stride: 20

loss:
  OZ_kl:
    type: KL
    key: posterior 
    target: prior
    weight: 1
  OZ_reconstruct:
    type: MSE
    key: image_reconstructed
    target: image
  OZ_collision_detection:
    type: BCE
    key: collision_reconstructed
    target: collision
optimizer:
  type: Adam
  lr: 0.0001
  amsgrad: True
trainer:
  batch_size: 32
  log_epoch: 10
  mixed_precision: true
  num_epochs: 10000
  num_workers: 0
  save_epoch: 100
  vis_batch_size: 2
  vis_epoch:
    after_rate: 100
    before_rate: 1
    n: 3
  warmup: 10
  # vis_args: 
  #   keys:
  #     - image
  #     - image_reconstructed
  # vis_mapping:
  #     image: ["image_reconstructed"]
   
model:
  type: Conv
  num_states: 32
  num_actions: 3
  observations:
    image: 
      type: Conv
      input_shape: [3, 56, 56]
      channels: [8, 16,32]
      
    collision: 
      type: Bool
      input_shape: 1
      hidden_layers: [16,8]
      mlp_activation: Hardsigmoid
      activation: Identity
      #negative_slope: 1
  lstm_cells: [256]
  lstm_posterior: true
  hidden_layers: [256]


```

`hierarchical-nav/experiments/OZ/models/__init__.py`:

```py
from experiments.OZ.models.oz_base_model import BaseModel
from experiments.OZ.models.oz_conv_model import ConvModel
from experiments.OZ.models.modules import *

```

`hierarchical-nav/experiments/OZ/models/modules.py`:

```py
from dommel_library.datastructs import TensorDict, cat 




def model_rollout(model, sequence,reset=True):
    ''' Calculate a sequence of prior and posterior distributions
        and likelihoods for a given sequence
        `model`               - the model with a prior and likelihood function
        `sequence`            - the sequence to process
    '''
    result = []
    sequence = sequence.to(model.device)
    if reset:
        model.reset()
    for j in range(sequence.action.shape[1]):
        action = sequence.action[:, j, :]
        observations = {k: sequence[k][:, j, :] for k in model.observations()}
        #vis_image(observations['image'].squeeze(0), show=True)
        
        step = model.forward(action, observations)
        #print(step.image.squeeze(0).shape)
        
        result.append(TensorDict(step).unsqueeze(1))

    return cat(*result)

def imagine_rollout(model,
                    agent,
                    max_sequence_length=100):
    '''
    Imagine a single experience sequence of length `max_sequence_length`
    by doing a role-out with the agent and a model's prior and likelihood,
    the agent and model should be compatible
    Arguments:
    `model`               - the model with a prior and likelihood function
    `agent`               - an agent compatible with the environment
    `max_sequence_length` - the max length of a sequence, default 100
    '''
    result = []
    agent.reset()

    step = TensorDict({})
    step.state = model.get_state()
    action, _ = agent.act(step)

    for _ in range(max_sequence_length):
        t = model.forward(action.to(model.device))
        step = TensorDict({})
        step.action = action
        
        step.update(t)

        action, _ = agent.act(step)
        result.append(step.unsqueeze(1))

    return cat(*result)






class ReplayAgent():

    def __init__(self, actions, start_index=0):
        super().__init__()
        self.actions = actions
        self.start_index = start_index
        self.i = self.start_index

    def action_size(self):
        return self.actions.shape[-1]

    def act(self, observations):
        action = self.actions[:, self.i, ...]
        if self.i < self.actions.shape[1] - 1:
            self.i += 1
        return action, TensorDict({})

    def reset(self):
        self.i = self.start_index

```

`hierarchical-nav/experiments/OZ/models/oz_base_model.py`:

```py
import copy
import torch

from dommel_library.nn import module_factory, summary
from dommel_library.datastructs import TensorDict, cat
from dommel_library.distributions import StandardNormal


class BaseModel( torch.nn.Module):

    def __init__(
        self,
        num_states,
        num_actions,
        observations,
        prior,
        posterior,
        likelihood,
        device="cpu",
        **kwargs
    ):
        super(BaseModel, self).__init__()
        self._observations = observations
        self._num_actions = num_actions
        self._num_states = num_states

        self._prior, self._posterior, self._likelihoods = None, None, None

        # switch behaviour based on whether we get a Module or a config 
        if hasattr(prior, "update"):
            self._prior = module_factory(**prior)
            self._posterior = module_factory(**posterior)
            self._likelihoods = torch.nn.ModuleDict()
            if type(likelihood) == type(torch.nn.ModuleDict()):
                self._likelihoods = likelihood
            else:
                self._likelihoods= torch.nn.ModuleDict()
                for key in self._observations.keys():
                    if key in likelihood:
                        self._likelihoods[key] = module_factory(**likelihood[key])
                    elif 'modules' in likelihood:
                        self._likelihoods[key] = module_factory(**likelihood)
        
        else:
            self._prior = prior
            self._posterior = posterior
            self._likelihoods = self.check_moduleDict_observation(likelihood)
        
    
        self._state = None
        self._hidden = None
        self._posterior_dist = None

        self.to(device)


    @property
    def device(self):
        return next(self.parameters()).device

    def get_state(self):
        return self._state
    
    def get_post(self):
        return self._posterior_dist

    def state_size(self):
        return self._num_states

    def action_size(self):
        return self._num_actions

    def observations(self):
        return [*self._observations.keys()]

    def observation_size(self, key):
        return self._observations[key]
        
    def reset(self, state=None, hidden=None, posterior = None):
        self._state = state
        self._hidden = hidden
        self._posterior_dist = posterior
    
    def fork(self, batch_size=None):
        c = copy.copy(self)
        if batch_size is None:
            c._state = self._state.clone().detach()
        else:
            if len(self._state.shape) < 2:
                c._state = (self._state.expand(batch_size, self.state_size())
                        .clone().detach())
            else:

                cont = torch.mean(self._state, dim=0)

                c._state = (cont.repeat(batch_size, 1).clone().detach())

        if self._hidden is not None:
            hidden_copy = TensorDict({})
            for k, h in self._hidden.items():
                if batch_size is None:
                    hh = h.clone().detach()
                else:
                    if len(h.shape) < 2:
                        hh = h.expand(batch_size, h.shape[-1]).clone().detach()
                    else:
                      
                        hh = torch.mean(h, dim=0)
                        hh = (hh.repeat(batch_size, 1)
                                .clone().detach())
                hidden_copy[k] = hh

            c._hidden = hidden_copy

        return c

    def check_moduleDict_observation(self,input):
        if type(input) == type(torch.nn.ModuleDict()):
                output = input
        else:
            output= torch.nn.ModuleDict()
            for key in self._observations.keys():
                if key in input:
                    output[key] = input[key]
                elif 'modules' in input:
                    output[key] = input
        return output

    def prior(self, state, action):

        inputs = TensorDict({"state": state, "action": action})
        hidden = None
        inputs.update(hidden=self._hidden)
        output = self._prior(inputs)
        if "hidden" in output.keys():
            hidden = output["hidden"]
            del output["hidden"]

        return output, hidden

    def posterior(self, state, action, observations):
        inputs = TensorDict({key: value
                             for key, value in observations.items()})
        inputs.update(state=state, action=action)
        if self._hidden is not None:
            inputs.update(self._hidden)

        out = self._posterior(inputs)
        
        for key in observations.keys():
            if "features_"+key in out: 
                del out["features_"+key]
        return out

    def likelihood(self, state, key):
        inputs = TensorDict({"state": state})
        if self._hidden is not None:
            inputs.update(self._hidden)
        # discriminate between multiple likelihoods
        likelihood=self._likelihoods[key]
        out = likelihood(inputs)
        
        return out

    def __call__(self, input_dict, reset=True, **kwargs):
        if reset:
            self.reset()
        if len(input_dict.shape) == 1:
            input_dict = input_dict.unsqueeze(1)
        res = None
        for timestep in range(input_dict.shape[1]):
            step = input_dict[:, timestep]
            action = step["action"]
            obs = TensorDict({key: value for key, value in step.items()
                              if key is not action})

            reconstruct = kwargs.get('reconstruct', True)
            o = self.forward(action=action, observations=obs, reconstruct=reconstruct, **kwargs)
            o = o.unsqueeze(1)
            if res is None:
                res = o
            else:
                res = cat(res, o)
        return res

    def forward(self, action=None, observations=None, reconstruct=True, **kwargs):
        if self._state is None:
            self._state = torch.zeros(action.shape[0],
                                      self._num_states,
                                      dtype=torch.float32
                                      ).to(self.device)
      
        result, self._hidden = self.prior(self._state, action)

        if observations is None:
            if "prior" not in result.keys():
                raise Exception("Model should have a `prior` output")
            self._state = result.prior.sample()
            result.update(state=self._state)
            
        else:
            posterior_output = self.posterior(
                self._state, action, observations)
            if "posterior" not in posterior_output.keys():
                raise Exception("Model should have a `posterior` output")
            
            self._posterior_dist = posterior_output.posterior
            self._state = self._posterior_dist.sample()
            result.update(posterior_output, state=self._state)

        if reconstruct:

            for key in self._observations.keys():             
                out = self.likelihood(self._state, key)
                result.update(out)

        # remove the hidden key if we have a recurrent prior
        if "hidden" in result.keys():
            del result.hidden

        return TensorDict(result)

    def save(self, path):
        torch.save(self.state_dict(), path)
        new_path = path[:-2] + "full.pt"
        torch.save(self, new_path)

    def load(self, path, map_location= None):
        params = torch.load(path,
                            map_location=map_location)
        self.load_state_dict(params)

    @staticmethod
    def load_full(path):
        return torch.load(path)

    def global_prior(self):
        return StandardNormal(self._state.shape[0], self._state.shape[1])

    def summary(self):
        inputs = TensorDict({})
        for key, shape in self._observations.items():
            inputs[key] = torch.zeros(shape)
        inputs["action"] = torch.zeros(self._num_actions)
        inputs = inputs.unsqueeze(0).unsqueeze(0).to(self.device)
        summary(self, inputs)

```

`hierarchical-nav/experiments/OZ/models/oz_conv_model.py`:

```py
import torch
from dommel_library.nn import (
    ConvPipeline,
    UpConvPipeline,
    Cat,
    VariationalMLP,
    VariationalGRU,
    MLP,
    VariationalLSTM,
    Activation,
    get_activation,
)
from dommel_library.nn.composable_module import ComposableModule
from dommel_library.datastructs import Dict

from .oz_base_model import BaseModel


class ConvModel(BaseModel):

    def __init__(
        self,
        num_states,
        num_actions,
        observations,
        channels = [],
        hidden_layers = None,
        lstm_cells=None,
        lstm_posterior=False,
        gru_cells=None,
        gru_posterior=False,
        conv_block="Conv",
        decoder_film=False,
        decoder_act=None,
        device="cpu",
        **kwargs
    ):
        # observation_type=[]
        # observation_input=[]
        # for key, value in observations.items():
        #     observation_type.append(key)
        #     observation_input.append(value)


        blocks = ["Conv", conv_block, ...]

        prior_cat = Dict({"module": Cat(dim=1),
                          "input": ["action", "state"],
                          })

        # prior_lin = Dict({"module":
        #                     MLP(
        #                         num_states,
        #                         num_features,
        #                         hidden_layer,
        #                         activation=dict_input.get(
        #                             "activation", "Activation")),
        #                     "input": "state",
        #                     })
        if lstm_cells is not None:
            prior_nn = Dict({"module":
                             VariationalLSTM(num_states + num_actions,
                                             num_states,
                                             lstm_cells),
                             "input": ["...", "hidden"],
                             "output": ["prior", "hidden"],
                             })
        elif gru_cells is not None:
            prior_nn = Dict({"module":
                             VariationalGRU(num_states + num_actions,
                                            num_states,
                                            gru_cells),
                             "input": ["...", "hidden"],
                             "output": ["prior", "hidden"],
                             })
        else:
            prior_nn = Dict({"module":
                             VariationalMLP(
                                 num_states + num_actions,
                                 num_states,
                                 [hidden_layers[0], hidden_layers[0]],
                                 activation=kwargs.get(
                                     "activation", "Activation")),
                             "output": "prior",
                             })                   
        
        prior = ComposableModule([prior_cat,
                                  prior_nn])

       
        posterior_conv=[]
        posterior_input= ["action", "state"]
        output_types = []
        posterior_num_features=0

        likelihoods = torch.nn.ModuleDict()
        #if several sensors observations
        observation_dict = {}
        for observation_type, value in observations.items():
            
            #if we have different models for each observation
            if isinstance(value,(dict, Dict)):
                #TODO: THIS PART WAS NOT TESTED IN REAL SCENARIOS
                #observation_type = next(iter(value['observations']))
                type = value['type']
                input_shape = value['input_shape'] #value['observations'][next(iter(value['observations']))]
                del value['input_shape']
                if 'channels' in value:
                    channel = value['channels']
                    del value['channels']
                else:
                    channel = channels
                
                if 'hidden_layers' in value:
                    hidden_layer = value['hidden_layers']
                    del value['hidden_layers']
                else:
                    hidden_layer = hidden_layers

                dict_input = value
                
                
            else:
                type = 'Conv'
                input_shape = value
                dict_input = kwargs
                channel = channels
                hidden_layer = hidden_layers
            output_type = "features_"+str(observation_type) 
            observation_dict[observation_type] = input_shape
            if type == 'Conv':
                posterior_conv.append(Dict({"module":
                                    ConvPipeline(input_shape,
                                                    channel,
                                                    block=blocks,
                                                    **dict_input
                                                    ),
                                    "input": observation_type,
                                    "output": output_type,
                                    }))
                num_features = posterior_conv[-1].module.output_length
                posterior_num_features+= num_features
                output_types.append(output_type)
                posterior_input.append(output_type)   
               
                # stride becomes interpolate in decoder
                if "stride" in dict_input.keys():
                    stride = dict_input["stride"]
                    if isinstance(stride, list):
                        interpolate = stride[::-1]
                    else:
                        interpolate = stride
                    dict_input["interpolate"] = interpolate
                    del dict_input["stride"]

                if decoder_film:
                    likelihood_conv = Dict({"module":
                                            UpConvPipeline(
                                                input_shape, #input_shape
                                                channel[::-1],
                                                block=blocks[::-1],
                                                condition_size=num_states,
                                                **dict_input
                                            ),
                                            "input": ["...", "state"],
                                            "output": observation_type+"_reconstructed",
                                            })
                else:
                    likelihood_conv = Dict({"module":
                                            UpConvPipeline(
                                                input_shape,
                                                channel[::-1],
                                                block=blocks[::-1],
                                                **dict_input
                                            ),
                                            "output": observation_type+"_reconstructed",
                                            })
                likelihood_connect = Dict({"module": Activation()})
            
            elif type == 'Bool':
                num_features = input_shape
                likelihood_connect = Dict({"module": #TODO: I WANT SIGMOID WITH TH 0.5 -doesn't exist TT-
                                    get_activation(activation= dict_input.get('mlp_activation', Activation()))}) 
                likelihood_conv = Dict({"module": #TODO: I want NOTHING, doing twice same activation == to doing nothing
                                    get_activation(activation= 'Identity'), 
                                    "output": observation_type+"_reconstructed"}) 
            

            likelihood_mlp = Dict({"module":
                            MLP(
                                num_states,
                                num_features,
                                hidden_layer,
                                activation=dict_input.get(
                                    "activation", "Activation")),
                            "input": "state",
                            })
            

            likelihood_layers = [likelihood_mlp,
                                likelihood_connect,
                                likelihood_conv
                                ]
            if decoder_act:
                likelihood_layers.append(
                    Dict({"module": get_activation(decoder_act)}))
                        
            likelihoods[observation_type]= ComposableModule(likelihood_layers)

        #posterior_cat = Dict({"module": Cat(dim=1),
        #                    "input": posterior_input
        #                    })
        
        # posterior_mlp = Dict({"module":
        #                     VariationalMLP(
        #                         posterior_num_features + num_states + num_actions,
        #                         num_states,
        #                         hidden_layers,
        #                         activation=kwargs.get(
        #                             "activation", "Activation")),
        #                     "output": "posterior",
        #                     })
       
        if not (lstm_posterior or gru_posterior):
        
            posterior_cat = Dict({"module": Cat(dim=1),
                                    "input": posterior_input,
                                    })
            posterior_mlp = Dict({"module":
                                    VariationalMLP(
                                        posterior_num_features + num_states + num_actions,
                                        num_states,
                                        hidden_layers,
                                        activation=kwargs.get(
                                            "activation", "Activation")),
                                    "output": "posterior",
                                    })
        else:
            if lstm_posterior:
                hidden_key = "hidden" + str(len(lstm_cells) - 1)
                num_hidden = lstm_cells[-1]

            elif gru_posterior:
                hidden_key = "gru" + str(len(gru_cells) - 1)
                num_hidden = gru_cells[-1]
            
            output_types.insert(0,hidden_key)
            posterior_cat = Dict({"module": Cat(dim=1),
                                "input": output_types,
                                })
     
            posterior_mlp = Dict({"module":
                                VariationalMLP(
                                    posterior_num_features + num_hidden,
                                    num_states,
                                    hidden_layers,
                                    activation=kwargs.get(
                                        "activation", "Activation")),
                                "output": "posterior",
                                })

        
        modules_list = posterior_conv.copy()           
        modules_list.append(posterior_cat)
        modules_list.append(posterior_mlp)
        posterior=ComposableModule(modules_list)
        
           
        BaseModel.__init__(self,
                           num_states,
                           num_actions,
                           observation_dict,
                           prior,
                           posterior,
                           likelihoods,
                           device="cpu",
                           **kwargs)

```

`hierarchical-nav/experiments/OZ/train/__init__.py`:

```py
from experiments.OZ.train.model_trainer import ModelTrainer # noqa f401

```

`hierarchical-nav/experiments/OZ/train/model_trainer.py`:

```py
import torch

from dommel_library.datastructs import cat
from dommel_library.train import Trainer
from dommel_library.modules.visualize import visualize_sequence

from experiments.OZ.models import ReplayAgent, model_rollout, imagine_rollout

class ModelTrainer(Trainer):
    def __init__(self, model, train_dataset, loss, optimizer, log_dir,
                 warmup=1, **kwargs):
        # divide loss by the sequence length to have normalized loss value
        sequence_length = train_dataset.sample(1).shape[1]
        for _, loss_dict in loss._losses.items():
            weight = loss_dict.get("weight", 1.0)
            weight /= sequence_length
            loss_dict["weight"] = weight

        Trainer.__init__(self, model, train_dataset, loss,
                         optimizer, log_dir, **kwargs)
        self._warmup = warmup

    def _visualize_prior(self, batch):
        with torch.no_grad():
            replay = ReplayAgent(batch.action[:, self._warmup:, ...])
            bootstrap = model_rollout(
                self._model, batch[:, 0:self._warmup, ...])
            prior = imagine_rollout(self._model,
                                    replay,
                                    batch.shape[1] - self._warmup)
            sequence = cat(bootstrap, prior)
        prior_images = visualize_sequence(sequence, **self._vis_args)
        return prior_images

    def _log_visualization(self, train_logs, val_logs, epoch):
        train_logs, val_logs = super()._log_visualization(train_logs, val_logs, epoch)

        # add prior visualizations
        prior_images = self._visualize_prior(self._visualize_batch["train"])
        for k, img in prior_images.items():
            train_logs.add("prior/" + k, img, "image")

        if self._val_loader:
            prior_images = self._visualize_prior(self._visualize_batch["val"])
            for k, img in prior_images.items():
                val_logs.add("prior/" + k, img, "image")

        return train_logs, val_logs

```

`hierarchical-nav/gym-minigrid_minimal-1/LICENSE`:

```
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2019 Maxime Chevalier-Boisvert

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

```

`hierarchical-nav/gym-minigrid_minimal-1/README.md`:

```md
# Update

See below an example of what our custom environment can generate for 2 rows 3 columns and seed 218:

<p align="center">
  <img src="env_example.png" width="300" title="3x4 env seed 218">
</p>

# ORIGINAL SOURCE
# Minimalistic Gridworld Environment (MiniGrid)

[![Build Status](https://travis-ci.org/maximecb/gym-minigrid.svg?branch=master)](https://travis-ci.org/maximecb/gym-minigrid)

There are other gridworld Gym environments out there, but this one is
designed to be particularly simple, lightweight and fast. The code has very few
dependencies, making it less likely to break or fail to install. It loads no
external sprites/textures, and it can run at up to 5000 FPS on a Core i7
laptop, which means you can run your experiments faster. A known-working RL
implementation can be found [in this repository](https://github.com/lcswillems/torch-rl).

Requirements:
- Python 3.5+
- OpenAI Gym
- NumPy
- Matplotlib (optional, only needed for display)

Please use this bibtex if you want to cite this repository in your publications:

```
@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}
```

List of publications & submissions using MiniGrid or BabyAI (please open a pull request to add missing entries):
- [In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications](https://openreview.net/pdf?id=rUwm9wCjURV) (Imperial College London, ICLR 2022)
- [Interesting Object, Curious Agent: Learning Task-Agnostic Exploration](https://arxiv.org/abs/2111.13119) (Meta AI Research, NeurIPS 2021)
- [Safe Policy Optimization with Local Generalized Linear Function Approximations](https://arxiv.org/abs/2111.04894) (IBM Research, Tsinghua University, NeurIPS 2021)
- [A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning](https://arxiv.org/abs/2106.02097) (Mila, McGill University, 2021)
- [SPOTTER: Extending Symbolic Planning Operators through Targeted Reinforcement Learning](http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1118.pdf) (Tufts University, SIFT, AAMAS 2021)
- [Grid-to-Graph: Flexible Spatial Relational Inductive Biases for Reinforcement Learning](https://arxiv.org/abs/2102.04220) (UCL, AAMAS 2021)
- [Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments](https://openreview.net/forum?id=MtEE0CktZht) (Texas A&M University, Kuai Inc., ICLR 2021)
- [Adversarially Guided Actor-Critic](https://openreview.net/forum?id=_mQp5cr_iNy) (INRIA, Google Brain, ICLR 2021)
- [Information-theoretic Task Selection for Meta-Reinforcement Learning](https://papers.nips.cc/paper/2020/file/ec3183a7f107d1b8dbb90cb3c01ea7d5-Paper.pdf) (University of Leeds, NeurIPS 2020)
- [BeBold: Exploration Beyond the Boundary of Explored Regions](https://arxiv.org/pdf/2012.08621.pdf) (UCB, December 2020)
- [Approximate Information State for Approximate Planning and Reinforcement Learning in Partially Observed Systems](https://arxiv.org/abs/2010.08843) (McGill, October 2020)
- [Prioritized Level Replay](https://arxiv.org/pdf/2010.03934.pdf) (FAIR, October 2020)
- [AllenAct: A Framework for Embodied AI Research](https://arxiv.org/pdf/2008.12760.pdf) (Allen Institute for AI, August 2020)
- [Learning with AMIGO: Adversarially Motivated Intrinsic Goals](https://arxiv.org/pdf/2006.12122.pdf) (MIT, FAIR, ICLR 2021)
- [RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments](https://openreview.net/forum?id=rkg-TJBFPB) (FAIR, ICLR 2020)
- [Learning to Request Guidance in Emergent Communication](https://arxiv.org/pdf/1912.05525.pdf) (University of Amsterdam, Dec 2019)
- [Working Memory Graphs](https://arxiv.org/abs/1911.07141) (MSR, Nov 2019)
- [Fast Task-Adaptation for Tasks Labeled Using
Natural Language in Reinforcement Learning](https://arxiv.org/pdf/1910.04040.pdf) (Oct 2019, University of Antwerp)
- [Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck
](https://arxiv.org/abs/1910.12911) (MSR, NeurIPS, Oct 2019)
- [Recurrent Independent Mechanisms](https://arxiv.org/pdf/1909.10893.pdf) (Mila, Sept 2019) 
- [Learning Effective Subgoals with Multi-Task Hierarchical Reinforcement Learning](http://surl.tirl.info/proceedings/SURL-2019_paper_10.pdf) (Tsinghua University, August 2019)
- [Mastering emergent language: learning to guide in simulated navigation](https://arxiv.org/abs/1908.05135) (University of Amsterdam, Aug 2019)
- [Transfer Learning by Modeling a Distribution over Policies](https://arxiv.org/abs/1906.03574) (Mila, June 2019)
- [Reinforcement Learning with Competitive Ensembles
of Information-Constrained Primitives](https://arxiv.org/abs/1906.10667) (Mila, June 2019)
- [Learning distant cause and effect using only local and immediate credit assignment](https://arxiv.org/abs/1905.11589) (Incubator 491, May 2019)
- [Practical Open-Loop Optimistic Planning](https://arxiv.org/abs/1904.04700) (INRIA, April 2019)
- [Learning World Graphs to Accelerate Hierarchical Reinforcement Learning](https://arxiv.org/abs/1907.00664) (Salesforce Research, 2019)
- [Variational State Encoding as Intrinsic Motivation in Reinforcement Learning](https://mila.quebec/wp-content/uploads/2019/05/WebPage.pdf) (Mila, TARL 2019)
- [Unsupervised Discovery of Decision States Through Intrinsic Control](https://tarl2019.github.io/assets/papers/modhe2019unsupervised.pdf) (Georgia Tech, TARL 2019)
- [Modeling the Long Term Future in Model-Based Reinforcement Learning](https://openreview.net/forum?id=SkgQBn0cF7) (Mila, ICLR 2019)
- [Unifying Ensemble Methods for Q-learning via Social Choice Theory](https://arxiv.org/pdf/1902.10646.pdf) (Max Planck Institute, Feb 2019)
- [Planning Beyond The Sensing Horizon Using a Learned Context](https://personalrobotics.cs.washington.edu/workshops/mlmp2018/assets/docs/18_CameraReadySubmission.pdf) (MLMP@IROS, 2018)
- [Guiding Policies with Language via Meta-Learning](https://arxiv.org/abs/1811.07882) (UC Berkeley, Nov 2018)
- [On the Complexity of Exploration in Goal-Driven Navigation](https://arxiv.org/abs/1811.06889) (CMU, NeurIPS, Nov 2018)
- [Transfer and Exploration via the Information Bottleneck](https://openreview.net/forum?id=rJg8yhAqKm) (Mila, Nov 2018)
- [Creating safer reward functions for reinforcement learning agents in the gridworld](https://gupea.ub.gu.se/bitstream/2077/62445/1/gupea_2077_62445_1.pdf) (University of Gothenburg, 2018)
- [BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop](https://arxiv.org/abs/1810.08272) (Mila, ICLR, Oct 2018)

This environment has been built as part of work done at [Mila](https://mila.quebec). The Dynamic obstacles environment has been added as part of work done at [IAS in TU Darmstadt](https://www.ias.informatik.tu-darmstadt.de/) and the University of Genoa for mobile robot navigation with dynamic obstacles.


## Basic Usage

There is a UI application which allows you to manually control the agent with the arrow keys, the action/vel and observation are saved when the script is killed :

```
./manual_control.py
```

The environment being run can be selected with the `--env` option, eg:

```
./manual_control.py --env MiniGrid-Train-Rooms-v0
```

## Reinforcement Learning

If you want to train an agent with reinforcement learning, I recommend using the code found in the [torch-rl](https://github.com/lcswillems/torch-rl) repository. This code has been tested and is known to work with this environment. The default hyper-parameters are also known to converge.

A sample training command is:

```
cd torch-rl
python3 -m scripts.train --env MiniGrid-Empty-8x8-v0 --algo ppo
```

## Wrappers

MiniGrid is built to support tasks involving natural language and sparse rewards.
The observations are dictionaries, with an 'image' field, partially observable
view of the environment, a 'mission' field which is a textual string
describing the objective the agent should reach to get a reward, and a 'direction'
field which can be used as an optional compass. Using dictionaries makes it
easy for you to add additional information to observations
if you need to, without having to encode everything into a single tensor.

There are a variery of wrappers to change the observation format available in [gym_minigrid/wrappers.py](/gym_minigrid/wrappers.py). If your RL code expects one single tensor for observations, take a look at
`FlatObsWrapper`. There is also an `ImgObsWrapper` that gets rid of the 'mission' field in observations,
leaving only the image field tensor.

Please note that the default observation format is a partially observable view of the environment using a
compact and efficient encoding, with 3 input values per visible grid cell, 7x7x3 values total.
These values are **not pixels**. If you want to obtain an array of RGB pixels as observations instead,
use the `RGBImgPartialObsWrapper`. You can use it as follows:

```
from gym_minigrid.wrappers import *
env = gym.make('MiniGrid-Empty-8x8-v0')
env = RGBImgPartialObsWrapper(env) # Get pixel observations
env = ImgObsWrapper(env) # Get rid of the 'mission' field
obs = env.reset() # This now produces an RGB tensor only
```

## Design

Structure of the world:
- The world is an NxM grid of tiles
- Each tile in the grid world contains zero or one object
  - Cells that do not contain an object have the value `None`
- Each object has an associated discrete color (string)
- Each object has an associated type (string)
  - Provided object types are: wall, floor, lava, door, key, ball, box and goal
- The agent can pick up and carry exactly one object (eg: ball or key)
- To open a locked door, the agent has to be carrying a key matching the door's color

Actions in the basic environment:
- Turn left
- Turn right
- Move forward
- Pick up an object
- Drop the object being carried
- Toggle (open doors, interact with objects)
- Done (task completed, optional)

Default tile/observation encoding:
- Each tile is encoded as a 3 dimensional tuple: (OBJECT_IDX, COLOR_IDX, STATE) 
- OBJECT_TO_IDX and COLOR_TO_IDX mapping can be found in [gym_minigrid/minigrid.py](gym_minigrid/minigrid.py)
- e.g. door STATE -> 0: open, 1: closed, 2: locked

By default, sparse rewards are given for reaching a green goal tile. A
reward of 1 is given for success, and zero for failure. There is also an
environment-specific time step limit for completing the task.
You can define your own reward function by creating a class derived
from `MiniGridEnv`. Extending the environment with new object types or new actions
should be very easy. If you wish to do this, you should take a look at the
[gym_minigrid/minigrid.py](gym_minigrid/minigrid.py) source file.

## Included Environments

The environments listed below are implemented in the [gym_minigrid/envs](/gym_minigrid/envs) directory.
Each environment provides one or more configurations registered with OpenAI gym. Each environment
is also programmatically tunable in terms of size/complexity, which is useful for curriculum learning
or to fine-tune difficulty.

### Empty environment

Registered configurations:
- `MiniGrid-Empty-5x5-v0`
- `MiniGrid-Empty-Random-5x5-v0`
- `MiniGrid-Empty-6x6-v0`
- `MiniGrid-Empty-Random-6x6-v0`
- `MiniGrid-Empty-8x8-v0`
- `MiniGrid-Empty-16x16-v0`

<p align="center">
<img src="/figures/empty-env.png" width=250>
</p>

This environment is an empty room, and the goal of the agent is to reach the
green goal square, which provides a sparse reward. A small penalty is
subtracted for the number of steps to reach the goal. This environment is
useful, with small rooms, to validate that your RL algorithm works correctly,
and with large rooms to experiment with sparse rewards and exploration.
The random variants of the environment have the agent starting at a random
position for each episode, while the regular variants have the agent always
starting in the corner opposite to the goal.

### Four rooms environment

Registered configurations:
- `MiniGrid-FourRooms-v0`

<p align="center">
<img src="/figures/four-rooms-env.png" width=380>
</p>

Classic four room reinforcement learning environment. The agent must navigate
in a maze composed of four rooms interconnected by 4 gaps in the walls. To
obtain a reward, the agent must reach the green goal square. Both the agent
and the goal square are randomly placed in any of the four rooms.

### Door & key environment

Registered configurations:
- `MiniGrid-DoorKey-5x5-v0`
- `MiniGrid-DoorKey-6x6-v0`
- `MiniGrid-DoorKey-8x8-v0`
- `MiniGrid-DoorKey-16x16-v0`

<p align="center">
<img src="/figures/door-key-env.png">
</p>

This environment has a key that the agent must pick up in order to unlock
a goal and then get to the green goal square. This environment is difficult,
because of the sparse reward, to solve using classical RL algorithms. It is
useful to experiment with curiosity or curriculum learning.

### Multi-room environment

Registered configurations:
- `MiniGrid-MultiRoom-N2-S4-v0` (two small rooms)
- `MiniGrid-MultiRoom-N4-S5-v0` (four rooms)
- `MiniGrid-MultiRoom-N6-v0` (six rooms)

<p align="center">
<img src="/figures/multi-room.gif" width=416 height=424>
</p>

This environment has a series of connected rooms with doors that must be
opened in order to get to the next room. The final room has the green goal
square the agent must get to. This environment is extremely difficult to
solve using RL alone. However, by gradually increasing the number of
rooms and building a curriculum, the environment can be solved.

### Fetch environment

Registered configurations:
- `MiniGrid-Fetch-5x5-N2-v0`
- `MiniGrid-Fetch-6x6-N2-v0`
- `MiniGrid-Fetch-8x8-N3-v0`

<p align="center">
<img src="/figures/fetch-env.png" width=450>
</p>

This environment has multiple objects of assorted types and colors. The
agent receives a textual string as part of its observation telling it
which object to pick up. Picking up the wrong object terminates the
episode with zero reward.

### Go-to-door environment

Registered configurations:
- `MiniGrid-GoToDoor-5x5-v0`
- `MiniGrid-GoToDoor-6x6-v0`
- `MiniGrid-GoToDoor-8x8-v0`

<p align="center">
<img src="/figures/gotodoor-6x6.png" width=400>
</p>

This environment is a room with four doors, one on each wall. The agent
receives a textual (mission) string as input, telling it which door to go to,
(eg: "go to the red door"). It receives a positive reward for performing the
`done` action next to the correct door, as indicated in the mission string.

### Put-near environment

Registered configurations:
- `MiniGrid-PutNear-6x6-N2-v0`
- `MiniGrid-PutNear-8x8-N3-v0`

The agent is instructed through a textual string to pick up an object and
place it next to another object. This environment is easy to solve with two
objects, but difficult to solve with more, as it involves both textual
understanding and spatial reasoning involving multiple objects.

### Red and blue doors environment

Registered configurations:
- `MiniGrid-RedBlueDoors-6x6-v0`
- `MiniGrid-RedBlueDoors-8x8-v0`

The agent is randomly placed within a room with one red and one blue door
facing opposite directions. The agent has to open the red door and then open
the blue door, in that order. Note that, surprisingly, this environment is
solvable without memory.

### Memory environment

Registered configurations:
- `MiniGrid-MemoryS17Random-v0`
- `MiniGrid-MemoryS13Random-v0`
- `MiniGrid-MemoryS13-v0`
- `MiniGrid-MemoryS11-v0`

This environment is a memory test. The agent starts in a small room
where it sees an object. It then has to go through a narrow hallway
which ends in a split. At each end of the split there is an object,
one of which is the same as the object in the starting room. The
agent has to remember the initial object, and go to the matching
object at split.

### Locked room environment

Registed configurations:
- `MiniGrid-LockedRoom-v0`

The environment has six rooms, one of which is locked. The agent receives
a textual mission string as input, telling it which room to go to in order
to get the key that opens the locked room. It then has to go into the locked
room in order to reach the final goal. This environment is extremely difficult
to solve with vanilla reinforcement learning alone.

### Key corridor environment

Registed configurations:
- `MiniGrid-KeyCorridorS3R1-v0`
- `MiniGrid-KeyCorridorS3R2-v0`
- `MiniGrid-KeyCorridorS3R3-v0`
- `MiniGrid-KeyCorridorS4R3-v0`
- `MiniGrid-KeyCorridorS5R3-v0`
- `MiniGrid-KeyCorridorS6R3-v0`

<p align="center">
    <img src="figures/KeyCorridorS3R1.png" width="250">
    <img src="figures/KeyCorridorS3R2.png" width="250">
    <img src="figures/KeyCorridorS3R3.png" width="250">
    <img src="figures/KeyCorridorS4R3.png" width="250">
    <img src="figures/KeyCorridorS5R3.png" width="250">
    <img src="figures/KeyCorridorS6R3.png" width="250">
</p>

This environment is similar to the locked room environment, but there are
multiple registered environment configurations of increasing size,
making it easier to use curriculum learning to train an agent to solve it.
The agent has to pick up an object which is behind a locked door. The key is
hidden in another room, and the agent has to explore the environment to find
it. The mission string does not give the agent any clues as to where the
key is placed. This environment can be solved without relying on language.

### Unlock environment

Registed configurations:
- `MiniGrid-Unlock-v0`

<p align="center">
    <img src="figures/Unlock.png" width="200">
</p>

The agent has to open a locked door. This environment can be solved without
relying on language.

### Unlock pickup environment

Registed configurations:
- `MiniGrid-UnlockPickup-v0`

<p align="center">
    <img src="figures/UnlockPickup.png" width="250">
</p>

The agent has to pick up a box which is placed in another room, behind a
locked door. This environment can be solved without relying on language.

### Blocked unlock pickup environment

Registed configurations:
- `MiniGrid-BlockedUnlockPickup-v0`

<p align="center">
    <img src="figures/BlockedUnlockPickup.png" width="250">
</p>

The agent has to pick up a box which is placed in another room, behind a
locked door. The door is also blocked by a ball which the agent has to move
before it can unlock the door. Hence, the agent has to learn to move the ball,
pick up the key, open the door and pick up the object in the other room.
This environment can be solved without relying on language.

## Obstructed maze environment

Registered configurations:
- `MiniGrid-ObstructedMaze-1Dl-v0`
- `MiniGrid-ObstructedMaze-1Dlh-v0`
- `MiniGrid-ObstructedMaze-1Dlhb-v0`
- `MiniGrid-ObstructedMaze-2Dl-v0`
- `MiniGrid-ObstructedMaze-2Dlh-v0`
- `MiniGrid-ObstructedMaze-2Dlhb-v0`
- `MiniGrid-ObstructedMaze-1Q-v0`
- `MiniGrid-ObstructedMaze-2Q-v0`
- `MiniGrid-ObstructedMaze-Full-v0`

<p align="center">
  <img src="figures/ObstructedMaze-1Dl.png" width="250">
  <img src="figures/ObstructedMaze-1Dlh.png" width="250">
  <img src="figures/ObstructedMaze-1Dlhb.png" width="250">
  <img src="figures/ObstructedMaze-2Dl.png" width="100">
  <img src="figures/ObstructedMaze-2Dlh.png" width="100">
  <img src="figures/ObstructedMaze-2Dlhb.png" width="100">
  <img src="figures/ObstructedMaze-1Q.png" width="250">
  <img src="figures/ObstructedMaze-2Q.png" width="250">
  <img src="figures/ObstructedMaze-4Q.png" width="250">
</p>

The agent has to pick up a box which is placed in a corner of a 3x3 maze.
The doors are locked, the keys are hidden in boxes and doors are obstructed
by balls. This environment can be solved without relying on language.

## Distributional shift environment

Registered configurations:
- `MiniGrid-DistShift1-v0`
- `MiniGrid-DistShift2-v0`

This environment is based on one of the DeepMind [AI safety gridworlds](https://github.com/deepmind/ai-safety-gridworlds).
The agent starts in the top-left corner and must reach the goal which is in the top-right corner, but has to avoid stepping
into lava on its way. The aim of this environment is to test an agent's ability to generalize. There are two slightly
different variants of the environment, so that the agent can be trained on one variant and tested on the other.

<p align="center">
  <img src="figures/DistShift1.png" width="200">
  <img src="figures/DistShift2.png" width="200">
</p>

## Lava gap environment

Registered configurations:
- `MiniGrid-LavaGapS5-v0`
- `MiniGrid-LavaGapS6-v0`
- `MiniGrid-LavaGapS7-v0`

<p align="center">
  <img src="figures/LavaGapS6.png" width="200">
</p>

The agent has to reach the green goal square at the opposite corner of the room,
and must pass through a narrow gap in a vertical strip of deadly lava. Touching
the lava terminate the episode with a zero reward. This environment is useful
for studying safety and safe exploration.

## Lava crossing environment

Registered configurations:
- `MiniGrid-LavaCrossingS9N1-v0`
- `MiniGrid-LavaCrossingS9N2-v0`
- `MiniGrid-LavaCrossingS9N3-v0`
- `MiniGrid-LavaCrossingS11N5-v0`

<p align="center">
  <img src="figures/LavaCrossingS9N1.png" width="200">
  <img src="figures/LavaCrossingS9N2.png" width="200">
  <img src="figures/LavaCrossingS9N3.png" width="200">
  <img src="figures/LavaCrossingS11N5.png" width="250">
</p>

The agent has to reach the green goal square on the other corner of the room
while avoiding rivers of deadly lava which terminate the episode in failure.
Each lava stream runs across the room either horizontally or vertically, and
has a single crossing point which can be safely used;  Luckily, a path to the
goal is guaranteed to exist. This environment is useful for studying safety and
safe exploration.

## Simple crossing environment

Registered configurations:
- `MiniGrid-SimpleCrossingS9N1-v0`
- `MiniGrid-SimpleCrossingS9N2-v0`
- `MiniGrid-SimpleCrossingS9N3-v0`
- `MiniGrid-SimpleCrossingS11N5-v0`

<p align="center">
  <img src="figures/SimpleCrossingS9N1.png" width="200">
  <img src="figures/SimpleCrossingS9N2.png" width="200">
  <img src="figures/SimpleCrossingS9N3.png" width="200">
  <img src="figures/SimpleCrossingS11N5.png" width="250">
</p>

Similar to the `LavaCrossing` environment, the agent has to reach the green
goal square on the other corner of the room, however lava is replaced by
walls. This MDP is therefore much easier and and maybe useful for quickly
testing your algorithms.

### Dynamic obstacles environment

Registered configurations:
- `MiniGrid-Dynamic-Obstacles-5x5-v0`
- `MiniGrid-Dynamic-Obstacles-Random-5x5-v0`
- `MiniGrid-Dynamic-Obstacles-6x6-v0`
- `MiniGrid-Dynamic-Obstacles-Random-6x6-v0`
- `MiniGrid-Dynamic-Obstacles-8x8-v0`
- `MiniGrid-Dynamic-Obstacles-16x16-v0`

<p align="center">
<img src="/figures/dynamic_obstacles.gif">
</p>

This environment is an empty room with moving obstacles. The goal of the agent is to reach the green goal square without colliding with any obstacle. A large penalty is subtracted if the agent collides with an obstacle and the episode finishes. This environment is useful to test Dynamic Obstacle Avoidance for mobile robots with Reinforcement Learning in Partial Observability.

```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/__init__.py`:

```py
# Import the envs module so that envs register themselves
import gym_minigrid.envs

# Import wrappers so it's accessible when installing with pip
import gym_minigrid.wrappers

```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/envs/__init__.py`:

```py
from gym_minigrid.envs.aisle_door_rooms import *


```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/envs/aisle_door_rooms.py`:

```py
from gym_minigrid.minigrid import *
from gym_minigrid.register import register
import numpy as np

# Map of agent direction indices to vectors
DIR_TO_VEC = [
    # Pointing right (positive X)
    np.array((1, 0)),
    # Down (positive Y)
    np.array((0, 1)),
    # Pointing left (negative X)
    np.array((-1, 0)),
    # Up (negative Y)
    np.array((0, -1)),
]

class AisleDoorRooms(MiniGridEnv):
    """
    colored rooms connected by x tiles corridors
    """

    def __init__(
        self,
        size= 24,
        agent_start_pos= None,
        agent_start_dir= 0,
        rooms_size = 4,
        max_steps= None,
        rooms_in_row = 3,
        rooms_in_col = 3,
        corridor_length =5,
        automatic_door = True,
        wT_around = 6,
        wT_size = 1

    ):
        self.agent_start_pos = agent_start_pos
        self.agent_start_dir = agent_start_dir
        self.automatic_door = automatic_door
        self.wT_around = wT_around
        self.wT_size= wT_size

        self.rooms_size = rooms_size
        
        if isinstance(corridor_length, int) and corridor_length>0:
            self.corridor_length = corridor_length
            self.random_corridor_length = False
        else:
            self.random_corridor_length = True

        if self.random_corridor_length:
            self.corridor_length = self._rand_int(3,7)

        self.rooms_in_row = rooms_in_row
        self.rooms_in_col = rooms_in_col        

        width = (self.rooms_size * rooms_in_col + (self.corridor_length+1) * (rooms_in_col-1)-1  )
        height = (self.rooms_size * rooms_in_row + (self.corridor_length+1) * (rooms_in_row-1)-1 )
        #print('width, height',width, height)
        if width <= 20 :
            width+=1
        if height <= 20 :
            height+=1

        if width >= 30 :
            width-=1
        if height >= 30 :
            height-=1
        # print(width, height)
        if width >= 40 :
            width-=1
        if height >= 40 :
            height-=1
        
        # self.static_room = static_room
        # self.closed = closed
        self.color_idx = {
            'red' : 0,
            'green' : 1,
            'blue'  : 2,
            'purple': 3,
            'white' : 4,
            
        }

        super().__init__(
            grid_size=None,
            width=width,
            height=height,
            max_steps=max_steps,
            # Set this to True for maximum speed
            see_through_walls=False,
            
            
        )     


    def _gen_grid(self, width, height):
               
        #define the number of room in col/row
        rooms_in_row = self.rooms_in_row
        rooms_in_col = self.rooms_in_col

        room_w = self.rooms_size
        room_h = self.rooms_size
       
        # Create an empty grid
        self.grid = Grid(width, height)
        self.grid.grid = [Floor('black')] * width * height

        # Generate the surrounding walls
        self.grid.horz_wall(0, 0)
        self.grid.horz_wall(0, height - 1)
        self.grid.vert_wall(0, 0)
        self.grid.vert_wall(width - 1, 0)

        # Generate the surrounding walls
        self.grid.wall_rect(0, 0, width, height)
        balls_rooms = []
        while len(balls_rooms) < self.wT_around:
            col_room = self._rand_int(0, rooms_in_col)
            row_room = self._rand_int(0, rooms_in_row)
            if [col_room, row_room] not in balls_rooms:
                balls_rooms.append([col_room, row_room])

        pos_vert_R = None
        pos_horz_B = [None] * rooms_in_col
        agent_pose_options = []

     
        # For each row of rooms
        for row_inc in range(0, rooms_in_row):
            # For each column
            for col_inc in range(0, rooms_in_col):
                
                xL = col_inc * (room_w + self.corridor_length)
                yT = row_inc * (room_h + self.corridor_length)
                xR = xL + room_w 
                yB = yT + room_h
       
                color = list(self.color_idx.keys())[list(self.color_idx.values()).index(self._rand_int(0, 4))]

                for b in range(xL+1, xR):
                    for c in range(yT+1,yB):
                        self.put_obj(Floor(color),b, c)

                #upper wall and door
                if col_inc > 0:
                    self.grid.vert_wall(xL, yT, room_h+1)
                    self.grid.set(xL,pos_vert_R[1], Floor('black'))
                    self.put_obj(Door(color='white'), xL-int(self.corridor_length/2), pos_vert_R[1] )
                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((xL-int(self.corridor_length/2)-corridor_depth,pos_vert_R[1],0))
    
                # Bottom wall and door
                if col_inc + 1 < rooms_in_col:
                    self.grid.vert_wall(xR, yT, room_h+1)
                    pos_vert_R = (xR, self._rand_int(yT + 1, yB))
                    self.grid.set(*pos_vert_R, Floor('black'))
                    self.grid.horz_wall(xR, pos_vert_R[1]-1, self.corridor_length)  
                    self.grid.horz_wall(xR, pos_vert_R[1]+1, self.corridor_length)
                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((xR+int(self.corridor_length/2)+corridor_depth, pos_vert_R[1],2))  

                if row_inc > 0:
                    self.grid.horz_wall(xL, yT, room_w+1)
                    self.grid.set(pos_horz_B[col_inc][0],yT, Floor('black'))
                    self.put_obj(Door(color='white'), pos_horz_B[col_inc][0] , yT-int(self.corridor_length/2))
                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((pos_horz_B[col_inc][0],yT-int(self.corridor_length/2)-corridor_depth,1))
                   
                # Bottom wall and door
                if row_inc + 1 < rooms_in_row:
                    self.grid.horz_wall(xL, yB, room_w+1)
                    pos_horz_B[col_inc] = (self._rand_int(xL + 1, xR), yB)
                    self.grid.set(*pos_horz_B[col_inc], Floor('black'))
                    self.grid.vert_wall(pos_horz_B[col_inc][0]-1, yB, self.corridor_length)  
                    self.grid.vert_wall(pos_horz_B[col_inc][0]+1, yB, self.corridor_length)  
                    corridor_depth = self._rand_int(0, 2)
                    
                    agent_pose_options.append((pos_horz_B[col_inc][0], yB+int(self.corridor_length/2)+corridor_depth, 3))  


                if [col_inc, row_inc] in balls_rooms:
                    
                    ##1 by 1 wT
                    if self.wT_size == 1:
                        x, y = self._rand_int(xL+1, xR), self._rand_int(yT+1, yB)
                        self.put_obj(Goal('white'), x,y)
                        self.goal = (color, x,y)
                        #print('goal x,y', x,y)
                      
                    ##4 by 4 wTiles
                    else:
                        x = self._rand_int(xL+1, xR-1)
                        y = self._rand_int(yT+1, yB-1)
                        self.put_obj(Goal('white'), x, y)
                        self.put_obj(Goal('white'), x+1, y)
                        self.put_obj(Goal('white'), x, y+1)
                        self.put_obj(Goal('white'), x+1, y+1)
            
              
        # Place the agent 
        if self.agent_start_pos == None:
            index= self._rand_int(0, len(agent_pose_options))
            self.starting_agent_pos = np.array([agent_pose_options[index][0], agent_pose_options[index][1]])
            self.agent_start_dir = agent_pose_options[index][2]

            self.agent_pos = self.starting_agent_pos   
            self.agent_dir = self.agent_start_dir
     
        else:
            self.agent_pos = np.asarray(self.agent_start_pos)
            self.agent_dir = self.agent_start_dir
         
        
        self.vel_ob = [0,0]
        self.encoded_action = None      

        if self.grid.get(*self.agent_pos).type == 'door' :
            self.door_open = self.agent_pos
            self.grid.get(*self.agent_pos).toggle(self, self.agent_pos)
        elif self.grid.get(*self.front_pos).type == 'door':
            self.door_open = self.front_pos
            self.grid.get(*self.front_pos).toggle(self, self.front_pos)
        else:
            self.door_open = [0,0]
        
        # self.put_obj(Goal(),self.agent_pos[0]+1, self.agent_pos[1])
        self.mission = "Motion in color distinct rooms environment"



    def step(self, action):
        obs, reward, done, info = super().step(action)

        fwd_cell = self.grid.get(*self.front_pos)
        if self.automatic_door == True and fwd_cell != None and fwd_cell.type == 'door':
            #if a door up front, open it
            obs_2,_,_,_= super().step(self.actions.toggle, automatic_door=self.automatic_door)
            obs['door_open'] = obs_2['door_open']
            obs['image'] = obs_2['image']


        if obs['vel_ob'][0] ==obs['vel_ob'][1] :
            real_action = [0,0,0]
        else:
            real_action = obs['action'].copy()

        self.current_pose = self.action_to_pose(real_action, self.current_pose) 
        
        obs['pose'] = self.current_pose
        return obs, reward, done, info
    
    def action_to_pose(self,action,current_pose):
        
        if action[0] == 1:
            current_pose[:2] = DIR_TO_VEC[current_pose[2]] + current_pose[:2]
        elif action[1] == 1:
            current_pose[2] = (current_pose[2]+1)%4
        elif action[2] == 1:
            current_pose[2] = (current_pose[2]-1)%4

        return current_pose


class AisleDoorFourTilesRoomsE(AisleDoorRooms):
    def __init__(self, size=120, rooms_size=5, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None, corridor_length=4, max_steps = 400):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col,  wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length, max_steps = max_steps) #random_corridor_length if corridor_length 0 or not int
class AisleDoorFiveTilesRoomsE(AisleDoorRooms):
    def __init__(self,size=120, rooms_size=6, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None, 
                 corridor_length=4, max_steps = 400):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col,  \
                         wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length,\
                           max_steps = max_steps) 
class AisleDoorSixTilesRoomsE(AisleDoorRooms):
    def __init__(self,size=120, rooms_size=7, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None,
                  corridor_length=4, max_steps = 400):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col, wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length, max_steps = max_steps) 
class AisleDoorSevenTilesRoomsE(AisleDoorRooms):
    def __init__(self,size=120, rooms_size=8, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None, 
                 corridor_length=4, max_steps = 400):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col,  \
                         wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length, \
                         max_steps = max_steps) 
class AisleDoorEightTilesRoomsE(AisleDoorRooms):
    def __init__(self,size=120, rooms_size=9, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None, 
                 corridor_length=4, max_steps = 40):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col,  \
                         wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length, \
                         max_steps = max_steps) 
register(
    id='MiniGrid-4-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorFourTilesRoomsE'
)
register(
    id='MiniGrid-5-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorFiveTilesRoomsE'
)
register(
    id='MiniGrid-6-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorSixTilesRoomsE'
)
register(
    id='MiniGrid-7-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorSevenTilesRoomsE'
)
register(
    id='MiniGrid-8-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorEightTilesRoomsE'
)



```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/minigrid.py`:

```py
import math
import hashlib
import gym
from enum import IntEnum
import numpy as np
from gym import error, spaces, utils
from gym.utils import seeding
from .rendering import *
import random 

# Size in pixels of a tile in the full-scale human view
TILE_PIXELS = 32

# Map of color names to RGB values
COLORS = {
    'red'   : np.array([255, 0, 0]),
    'green' : np.array([0, 255, 0]),
    'blue'  : np.array([0, 0, 255]),
    'purple': np.array([112, 39, 195]),
    'yellow': np.array([255, 255, 0]),
    'grey'  : np.array([100, 100, 100]),
    'black'  : np.array([0, 0, 0]),
    'white'  : np.array([255, 255, 255]),
    'white_floor'  : np.array([255*2, 255*2, 255*2])
}

COLOR_NAMES = sorted(list(COLORS.keys()))

# Used to map colors to integers
COLOR_TO_IDX = {
    'red'   : 0,
    'green' : 1,
    'blue'  : 2,
    'purple': 3,
    'yellow': 4,
    'grey'  : 5,
    'black' : 6,
    'white' : 7,
    'white_floor':8,
}
IDX_TO_COLOR = dict(zip(COLOR_TO_IDX.values(), COLOR_TO_IDX.keys()))

# Map of object type to integers
OBJECT_TO_IDX = {
    'unseen'        : 0,
    'empty'         : 1,
    'wall'          : 2,
    'floor'         : 3,
    'door'          : 4,
    'key'           : 5,
    'ball'          : 6,
    'box'           : 7,
    'goal'          : 8,
    'lava'          : 9,
    'agent'         : 10,
}

IDX_TO_OBJECT = dict(zip(OBJECT_TO_IDX.values(), OBJECT_TO_IDX.keys()))

# Map of state names to integers
STATE_TO_IDX = {
    'open'  : 0,
    'closed': 1,
    'locked': 2,
}

# Map of agent direction indices to vectors
DIR_TO_VEC = [
    # Pointing right (positive X)
    np.array((1, 0)),
    # Down (positive Y)
    np.array((0, 1)),
    # Pointing left (negative X)
    np.array((-1, 0)),
    # Up (negative Y)
    np.array((0, -1)),
]

class WorldObj:
    """
    Base class for grid world objects
    """

    def __init__(self, type, color):
        assert type in OBJECT_TO_IDX, type
        assert color in COLOR_TO_IDX, color
        self.type = type
        self.color = color
        self.contains = None

        # Initial position of the object
        self.init_pos = None

        # Current position of the object
        self.cur_pos = None

    def can_overlap(self):
        """Can the agent overlap with this?"""
        return False

    def can_pickup(self):
        """Can the agent pick this up?"""
        return False

    def can_contain(self):
        """Can this contain another object?"""
        return False

    def see_behind(self):
        """Can the agent see behind this object?"""
        return True

    def toggle(self, env, pos):
        """Method to trigger/toggle an action this object performs"""
        return False

    def encode(self):
        """Encode the a description of this object as a 3-tuple of integers"""
        return (OBJECT_TO_IDX[self.type], COLOR_TO_IDX[self.color], 0)

    @staticmethod
    def decode(type_idx, color_idx, state):
        """Create an object from a 3-tuple state description"""

        obj_type = IDX_TO_OBJECT[type_idx]
        color = IDX_TO_COLOR[color_idx]

        if obj_type == 'empty' or obj_type == 'unseen':
            return None

        # State, 0: open, 1: closed, 2: locked
        is_open = state == 0
        is_locked = state == 2

        if obj_type == 'wall':
            v = Wall(color)
        elif obj_type == 'floor':
            v = Floor(color)
        elif obj_type == 'ball':
            v = Ball(color)
        elif obj_type == 'key':
            v = Key(color)
        elif obj_type == 'box':
            v = Box(color)
        elif obj_type == 'door':
            v = Door(color, is_open, is_locked)
        elif obj_type == 'goal':
            v = Goal(color)
        elif obj_type == 'lava':
            v = Lava()
        else:
            assert False, "unknown object type in decode '%s'" % obj_type

        return v

    def render(self, r):
        """Draw this object with the given renderer"""
        raise NotImplementedError

class Goal(WorldObj):
    def __init__(self, color= 'green'):
        super().__init__('goal', color)

    def can_overlap(self):
        return True

    def render(self, img):
        fill_coords(img, point_in_rect(0, 1, 0, 1), COLORS[self.color])

class Floor(WorldObj):
    """
    Colored floor tile the agent can walk over
    """

    def __init__(self, color='blue'):
        super().__init__('floor', color)

    def can_overlap(self):
        return True

    def render(self, img):
        # Give the floor a pale color
        color = COLORS[self.color] / 2
        fill_coords(img, point_in_rect(0.031, 1, 0.031, 1), color)


class Lava(WorldObj):
    def __init__(self):
        super().__init__('lava', 'red')

    def can_overlap(self):
        return True

    def render(self, img):
        c = (255, 128, 0)

        # Background color
        fill_coords(img, point_in_rect(0, 1, 0, 1), c)

        # Little waves
        for i in range(3):
            ylo = 0.3 + 0.2 * i
            yhi = 0.4 + 0.2 * i
            fill_coords(img, point_in_line(0.1, ylo, 0.3, yhi, r=0.03), (0,0,0))
            fill_coords(img, point_in_line(0.3, yhi, 0.5, ylo, r=0.03), (0,0,0))
            fill_coords(img, point_in_line(0.5, ylo, 0.7, yhi, r=0.03), (0,0,0))
            fill_coords(img, point_in_line(0.7, yhi, 0.9, ylo, r=0.03), (0,0,0))

class Wall(WorldObj):
    def __init__(self, color='grey'):
        super().__init__('wall', color)

    def see_behind(self):
        return False

    def render(self, img):
        fill_coords(img, point_in_rect(0, 1, 0, 1), COLORS[self.color])

class Door(WorldObj):
    def __init__(self, color, is_open=False, is_locked=False):
        super().__init__('door', color)
        self.is_open = is_open
        self.is_locked = is_locked

    def can_overlap(self):
        """The agent can only walk over this cell when the door is open if 'return self.is_open' """
        return self.is_open

    def see_behind(self):
        return self.is_open

    def toggle(self, env, pos):
        # If the player has the right key to open the door
        if self.is_locked:
            if isinstance(env.carrying, Key) and env.carrying.color == self.color:
                self.is_locked = False
                self.is_open = True
                return True
            return False


        self.is_open = not self.is_open
        return True

    def encode(self):
        """Encode the a description of this object as a 3-tuple of integers"""

        # State, 0: open, 1: closed, 2: locked
        if self.is_open:
            state = 0
        elif self.is_locked:
            state = 2
        elif not self.is_open:
            state = 1

        return (OBJECT_TO_IDX[self.type], COLOR_TO_IDX[self.color], state)

    def render(self, img):
        c = COLORS[self.color]

        if self.is_open:
            fill_coords(img, point_in_rect(0.88, 1.00, 0.00, 1.00), c)
            fill_coords(img, point_in_rect(0.92, 0.96, 0.04, 0.96), (0,0,0))
            return

        # Door frame and door
        if self.is_locked:
            fill_coords(img, point_in_rect(0.00, 1.00, 0.00, 1.00), c)
            fill_coords(img, point_in_rect(0.06, 0.94, 0.06, 0.94), 0.45 * np.array(c))

            # Draw key slot
            fill_coords(img, point_in_rect(0.52, 0.75, 0.50, 0.56), c)
        else:
            fill_coords(img, point_in_rect(0.00, 1.00, 0.00, 1.00), c)
            fill_coords(img, point_in_rect(0.04, 0.96, 0.04, 0.96), (0,0,0))
            fill_coords(img, point_in_rect(0.08, 0.92, 0.08, 0.92), c)
            fill_coords(img, point_in_rect(0.12, 0.88, 0.12, 0.88), (0,0,0))

            # Draw door handle
            fill_coords(img, point_in_circle(cx=0.75, cy=0.50, r=0.08), c)

class Key(WorldObj):
    def __init__(self, color='blue'):
        super(Key, self).__init__('key', color)

    def can_pickup(self):
        return True

    def render(self, img):
        c = COLORS[self.color]

        # Vertical quad
        fill_coords(img, point_in_rect(0.50, 0.63, 0.31, 0.88), c)

        # Teeth
        fill_coords(img, point_in_rect(0.38, 0.50, 0.59, 0.66), c)
        fill_coords(img, point_in_rect(0.38, 0.50, 0.81, 0.88), c)

        # Ring
        fill_coords(img, point_in_circle(cx=0.56, cy=0.28, r=0.190), c)
        fill_coords(img, point_in_circle(cx=0.56, cy=0.28, r=0.064), (0,0,0))

class Ball(WorldObj):
    def __init__(self, color='blue'):
        super(Ball, self).__init__('ball', color)

    def can_pickup(self):
        return True
    
    def can_overlap(self):
        return True

    def render(self, img):
        fill_coords(img, point_in_circle(0.5, 0.5, 0.31), COLORS[self.color])

class Box(WorldObj):
    def __init__(self, color, contains=None):
        super(Box, self).__init__('box', color)
        self.contains = contains

    def can_pickup(self):
        return True
    
    def can_overlap(self):
        return True

    def render(self, img):
        c = COLORS[self.color]

        # Outline
        fill_coords(img, point_in_rect(0.12, 0.88, 0.12, 0.88), c)
        fill_coords(img, point_in_rect(0.18, 0.82, 0.18, 0.82), (0,0,0))

        # Horizontal slit
        fill_coords(img, point_in_rect(0.16, 0.84, 0.47, 0.53), c)

    def toggle(self, env, pos):
        # Replace the box by its contents
        env.grid.set(*pos, self.contains)
        return True

class Grid:
    """
    Represent a grid and operations on it
    """

    # Static cache of pre-renderer tiles
    tile_cache = {}

    def __init__(self, width, height):
        assert width >= 3
        assert height >= 3

        self.width = width
        self.height = height

        self.grid = [None] * width * height

    def __contains__(self, key):
        if isinstance(key, WorldObj):
            for e in self.grid:
                if e is key:
                    return True
        elif isinstance(key, tuple):
            for e in self.grid:
                if e is None:
                    continue
                if (e.color, e.type) == key:
                    return True
                if key[0] is None and key[1] == e.type:
                    return True
        return False

    def __eq__(self, other):
        grid1  = self.encode()
        grid2 = other.encode()
        return np.array_equal(grid2, grid1)

    def __ne__(self, other):
        return not self == other

    def copy(self):
        from copy import deepcopy
        return deepcopy(self)

    def set(self, i, j, v):
        assert i >= 0 and i < self.width
        assert j >= 0 and j < self.height
        self.grid[j * self.width + i] = v

    def get(self, i, j):
        assert i >= 0 and i < self.width
        assert j >= 0 and j < self.height
        return self.grid[j * self.width + i]

    def horz_wall(self, x, y, length=None, obj_type=Wall):
        if length is None:
            length = self.width - x
        for i in range(0, length):
            self.set(x + i, y, obj_type())

    def vert_wall(self, x, y, length=None, obj_type=Wall):
        if length is None:
            length = self.height - y
        for j in range(0, length):
            self.set(x, y + j, obj_type())

    def wall_rect(self, x, y, w, h):
        self.horz_wall(x, y, w)
        self.horz_wall(x, y+h-1, w)
        self.vert_wall(x, y, h)
        self.vert_wall(x+w-1, y, h)

    def rotate_left(self):
        """
        Rotate the grid to the left (counter-clockwise)
        """

        grid = Grid(self.height, self.width)

        for i in range(self.width):
            for j in range(self.height):
                v = self.get(i, j)
                grid.set(j, grid.height - 1 - i, v)

        return grid

    def slice(self, topX, topY, width, height):
        """
        Get a subset of the grid
        """

        grid = Grid(width, height)

        for j in range(0, height):
            for i in range(0, width):
                x = topX + i
                y = topY + j

                if x >= 0 and x < self.width and \
                   y >= 0 and y < self.height:
                    v = self.get(x, y)
                else:
                    v = Wall()
                grid.set(i, j, v)

        return grid

    @classmethod
    def render_tile(
        cls,
        obj,
        agent_dir=None,
        highlight=False,
        tile_size=TILE_PIXELS,
        subdivs=3
    ):
        """
        Render a tile and cache the result
        """

        # Hash map lookup key for the cache
        key = (agent_dir, highlight, tile_size)
        key = obj.encode() + key if obj else key

        if key in cls.tile_cache:
            return cls.tile_cache[key]

        img = np.zeros(shape=(tile_size * subdivs, tile_size * subdivs, 3), dtype=np.uint8)

        # Draw the grid lines (top and left edges)
        fill_coords(img, point_in_rect(0, 0.031, 0, 1), (100, 100, 100))
        fill_coords(img, point_in_rect(0, 1, 0, 0.031), (100, 100, 100))

        if obj != None:
            obj.render(img)

        # Overlay the agent on top
        if agent_dir is not None:
            tri_fn = point_in_triangle(
                (0.12, 0.19),
                (0.87, 0.50),
                (0.12, 0.81),
            )

            # Rotate the agent based on its direction
            tri_fn = rotate_fn(tri_fn, cx=0.5, cy=0.5, theta=0.5*math.pi*agent_dir)
            fill_coords(img, tri_fn, (255, 0, 0))

        # Highlight the cell if needed
        if highlight:
            highlight_img(img)

        # Downsample the image to perform supersampling/anti-aliasing
        img = downsample(img, subdivs)

        # Cache the rendered tile
        cls.tile_cache[key] = img

        return img

    def render(
        self,
        tile_size,
        agent_pos=None,
        agent_dir=None,
        highlight_mask=None,
        agent_view_poses = [],
    ):
        """
        Render this grid at a given scale
        :param r: target renderer object
        :param tile_size: tile size in pixels
        :param agent_view: wether we render whole grid or just agent ob
        """

        if highlight_mask is None:
            highlight_mask = np.zeros(shape=(self.width, self.height), dtype=bool)

        
        if len(agent_view_poses) > 0:
            agent_view_size = int(np.sqrt(len(agent_view_poses)))
            img = np.zeros(shape=(agent_view_size * tile_size, agent_view_size * tile_size, 3), dtype=np.uint8)
           

        else:
            # Compute the total grid size
            width_px = self.width * tile_size
            height_px = self.height * tile_size
            img = np.zeros(shape=(height_px, width_px, 3), dtype=np.uint8)
        
        
        
        # Render the agent view
        if len(agent_view_poses)> 0:
            n = 0 
            agent_dir = 3
            # if np.all(dir_vec == (0,1)):
            #     agent_view_poses = agent_view_poses[::-1]
            for v in agent_view_poses:
                i,j = v[0], v[1]
                try:
                    cell = self.get(i, j)
                    agent_here = np.array_equal(agent_pos, (i, j))
                    
                    tile_img = Grid.render_tile(
                        cell,
                        agent_dir=agent_dir if agent_here else None,
                        highlight=highlight_mask[i, j],
                        tile_size=tile_size
                    )
                except AssertionError:
                    tile_img = (0,0,0)
            
                #if dir_vec[0] != 0:
                #t_in_img = [n // agent_view_size, n % agent_view_size]
                # else:
                t_in_img = [n % agent_view_size, n // agent_view_size,]
                
                ymin = t_in_img[1] * tile_size
                ymax = (t_in_img[1]+1) * tile_size
                xmin = t_in_img[0] * tile_size
                xmax = (t_in_img[0]+1) * tile_size

                n+=1
                try:
                    if highlight_mask[i,j] == False:
                        tile_img = (0,0,0)
                except IndexError:
                    tile_img = (0,0,0)
                img[ymin:ymax, xmin:xmax, :] = tile_img


        # Render the grid
        else:
            for j in range(0, self.height):
                for i in range(0, self.width):
                    cell = self.get(i, j)
                    agent_here = np.array_equal(agent_pos, (i, j))
                    tile_img = Grid.render_tile(
                        cell,
                        agent_dir=agent_dir if agent_here else None,
                        highlight=highlight_mask[i, j],
                        tile_size=tile_size
                    )
                    
                    
                    ymin = j * tile_size
                    ymax = (j+1) * tile_size
                    xmin = i * tile_size
                    xmax = (i+1) * tile_size
                    img[ymin:ymax, xmin:xmax, :] = tile_img

        return img

    def encode(self, vis_mask=None):
        """
        Produce a compact numpy encoding of the grid
        """

        if vis_mask is None:
            vis_mask = np.ones((self.width, self.height), dtype=bool)

        array = np.zeros((self.width, self.height, 3), dtype='uint8')

        for i in range(self.width):
            for j in range(self.height):
                if vis_mask[i, j]:
                    v = self.get(i, j)

                    if v is None:
                        array[i, j, 0] = OBJECT_TO_IDX['empty']
                        array[i, j, 1] = 0
                        array[i, j, 2] = 0

                    else:
                        array[i, j, :] = v.encode()

        return array

    @staticmethod
    def decode(array):
        """
        Decode an array grid encoding back into a grid
        """

        width, height, channels = array.shape
        assert channels == 3

        vis_mask = np.ones(shape=(width, height), dtype=bool)

        grid = Grid(width, height)
        for i in range(width):
            for j in range(height):
                type_idx, color_idx, state = array[i, j]
                v = WorldObj.decode(type_idx, color_idx, state)
                grid.set(i, j, v)
                vis_mask[i, j] = (type_idx != OBJECT_TO_IDX['unseen'])

        return grid, vis_mask

    def process_vis(grid, agent_pos):
        mask = np.zeros(shape=(grid.width, grid.height), dtype=bool)

        mask[agent_pos[0], agent_pos[1]] = True

        for j in reversed(range(0, grid.height)):
            for i in range(0, grid.width-1):
                if not mask[i, j]:
                    continue

                cell = grid.get(i, j)
                if cell and not cell.see_behind():
                    continue

                mask[i+1, j] = True
                if j > 0:
                    mask[i+1, j-1] = True
                    mask[i, j-1] = True

            for i in reversed(range(1, grid.width)):
                if not mask[i, j]:
                    continue

                cell = grid.get(i, j)
                if cell and not cell.see_behind():
                    continue

                mask[i-1, j] = True
                if j > 0:
                    mask[i-1, j-1] = True
                    mask[i, j-1] = True

        for j in range(0, grid.height):
            for i in range(0, grid.width):
                if not mask[i, j]:
                    grid.set(i, j, None)

        return mask

class MiniGridEnv(gym.Env):
    """
    2D grid world game environment
    """

    metadata = {
        'render.modes': ['human', 'rgb_array'],
        'video.frames_per_second' : 10
    }

    # Enumeration of possible actions
    class Actions(IntEnum):
        # Turn left, turn right, move forward
        left = 0
        right = 1
        forward = 2

        # Pick up an object
        pickup = 3
        # Drop an object
        drop = 4
        # Toggle/activate an object
        toggle = 5

        # Done completing task
        done = 6

    def __init__(
        self,
        grid_size=None,
        width=None,
        height=None,
        max_steps=100,
        see_through_walls=False,
        seed=1337,
        agent_view_size=7
    ):
        # Can't set both grid_size and width/height
        if grid_size:
            assert width == None and height == None
            width = grid_size
            height = grid_size

        # Action enumeration for this environment
        self.actions = MiniGridEnv.Actions

        # Actions are discrete integer values
        self.action_space = spaces.Discrete(len(self.actions))

        # Number of cells (width and height) in the agent view
        assert agent_view_size % 2 == 1
        assert agent_view_size >= 3
        self.agent_view_size = agent_view_size

        # Observations are dictionaries containing an
        # encoding of the grid and a textual 'mission' string
        self.observation_space = spaces.Box(
            low=0,
            high=255,
            shape=(self.agent_view_size, self.agent_view_size, 3),
            dtype='uint8'
        )
        self.observation_space = spaces.Dict({
            'image': self.observation_space
        })

        # Range of possible rewards
        self.reward_range = (0, 1)

        # Window to use for human rendering mode
        self.window = None

        # Environment configuration
        self.width = width
        self.height = height
        self.max_steps = max_steps
        self.see_through_walls = see_through_walls

        # Current position and direction of the agent
        self.agent_pos = None
        self.agent_dir = None

        # Initialize the RNG
        self.seed(seed=seed)

        # Initialize the state
        self.reset()

    def reset(self, test=False):
        # Current position and direction of the agent
        self.agent_pos = None
        self.agent_dir = None
        self.current_pose = np.array([0,0,0])

        # Generate a new random grid at the start of each episode
        # To keep the same grid for each episode, call env.seed() with
        # the same seed before calling env.reset()
        self._gen_grid(self.width, self.height)
        
        if test == True:
            print('CHECK GOAL POSE', self.goal)   
            self.put_obj(Floor(self.goal[0]),self.goal[1], self.goal[2])
            x, y = self._rand_int(1, self.width), self._rand_int(1, self.height)
            tile = self.grid.get(x,y)
            while tile.type != 'floor' or tile.color == 'black':
                x, y = self._rand_int(1, self.width), self._rand_int(1, self.height)
                # print(x,y)
                tile = self.grid.get(x,y)
            
            self.put_obj(Goal('white'), x, y)
            print('NEW GOAL POSE', x,y)   

        # These fields should be defined by _gen_grid
        assert self.agent_pos is not None
        assert self.agent_dir is not None

        # Check that the agent doesn't overlap with an object
        start_cell = self.grid.get(*self.agent_pos)
        assert start_cell is None or start_cell.can_overlap()

        # Item picked up, being carried, initially nothing
        self.carrying = None

        # Step count since episode start
        self.step_count = 0

        # Return first observation
        obs = self.gen_obs()
        return obs

    def seed(self, seed=1337):
        # Seed the random number generator
        self.np_random, _ = seeding.np_random(seed)
        return [seed]

    def hash(self, size=16):
        """Compute a hash that uniquely identifies the current state of the environment.
        :param size: Size of the hashing
        """
        sample_hash = hashlib.sha256()

        to_encode = [self.grid.encode().tolist(), self.agent_pos, self.agent_dir]
        for item in to_encode:
            sample_hash.update(str(item).encode('utf8'))

        return sample_hash.hexdigest()[:size]

    @property
    def steps_remaining(self):
        if self.max_steps:
            return self.max_steps - self.step_count
        else:
            return 1

    def __str__(self):
        """
        Produce a pretty string of the environment's grid along with the agent.
        A grid cell is represented by 2-character string, the first one for
        the object and the second one for the color.
        """

        # Map of object types to short string
        OBJECT_TO_STR = {
            'wall'          : 'W',
            'floor'         : 'F',
            'door'          : 'D',
            'key'           : 'K',
            'ball'          : 'A',
            'box'           : 'B',
            'goal'          : 'G',
            'lava'          : 'V',
        }

        # Short string for opened door
        OPENDED_DOOR_IDS = '_'

        # Map agent's direction to short string
        AGENT_DIR_TO_STR = {
            0: '>',
            1: 'V',
            2: '<',
            3: '^'
        }

        str = ''

        for j in range(self.grid.height):

            for i in range(self.grid.width):
                if i == self.agent_pos[0] and j == self.agent_pos[1]:
                    str += 2 * AGENT_DIR_TO_STR[self.agent_dir]
                    continue

                c = self.grid.get(i, j)

                if c == None:
                    str += '  '
                    continue

                if c.type == 'door':
                    if c.is_open:
                        str += '__'
                    elif c.is_locked:
                        str += 'L' + c.color[0].upper()
                    else:
                        str += 'D' + c.color[0].upper()
                    continue

                str += OBJECT_TO_STR[c.type] + c.color[0].upper()

            if j < self.grid.height - 1:
                str += '\n'

        return str

    def _gen_grid(self, width, height):
        assert False, "_gen_grid needs to be implemented by each environment"

    def _reward(self):
        """
        Compute the reward to be given upon success
        """
        if self.max_steps:
            return 1 - 0.9 * (self.step_count / self.max_steps)
        else:
            return 1

    def _rand_int(self, low, high):
        """
        Generate random integer in [low,high[
        """

        return self.np_random.randint(low, high)

    def _rand_float(self, low, high):
        """
        Generate random float in [low,high[
        """

        return self.np_random.uniform(low, high)

    def _rand_bool(self):
        """
        Generate random boolean value
        """

        return (self.np_random.randint(0, 2) == 0)

    def _rand_elem(self, iterable):
        """
        Pick a random element in a list
        """

        lst = list(iterable)
        idx = self._rand_int(0, len(lst))
        return lst[idx]

    def _rand_subset(self, iterable, num_elems):
        """
        Sample a random subset of distinct elements of a list
        """

        lst = list(iterable)
        assert num_elems <= len(lst)

        out = []

        while len(out) < num_elems:
            elem = self._rand_elem(lst)
            lst.remove(elem)
            out.append(elem)

        return out

    def _rand_color(self):
        """
        Generate a random color name (string)
        """

        return self._rand_elem(COLOR_NAMES)

    def _rand_pos(self, xLow, xHigh, yLow, yHigh):
        """
        Generate a random (x,y) position tuple
        """

        return (
            self.np_random.randint(xLow, xHigh),
            self.np_random.randint(yLow, yHigh)
        )

    def place_obj(self,
        obj,
        top=None,
        size=None,
        reject_fn=None,
        max_tries=math.inf
    ):
        """
        Place an object at an empty position in the grid

        :param top: top-left position of the rectangle where to place
        :param size: size of the rectangle where to place
        :param reject_fn: function to filter out potential positions
        """

        if top is None:
            top = (0, 0)
        else:
            top = (max(top[0], 0), max(top[1], 0))

        if size is None:
            size = (self.grid.width, self.grid.height)

        num_tries = 0

        while True:
            # This is to handle with rare cases where rejection sampling
            # gets stuck in an infinite loop
            if num_tries > max_tries:
                raise RecursionError('rejection sampling failed in place_obj')

            num_tries += 1

            pos = np.array((
                self._rand_int(top[0], min(top[0] + size[0], self.grid.width)),
                self._rand_int(top[1], min(top[1] + size[1], self.grid.height))
            ))

            # Don't place the object on top of another object
            grid_content = self.grid.get(*pos) 
            if grid_content!= None:
                if isinstance(grid_content, Floor):
                    break
                else:
                    continue

            # Don't place the object where the agent is
            if np.array_equal(pos, self.agent_pos):
                continue

            # Check if there is a filtering criterion
            if reject_fn and reject_fn(self, pos):
                continue

            break

        #self.grid.set(*pos, obj)

        if obj is not None:
            obj.init_pos = pos
            obj.cur_pos = pos

        return pos

    def put_obj(self, obj, i, j):
        """
        Put an object at a specific position in the grid
        """

        self.grid.set(i, j, obj)
        obj.init_pos = (i, j)
        obj.cur_pos = (i, j)

    def place_agent(
        self,
        top=None,
        size=None,
        rand_dir=True,
        max_tries=math.inf
    ):
        """
        Set the agent's starting point at an empty position in the grid
        """
        self.agent_pos = None
        self.encoded_action = None
        self.vel_ob = [0,0]
        pos = self.place_obj(None, top, size, max_tries=max_tries)
        self.agent_pos = pos
        #self.door_open=False
        if rand_dir:
            self.agent_dir = self._rand_int(0, 4)

        return pos

    @property
    def dir_vec(self):
        """
        Get the direction vector for the agent, pointing in the direction
        of forward movement.
        """

        assert self.agent_dir >= 0 and self.agent_dir < 4
        return DIR_TO_VEC[self.agent_dir]

    @property
    def right_vec(self):
        """
        Get the vector pointing to the right of the agent.
        """

        dx, dy = self.dir_vec
        return np.array((-dy, dx))

    @property
    def front_pos(self):
        """
        Get the position of the cell that is right in front of the agent
        """

        return self.agent_pos + self.dir_vec

    def get_view_coords(self, i, j):
        """
        Translate and rotate absolute grid coordinates (i, j) into the
        agent's partially observable view (sub-grid). Note that the resulting
        coordinates may be negative or outside of the agent's view size.
        """

        ax, ay = self.agent_pos
        dx, dy = self.dir_vec
        rx, ry = self.right_vec

        # Compute the absolute coordinates of the top-left view corner
        sz = self.agent_view_size
        hs = self.agent_view_size // 2
        tx = ax + (dx * (sz-1)) - (rx * hs)
        ty = ay + (dy * (sz-1)) - (ry * hs)

        lx = i - tx
        ly = j - ty

        # Project the coordinates of the object relative to the top-left
        # corner onto the agent's own coordinate system
        vx = (rx*lx + ry*ly)
        vy = -(dx*lx + dy*ly)

        return vx, vy

    def get_view_exts(self):
        """
        Get the extents of the square set of tiles visible to the agent
        Note: the bottom extent indices are not included in the set
        """

        # Facing right
        if self.agent_dir == 0:
            topX = self.agent_pos[0]
            topY = self.agent_pos[1] - self.agent_view_size // 2
        # Facing down
        elif self.agent_dir == 1:
            topX = self.agent_pos[0] - self.agent_view_size // 2
            topY = self.agent_pos[1]
        # Facing left
        elif self.agent_dir == 2:
            topX = self.agent_pos[0] - self.agent_view_size + 1
            topY = self.agent_pos[1] - self.agent_view_size // 2
        # Facing up
        elif self.agent_dir == 3:
            topX = self.agent_pos[0] - self.agent_view_size // 2
            topY = self.agent_pos[1] - self.agent_view_size + 1
        else:
            assert False, "invalid agent direction"

        botX = topX + self.agent_view_size
        botY = topY + self.agent_view_size

        return (topX, topY, botX, botY)

    def relative_coords(self, x, y):
        """
        Check if a grid position belongs to the agent's field of view, and returns the corresponding coordinates
        """

        vx, vy = self.get_view_coords(x, y)

        if vx < 0 or vy < 0 or vx >= self.agent_view_size or vy >= self.agent_view_size:
            return None

        return vx, vy

    def in_view(self, x, y):
        """
        check if a grid position is visible to the agent
        """

        return self.relative_coords(x, y) is not None

    def agent_sees(self, x, y):
        """
        Check if a non-empty grid position is visible to the agent
        """

        coordinates = self.relative_coords(x, y)
        if coordinates is None:
            return False
        vx, vy = coordinates

        obs = self.gen_obs()
        obs_grid, _ = Grid.decode(obs['image'])
        obs_cell = obs_grid.get(vx, vy)
        world_cell = self.grid.get(x, y)

        return obs_cell is not None and obs_cell.type == world_cell.type

    def step(self, action, automatic_door=False):
        self.step_count += 1

        #print('door_open',self.door_open)
        reward = 0
        done = False
        self.vel_ob = [0,0] 

        # Get the position in front of the agent
        fwd_pos = self.front_pos

        # Get the contents of the cell in front of the agent
        fwd_cell = self.grid.get(*fwd_pos)
        
        # Rotate left
        if action == self.actions.left:
            self.encoded_action = [0,0,1]
            self.agent_dir -= 1
            if self.agent_dir < 0:
                self.agent_dir += 4
            self.vel_ob = [0,-1] 
            # if automatic_door:
            if (np.array(self.door_open) != [0,0]).all() and not ((self.agent_pos == self.door_open).all()):
                fwd_door_cell = self.grid.get(*self.door_open)
                if fwd_door_cell is not None:
                    fwd_door_cell.toggle(self, self.door_open)
                    self.door_open = [0,0]

        # Rotate right
        elif action == self.actions.right:
            self.encoded_action = [0,1,0]
            self.agent_dir = (self.agent_dir + 1) % 4
            self.vel_ob = [0,1]
            # if automatic_door:
            if (np.array(self.door_open) != [0,0]).all() and not ((self.agent_pos == self.door_open).all()):
                fwd_door_cell = self.grid.get(*self.door_open)
                if fwd_door_cell is not None :
                    fwd_door_cell.toggle(self, self.door_open)
                    self.door_open = [0,0]

        # Move forward
        elif action == self.actions.forward:
            self.encoded_action = [1,0,0]
            if fwd_cell == None or fwd_cell.can_overlap():
                self.agent_pos = fwd_pos
                self.vel_ob = [1,0]
            if fwd_cell != None and fwd_cell.type == 'goal':
                #done = True
                self.agent_pos = fwd_pos
                self.vel_ob = [1,0]
                reward = self._reward()
            if fwd_cell != None and fwd_cell.type == 'lava':
                done = True
                self.vel_ob = [1,0]
                

        # Pick up an object
        elif action == self.actions.pickup:
            if fwd_cell and fwd_cell.can_pickup():
                if self.carrying is None:
                    self.carrying = fwd_cell
                    self.carrying.cur_pos = np.array([-1, -1])
                    self.grid.set(*fwd_pos, None)

        # Drop an object
        elif action == self.actions.drop:
            if not fwd_cell and self.carrying:
                self.grid.set(*fwd_pos, self.carrying)
                self.carrying.cur_pos = fwd_pos
                self.carrying = None

        # Toggle/activate an object
        elif action == self.actions.toggle:
            if fwd_cell:
                fwd_cell.toggle(self, fwd_pos)
                self.door_open = fwd_pos
                self.step_count -=1 #THIS SHOULD NOT COUNT AS AN ACTION

        # Done action (not used by default)
        elif action == self.actions.done:
            pass

      
        else:
            assert False, "unknown action"
        #If we opened a door, whatever the motion -except passing it-, we close it (consider we can overlap doors)
        if (np.array(self.door_open) != [0,0]).all()  and not ((self.agent_pos == self.door_open).all() or (fwd_pos == self.door_open).all()):
            fwd_door_cell = self.grid.get(*self.door_open)
            if fwd_door_cell is not None:
                fwd_door_cell.toggle(self, self.door_open)
                self.door_open = [0,0]


        if self.max_steps:
            if self.step_count >= self.max_steps:
                done = True

        obs = self.gen_obs()

        return obs, reward, done, {}


    def gen_obs_grid(self):
        """
        Generate the sub-grid observed by the agent.
        This method also outputs a visibility mask telling us which grid
        cells the agent can actually see.
        """

        topX, topY, botX, botY = self.get_view_exts()

        grid = self.grid.slice(topX, topY, self.agent_view_size, self.agent_view_size)

        for i in range(self.agent_dir + 1):
            grid = grid.rotate_left()

        # Process occluders and visibility
        # Note that this incurs some performance cost
        if not self.see_through_walls:
            vis_mask = grid.process_vis(agent_pos=(self.agent_view_size // 2 , self.agent_view_size - 1))
        else:
            vis_mask = np.ones(shape=(grid.width, grid.height), dtype=bool)

        # Make it so the agent sees what it's carrying
        # We do this by placing the carried object at the agent's position
        # in the agent's partially observable view
        agent_pos = grid.width // 2, grid.height - 1
        if self.carrying:
            grid.set(*agent_pos, self.carrying)
        else:
            grid.set(*agent_pos, None)
        
        return grid, vis_mask
    
    
    def gen_obs(self):
        """
        Generate the agent's view (partially observable, low-resolution encoding)
        """

        grid, vis_mask = self.gen_obs_grid()

        # Encode the partially observable view into a numpy array
        image = grid.encode(vis_mask)

        assert hasattr(self, 'mission'), "environments must define a textual mission string"
        
        # Observations are dictionaries containing:
        # - an image (partially observable view of the environment)
        # - the agent's direction/orientation (acting as a compass)
        # - the agent actual motion after the command
        # - a textual mission string (instructions for the agent)
        try:
            if isinstance(self.door_open, np.ndarray):
                door_open = self.door_open.tolist()
            else:
                door_open = self.door_open
        except AttributeError:
                self.door_open = [0,0]
                door_open = self.door_open
        obs = {
            'image': image,
            'direction': self.agent_dir,
            'vel_ob': self.vel_ob,
            'action':self.encoded_action,
            'door_open':door_open,
            'mission': self.mission
        }
        
        return obs
    


    def get_obs_render(self, obs, tile_size=TILE_PIXELS//2):
        """
        Render an agent observation for visualization
        """

        grid, vis_mask = Grid.decode(obs)

        # Render the whole grid
        img = grid.render(
            tile_size,
            agent_pos=(self.agent_view_size // 2, self.agent_view_size - 1),
            agent_dir=3,
            highlight_mask=vis_mask
        )

        return img

    def render(self, mode='human', agent_view= False, close=False, highlight=True, tile_size=TILE_PIXELS):
        """
        Render the whole-grid human view
        """

        if close:
            if self.window:
                self.window.close()
            return

        if mode == 'human' and not self.window:
            import gym_minigrid.window
            self.window = gym_minigrid.window.Window('gym_minigrid')
            self.window.show(block=False)

        # Compute which cells are visible to the agent
        _, vis_mask = self.gen_obs_grid()

        # Compute the world coordinates of the bottom-left corner
        # of the agent's view area
        f_vec = self.dir_vec
        r_vec = self.right_vec
        top_left = self.agent_pos + f_vec * (self.agent_view_size-1) - r_vec * (self.agent_view_size // 2)

        # Mask of which cells to highlight
        highlight_mask = np.zeros(shape=(self.width, self.height), dtype=bool)

        #absolute_pose_agent_view
        abs_ij_agent_view = []
        

        # For each cell in the visibility mask
        for vis_j in range(0, self.agent_view_size):
            for vis_i in range(0, self.agent_view_size):
                

                # Compute the world coordinates of this cell
                abs_i, abs_j = top_left - (f_vec * vis_j) + (r_vec * vis_i)
                if agent_view:
                    abs_ij_agent_view.append([abs_i, abs_j])
                # If this cell is not visible, don't highlight it
                if not vis_mask[vis_i, vis_j]:
                    continue

                if abs_i < 0 or abs_i >= self.width:
                    continue
                if abs_j < 0 or abs_j >= self.height:
                    continue

                # Mark this cell to be highlighted
                highlight_mask[abs_i, abs_j] = True

        # Render the whole grid
        img = self.grid.render(
            tile_size,
            self.agent_pos,
            self.agent_dir,
            highlight_mask=highlight_mask if highlight else None,
            agent_view_poses = abs_ij_agent_view,
            
        )
      
        
        if mode == 'human':
            self.window.set_caption(self.mission)
            self.window.show_img(img)

        return img

    def close(self):
        if self.window:
            self.window.close()
        return

```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/register.py`:

```py
from gym.envs.registration import register as gym_register

env_list = []

def register(
    id,
    entry_point,
    reward_threshold=0.95
):
    assert id.startswith("MiniGrid-")
    assert id not in env_list

    # Register the environment with OpenAI gym
    gym_register(
        id=id,
        entry_point=entry_point,
        reward_threshold=reward_threshold
    )

    # Add the environment to the set
    env_list.append(id)

```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/rendering.py`:

```py
import math
import numpy as np

def downsample(img, factor):
    """
    Downsample an image along both dimensions by some factor
    """

    assert img.shape[0] % factor == 0
    assert img.shape[1] % factor == 0

    img = img.reshape([img.shape[0]//factor, factor, img.shape[1]//factor, factor, 3])
    img = img.mean(axis=3)
    img = img.mean(axis=1)

    return img

def fill_coords(img, fn, color):
    """
    Fill pixels of an image with coordinates matching a filter function
    """

    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            yf = (y + 0.5) / img.shape[0]
            xf = (x + 0.5) / img.shape[1]
            if fn(xf, yf):
                img[y, x] = color

    return img

def rotate_fn(fin, cx, cy, theta):
    def fout(x, y):
        x = x - cx
        y = y - cy

        x2 = cx + x * math.cos(-theta) - y * math.sin(-theta)
        y2 = cy + y * math.cos(-theta) + x * math.sin(-theta)

        return fin(x2, y2)

    return fout

def point_in_line(x0, y0, x1, y1, r):
    p0 = np.array([x0, y0])
    p1 = np.array([x1, y1])
    dir = p1 - p0
    dist = np.linalg.norm(dir)
    dir = dir / dist

    xmin = min(x0, x1) - r
    xmax = max(x0, x1) + r
    ymin = min(y0, y1) - r
    ymax = max(y0, y1) + r

    def fn(x, y):
        # Fast, early escape test
        if x < xmin or x > xmax or y < ymin or y > ymax:
            return False

        q = np.array([x, y])
        pq = q - p0

        # Closest point on line
        a = np.dot(pq, dir)
        a = np.clip(a, 0, dist)
        p = p0 + a * dir

        dist_to_line = np.linalg.norm(q - p)
        return dist_to_line <= r

    return fn

def point_in_circle(cx, cy, r):
    def fn(x, y):
        return (x-cx)*(x-cx) + (y-cy)*(y-cy) <= r * r
    return fn

def point_in_rect(xmin, xmax, ymin, ymax):
    def fn(x, y):
        return x >= xmin and x <= xmax and y >= ymin and y <= ymax
    return fn

def point_in_triangle(a, b, c):
    a = np.array(a)
    b = np.array(b)
    c = np.array(c)

    def fn(x, y):
        v0 = c - a
        v1 = b - a
        v2 = np.array((x, y)) - a

        # Compute dot products
        dot00 = np.dot(v0, v0)
        dot01 = np.dot(v0, v1)
        dot02 = np.dot(v0, v2)
        dot11 = np.dot(v1, v1)
        dot12 = np.dot(v1, v2)

        # Compute barycentric coordinates
        inv_denom = 1 / (dot00 * dot11 - dot01 * dot01)
        u = (dot11 * dot02 - dot01 * dot12) * inv_denom
        v = (dot00 * dot12 - dot01 * dot02) * inv_denom

        # Check if point is in triangle
        return (u >= 0) and (v >= 0) and (u + v) < 1

    return fn

def highlight_img(img, color=(255, 255, 255), alpha=0.30):
    """
    Add highlighting to an image
    """

    blend_img = img + alpha * (np.array(color, dtype=np.uint8) - img)
    blend_img = blend_img.clip(0, 255).astype(np.uint8)
    img[:, :, :] = blend_img

```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/roomgrid.py`:

```py
from .minigrid import *

def reject_next_to(env, pos):
    """
    Function to filter out object positions that are right next to
    the agent's starting point
    """

    sx, sy = env.agent_pos
    x, y = pos
    d = abs(sx - x) + abs(sy - y)
    return d < 2

class Room:
    def __init__(
        self,
        top,
        size
    ):
        # Top-left corner and size (tuples)
        self.top = top
        self.size = size

        # List of door objects and door positions
        # Order of the doors is right, down, left, up
        self.doors = [None] * 4
        self.door_pos = [None] * 4

        # List of rooms adjacent to this one
        # Order of the neighbors is right, down, left, up
        self.neighbors = [None] * 4

        # Indicates if this room is behind a locked door
        self.locked = False

        # List of objects contained
        self.objs = []

    def rand_pos(self, env):
        topX, topY = self.top
        sizeX, sizeY = self.size
        return env._randPos(
            topX + 1, topX + sizeX - 1,
            topY + 1, topY + sizeY - 1
        )

    def pos_inside(self, x, y):
        """
        Check if a position is within the bounds of this room
        """

        topX, topY = self.top
        sizeX, sizeY = self.size

        if x < topX or y < topY:
            return False

        if x >= topX + sizeX or y >= topY + sizeY:
            return False

        return True

class RoomGrid(MiniGridEnv):
    """
    Environment with multiple rooms and random objects.
    This is meant to serve as a base class for other environments.
    """

    def __init__(
        self,
        room_size=7,
        num_rows=3,
        num_cols=3,
        max_steps=100,
        seed=0,
        agent_view_size=7
    ):
        assert room_size > 0
        assert room_size >= 3
        assert num_rows > 0
        assert num_cols > 0
        self.room_size = room_size
        self.num_rows = num_rows
        self.num_cols = num_cols

        height = (room_size - 1) * num_rows + 1
        width = (room_size - 1) * num_cols + 1

        # By default, this environment has no mission
        self.mission = ''

        super().__init__(
            width=width,
            height=height,
            max_steps=max_steps,
            see_through_walls=False,
            seed=seed,
            agent_view_size=agent_view_size
        )

    def room_from_pos(self, x, y):
        """Get the room a given position maps to"""

        assert x >= 0
        assert y >= 0

        i = x // (self.room_size-1)
        j = y // (self.room_size-1)

        assert i < self.num_cols
        assert j < self.num_rows

        return self.room_grid[j][i]

    def get_room(self, i, j):
        assert i < self.num_cols
        assert j < self.num_rows
        return self.room_grid[j][i]

    def _gen_grid(self, width, height):
        # Create the grid
        self.grid = Grid(width, height)

        self.room_grid = []

        # For each row of rooms
        for j in range(0, self.num_rows):
            row = []

            # For each column of rooms
            for i in range(0, self.num_cols):
                room = Room(
                    (i * (self.room_size-1), j * (self.room_size-1)),
                    (self.room_size, self.room_size)
                )
                row.append(room)

                # Generate the walls for this room
                self.grid.wall_rect(*room.top, *room.size)

            self.room_grid.append(row)

        # For each row of rooms
        for j in range(0, self.num_rows):
            # For each column of rooms
            for i in range(0, self.num_cols):
                room = self.room_grid[j][i]

                x_l, y_l = (room.top[0] + 1, room.top[1] + 1)
                x_m, y_m = (room.top[0] + room.size[0] - 1, room.top[1] + room.size[1] - 1)

                # Door positions, order is right, down, left, up
                if i < self.num_cols - 1:
                    room.neighbors[0] = self.room_grid[j][i+1]
                    room.door_pos[0] = (x_m, self._rand_int(y_l, y_m))
                if j < self.num_rows - 1:
                    room.neighbors[1] = self.room_grid[j+1][i]
                    room.door_pos[1] = (self._rand_int(x_l, x_m), y_m)
                if i > 0:
                    room.neighbors[2] = self.room_grid[j][i-1]
                    room.door_pos[2] = room.neighbors[2].door_pos[0]
                if j > 0:
                    room.neighbors[3] = self.room_grid[j-1][i]
                    room.door_pos[3] = room.neighbors[3].door_pos[1]

        # The agent starts in the middle, facing right
        self.agent_pos = (
            (self.num_cols // 2) * (self.room_size-1) + (self.room_size // 2),
            (self.num_rows // 2) * (self.room_size-1) + (self.room_size // 2)
        )
        self.agent_dir = 0

    def place_in_room(self, i, j, obj):
        """
        Add an existing object to room (i, j)
        """

        room = self.get_room(i, j)

        pos = self.place_obj(
            obj,
            room.top,
            room.size,
            reject_fn=reject_next_to,
            max_tries=1000
        )

        room.objs.append(obj)

        return obj, pos

    def add_object(self, i, j, kind=None, color=None):
        """
        Add a new object to room (i, j)
        """

        if kind == None:
            kind = self._rand_elem(['key', 'ball', 'box'])

        if color == None:
            color = self._rand_color()

        # TODO: we probably want to add an Object.make helper function
        assert kind in ['key', 'ball', 'box']
        if kind == 'key':
            obj = Key(color)
        elif kind == 'ball':
            obj = Ball(color)
        elif kind == 'box':
            obj = Box(color)

        return self.place_in_room(i, j, obj)

    def add_door(self, i, j, door_idx=None, color=None, locked=None):
        """
        Add a door to a room, connecting it to a neighbor
        """

        room = self.get_room(i, j)

        if door_idx == None:
            # Need to make sure that there is a neighbor along this wall
            # and that there is not already a door
            while True:
                door_idx = self._rand_int(0, 4)
                if room.neighbors[door_idx] and room.doors[door_idx] is None:
                    break

        if color == None:
            color = self._rand_color()

        if locked is None:
            locked = self._rand_bool()

        assert room.doors[door_idx] is None, "door already exists"

        room.locked = locked
        door = Door(color, is_locked=locked)

        pos = room.door_pos[door_idx]
        self.grid.set(*pos, door)
        door.cur_pos = pos

        neighbor = room.neighbors[door_idx]
        room.doors[door_idx] = door
        neighbor.doors[(door_idx+2) % 4] = door

        return door, pos

    def remove_wall(self, i, j, wall_idx):
        """
        Remove a wall between two rooms
        """

        room = self.get_room(i, j)

        assert wall_idx >= 0 and wall_idx < 4
        assert room.doors[wall_idx] is None, "door exists on this wall"
        assert room.neighbors[wall_idx], "invalid wall"

        neighbor = room.neighbors[wall_idx]

        tx, ty = room.top
        w, h = room.size

        # Ordering of walls is right, down, left, up
        if wall_idx == 0:
            for i in range(1, h - 1):
                self.grid.set(tx + w - 1, ty + i, None)
        elif wall_idx == 1:
            for i in range(1, w - 1):
                self.grid.set(tx + i, ty + h - 1, None)
        elif wall_idx == 2:
            for i in range(1, h - 1):
                self.grid.set(tx, ty + i, None)
        elif wall_idx == 3:
            for i in range(1, w - 1):
                self.grid.set(tx + i, ty, None)
        else:
            assert False, "invalid wall index"

        # Mark the rooms as connected
        room.doors[wall_idx] = True
        neighbor.doors[(wall_idx+2) % 4] = True

    def place_agent(self, i=None, j=None, rand_dir=True):
        """
        Place the agent in a room
        """

        if i == None:
            i = self._rand_int(0, self.num_cols)
        if j == None:
            j = self._rand_int(0, self.num_rows)

        room = self.room_grid[j][i]

        # Find a position that is not right in front of an object
        while True:
            super().place_agent(room.top, room.size, rand_dir, max_tries=1000)
            front_cell = self.grid.get(*self.front_pos)
            if front_cell is None or front_cell.type == 'wall':
                break

        return self.agent_pos

    def connect_all(self, door_colors=COLOR_NAMES, max_itrs=5000):
        """
        Make sure that all rooms are reachable by the agent from its
        starting position
        """

        start_room = self.room_from_pos(*self.agent_pos)

        added_doors = []

        def find_reach():
            reach = set()
            stack = [start_room]
            while len(stack) > 0:
                room = stack.pop()
                if room in reach:
                    continue
                reach.add(room)
                for i in range(0, 4):
                    if room.doors[i]:
                        stack.append(room.neighbors[i])
            return reach

        num_itrs = 0

        while True:
            # This is to handle rare situations where random sampling produces
            # a level that cannot be connected, producing in an infinite loop
            if num_itrs > max_itrs:
                raise RecursionError('connect_all failed')
            num_itrs += 1

            # If all rooms are reachable, stop
            reach = find_reach()
            if len(reach) == self.num_rows * self.num_cols:
                break

            # Pick a random room and door position
            i = self._rand_int(0, self.num_cols)
            j = self._rand_int(0, self.num_rows)
            k = self._rand_int(0, 4)
            room = self.get_room(i, j)

            # If there is already a door there, skip
            if not room.door_pos[k] or room.doors[k]:
                continue

            if room.locked or room.neighbors[k].locked:
                continue

            color = self._rand_elem(door_colors)
            door, _ = self.add_door(i, j, k, color, False)
            added_doors.append(door)

        return added_doors

    def add_distractors(self, i=None, j=None, num_distractors=10, all_unique=True):
        """
        Add random objects that can potentially distract/confuse the agent.
        """

        # Collect a list of existing objects
        objs = []
        for row in self.room_grid:
            for room in row:
                for obj in room.objs:
                    objs.append((obj.type, obj.color))

        # List of distractors added
        dists = []

        while len(dists) < num_distractors:
            color = self._rand_elem(COLOR_NAMES)
            type = self._rand_elem(['key', 'ball', 'box'])
            obj = (type, color)

            if all_unique and obj in objs:
                continue

            # Add the object to a random room if no room specified
            room_i = i
            room_j = j
            if room_i == None:
                room_i = self._rand_int(0, self.num_cols)
            if room_j == None:
                room_j = self._rand_int(0, self.num_rows)

            dist, pos = self.add_object(room_i, room_j, *obj)

            objs.append(obj)
            dists.append(dist)

        return dists

```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/window.py`:

```py
import sys
import numpy as np

# Only ask users to install matplotlib if they actually need it
try:
    import matplotlib.pyplot as plt
except:
    print('To display the environment in a window, please install matplotlib, eg:')
    print('pip3 install --user matplotlib')
    sys.exit(-1)

class Window:
    """
    Window to draw a gridworld instance using Matplotlib
    """

    def __init__(self, title):
        self.fig = None

        self.imshow_obj = None

        # Create the figure and axes
        self.fig =plt.figure()
        self.ax = plt.subplot()

        # Show the env name in the window title
        #self.fig.canvas.set_window_title(title)

        # Turn off x/y axis numbering/ticks
        self.ax.xaxis.set_ticks_position('none')
        self.ax.yaxis.set_ticks_position('none')
        _ = self.ax.set_xticklabels([])
        _ = self.ax.set_yticklabels([])

        # Flag indicating the window was closed
        self.closed = False

        def close_handler(evt):
            self.closed = True

        self.fig.canvas.mpl_connect('close_event', close_handler)

    def show_img(self, img, pause = None):
        """
        Show an image or update the image being shown
        """

        # If no image has been shown yet,
        # show the first image of the environment
        if self.imshow_obj is None:
            self.imshow_obj = self.ax.imshow(img, interpolation='bilinear')

        # Update the image data
        self.imshow_obj.set_data(img)

        # Request the window be redrawn
        self.fig.canvas.draw_idle()
        self.fig.canvas.flush_events()

        # Let matplotlib process UI events
        if pause:
            plt.pause(pause)
        else:
            plt.pause(0.001)


    def set_caption(self, text):
        """
        Set/update the caption text below the image
        """

        plt.xlabel(text)

    def reg_key_handler(self, key_handler):
        """
        Register a keyboard event handler
        """

        # Keyboard handler
        self.fig.canvas.mpl_connect('key_press_event', key_handler)

    def show(self, block=True):
        """
        Show the window, and start an event loop
        """

        # If not blocking, trigger interactive mode
        if not block:
            plt.ion()

        # Show the plot
        # In non-interative mode, this enters the matplotlib event loop
        # In interactive mode, this call does not block
        plt.show()

    def close(self):
        """
        Close the window
        """

        plt.close()
        self.closed = True

```

`hierarchical-nav/gym-minigrid_minimal-1/build/lib/gym_minigrid/wrappers.py`:

```py
import math
import operator
from functools import reduce

import numpy as np
import gym
from gym import error, spaces, utils
from .minigrid import OBJECT_TO_IDX, COLOR_TO_IDX, STATE_TO_IDX, Goal

class ReseedWrapper(gym.core.Wrapper):
    """
    Wrapper to always regenerate an environment with the same set of seeds.
    This can be used to force an environment to always keep the same
    configuration when reset.
    """

    def __init__(self, env, seeds=[0], seed_idx=0):
        self.seeds = list(seeds)
        self.seed_idx = seed_idx
        super().__init__(env)

    def reset(self, **kwargs):
        seed = self.seeds[self.seed_idx]
        self.seed_idx = (self.seed_idx + 1) % len(self.seeds)
        self.env.seed(seed)
        return self.env.reset(**kwargs)

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        return obs, reward, done, info

class ActionBonus(gym.core.Wrapper):
    """
    Wrapper which adds an exploration bonus.
    This is a reward to encourage exploration of less
    visited (state,action) pairs.
    """

    def __init__(self, env):
        super().__init__(env)
        self.counts = {}

    def step(self, action):
        obs, reward, done, info = self.env.step(action)

        env = self.unwrapped
        tup = (tuple(env.agent_pos), env.agent_dir, action)

        # Get the count for this (s,a) pair
        pre_count = 0
        if tup in self.counts:
            pre_count = self.counts[tup]

        # Update the count for this (s,a) pair
        new_count = pre_count + 1
        self.counts[tup] = new_count

        bonus = 1 / math.sqrt(new_count)
        reward += bonus

        return obs, reward, done, info

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

class StateBonus(gym.core.Wrapper):
    """
    Adds an exploration bonus based on which positions
    are visited on the grid.
    """

    def __init__(self, env):
        super().__init__(env)
        self.counts = {}

    def step(self, action):
        obs, reward, done, info = self.env.step(action)

        # Tuple based on which we index the counts
        # We use the position after an update
        env = self.unwrapped
        tup = (tuple(env.agent_pos))

        # Get the count for this key
        pre_count = 0
        if tup in self.counts:
            pre_count = self.counts[tup]

        # Update the count for this key
        new_count = pre_count + 1
        self.counts[tup] = new_count

        bonus = 1 / math.sqrt(new_count)
        reward += bonus

        return obs, reward, done, info

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

class ImgObsWrapper(gym.core.ObservationWrapper):
    """
    Use the image as the only observation output, no language/mission.
    """

    def __init__(self, env):
        super().__init__(env)
        self.observation_space = env.observation_space.spaces['image']

    def observation(self, obs):
        return obs['image']

class ImgActionObsWrapper(gym.core.ObservationWrapper):
    """
    Use the image abd the effectued action as observation output, no language/mission.
    """

    def __init__(self, env):
        super().__init__(env)
        self.observation_space = env.observation_space.spaces['image']

    def observation(self, obs):
        if 'mission' in obs:
            del obs['mission']
        return obs

class OneHotPartialObsWrapper(gym.core.ObservationWrapper):
    """
    Wrapper to get a one-hot encoding of a partially observable
    agent view as observation.
    """

    def __init__(self, env, tile_size=8):
        super().__init__(env)

        self.tile_size = tile_size

        obs_shape = env.observation_space['image'].shape

        # Number of bits per cell
        num_bits = len(OBJECT_TO_IDX) + len(COLOR_TO_IDX) + len(STATE_TO_IDX)

        self.observation_space.spaces["image"] = spaces.Box(
            low=0,
            high=255,
            shape=(obs_shape[0], obs_shape[1], num_bits),
            dtype='uint8'
        )

    def observation(self, obs):
        img = obs['image']
        out = np.zeros(self.observation_space.spaces['image'].shape, dtype='uint8')

        for i in range(img.shape[0]):
            for j in range(img.shape[1]):
                type = img[i, j, 0]
                color = img[i, j, 1]
                state = img[i, j, 2]

                out[i, j, type] = 1
                out[i, j, len(OBJECT_TO_IDX) + color] = 1
                out[i, j, len(OBJECT_TO_IDX) + len(COLOR_TO_IDX) + state] = 1

        return {
            'action': obs['action'],
            'vel_ob': obs['vel_ob'],
            'image': out
        }

class RGBImgObsWrapper(gym.core.ObservationWrapper):
    """
    Wrapper to use fully observable RGB image as observation,
    This can be used to have the agent to solve the gridworld in pixel space.
    """

    def __init__(self, env, tile_size=8):
        super().__init__(env)

        self.tile_size = tile_size

        self.observation_space.spaces['image'] = spaces.Box(
            low=0,
            high=255,
            shape=(self.env.width * tile_size, self.env.height * tile_size, 3),
            dtype='uint8'
        )

    def observation(self, obs):
        env = self.unwrapped

        rgb_img = env.render(
            mode='rgb_array',
            highlight=False,
            tile_size=self.tile_size
        )

        return {
            'mission': obs['mission'],
            'image': rgb_img
        }


class RGBImgPartialObsWrapper(gym.core.ObservationWrapper):
    """
    Wrapper to use partially observable RGB image as observation.
    This can be used to have the agent to solve the gridworld in pixel space.
    """

    def __init__(self, env, tile_size=8):
        super().__init__(env)

        self.tile_size = tile_size

        obs_shape = env.observation_space.spaces['image'].shape
        self.observation_space.spaces['image'] = spaces.Box(
            low=0,
            high=255,
            shape=(obs_shape[0] * tile_size, obs_shape[1] * tile_size, 3),
            dtype='uint8'
        )

    def observation(self, obs):
        env = self.unwrapped

        rgb_img_partial = env.get_obs_render(
            obs['image'],
            tile_size=self.tile_size
        )
        obs['image'] = rgb_img_partial
        return obs

class FullyObsWrapper(gym.core.ObservationWrapper):
    """
    Fully observable gridworld using a compact grid encoding
    """

    def __init__(self, env):
        super().__init__(env)

        self.observation_space.spaces["image"] = spaces.Box(
            low=0,
            high=255,
            shape=(self.env.width, self.env.height, 3),  # number of cells
            dtype='uint8'
        )

    def observation(self, obs):
        env = self.unwrapped
        full_grid = env.grid.encode()
        full_grid[env.agent_pos[0]][env.agent_pos[1]] = np.array([
            OBJECT_TO_IDX['agent'],
            COLOR_TO_IDX['red'],
            env.agent_dir
        ])

        return {
            'mission': obs['mission'],
            'image': full_grid
        }

class FlatObsWrapper(gym.core.ObservationWrapper):
    """
    Encode mission strings using a one-hot scheme,
    and combine these with observed images into one flat array
    """

    def __init__(self, env, maxStrLen=96):
        super().__init__(env)

        self.maxStrLen = maxStrLen
        self.numCharCodes = 27

        imgSpace = env.observation_space.spaces['image']
        imgSize = reduce(operator.mul, imgSpace.shape, 1)

        self.observation_space = spaces.Box(
            low=0,
            high=255,
            shape=(imgSize + self.numCharCodes * self.maxStrLen,),
            dtype='uint8'
        )

        self.cachedStr = None
        self.cachedArray = None

    def observation(self, obs):
        image = obs['image']
        mission = obs['mission']

        # Cache the last-encoded mission string
        if mission != self.cachedStr:
            assert len(mission) <= self.maxStrLen, 'mission string too long ({} chars)'.format(len(mission))
            mission = mission.lower()

            strArray = np.zeros(shape=(self.maxStrLen, self.numCharCodes), dtype='float32')

            for idx, ch in enumerate(mission):
                if ch >= 'a' and ch <= 'z':
                    chNo = ord(ch) - ord('a')
                elif ch == ' ':
                    chNo = ord('z') - ord('a') + 1
                assert chNo < self.numCharCodes, '%s : %d' % (ch, chNo)
                strArray[idx, chNo] = 1

            self.cachedStr = mission
            self.cachedArray = strArray

        obs = np.concatenate((image.flatten(), self.cachedArray.flatten()))

        return obs

class ViewSizeWrapper(gym.core.Wrapper):
    """
    Wrapper to customize the agent field of view size.
    This cannot be used with fully observable wrappers.
    """

    def __init__(self, env, agent_view_size=7):
        super().__init__(env)

        assert agent_view_size % 2 == 1
        assert agent_view_size >= 3

        # Override default view size
        env.unwrapped.agent_view_size = agent_view_size

        # Compute observation space with specified view size
        observation_space = gym.spaces.Box(
            low=0,
            high=255,
            shape=(agent_view_size, agent_view_size, 3),
            dtype='uint8'
        )

        # Override the environment's observation space
        self.observation_space = spaces.Dict({
            'image': observation_space
        })

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

    def step(self, action):
        return self.env.step(action)

class DirectionObsWrapper(gym.core.ObservationWrapper):
    """
    Provides the slope/angular direction to the goal with the observations as modeled by (y2 - y2 )/( x2 - x1)
    type = {slope , angle}
    """
    def __init__(self, env,type='slope'):
        super().__init__(env)
        self.goal_position = None
        self.type = type

    def reset(self):
        obs = self.env.reset()
        if not self.goal_position:
            self.goal_position = [x for x,y in enumerate(self.grid.grid) if isinstance(y,(Goal) ) ]
            if len(self.goal_position) >= 1: # in case there are multiple goals , needs to be handled for other env types
                self.goal_position = (int(self.goal_position[0]/self.height) , self.goal_position[0]%self.width)
        return obs

    def observation(self, obs):
        slope = np.divide( self.goal_position[1] - self.agent_pos[1] ,  self.goal_position[0] - self.agent_pos[0])
        obs['goal_direction'] = np.arctan( slope ) if self.type == 'angle' else slope
        return obs

class SymbolicObsWrapper(gym.core.ObservationWrapper):
    """
    Fully observable grid with a symbolic state representation.
    The symbol is a triple of (X, Y, IDX), where X and Y are
    the coordinates on the grid, and IDX is the id of the object.
    """

    def __init__(self, env):
        super().__init__(env)

        self.observation_space.spaces["image"] = spaces.Box(
            low=0,
            high=max(OBJECT_TO_IDX.values()),
            shape=(self.env.width, self.env.height, 3),  # number of cells
            dtype="uint8",
        )

    def observation(self, obs):
        objects = np.array(
            [OBJECT_TO_IDX[o.type] if o is not None else -1 for o in self.grid.grid]
        )
        w, h = self.width, self.height
        grid = np.mgrid[:w, :h]
        grid = np.concatenate([grid, objects.reshape(1, w, h)])
        grid = np.transpose(grid, (1, 2, 0))
        obs['image'] = grid
        return obs

```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/__init__.py`:

```py
# Import the envs module so that envs register themselves
import gym_minigrid.envs

# Import wrappers so it's accessible when installing with pip
import gym_minigrid.wrappers

```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/envs/__init__.py`:

```py
from gym_minigrid.envs.aisle_door_rooms import *


```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/envs/aisle_door_rooms.py`:

```py
from gym_minigrid.minigrid import *
from gym_minigrid.register import register
import numpy as np
import random 
from collections import deque

# Map of agent direction indices to vectors
DIR_TO_VEC = [
    # Pointing right (positive X)
    np.array((1, 0)),
    # Down (positive Y)
    np.array((0, 1)),
    # Pointing left (negative X)
    np.array((-1, 0)),
    # Up (negative Y)
    np.array((0, -1)),
]

class AisleDoorRooms(MiniGridEnv):
    """
    colored rooms connected by x tiles corridors
    """

    def __init__(
        self,
        size= 24,
        agent_start_pos= None,
        agent_start_dir= 0,
        rooms_size = 4,
        max_steps= None,
        rooms_in_row = 3,
        rooms_in_col = 3,
        corridor_length =5,
        automatic_door = True,
        wT_around = 6,
        wT_size = 1,
        obstacle_rate=0.15,          # Add this
        debug=True

    ):
        self.agent_start_pos = agent_start_pos
        self.agent_start_dir = agent_start_dir
        self.automatic_door = automatic_door
        self.wT_around = wT_around
        self.wT_size= wT_size
        self.obstacle_rate = obstacle_rate
        self.debug = debug
        self.obstacles_spawned = False  
        self.current_obstacles = set()
        self.color_idx = {
            'red' : 0,
            'green' : 1,
            'blue'  : 2,
            'purple': 3,
            'white' : 4,   # keep for doors/goals
        }
        # Prefer not to paint rooms 'white' since doors/goals are white
        self._room_palette = [c for c in self.color_idx.keys() if c != 'white']


        self.rooms_size = rooms_size
        
        if isinstance(corridor_length, int) and corridor_length>0:
            self.corridor_length = corridor_length
            self.random_corridor_length = False
        else:
            self.random_corridor_length = True

        if self.random_corridor_length:
            self.corridor_length = self._rand_int(3,7)

        self.rooms_in_row = rooms_in_row
        self.rooms_in_col = rooms_in_col        

        width = (self.rooms_size * rooms_in_col + (self.corridor_length+1) * (rooms_in_col-1)-1  )
        height = (self.rooms_size * rooms_in_row + (self.corridor_length+1) * (rooms_in_row-1)-1 )
        #print('width, height',width, height)
        if width <= 20 :
            width+=1
        if height <= 20 :
            height+=1

        if width >= 30 :
            width-=1
        if height >= 30 :
            height-=1
        # print(width, height)
        if width >= 40 :
            width-=1
        if height >= 40 :
            height-=1
        
        # self.static_room = static_room
        # self.closed = closed
        self.color_idx = {
            'red' : 0,
            'green' : 1,
            'blue'  : 2,
            'purple': 3,
            'white' : 4,
            
        }

        super().__init__(
            grid_size=None,
            width=width,
            height=height,
            max_steps=max_steps,
            # Set this to True for maximum speed
            see_through_walls=False,
            
            
        )     

    
    def _gen_grid(self, width, height):
        print("1TFFFFFFFFF")       
        rooms_in_row = self.rooms_in_row
        rooms_in_col = self.rooms_in_col
        room_w = self.rooms_size
        room_h = self.rooms_size

        # >>> PATCH 1: initialize tracking
        self._init_room_tracking(width, height)

        # Create an empty grid ...
        self.grid = Grid(width, height)
        self.grid.grid = [Floor('black')] * width * height

        # Surrounding walls ...
        self.grid.horz_wall(0, 0)
        self.grid.horz_wall(0, height - 1)
        self.grid.vert_wall(0, 0)
        self.grid.vert_wall(width - 1, 0)
        self.grid.wall_rect(0, 0, width, height)

        balls_rooms = []
        while len(balls_rooms) < self.wT_around:
            col_room = self._rand_int(0, rooms_in_col)
            row_room = self._rand_int(0, rooms_in_row)
            if [col_room, row_room] not in balls_rooms:
                balls_rooms.append([col_room, row_room])

        pos_vert_R = None
        pos_horz_B = [None] * rooms_in_col
        agent_pose_options = []

        # For each row of rooms
        for row_inc in range(0, rooms_in_row):
            pos_vert_R = None
            for col_inc in range(0, rooms_in_col):
                xL = col_inc * (room_w + self.corridor_length)
                yT = row_inc * (room_h + self.corridor_length)
                xR = xL + room_w 
                yB = yT + room_h

                # choose color
                if hasattr(self, "_rand_elem"):
                    color = self._rand_elem(self._room_palette)
                else:
                    import random
                    color = random.choice(self._room_palette)
                # >>> PATCH 2: map room area + store meta (color, bounds)
                self._register_room_area(col_inc, row_inc, xL, yT, xR, yB, color)

                # fill interior with colored floor (your existing loop)
                for b in range(xL+1, xR):
                    for c in range(yT+1, yB):
                        self.put_obj(Floor(color), b, c)

                # ----- LEFT neighbor connection (upper wall/door in your terms) -----
                if col_inc > 0:
                    self.grid.vert_wall(xL, yT, room_h+1)
                    self.grid.set(xL, pos_vert_R[1], Floor('black'))

                    # door between left room and current corridor
                    door_x = xL - int(self.corridor_length/2)
                    door_y = pos_vert_R[1]
                    self.put_obj(Door(color='black'), door_x, door_y)

                    self._register_door(door_x, door_y, roomA=(col_inc-1, row_inc), roomB=(col_inc, row_inc))
                    self._register_corridor_run(
                        range(max(0, xL - self.corridor_length), xL),  # full length, clamped at 0
                        [door_y]
                    )

                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((door_x - corridor_depth, door_y, 0))

                # ----- RIGHT neighbor preparation (creates corridor to the right) -----
                if col_inc + 1 < rooms_in_col:
                    self.grid.vert_wall(xR, yT, room_h+1)
                    pos_vert_R = (xR, self._rand_int(yT + 1, yB))
                    self.grid.set(*pos_vert_R, Floor('black'))

                    # walls above/below corridor; walkable stripe at y=pos_vert_R[1]
                    self.grid.horz_wall(xR, pos_vert_R[1]-1, self.corridor_length)  
                    self.grid.horz_wall(xR, pos_vert_R[1]+1, self.corridor_length)

                    # >>> PATCH 3b: record corridor run to the right
                    self._register_corridor_run(range(xR, xR + self.corridor_length), [pos_vert_R[1]])

                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((xR + int(self.corridor_length/2) + corridor_depth, pos_vert_R[1], 2))

                # ----- TOP neighbor connection -----
                if row_inc > 0:
                    self.grid.horz_wall(xL, yT, room_w+1)
                    self.grid.set(pos_horz_B[col_inc][0], yT, Floor('black'))

                    door_x = pos_horz_B[col_inc][0]
                    door_y = yT - int(self.corridor_length/2)
                    self.put_obj(Door(color='black'), door_x, door_y)

                    # >>> PATCH 3c: record doorway + corridor stripe up
                    self._register_door(door_x, door_y, roomA=(col_inc, row_inc-1), roomB=(col_inc, row_inc))
                    self._register_corridor_run(
                        [door_x],
                        range(max(0, yT - self.corridor_length), yT)  # full length, clamped
                    )

                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((door_x, door_y - corridor_depth, 1))

                # ----- BOTTOM neighbor preparation -----
                if row_inc + 1 < rooms_in_row:
                    self.grid.horz_wall(xL, yB, room_w+1)
                    pos_horz_B[col_inc] = (self._rand_int(xL + 1, xR), yB)
                    self.grid.set(*pos_horz_B[col_inc], Floor('black'))

                    # corridor vertical walls; walkable stripe at x=pos_horz_B[col_inc][0]
                    self.grid.vert_wall(pos_horz_B[col_inc][0]-1, yB, self.corridor_length)  
                    self.grid.vert_wall(pos_horz_B[col_inc][0]+1, yB, self.corridor_length)

                    # >>> PATCH 3d: record corridor run downward
                    cx = pos_horz_B[col_inc][0]
                    self._register_corridor_run([cx], range(yB, yB + self.corridor_length))
                    

                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((cx, yB + int(self.corridor_length/2) + corridor_depth, 3))

                # Goals in selected rooms (unchanged)
                """if [col_inc, row_inc] in balls_rooms:
                    if self.wT_size == 1:
                        x, y = self._rand_int(xL+1, xR), self._rand_int(yT+1, yB)
                        self.put_obj(Goal('white'), x, y)
                        self.goal = (color, x, y)
                    else:
                        x = self._rand_int(xL+1, xR-1)
                        y = self._rand_int(yT+1, yB-1)
                        self.put_obj(Goal('white'), x, y)
                        self.put_obj(Goal('white'), x+1, y)
                        self.put_obj(Goal('white'), x, y+1)
                        self.put_obj(Goal('white'), x+1, y+1)"""

        # Place the agent (unchanged)
        if self.agent_start_pos == None:
            index = self._rand_int(0, len(agent_pose_options))
            self.starting_agent_pos = np.array([agent_pose_options[index][0], agent_pose_options[index][1]])
            self.agent_start_dir = agent_pose_options[index][2]
            self.agent_pos = self.starting_agent_pos   
            self.agent_dir = self.agent_start_dir
        else:
            self.agent_pos = np.asarray(self.agent_start_pos)
            self.agent_dir = self.agent_start_dir

        # >>> PATCH 4: seed visit tracker with starting location
        self._last_room_id = self.get_room_id_at(self.agent_pos[0], self.agent_pos[1])
        if self._last_room_id is not None and self._last_room_id not in self._visited_rooms_set:
            self._visited_rooms_set.add(self._last_room_id)
            self.visited_rooms_order.append({
                'room': self._last_room_id,
                'color': self.get_room_color_by_room(self._last_room_id),
                'entered_at_step': 0,
                'entry_xy': (int(self.agent_pos[0]), int(self.agent_pos[1])),
            })
        
        self.vel_ob = [0,0]
        self.encoded_action = None      

        if self.grid.get(*self.agent_pos).type == 'door' :
            self.door_open = self.agent_pos
            self.grid.get(*self.agent_pos).toggle(self, self.agent_pos)
        elif self.grid.get(*self.front_pos).type == 'door':
            self.door_open = self.front_pos
            self.grid.get(*self.front_pos).toggle(self, self.front_pos)
        else:
            self.door_open = [0,0]

        
        #self.grid.set(14, 10, Wall)
        #self.grid.set(3,4,Wall)
        #self.grid.vert_wall(21, 11, 1)
        #self.grid.vert_wall(22, 11, 1)
        #self.grid.vert_wall(20, 11, 1)
        # self.put_obj(Goal(),self.agent_pos[0]+1, self.agent_pos[1])
        self.mission = "Motion in color distinct rooms environment"



    def step(self, action):
        obs, reward, done, info = super().step(action)

        fwd_cell = self.grid.get(*self.front_pos)
        if self.automatic_door == True and fwd_cell != None and fwd_cell.type == 'door':
            #if a door up front, open it
            obs_2,_,_,_= super().step(self.actions.toggle, automatic_door=self.automatic_door)
            obs['door_open'] = obs_2['door_open']
            obs['image'] = obs_2['image']


        if obs['vel_ob'][0] ==obs['vel_ob'][1] :
            real_action = [0,0,0]
        else:
            real_action = obs['action'].copy()

        self.current_pose = self.action_to_pose(real_action, self.current_pose) 
        try:
            self._mark_room_visit(self.agent_pos[0], self.agent_pos[1])
        except Exception:
            pass
        obs['pose'] = self.current_pose
        return obs, reward, done, info
    
    def _init_room_tracking(self, width: int, height: int):
        """Call at the very start of _gen_grid()."""
        # Per-room metadata
        self.room_meta = {}             # dict[(col,row)] -> {'bounds':(x1,y1,x2,y2), 'color':str, 'doorways':[(x,y),...]}
        # Reverse index: grid cell -> room id
        self.xy_to_room = {}            # dict[(x,y)] -> (col,row)
        # Corridor and door coordinates
        self.corridor_xy = set()        # set[(x,y)]
        self.door_xy = []               # list[(x,y)]
        # Visit tracking (discovery order)
        self.visited_rooms_order = []   # list[{'room':(col,row), 'color':str, 'entered_at_step':int, 'entry_xy':(x,y)}]
        self._visited_rooms_set = set()
        self._last_room_id = None

    def _register_room_area(self, col: int, row: int, xL: int, yT: int, xR: int, yB: int, color: str):
        """
        Record per-room metadata and map interior floor cells (exclude walls).
        xL,yT inclusive left/top; xR,yB exclusive right/bottom boundary used by your code.
        """
        # Interior floor (exclude the 1-cell walls you build around)
        x1, y1 = xL + 1, yT + 1
        x2, y2 = xR - 1, yB - 1
        self.room_meta[(col, row)] = {
            'bounds': (x1, y1, x2, y2),
            'color':  color,
            'doorways': []
        }
        for x in range(x1, xR):
            for y in range(y1, yB):
                self.xy_to_room[(x, y)] = (col, row)

    def _register_door(self, x: int, y: int, roomA: tuple[int,int] | None = None, roomB: tuple[int,int] | None = None):
        self.door_xy.append((x, y))
        # track discovered doorway cells; also mark as corridor for indexing
        self.corridor_xy.add((x, y))
        # Optional: attach doorway to adjacent rooms if provided
        if roomA in self.room_meta:
            self.room_meta[roomA]['doorways'].append((x, y))
        if roomB in self.room_meta:
            self.room_meta[roomB]['doorways'].append((x, y))

    def _register_corridor_run(self, xs: range | list[int], ys: range | list[int]):
        """Mark a corridor 'walkable stripe' we created between rooms."""
        for x in (xs if isinstance(xs, range) else list(xs)):
            for y in (ys if isinstance(ys, range) else list(ys)):
                self.corridor_xy.add((x, y))

    def _mark_room_visit(self, x: int, y: int):
        """
        Call after agent position is updated (each step). Uses self.step_count if available.
        Detects transitions and logs first-time visits.
        """
        rid = self.xy_to_room.get((int(x), int(y)))  # (col,row) or None (corridor)
        # Transition detection
        if rid != self._last_room_id:
            if rid is not None and rid not in self._visited_rooms_set:
                self._visited_rooms_set.add(rid)
                color = self.room_meta.get(rid, {}).get('color', 'gray')
                step_no = int(getattr(self, 'step_count', 0))
                self.visited_rooms_order.append({
                    'room': rid,
                    'color': color,
                    'entered_at_step': step_no,
                    'entry_xy': (int(x), int(y)),
                })
        self._last_room_id = rid

    # --- Convenience getters (useful for plotting and EMAP color bridging) -----

    def get_room_id_at(self, x: int, y: int):
        return self.xy_to_room.get((int(x), int(y)))

    def get_room_color_by_room(self, room: tuple[int,int] | None):
        if room is None: return None
        meta = self.room_meta.get(room)
        return meta['color'] if meta else None

    def get_room_color_by_xy(self, x: int, y: int):
        return self.get_room_color_by_room(self.get_room_id_at(x, y))

    def get_visited_rooms_order(self) -> list[tuple[int,int]]:
        """Return discovery order as list of (col,row)."""
        return [rec['room'] for rec in self.visited_rooms_order]

    def get_rooms_layout_summary(self) -> dict:
        """Lightweight summary for debugging/visuals."""
        return {
            'rooms_in_row': self.rooms_in_row,
            'rooms_in_col': self.rooms_in_col,
            'n_rooms': len(self.room_meta),
            'n_corridor_cells': len(self.corridor_xy),
            'n_doors': len(self.door_xy),
            'first_visits': self.visited_rooms_order[:5]
        }
    # ---- END: room/corridor instrumentation helpers ---------------------------
    
    def Spawn_Obstacles(self, obstacle_rate= None):
        """
        Spawn grey 1×1 walls on coloured-room floor tiles while
        • leaving every corridor endpoint + its moat free,
        • ensuring the current agent corridor can still reach another corridor tile,
        • **never touching the agent's current position.**
        """
        print("DIS WHE")
        
        if obstacle_rate is None:
            obstacle_rate = self.obstacle_rate
        
        dirs = ((1, 0), (-1, 0), (0, 1), (0, -1))

        # ───────── collect room & corridor tiles ───────────────────────────
        coloured_floor, corridor_cells = [], []
        for x in range(1, self.grid.width - 1):
            for y in range(1, self.grid.height - 1):
                cell = self.grid.get(x, y)
                if isinstance(cell, Floor):
                    (corridor_cells if cell.color == "black" else coloured_floor).append((x, y))

        assert len(corridor_cells) >= 2, "environment must contain corridors"

        # ───────── build protected set: corridor endpoints + moat ───────────
        def degree(coord):
            x, y = coord
            return sum(((x + dx, y + dy) in corridor_cells) for dx, dy in dirs)

        protected = {c for c in corridor_cells if degree(c) == 1}          # endpoints
        for x, y in list(protected):                                        # 1-tile moat
            protected.update({(x + dx, y + dy) for dx, dy in dirs})

        # Protect the agent's CURRENT position (not spawn position)
        protected.add(tuple(self.agent_pos))

        # remove protected tiles from candidate list
        coloured_floor = [c for c in coloured_floor if c not in protected]

        # ───────── identify current agent corridor tile (for connectivity test) ─────
        ax, ay = self.agent_pos
        current_corridor = min(corridor_cells, key=lambda c: abs(c[0] - ax) + abs(c[1] - ay))
        
        # ───── helper: can current corridor still reach another corridor? ────
        def corridor_reachable(blocked: set) -> bool:
            q = deque([current_corridor])
            visited = {current_corridor}

            while q:
                x, y = q.popleft()
                for dx, dy in dirs:
                    nx, ny = x + dx, y + dy
                    if (nx, ny) in visited or (nx, ny) in blocked:
                        continue
                    if 0 <= nx < self.grid.width and 0 <= ny < self.grid.height:
                        cell = self.grid.get(nx, ny)
                        passable = (
                            cell is None
                            or (hasattr(cell, "can_overlap") and cell.can_overlap())
                            or isinstance(cell, Door)
                        )
                        if passable:
                            if (nx, ny) in corridor_cells and (nx, ny) != current_corridor:
                                return True       # found another corridor tile
                            visited.add((nx, ny))
                            q.append((nx, ny))
            return False                          # current corridor isolated

        # ───────────────────── greedy obstacle insertion ───────────────────
        random.shuffle(coloured_floor)
        target_n = int(len(coloured_floor) * obstacle_rate)
        new_obstacles = set()

        for (x, y) in coloured_floor:
            if len(new_obstacles) >= target_n:
                break
            candidate = self.current_obstacles | new_obstacles | {(x, y)}
            if corridor_reachable(candidate):
                new_obstacles.add((x, y))

        # ─────────────────────────── render walls ──────────────────────────
        for x, y in new_obstacles:
            self.grid.vert_wall(x, y, 1)
        
        # Update tracking
        self.current_obstacles.update(new_obstacles)

        if self.debug:
            print(f"Placed {len(new_obstacles)} new grey obstacles "
                f"(target {target_n}) – current corridor reachable & endpoints clear")
            print(f"Total obstacles now: {len(self.current_obstacles)}")
    def Remove_Obstacles(self):
        """Remove all current obstacles from the grid"""
        for x, y in self.current_obstacles:
            # Get the original floor color for this position
            # Find which room this position belongs to
            rooms_in_row = self.rooms_in_row
            rooms_in_col = self.rooms_in_col
            room_w = self.rooms_size
            room_h = self.rooms_size
            
            # Find which room this obstacle is in
            for row_inc in range(rooms_in_row):
                for col_inc in range(rooms_in_col):
                    xL = col_inc * (room_w + self.corridor_length)
                    yT = row_inc * (room_h + self.corridor_length)
                    xR = xL + room_w 
                    yB = yT + room_h
                    
                    if xL < x < xR and yT < y < yB:
                        # This obstacle is in this room, restore original color
                        color = list(self.color_idx.keys())[self._rand_int(0, 4)]
                        self.put_obj(Floor(color), x, y)
                        break
        
        self.current_obstacles.clear()
        if self.debug:
            print("Removed all obstacles")     
    def action_to_pose(self,action,current_pose):
        
        if action[0] == 1:
            current_pose[:2] = DIR_TO_VEC[current_pose[2]] + current_pose[:2]
        elif action[1] == 1:
            current_pose[2] = (current_pose[2]+1)%4
        elif action[2] == 1:
            current_pose[2] = (current_pose[2]-1)%4

        return current_pose


class AisleDoorFourTilesRoomsE(AisleDoorRooms):
    def __init__(self, size=120, rooms_size=5, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None, corridor_length=4, max_steps = 4000):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col,  wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length, max_steps = max_steps) #random_corridor_length if corridor_length 0 or not int
class AisleDoorFiveTilesRoomsE(AisleDoorRooms):
    def __init__(self,size=120, rooms_size=6, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None, 
                 corridor_length=4, max_steps = 400):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col,  \
                         wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length,\
                           max_steps = max_steps) 
class AisleDoorSixTilesRoomsE(AisleDoorRooms):
    def __init__(self,size=120, rooms_size=7, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None,
                  corridor_length=4, max_steps = 400):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col, wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length, max_steps = max_steps) 
class AisleDoorSevenTilesRoomsE(AisleDoorRooms):
    def __init__(self,size=120, rooms_size=8, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None, 
                 corridor_length=4, max_steps = 400):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col,  \
                         wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length, \
                         max_steps = max_steps) 
class AisleDoorEightTilesRoomsE(AisleDoorRooms):
    def __init__(self,size=120, rooms_size=9, rooms_in_row = 3, rooms_in_col = 3,  wT_around = 1, wT_size=1, agent_start_pos=None, 
                 corridor_length=4, max_steps = 40):
        super().__init__(size=size, rooms_size=rooms_size, rooms_in_row = rooms_in_row, rooms_in_col = rooms_in_col,  \
                         wT_around = wT_around, wT_size=wT_size, agent_start_pos=agent_start_pos, corridor_length=corridor_length, \
                         max_steps = max_steps) 
register(
    id='MiniGrid-4-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorFourTilesRoomsE'
)
register(
    id='MiniGrid-5-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorFiveTilesRoomsE'
)
register(
    id='MiniGrid-6-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorSixTilesRoomsE'
)
register(
    id='MiniGrid-7-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorSevenTilesRoomsE'
)
register(
    id='MiniGrid-8-tiles-ad-rooms-v0',
    entry_point='gym_minigrid.envs:AisleDoorEightTilesRoomsE'
)

class AisleDoorRoomsObstacles(MiniGridEnv):
    """
    Colored rooms connected by corridors *plus* random 1×1 grey obstacles.
    Rewards:
        +0.02  each safe step
        -1.0   on collision   → episode terminates
    Episode ends after 100 env steps if no collision occurs.
    """

    # ───────────────────────────────── init ──────────────────────────────────
    def __init__(
        self,
        size=120,
        agent_start_pos=None,
        agent_start_dir=0,
        rooms_size=5,
        rooms_in_row=3,
        rooms_in_col=3,
        corridor_length=5,
        automatic_door=True,
        wT_around=6,
        wT_size=1,
        obstacle_rate=0.15,          # ← 5 % of free cells become walls
        max_steps=100,               # ← horizon for “survival”
        debug=True,                 # ← print everything?
    ):
        # user-visible params
        self.automatic_door = automatic_door
        self.rooms_size = rooms_size
        self.rooms_in_row = rooms_in_row
        self.rooms_in_col = rooms_in_col
        self.obstacle_rate = obstacle_rate
        self.debug = debug
        self.wT_around=wT_around
        self.wT_size=wT_size
        self.size=size

        # store start pose if provided
        self.agent_start_pos = agent_start_pos
        self.agent_start_dir = agent_start_dir

        # allow fixed or random corridor length
        if isinstance(corridor_length, int) and corridor_length > 0:
            self.corridor_length = corridor_length
        else:
            self.corridor_length = self._rand_int(3, 7)

        # compute full grid size
        width = (rooms_in_col * rooms_size
                 + (rooms_in_col - 1) * (self.corridor_length + 1) - 1)
        height = (rooms_in_row * rooms_size
                  + (rooms_in_row - 1) * (self.corridor_length + 1) - 1)

        # small aesthetic tweaks so the grid isn’t exactly 20/30/40
        if width <= 20:
            width += 1
        if height <= 20:
            height += 1
        if width >= 30:
            width -= 1
        if height >= 30:
            height -= 1
        if width >= 40:
            width -= 1
        if height >= 40:
            height -= 1
        self.color_idx = {
            'red' : 0,
            'green' : 1,
            'blue'  : 2,
            'purple': 3,
            'white' : 4,
            
        }

        super().__init__(
            grid_size=None,
            width=width,
            height=height,
            max_steps=max_steps,
            see_through_walls=False,
        )

    # ─────────────────────────────── grid layout ─────────────────────────────
    def _gen_grid(self, width, height):
        if self.debug:
            print("Generating map …")

        super_grid_init = super()._gen_grid  # keep flake-8 calm
        print("4TFFFFFFFFF")       
        #define the number of room in col/row
        rooms_in_row = self.rooms_in_row
        rooms_in_col = self.rooms_in_col

        room_w = self.rooms_size
        room_h = self.rooms_size
       
        # Create an empty grid
        self.grid = Grid(width, height)
        self.grid.grid = [Floor('black')] * width * height

        # Generate the surrounding walls
    
        self.grid.horz_wall(0, 0)
        self.grid.horz_wall(0, height - 1)
        self.grid.vert_wall(0, 0)
        self.grid.vert_wall(width - 1, 0)

        # Generate the surrounding walls
        self.grid.wall_rect(0, 0, width, height)
        balls_rooms = []
        while len(balls_rooms) < self.wT_around:
            col_room = self._rand_int(0, rooms_in_col)
            row_room = self._rand_int(0, rooms_in_row)
            if [col_room, row_room] not in balls_rooms:
                balls_rooms.append([col_room, row_room])

        pos_vert_R = None
        pos_horz_B = [None] * rooms_in_col
        agent_pose_options = []

     
        # For each row of rooms
        for row_inc in range(0, rooms_in_row):
            # For each column
            for col_inc in range(0, rooms_in_col):
                
                xL = col_inc * (room_w + self.corridor_length)
                yT = row_inc * (room_h + self.corridor_length)
                xR = xL + room_w 
                yB = yT + room_h
       
                color = list(self.color_idx.keys())[list(self.color_idx.values()).index(self._rand_int(0, 4))]

                for b in range(xL+1, xR):
                    for c in range(yT+1,yB):
                        self.put_obj(Floor(color),b, c)

                #upper wall and door
                if col_inc > 0:
                    self.grid.vert_wall(xL, yT, room_h+1)
                    self.grid.set(xL,pos_vert_R[1], Floor('black'))
                    self.put_obj(Door(color='white'), xL-int(self.corridor_length/2), pos_vert_R[1] )
                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((xL-int(self.corridor_length/2)-corridor_depth,pos_vert_R[1],0))
    
                # Bottom wall and door
                if col_inc + 1 < rooms_in_col:
                    self.grid.vert_wall(xR, yT, room_h+1)
                    pos_vert_R = (xR, self._rand_int(yT + 1, yB))
                    self.grid.set(*pos_vert_R, Floor('black'))
                    self.grid.horz_wall(xR, pos_vert_R[1]-1, self.corridor_length)  
                    self.grid.horz_wall(xR, pos_vert_R[1]+1, self.corridor_length)
                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((xR+int(self.corridor_length/2)+corridor_depth, pos_vert_R[1],2))  

                if row_inc > 0:
                    self.grid.horz_wall(xL, yT, room_w+1)
                    self.grid.set(pos_horz_B[col_inc][0],yT, Floor('black'))
                    self.put_obj(Door(color='white'), pos_horz_B[col_inc][0] , yT-int(self.corridor_length/2))
                    corridor_depth = self._rand_int(0, 2)
                    agent_pose_options.append((pos_horz_B[col_inc][0],yT-int(self.corridor_length/2)-corridor_depth,1))
                   
                # Bottom wall and door
                if row_inc + 1 < rooms_in_row:
                    self.grid.horz_wall(xL, yB, room_w+1)
                    pos_horz_B[col_inc] = (self._rand_int(xL + 1, xR), yB)
                    self.grid.set(*pos_horz_B[col_inc], Floor('black'))
                    self.grid.vert_wall(pos_horz_B[col_inc][0]-1, yB, self.corridor_length)  
                    self.grid.vert_wall(pos_horz_B[col_inc][0]+1, yB, self.corridor_length)  
                    corridor_depth = self._rand_int(0, 2)
                    
                    agent_pose_options.append((pos_horz_B[col_inc][0], yB+int(self.corridor_length/2)+corridor_depth, 3))  

         
        self.agent_pos, self.agent_dir = self._pick_random_spawn()
        self.vel_ob = [0,0]
        self.encoded_action = None      
        self._scatter_obstacles()
        spawn_cell = self.grid.get(*self.agent_pos)
        if isinstance(spawn_cell, Door):
            spawn_cell.toggle(self, self.agent_pos)
        elif isinstance(self.grid.get(*self.front_pos), Door):
            self.grid.get(*self.front_pos).toggle(self, self.front_pos)
        self.mission = "Avoid grey obstacles"

    # ------------------------------------------------------------------ #
    def _pick_random_spawn(self):
        """
        Return (np.array([x, y]), dir_idx) such that the start tile is

        • a coloured room floor   OR
        • a black floor that neighbours a coloured floor/door (corridor),
        • not on an obstacle / wall / goal,
        • and has at least one 4-neighbour it can step onto.
        """
        dirs = ((1, 0), (-1, 0), (0, 1), (0, -1))
        candidates = []

        for x in range(1, self.grid.width - 1):
            for y in range(1, self.grid.height - 1):
                cell = self.grid.get(x, y)

                # we only ever spawn on plain Floor tiles
                if not isinstance(cell, Floor):
                    continue

                # ---------- colour logic ------------------------------------
                if cell.color == "black":
                    # Accept *only* if this black tile touches at least one
                    # coloured floor -- that marks it as a corridor segment
                    touches_room = False
                    for dx, dy in dirs:
                        ncell = self.grid.get(x + dx, y + dy)
                        if isinstance(ncell, Floor) and ncell.color != "black":
                            touches_room = True
                            break
                        if isinstance(ncell, Door):                # door counts
                            touches_room = True
                            break
                    if not touches_room:
                        continue        # outer-ring or isolated black tile → skip

                # ---------- boxed-in test -----------------------------------
                has_exit = False
                for dx, dy in dirs:
                    nx, ny  = x + dx, y + dy
                    ncell   = self.grid.get(nx, ny)
                    if (
                        ncell is None
                        or (hasattr(ncell, "can_overlap") and ncell.can_overlap())
                        or isinstance(ncell, Door)
                    ):
                        has_exit = True
                        break
                if not has_exit:
                    continue

                # passed all criteria
                candidates.append((x, y))

        assert candidates, "No valid spawn tiles after filtering!"

        x, y     = random.choice(candidates)
        dir_idx  = self._rand_int(0, 4)     # 0:→ 1:↓ 2:← 3:↑
        return np.array([x, y]), dir_idx
    # --------------------------------------------------------------------- #
    def _scatter_obstacles(self):
        """
        Drop grey 1×1 walls on coloured-room floor tiles while

        • leaving every corridor endpoint + its moat free,
        • ensuring the *spawn corridor* can still reach another corridor tile, and
        • **never touching the agent’s spawn tile itself.**
        """
        dirs = ((1, 0), (-1, 0), (0, 1), (0, -1))

        # ───────── collect room & corridor tiles ───────────────────────────
        coloured_floor, corridor_cells = [], []
        for x in range(1, self.grid.width - 1):
            for y in range(1, self.grid.height - 1):
                cell = self.grid.get(x, y)
                if isinstance(cell, Floor):
                    (corridor_cells if cell.color == "black" else coloured_floor).append((x, y))

        assert len(corridor_cells) >= 2, "environment must contain corridors"

        # ───────── build protected set: corridor endpoints + moat ───────────
        def degree(coord):
            x, y = coord
            return sum(((x + dx, y + dy) in corridor_cells) for dx, dy in dirs)

        protected = {c for c in corridor_cells if degree(c) == 1}          # endpoints
        for x, y in list(protected):                                        # 1-tile moat
            protected.update({(x + dx, y + dy) for dx, dy in dirs})

        # NEW ─────—— also protect the agent’s spawn position ───────────────
        protected.add(tuple(self.agent_pos))

        # remove protected tiles from candidate list
        coloured_floor = [c for c in coloured_floor if c not in protected]

        # ───────── identify spawn-corridor tile (for connectivity test) ─────
        ax, ay = self.agent_pos
        spawn_corridor = min(corridor_cells, key=lambda c: abs(c[0] - ax) + abs(c[1] - ay))
            # ───── helper: can spawn corridor still reach another corridor? ────
        def spawn_corridor_ok(blocked: set) -> bool:
            q       = deque([spawn_corridor])
            visited = {spawn_corridor}

            while q:
                x, y = q.popleft()
                for dx, dy in dirs:
                    nx, ny = x + dx, y + dy
                    if (nx, ny) in visited or (nx, ny) in blocked:
                        continue
                    if 0 <= nx < self.grid.width and 0 <= ny < self.grid.height:
                        cell = self.grid.get(nx, ny)
                        passable = (
                            cell is None
                            or (hasattr(cell, "can_overlap") and cell.can_overlap())
                            or isinstance(cell, Door)
                        )
                        if passable:
                            if (nx, ny) in corridor_cells and (nx, ny) != spawn_corridor:
                                return True       # found another corridor tile
                            visited.add((nx, ny))
                            q.append((nx, ny))
            return False                          # spawn corridor isolated

        # ───────────────────── greedy obstacle insertion ───────────────────
        random.shuffle(coloured_floor)
        target_n  = int(len(coloured_floor) * self.obstacle_rate)
        obstacles = set()

        for (x, y) in coloured_floor:
            if len(obstacles) >= target_n:
                break
            candidate = obstacles | {(x, y)}
            if spawn_corridor_ok(candidate):
                obstacles.add((x, y))

        # ─────────────────────────── render walls ──────────────────────────
        for x, y in obstacles:
            self.grid.vert_wall(x, y, 1)

        if self.debug:
            print(f"Placed {len(obstacles)} grey obstacles "
                f"(target {target_n}) – spawn corridor reachable & endpoints clear")
    # ─────────────────────────────── stepping ────────────────────────────────
    def step(self, action):
        """
        • Executes the primitive action via MiniGridEnv.step  
        • Opens automatic doors (old behaviour)  
        • Detects collisions and shapes reward  
        • Emits a boolean `info["collision"]`  
        • Terminates on collision or when max_steps is reached
        """
        # ------------------------------------------------------------------ #
        # 1) run the *real* env step (base reward ignored here)
        obs, _, done, info = super().step(action)
        # ------------------------------------------------------------------ #

        # ─────────────── automatic door helper ────────────────────────────
        if not done and self.automatic_door:
            fwd_cell = self.grid.get(*self.front_pos)
            if fwd_cell and fwd_cell.type == "door":
                # open the door, keep motion count unchanged
                obs2, _, _, _ = super().step(
                    self.actions.toggle, automatic_door=True
                )
                obs["door_open"] = obs2["door_open"]
                obs["image"] = obs2["image"]

        # ─────────────── collision detection / reward shaping ─────────────
        collided = (
            action == self.actions.forward and obs["vel_ob"] == [0, 0]
        )

        if collided:               # hit a wall or obstacle
            reward = -1.0
            done = True
        else:                      # survived one more step
            reward = 0.02

        info["collision"] = collided

        # ─────────────── bookkeeping for pose output (unchanged) ──────────
        real_action = [0, 0, 0] if obs["vel_ob"][0] == obs["vel_ob"][1] \
                               else obs["action"].copy()
        self.current_pose = self.action_to_pose(real_action, self.current_pose)
        obs["pose"] = self.current_pose

        if self.debug and (collided or done):
            flag = "COLLISION" if collided else "TIMEOUT"
            print(f"[step {self.step_count}] {flag} | reward {reward:.2f}")

        return obs, reward, done, info

    # ─────────────────────────── util for pose update ───────────────────────
    def action_to_pose(self, action, current_pose):
        if action[0] == 1:           # forward
            current_pose[:2] = DIR_TO_VEC[current_pose[2]] + current_pose[:2]
        elif action[1] == 1:         # turn right
            current_pose[2] = (current_pose[2] + 1) % 4
        elif action[2] == 1:         # turn left
            current_pose[2] = (current_pose[2] - 1) % 4
        return current_pose


# ─────────────────────────────── registry ───────────────────────────────────
register(
    id="MiniGrid-ADRooms-Collision-v0",
    entry_point="gym_minigrid.envs:AisleDoorRoomsObstacles",
)

```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/minigrid.py`:

```py
import math
import hashlib
import gym
from enum import IntEnum
import numpy as np
from gym import error, spaces, utils
from gym.utils import seeding
from .rendering import *
import random 

# Size in pixels of a tile in the full-scale human view
TILE_PIXELS = 32

# Map of color names to RGB values
COLORS = {
    'red'   : np.array([255, 0, 0]),
    'green' : np.array([0, 255, 0]),
    'blue'  : np.array([0, 0, 255]),
    'purple': np.array([112, 39, 195]),
    'yellow': np.array([255, 255, 0]),
    'grey'  : np.array([100, 100, 100]),
    'black'  : np.array([0, 0, 0]),
    'white'  : np.array([255, 255, 255]),
    'white_floor'  : np.array([255*2, 255*2, 255*2])
}

COLOR_NAMES = sorted(list(COLORS.keys()))

# Used to map colors to integers
COLOR_TO_IDX = {
    'red'   : 0,
    'green' : 1,
    'blue'  : 2,
    'purple': 3,
    'yellow': 4,
    'grey'  : 5,
    'black' : 6,
    'white' : 7,
    'white_floor':8,
}
IDX_TO_COLOR = dict(zip(COLOR_TO_IDX.values(), COLOR_TO_IDX.keys()))

# Map of object type to integers
OBJECT_TO_IDX = {
    'unseen'        : 0,
    'empty'         : 1,
    'wall'          : 2,
    'floor'         : 3,
    'door'          : 4,
    'key'           : 5,
    'ball'          : 6,
    'box'           : 7,
    'goal'          : 8,
    'lava'          : 9,
    'agent'         : 10,
}

IDX_TO_OBJECT = dict(zip(OBJECT_TO_IDX.values(), OBJECT_TO_IDX.keys()))

# Map of state names to integers
STATE_TO_IDX = {
    'open'  : 0,
    'closed': 1,
    'locked': 2,
}

# Map of agent direction indices to vectors
DIR_TO_VEC = [
    # Pointing right (positive X)
    np.array((1, 0)),
    # Down (positive Y)
    np.array((0, 1)),
    # Pointing left (negative X)
    np.array((-1, 0)),
    # Up (negative Y)
    np.array((0, -1)),
]

class WorldObj:
    """
    Base class for grid world objects
    """

    def __init__(self, type, color):
        assert type in OBJECT_TO_IDX, type
        assert color in COLOR_TO_IDX, color
        self.type = type
        self.color = color
        self.contains = None

        # Initial position of the object
        self.init_pos = None

        # Current position of the object
        self.cur_pos = None

    def can_overlap(self):
        """Can the agent overlap with this?"""
        return False

    def can_pickup(self):
        """Can the agent pick this up?"""
        return False

    def can_contain(self):
        """Can this contain another object?"""
        return False

    def see_behind(self):
        """Can the agent see behind this object?"""
        return True

    def toggle(self, env, pos):
        """Method to trigger/toggle an action this object performs"""
        return False

    def encode(self):
        """Encode the a description of this object as a 3-tuple of integers"""
        return (OBJECT_TO_IDX[self.type], COLOR_TO_IDX[self.color], 0)

    @staticmethod
    def decode(type_idx, color_idx, state):
        """Create an object from a 3-tuple state description"""

        obj_type = IDX_TO_OBJECT[type_idx]
        color = IDX_TO_COLOR[color_idx]

        if obj_type == 'empty' or obj_type == 'unseen':
            return None

        # State, 0: open, 1: closed, 2: locked
        is_open = state == 0
        is_locked = state == 2

        if obj_type == 'wall':
            v = Wall(color)
        elif obj_type == 'floor':
            v = Floor(color)
        elif obj_type == 'ball':
            v = Ball(color)
        elif obj_type == 'key':
            v = Key(color)
        elif obj_type == 'box':
            v = Box(color)
        elif obj_type == 'door':
            v = Door(color, is_open, is_locked)
        elif obj_type == 'goal':
            v = Goal(color)
        elif obj_type == 'lava':
            v = Lava()
        else:
            assert False, "unknown object type in decode '%s'" % obj_type

        return v

    def render(self, r):
        """Draw this object with the given renderer"""
        raise NotImplementedError

class Goal(WorldObj):
    def __init__(self, color= 'green'):
        super().__init__('goal', color)

    def can_overlap(self):
        return True

    def render(self, img):
        fill_coords(img, point_in_rect(0, 1, 0, 1), COLORS[self.color])

class Floor(WorldObj):
    """
    Colored floor tile the agent can walk over
    """

    def __init__(self, color='blue'):
        super().__init__('floor', color)

    def can_overlap(self):
        return True

    def render(self, img):
        # Give the floor a pale color
        color = COLORS[self.color] / 2
        fill_coords(img, point_in_rect(0.031, 1, 0.031, 1), color)


class Lava(WorldObj):
    def __init__(self):
        super().__init__('lava', 'red')

    def can_overlap(self):
        return True

    def render(self, img):
        c = (255, 128, 0)

        # Background color
        fill_coords(img, point_in_rect(0, 1, 0, 1), c)

        # Little waves
        for i in range(3):
            ylo = 0.3 + 0.2 * i
            yhi = 0.4 + 0.2 * i
            fill_coords(img, point_in_line(0.1, ylo, 0.3, yhi, r=0.03), (0,0,0))
            fill_coords(img, point_in_line(0.3, yhi, 0.5, ylo, r=0.03), (0,0,0))
            fill_coords(img, point_in_line(0.5, ylo, 0.7, yhi, r=0.03), (0,0,0))
            fill_coords(img, point_in_line(0.7, yhi, 0.9, ylo, r=0.03), (0,0,0))

class Wall(WorldObj):
    def __init__(self, color='grey'):
        super().__init__('wall', color)

    def see_behind(self):
        return False

    def render(self, img):
        fill_coords(img, point_in_rect(0, 1, 0, 1), COLORS[self.color])

class Door(WorldObj):
    def __init__(self, color, is_open=False, is_locked=False):
        super().__init__('door', color)
        self.is_open = is_open
        self.is_locked = is_locked

    def can_overlap(self):
        """The agent can only walk over this cell when the door is open if 'return self.is_open' """
        return self.is_open

    def see_behind(self):
        return self.is_open

    def toggle(self, env, pos):
        # If the player has the right key to open the door
        if self.is_locked:
            if isinstance(env.carrying, Key) and env.carrying.color == self.color:
                self.is_locked = False
                self.is_open = True
                return True
            return False


        self.is_open = not self.is_open
        return True

    def encode(self):
        """Encode the a description of this object as a 3-tuple of integers"""

        # State, 0: open, 1: closed, 2: locked
        if self.is_open:
            state = 0
        elif self.is_locked:
            state = 2
        elif not self.is_open:
            state = 1

        return (OBJECT_TO_IDX[self.type], COLOR_TO_IDX[self.color], state)

    def render(self, img):
        c = COLORS[self.color]

        if self.is_open:
            fill_coords(img, point_in_rect(0.88, 1.00, 0.00, 1.00), c)
            fill_coords(img, point_in_rect(0.92, 0.96, 0.04, 0.96), (0,0,0))
            return

        # Door frame and door
        if self.is_locked:
            fill_coords(img, point_in_rect(0.00, 1.00, 0.00, 1.00), c)
            fill_coords(img, point_in_rect(0.06, 0.94, 0.06, 0.94), 0.45 * np.array(c))

            # Draw key slot
            fill_coords(img, point_in_rect(0.52, 0.75, 0.50, 0.56), c)
        else:
            fill_coords(img, point_in_rect(0.00, 1.00, 0.00, 1.00), c)
            fill_coords(img, point_in_rect(0.04, 0.96, 0.04, 0.96), (0,0,0))
            fill_coords(img, point_in_rect(0.08, 0.92, 0.08, 0.92), c)
            fill_coords(img, point_in_rect(0.12, 0.88, 0.12, 0.88), (0,0,0))

            # Draw door handle
            fill_coords(img, point_in_circle(cx=0.75, cy=0.50, r=0.08), c)

class Key(WorldObj):
    def __init__(self, color='blue'):
        super(Key, self).__init__('key', color)

    def can_pickup(self):
        return True

    def render(self, img):
        c = COLORS[self.color]

        # Vertical quad
        fill_coords(img, point_in_rect(0.50, 0.63, 0.31, 0.88), c)

        # Teeth
        fill_coords(img, point_in_rect(0.38, 0.50, 0.59, 0.66), c)
        fill_coords(img, point_in_rect(0.38, 0.50, 0.81, 0.88), c)

        # Ring
        fill_coords(img, point_in_circle(cx=0.56, cy=0.28, r=0.190), c)
        fill_coords(img, point_in_circle(cx=0.56, cy=0.28, r=0.064), (0,0,0))

class Ball(WorldObj):
    def __init__(self, color='blue'):
        super(Ball, self).__init__('ball', color)

    def can_pickup(self):
        return True
    
    def can_overlap(self):
        return True

    def render(self, img):
        fill_coords(img, point_in_circle(0.5, 0.5, 0.31), COLORS[self.color])

class Box(WorldObj):
    def __init__(self, color, contains=None):
        super(Box, self).__init__('box', color)
        self.contains = contains

    def can_pickup(self):
        return True
    
    def can_overlap(self):
        return True

    def render(self, img):
        c = COLORS[self.color]

        # Outline
        fill_coords(img, point_in_rect(0.12, 0.88, 0.12, 0.88), c)
        fill_coords(img, point_in_rect(0.18, 0.82, 0.18, 0.82), (0,0,0))

        # Horizontal slit
        fill_coords(img, point_in_rect(0.16, 0.84, 0.47, 0.53), c)

    def toggle(self, env, pos):
        # Replace the box by its contents
        env.grid.set(*pos, self.contains)
        return True

class Grid:
    """
    Represent a grid and operations on it
    """

    # Static cache of pre-renderer tiles
    tile_cache = {}

    def __init__(self, width, height):
        assert width >= 3
        assert height >= 3

        self.width = width
        self.height = height

        self.grid = [None] * width * height

    def __contains__(self, key):
        if isinstance(key, WorldObj):
            for e in self.grid:
                if e is key:
                    return True
        elif isinstance(key, tuple):
            for e in self.grid:
                if e is None:
                    continue
                if (e.color, e.type) == key:
                    return True
                if key[0] is None and key[1] == e.type:
                    return True
        return False

    def __eq__(self, other):
        grid1  = self.encode()
        grid2 = other.encode()
        return np.array_equal(grid2, grid1)

    def __ne__(self, other):
        return not self == other

    def copy(self):
        from copy import deepcopy
        return deepcopy(self)

    def set(self, i, j, v):
        assert i >= 0 and i < self.width
        assert j >= 0 and j < self.height
        self.grid[j * self.width + i] = v

    def get(self, i, j):
        assert i >= 0 and i < self.width
        assert j >= 0 and j < self.height
        return self.grid[j * self.width + i]

    def horz_wall(self, x, y, length=None, obj_type=Wall):
        if length is None:
            length = self.width - x
        for i in range(0, length):
            self.set(x + i, y, obj_type())

    def vert_wall(self, x, y, length=None, obj_type=Wall):
        if length is None:
            length = self.height - y
        for j in range(0, length):
            self.set(x, y + j, obj_type())

    def wall_rect(self, x, y, w, h):
        self.horz_wall(x, y, w)
        self.horz_wall(x, y+h-1, w)
        self.vert_wall(x, y, h)
        self.vert_wall(x+w-1, y, h)

    def rotate_left(self):
        """
        Rotate the grid to the left (counter-clockwise)
        """

        grid = Grid(self.height, self.width)

        for i in range(self.width):
            for j in range(self.height):
                v = self.get(i, j)
                grid.set(j, grid.height - 1 - i, v)

        return grid

    def slice(self, topX, topY, width, height):
        """
        Get a subset of the grid
        """

        grid = Grid(width, height)

        for j in range(0, height):
            for i in range(0, width):
                x = topX + i
                y = topY + j

                if x >= 0 and x < self.width and \
                   y >= 0 and y < self.height:
                    v = self.get(x, y)
                else:
                    v = Wall()
                grid.set(i, j, v)

        return grid

    @classmethod
    def render_tile(
        cls,
        obj,
        agent_dir=None,
        highlight=False,
        tile_size=TILE_PIXELS,
        subdivs=3
    ):
        """
        Render a tile and cache the result
        """

        # Hash map lookup key for the cache
        key = (agent_dir, highlight, tile_size)
        key = obj.encode() + key if obj else key

        if key in cls.tile_cache:
            return cls.tile_cache[key]

        img = np.zeros(shape=(tile_size * subdivs, tile_size * subdivs, 3), dtype=np.uint8)

        # Draw the grid lines (top and left edges)
        fill_coords(img, point_in_rect(0, 0.031, 0, 1), (100, 100, 100))
        fill_coords(img, point_in_rect(0, 1, 0, 0.031), (100, 100, 100))

        if obj != None:
            obj.render(img)

        # Overlay the agent on top
        if agent_dir is not None:
            tri_fn = point_in_triangle(
                (0.12, 0.19),
                (0.87, 0.50),
                (0.12, 0.81),
            )

            # Rotate the agent based on its direction
            tri_fn = rotate_fn(tri_fn, cx=0.5, cy=0.5, theta=0.5*math.pi*agent_dir)
            fill_coords(img, tri_fn, (255, 0, 0))

        # Highlight the cell if needed
        if highlight:
            highlight_img(img)

        # Downsample the image to perform supersampling/anti-aliasing
        img = downsample(img, subdivs)

        # Cache the rendered tile
        cls.tile_cache[key] = img

        return img

    def render(
        self,
        tile_size,
        agent_pos=None,
        agent_dir=None,
        highlight_mask=None,
        agent_view_poses = [],
    ):
        """
        Render this grid at a given scale
        :param r: target renderer object
        :param tile_size: tile size in pixels
        :param agent_view: wether we render whole grid or just agent ob
        """

        if highlight_mask is None:
            highlight_mask = np.zeros(shape=(self.width, self.height), dtype=bool)

        
        if len(agent_view_poses) > 0:
            agent_view_size = int(np.sqrt(len(agent_view_poses)))
            img = np.zeros(shape=(agent_view_size * tile_size, agent_view_size * tile_size, 3), dtype=np.uint8)
           

        else:
            # Compute the total grid size
            width_px = self.width * tile_size
            height_px = self.height * tile_size
            img = np.zeros(shape=(height_px, width_px, 3), dtype=np.uint8)
        
        
        
        # Render the agent view
        if len(agent_view_poses)> 0:
            n = 0 
            agent_dir = 3
            # if np.all(dir_vec == (0,1)):
            #     agent_view_poses = agent_view_poses[::-1]
            for v in agent_view_poses:
                i,j = v[0], v[1]
                try:
                    cell = self.get(i, j)
                    agent_here = np.array_equal(agent_pos, (i, j))
                    
                    tile_img = Grid.render_tile(
                        cell,
                        agent_dir=agent_dir if agent_here else None,
                        highlight=highlight_mask[i, j],
                        tile_size=tile_size
                    )
                except AssertionError:
                    tile_img = (0,0,0)
            
                #if dir_vec[0] != 0:
                #t_in_img = [n // agent_view_size, n % agent_view_size]
                # else:
                t_in_img = [n % agent_view_size, n // agent_view_size,]
                
                ymin = t_in_img[1] * tile_size
                ymax = (t_in_img[1]+1) * tile_size
                xmin = t_in_img[0] * tile_size
                xmax = (t_in_img[0]+1) * tile_size

                n+=1
                try:
                    if highlight_mask[i,j] == False:
                        tile_img = (0,0,0)
                except IndexError:
                    tile_img = (0,0,0)
                img[ymin:ymax, xmin:xmax, :] = tile_img


        # Render the grid
        else:
            for j in range(0, self.height):
                for i in range(0, self.width):
                    cell = self.get(i, j)
                    agent_here = np.array_equal(agent_pos, (i, j))
                    tile_img = Grid.render_tile(
                        cell,
                        agent_dir=agent_dir if agent_here else None,
                        highlight=highlight_mask[i, j],
                        tile_size=tile_size
                    )
                    
                    
                    ymin = j * tile_size
                    ymax = (j+1) * tile_size
                    xmin = i * tile_size
                    xmax = (i+1) * tile_size
                    img[ymin:ymax, xmin:xmax, :] = tile_img

        return img

    def encode(self, vis_mask=None):
        """
        Produce a compact numpy encoding of the grid
        """

        if vis_mask is None:
            vis_mask = np.ones((self.width, self.height), dtype=bool)

        array = np.zeros((self.width, self.height, 3), dtype='uint8')

        for i in range(self.width):
            for j in range(self.height):
                if vis_mask[i, j]:
                    v = self.get(i, j)

                    if v is None:
                        array[i, j, 0] = OBJECT_TO_IDX['empty']
                        array[i, j, 1] = 0
                        array[i, j, 2] = 0

                    else:
                        array[i, j, :] = v.encode()

        return array

    @staticmethod
    def decode(array):
        """
        Decode an array grid encoding back into a grid
        """

        width, height, channels = array.shape
        assert channels == 3

        vis_mask = np.ones(shape=(width, height), dtype=bool)

        grid = Grid(width, height)
        for i in range(width):
            for j in range(height):
                type_idx, color_idx, state = array[i, j]
                v = WorldObj.decode(type_idx, color_idx, state)
                grid.set(i, j, v)
                vis_mask[i, j] = (type_idx != OBJECT_TO_IDX['unseen'])

        return grid, vis_mask

    def process_vis(grid, agent_pos):
        mask = np.zeros(shape=(grid.width, grid.height), dtype=bool)

        mask[agent_pos[0], agent_pos[1]] = True

        for j in reversed(range(0, grid.height)):
            for i in range(0, grid.width-1):
                if not mask[i, j]:
                    continue

                cell = grid.get(i, j)
                if cell and not cell.see_behind():
                    continue

                mask[i+1, j] = True
                if j > 0:
                    mask[i+1, j-1] = True
                    mask[i, j-1] = True

            for i in reversed(range(1, grid.width)):
                if not mask[i, j]:
                    continue

                cell = grid.get(i, j)
                if cell and not cell.see_behind():
                    continue

                mask[i-1, j] = True
                if j > 0:
                    mask[i-1, j-1] = True
                    mask[i, j-1] = True

        for j in range(0, grid.height):
            for i in range(0, grid.width):
                if not mask[i, j]:
                    grid.set(i, j, None)

        return mask

class MiniGridEnv(gym.Env):
    """
    2D grid world game environment
    """

    metadata = {
        'render.modes': ['human', 'rgb_array'],
        'video.frames_per_second' : 10
    }

    # Enumeration of possible actions
    class Actions(IntEnum):
        # Turn left, turn right, move forward
        left = 0
        right = 1
        forward = 2

        # Pick up an object
        pickup = 3
        # Drop an object
        drop = 4
        # Toggle/activate an object
        toggle = 5

        # Done completing task
        done = 6

    def __init__(
        self,
        grid_size=None,
        width=None,
        height=None,
        max_steps=100,
        see_through_walls=False,
        seed=1337,
        agent_view_size=7
    ):
        # Can't set both grid_size and width/height
        if grid_size:
            assert width == None and height == None
            width = grid_size
            height = grid_size

        # Action enumeration for this environment
        self.actions = MiniGridEnv.Actions

        # Actions are discrete integer values
        self.action_space = spaces.Discrete(len(self.actions))

        # Number of cells (width and height) in the agent view
        assert agent_view_size % 2 == 1
        assert agent_view_size >= 3
        self.agent_view_size = agent_view_size

        # Observations are dictionaries containing an
        # encoding of the grid and a textual 'mission' string
        self.observation_space = spaces.Box(
            low=0,
            high=255,
            shape=(self.agent_view_size, self.agent_view_size, 3),
            dtype='uint8'
        )
        self.observation_space = spaces.Dict({
            'image': self.observation_space
        })

        # Range of possible rewards
        self.reward_range = (0, 1)

        # Window to use for human rendering mode
        self.window = None

        # Environment configuration
        self.width = width
        self.height = height
        self.max_steps = max_steps
        self.see_through_walls = see_through_walls

        # Current position and direction of the agent
        self.agent_pos = None
        self.agent_dir = None

        # Initialize the RNG
        self.seed(seed=seed)

        # Initialize the state
        self.reset()

    def reset(self, test=False):
        # Current position and direction of the agent
        self.agent_pos = None
        self.agent_dir = None
        self.current_pose = np.array([0,0,0])

        # Generate a new random grid at the start of each episode
        # To keep the same grid for each episode, call env.seed() with
        # the same seed before calling env.reset()
        self._gen_grid(self.width, self.height)
        
        if test == True:
            print('CHECK GOAL POSE', self.goal)   
            self.put_obj(Floor(self.goal[0]),self.goal[1], self.goal[2])
            x, y = self._rand_int(1, self.width), self._rand_int(1, self.height)
            tile = self.grid.get(x,y)
            while tile.type != 'floor' or tile.color == 'black':
                x, y = self._rand_int(1, self.width), self._rand_int(1, self.height)
                # print(x,y)
                tile = self.grid.get(x,y)
            
            self.put_obj(Goal('white'), x, y)
            print('NEW GOAL POSE', x,y)   

        # These fields should be defined by _gen_grid
        assert self.agent_pos is not None
        assert self.agent_dir is not None

        # Check that the agent doesn't overlap with an object
        start_cell = self.grid.get(*self.agent_pos)
        assert start_cell is None or start_cell.can_overlap()

        # Item picked up, being carried, initially nothing
        self.carrying = None

        # Step count since episode start
        self.step_count = 0

        # Return first observation
        obs = self.gen_obs()
        return obs

    def seed(self, seed=1337):
        # Seed the random number generator
        self.np_random, _ = seeding.np_random(seed)
        return [seed]

    def hash(self, size=16):
        """Compute a hash that uniquely identifies the current state of the environment.
        :param size: Size of the hashing
        """
        sample_hash = hashlib.sha256()

        to_encode = [self.grid.encode().tolist(), self.agent_pos, self.agent_dir]
        for item in to_encode:
            sample_hash.update(str(item).encode('utf8'))

        return sample_hash.hexdigest()[:size]

    @property
    def steps_remaining(self):
        if self.max_steps:
            return self.max_steps - self.step_count
        else:
            return 1

    def __str__(self):
        """
        Produce a pretty string of the environment's grid along with the agent.
        A grid cell is represented by 2-character string, the first one for
        the object and the second one for the color.
        """

        # Map of object types to short string
        OBJECT_TO_STR = {
            'wall'          : 'W',
            'floor'         : 'F',
            'door'          : 'D',
            'key'           : 'K',
            'ball'          : 'A',
            'box'           : 'B',
            'goal'          : 'G',
            'lava'          : 'V',
        }

        # Short string for opened door
        OPENDED_DOOR_IDS = '_'

        # Map agent's direction to short string
        AGENT_DIR_TO_STR = {
            0: '>',
            1: 'V',
            2: '<',
            3: '^'
        }

        str = ''

        for j in range(self.grid.height):

            for i in range(self.grid.width):
                if i == self.agent_pos[0] and j == self.agent_pos[1]:
                    str += 2 * AGENT_DIR_TO_STR[self.agent_dir]
                    continue

                c = self.grid.get(i, j)

                if c == None:
                    str += '  '
                    continue

                if c.type == 'door':
                    if c.is_open:
                        str += '__'
                    elif c.is_locked:
                        str += 'L' + c.color[0].upper()
                    else:
                        str += 'D' + c.color[0].upper()
                    continue

                str += OBJECT_TO_STR[c.type] + c.color[0].upper()

            if j < self.grid.height - 1:
                str += '\n'

        return str

    def _gen_grid(self, width, height):
        assert False, "_gen_grid needs to be implemented by each environment"

    def _reward(self):
        """
        Compute the reward to be given upon success
        """
        if self.max_steps:
            return 1 - 0.9 * (self.step_count / self.max_steps)
        else:
            return 1

    def _rand_int(self, low, high):
        """
        Generate random integer in [low,high[
        """

        return self.np_random.randint(low, high)

    def _rand_float(self, low, high):
        """
        Generate random float in [low,high[
        """

        return self.np_random.uniform(low, high)

    def _rand_bool(self):
        """
        Generate random boolean value
        """

        return (self.np_random.randint(0, 2) == 0)

    def _rand_elem(self, iterable):
        """
        Pick a random element in a list
        """

        lst = list(iterable)
        idx = self._rand_int(0, len(lst))
        return lst[idx]

    def _rand_subset(self, iterable, num_elems):
        """
        Sample a random subset of distinct elements of a list
        """

        lst = list(iterable)
        assert num_elems <= len(lst)

        out = []

        while len(out) < num_elems:
            elem = self._rand_elem(lst)
            lst.remove(elem)
            out.append(elem)

        return out

    def _rand_color(self):
        """
        Generate a random color name (string)
        """

        return self._rand_elem(COLOR_NAMES)

    def _rand_pos(self, xLow, xHigh, yLow, yHigh):
        """
        Generate a random (x,y) position tuple
        """

        return (
            self.np_random.randint(xLow, xHigh),
            self.np_random.randint(yLow, yHigh)
        )

    def place_obj(self,
        obj,
        top=None,
        size=None,
        reject_fn=None,
        max_tries=math.inf
    ):
        """
        Place an object at an empty position in the grid

        :param top: top-left position of the rectangle where to place
        :param size: size of the rectangle where to place
        :param reject_fn: function to filter out potential positions
        """

        if top is None:
            top = (0, 0)
        else:
            top = (max(top[0], 0), max(top[1], 0))

        if size is None:
            size = (self.grid.width, self.grid.height)

        num_tries = 0

        while True:
            # This is to handle with rare cases where rejection sampling
            # gets stuck in an infinite loop
            if num_tries > max_tries:
                raise RecursionError('rejection sampling failed in place_obj')

            num_tries += 1

            pos = np.array((
                self._rand_int(top[0], min(top[0] + size[0], self.grid.width)),
                self._rand_int(top[1], min(top[1] + size[1], self.grid.height))
            ))

            # Don't place the object on top of another object
            grid_content = self.grid.get(*pos) 
            if grid_content!= None:
                if isinstance(grid_content, Floor):
                    break
                else:
                    continue

            # Don't place the object where the agent is
            if np.array_equal(pos, self.agent_pos):
                continue

            # Check if there is a filtering criterion
            if reject_fn and reject_fn(self, pos):
                continue

            break

        #self.grid.set(*pos, obj)

        if obj is not None:
            obj.init_pos = pos
            obj.cur_pos = pos

        return pos

    def put_obj(self, obj, i, j):
        """
        Put an object at a specific position in the grid
        """

        self.grid.set(i, j, obj)
        obj.init_pos = (i, j)
        obj.cur_pos = (i, j)

    def place_agent(
        self,
        top=None,
        size=None,
        rand_dir=True,
        max_tries=math.inf
    ):
        """
        Set the agent's starting point at an empty position in the grid
        """
        self.agent_pos = None
        self.encoded_action = None
        self.vel_ob = [0,0]
        pos = self.place_obj(None, top, size, max_tries=max_tries)
        self.agent_pos = pos
        #self.door_open=False
        if rand_dir:
            self.agent_dir = self._rand_int(0, 4)

        return pos

    @property
    def dir_vec(self):
        """
        Get the direction vector for the agent, pointing in the direction
        of forward movement.
        """

        assert self.agent_dir >= 0 and self.agent_dir < 4
        return DIR_TO_VEC[self.agent_dir]

    @property
    def right_vec(self):
        """
        Get the vector pointing to the right of the agent.
        """

        dx, dy = self.dir_vec
        return np.array((-dy, dx))

    @property
    def front_pos(self):
        """
        Get the position of the cell that is right in front of the agent
        """

        return self.agent_pos + self.dir_vec

    def get_view_coords(self, i, j):
        """
        Translate and rotate absolute grid coordinates (i, j) into the
        agent's partially observable view (sub-grid). Note that the resulting
        coordinates may be negative or outside of the agent's view size.
        """

        ax, ay = self.agent_pos
        dx, dy = self.dir_vec
        rx, ry = self.right_vec

        # Compute the absolute coordinates of the top-left view corner
        sz = self.agent_view_size
        hs = self.agent_view_size // 2
        tx = ax + (dx * (sz-1)) - (rx * hs)
        ty = ay + (dy * (sz-1)) - (ry * hs)

        lx = i - tx
        ly = j - ty

        # Project the coordinates of the object relative to the top-left
        # corner onto the agent's own coordinate system
        vx = (rx*lx + ry*ly)
        vy = -(dx*lx + dy*ly)

        return vx, vy

    def get_view_exts(self):
        """
        Get the extents of the square set of tiles visible to the agent
        Note: the bottom extent indices are not included in the set
        """

        # Facing right
        if self.agent_dir == 0:
            topX = self.agent_pos[0]
            topY = self.agent_pos[1] - self.agent_view_size // 2
        # Facing down
        elif self.agent_dir == 1:
            topX = self.agent_pos[0] - self.agent_view_size // 2
            topY = self.agent_pos[1]
        # Facing left
        elif self.agent_dir == 2:
            topX = self.agent_pos[0] - self.agent_view_size + 1
            topY = self.agent_pos[1] - self.agent_view_size // 2
        # Facing up
        elif self.agent_dir == 3:
            topX = self.agent_pos[0] - self.agent_view_size // 2
            topY = self.agent_pos[1] - self.agent_view_size + 1
        else:
            assert False, "invalid agent direction"

        botX = topX + self.agent_view_size
        botY = topY + self.agent_view_size

        return (topX, topY, botX, botY)

    def relative_coords(self, x, y):
        """
        Check if a grid position belongs to the agent's field of view, and returns the corresponding coordinates
        """

        vx, vy = self.get_view_coords(x, y)

        if vx < 0 or vy < 0 or vx >= self.agent_view_size or vy >= self.agent_view_size:
            return None

        return vx, vy

    def in_view(self, x, y):
        """
        check if a grid position is visible to the agent
        """

        return self.relative_coords(x, y) is not None

    def agent_sees(self, x, y):
        """
        Check if a non-empty grid position is visible to the agent
        """

        coordinates = self.relative_coords(x, y)
        if coordinates is None:
            return False
        vx, vy = coordinates

        obs = self.gen_obs()
        obs_grid, _ = Grid.decode(obs['image'])
        obs_cell = obs_grid.get(vx, vy)
        world_cell = self.grid.get(x, y)

        return obs_cell is not None and obs_cell.type == world_cell.type

    def step(self, action, automatic_door=False):
        self.step_count += 1

        #print('door_open',self.door_open)
        reward = 0
        done = False
        self.vel_ob = [0,0] 

        # Get the position in front of the agent
        fwd_pos = self.front_pos

        # Get the contents of the cell in front of the agent
        fwd_cell = self.grid.get(*fwd_pos)
        
        # Rotate left
        if action == self.actions.left:
            self.encoded_action = [0,0,1]
            self.agent_dir -= 1
            if self.agent_dir < 0:
                self.agent_dir += 4
            self.vel_ob = [0,-1] 
            # if automatic_door:
            if (np.array(self.door_open) != [0,0]).all() and not ((self.agent_pos == self.door_open).all()):
                fwd_door_cell = self.grid.get(*self.door_open)
                if fwd_door_cell is not None:
                    fwd_door_cell.toggle(self, self.door_open)
                    self.door_open = [0,0]

        # Rotate right
        elif action == self.actions.right:
            self.encoded_action = [0,1,0]
            self.agent_dir = (self.agent_dir + 1) % 4
            self.vel_ob = [0,1]
            # if automatic_door:
            if (np.array(self.door_open) != [0,0]).all() and not ((self.agent_pos == self.door_open).all()):
                fwd_door_cell = self.grid.get(*self.door_open)
                if fwd_door_cell is not None :
                    fwd_door_cell.toggle(self, self.door_open)
                    self.door_open = [0,0]

        # Move forward
        elif action == self.actions.forward:
            self.encoded_action = [1,0,0]
            if fwd_cell == None or fwd_cell.can_overlap():
                self.agent_pos = fwd_pos
                self.vel_ob = [1,0]
            if fwd_cell != None and fwd_cell.type == 'goal':
                #done = True
                self.agent_pos = fwd_pos
                self.vel_ob = [1,0]
                reward = self._reward()
            if fwd_cell != None and fwd_cell.type == 'lava':
                done = True
                self.vel_ob = [1,0]
                

        # Pick up an object
        elif action == self.actions.pickup:
            if fwd_cell and fwd_cell.can_pickup():
                if self.carrying is None:
                    self.carrying = fwd_cell
                    self.carrying.cur_pos = np.array([-1, -1])
                    self.grid.set(*fwd_pos, None)

        # Drop an object
        elif action == self.actions.drop:
            if not fwd_cell and self.carrying:
                self.grid.set(*fwd_pos, self.carrying)
                self.carrying.cur_pos = fwd_pos
                self.carrying = None

        # Toggle/activate an object
        elif action == self.actions.toggle:
            if fwd_cell:
                fwd_cell.toggle(self, fwd_pos)
                self.door_open = fwd_pos
                self.step_count -=1 #THIS SHOULD NOT COUNT AS AN ACTION

        # Done action (not used by default)
        elif action == self.actions.done:
            pass

      
        else:
            assert False, "unknown action"
        #If we opened a door, whatever the motion -except passing it-, we close it (consider we can overlap doors)
        if (np.array(self.door_open) != [0,0]).all()  and not ((self.agent_pos == self.door_open).all() or (fwd_pos == self.door_open).all()):
            fwd_door_cell = self.grid.get(*self.door_open)
            if fwd_door_cell is not None:
                fwd_door_cell.toggle(self, self.door_open)
                self.door_open = [0,0]


        if self.max_steps:
            if self.step_count >= self.max_steps:
                done = True

        obs = self.gen_obs()

        return obs, reward, done, {}


    def gen_obs_grid(self):
        """
        Generate the sub-grid observed by the agent.
        This method also outputs a visibility mask telling us which grid
        cells the agent can actually see.
        """

        topX, topY, botX, botY = self.get_view_exts()

        grid = self.grid.slice(topX, topY, self.agent_view_size, self.agent_view_size)

        for i in range(self.agent_dir + 1):
            grid = grid.rotate_left()

        # Process occluders and visibility
        # Note that this incurs some performance cost
        if not self.see_through_walls:
            vis_mask = grid.process_vis(agent_pos=(self.agent_view_size // 2 , self.agent_view_size - 1))
        else:
            vis_mask = np.ones(shape=(grid.width, grid.height), dtype=bool)

        # Make it so the agent sees what it's carrying
        # We do this by placing the carried object at the agent's position
        # in the agent's partially observable view
        agent_pos = grid.width // 2, grid.height - 1
        if self.carrying:
            grid.set(*agent_pos, self.carrying)
        else:
            grid.set(*agent_pos, None)
        
        return grid, vis_mask
    
    
    def gen_obs(self):
        """
        Generate the agent's view (partially observable, low-resolution encoding)
        """

        grid, vis_mask = self.gen_obs_grid()

        # Encode the partially observable view into a numpy array
        image = grid.encode(vis_mask)

        assert hasattr(self, 'mission'), "environments must define a textual mission string"
        
        # Observations are dictionaries containing:
        # - an image (partially observable view of the environment)
        # - the agent's direction/orientation (acting as a compass)
        # - the agent actual motion after the command
        # - a textual mission string (instructions for the agent)
        try:
            if isinstance(self.door_open, np.ndarray):
                door_open = self.door_open.tolist()
            else:
                door_open = self.door_open
        except AttributeError:
                self.door_open = [0,0]
                door_open = self.door_open
        obs = {
            'image': image,
            'direction': self.agent_dir,
            'vel_ob': self.vel_ob,
            'action':self.encoded_action,
            'door_open':door_open,
            'mission': self.mission
        }
        
        return obs
    


    def get_obs_render(self, obs, tile_size=TILE_PIXELS//2):
        """
        Render an agent observation for visualization
        """

        grid, vis_mask = Grid.decode(obs)

        # Render the whole grid
        img = grid.render(
            tile_size,
            agent_pos=(self.agent_view_size // 2, self.agent_view_size - 1),
            agent_dir=3,
            highlight_mask=vis_mask
        )

        return img

    def render(self, mode='human', agent_view= False, close=False, highlight=True, tile_size=TILE_PIXELS):
        """
        Render the whole-grid human view
        """

        if close:
            if self.window:
                self.window.close()
            return

        if mode == 'human' and not self.window:
            import gym_minigrid.window
            self.window = gym_minigrid.window.Window('gym_minigrid')
            self.window.show(block=False)

        # Compute which cells are visible to the agent
        _, vis_mask = self.gen_obs_grid()

        # Compute the world coordinates of the bottom-left corner
        # of the agent's view area
        f_vec = self.dir_vec
        r_vec = self.right_vec
        top_left = self.agent_pos + f_vec * (self.agent_view_size-1) - r_vec * (self.agent_view_size // 2)

        # Mask of which cells to highlight
        highlight_mask = np.zeros(shape=(self.width, self.height), dtype=bool)

        #absolute_pose_agent_view
        abs_ij_agent_view = []
        

        # For each cell in the visibility mask
        for vis_j in range(0, self.agent_view_size):
            for vis_i in range(0, self.agent_view_size):
                

                # Compute the world coordinates of this cell
                abs_i, abs_j = top_left - (f_vec * vis_j) + (r_vec * vis_i)
                if agent_view:
                    abs_ij_agent_view.append([abs_i, abs_j])
                # If this cell is not visible, don't highlight it
                if not vis_mask[vis_i, vis_j]:
                    continue

                if abs_i < 0 or abs_i >= self.width:
                    continue
                if abs_j < 0 or abs_j >= self.height:
                    continue

                # Mark this cell to be highlighted
                highlight_mask[abs_i, abs_j] = True

        # Render the whole grid
        img = self.grid.render(
            tile_size,
            self.agent_pos,
            self.agent_dir,
            highlight_mask=highlight_mask if highlight else None,
            agent_view_poses = abs_ij_agent_view,
            
        )
      
        
        if mode == 'human':
            self.window.set_caption(self.mission)
            self.window.show_img(img)

        return img

    def close(self):
        if self.window:
            self.window.close()
        return

```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/register.py`:

```py
from gym.envs.registration import register as gym_register

env_list = []

def register(
    id,
    entry_point,
    reward_threshold=0.95
):
    assert id.startswith("MiniGrid-")
    assert id not in env_list

    # Register the environment with OpenAI gym
    gym_register(
        id=id,
        entry_point=entry_point,
        reward_threshold=reward_threshold
    )

    # Add the environment to the set
    env_list.append(id)

```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/rendering.py`:

```py
import math
import numpy as np

def downsample(img, factor):
    """
    Downsample an image along both dimensions by some factor
    """

    assert img.shape[0] % factor == 0
    assert img.shape[1] % factor == 0

    img = img.reshape([img.shape[0]//factor, factor, img.shape[1]//factor, factor, 3])
    img = img.mean(axis=3)
    img = img.mean(axis=1)

    return img

def fill_coords(img, fn, color):
    """
    Fill pixels of an image with coordinates matching a filter function
    """

    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            yf = (y + 0.5) / img.shape[0]
            xf = (x + 0.5) / img.shape[1]
            if fn(xf, yf):
                img[y, x] = color

    return img

def rotate_fn(fin, cx, cy, theta):
    def fout(x, y):
        x = x - cx
        y = y - cy

        x2 = cx + x * math.cos(-theta) - y * math.sin(-theta)
        y2 = cy + y * math.cos(-theta) + x * math.sin(-theta)

        return fin(x2, y2)

    return fout

def point_in_line(x0, y0, x1, y1, r):
    p0 = np.array([x0, y0])
    p1 = np.array([x1, y1])
    dir = p1 - p0
    dist = np.linalg.norm(dir)
    dir = dir / dist

    xmin = min(x0, x1) - r
    xmax = max(x0, x1) + r
    ymin = min(y0, y1) - r
    ymax = max(y0, y1) + r

    def fn(x, y):
        # Fast, early escape test
        if x < xmin or x > xmax or y < ymin or y > ymax:
            return False

        q = np.array([x, y])
        pq = q - p0

        # Closest point on line
        a = np.dot(pq, dir)
        a = np.clip(a, 0, dist)
        p = p0 + a * dir

        dist_to_line = np.linalg.norm(q - p)
        return dist_to_line <= r

    return fn

def point_in_circle(cx, cy, r):
    def fn(x, y):
        return (x-cx)*(x-cx) + (y-cy)*(y-cy) <= r * r
    return fn

def point_in_rect(xmin, xmax, ymin, ymax):
    def fn(x, y):
        return x >= xmin and x <= xmax and y >= ymin and y <= ymax
    return fn

def point_in_triangle(a, b, c):
    a = np.array(a)
    b = np.array(b)
    c = np.array(c)

    def fn(x, y):
        v0 = c - a
        v1 = b - a
        v2 = np.array((x, y)) - a

        # Compute dot products
        dot00 = np.dot(v0, v0)
        dot01 = np.dot(v0, v1)
        dot02 = np.dot(v0, v2)
        dot11 = np.dot(v1, v1)
        dot12 = np.dot(v1, v2)

        # Compute barycentric coordinates
        inv_denom = 1 / (dot00 * dot11 - dot01 * dot01)
        u = (dot11 * dot02 - dot01 * dot12) * inv_denom
        v = (dot00 * dot12 - dot01 * dot02) * inv_denom

        # Check if point is in triangle
        return (u >= 0) and (v >= 0) and (u + v) < 1

    return fn

def highlight_img(img, color=(255, 255, 255), alpha=0.30):
    """
    Add highlighting to an image
    """

    blend_img = img + alpha * (np.array(color, dtype=np.uint8) - img)
    blend_img = blend_img.clip(0, 255).astype(np.uint8)
    img[:, :, :] = blend_img

```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/roomgrid.py`:

```py
from .minigrid import *

def reject_next_to(env, pos):
    """
    Function to filter out object positions that are right next to
    the agent's starting point
    """

    sx, sy = env.agent_pos
    x, y = pos
    d = abs(sx - x) + abs(sy - y)
    return d < 2

class Room:
    def __init__(
        self,
        top,
        size
    ):
        # Top-left corner and size (tuples)
        self.top = top
        self.size = size

        # List of door objects and door positions
        # Order of the doors is right, down, left, up
        self.doors = [None] * 4
        self.door_pos = [None] * 4

        # List of rooms adjacent to this one
        # Order of the neighbors is right, down, left, up
        self.neighbors = [None] * 4

        # Indicates if this room is behind a locked door
        self.locked = False

        # List of objects contained
        self.objs = []

    def rand_pos(self, env):
        topX, topY = self.top
        sizeX, sizeY = self.size
        return env._randPos(
            topX + 1, topX + sizeX - 1,
            topY + 1, topY + sizeY - 1
        )

    def pos_inside(self, x, y):
        """
        Check if a position is within the bounds of this room
        """

        topX, topY = self.top
        sizeX, sizeY = self.size

        if x < topX or y < topY:
            return False

        if x >= topX + sizeX or y >= topY + sizeY:
            return False

        return True

class RoomGrid(MiniGridEnv):
    """
    Environment with multiple rooms and random objects.
    This is meant to serve as a base class for other environments.
    """

    def __init__(
        self,
        room_size=7,
        num_rows=3,
        num_cols=3,
        max_steps=100,
        seed=0,
        agent_view_size=7
    ):
        assert room_size > 0
        assert room_size >= 3
        assert num_rows > 0
        assert num_cols > 0
        self.room_size = room_size
        self.num_rows = num_rows
        self.num_cols = num_cols

        height = (room_size - 1) * num_rows + 1
        width = (room_size - 1) * num_cols + 1

        # By default, this environment has no mission
        self.mission = ''

        super().__init__(
            width=width,
            height=height,
            max_steps=max_steps,
            see_through_walls=False,
            seed=seed,
            agent_view_size=agent_view_size
        )

    def room_from_pos(self, x, y):
        """Get the room a given position maps to"""

        assert x >= 0
        assert y >= 0

        i = x // (self.room_size-1)
        j = y // (self.room_size-1)

        assert i < self.num_cols
        assert j < self.num_rows

        return self.room_grid[j][i]

    def get_room(self, i, j):
        assert i < self.num_cols
        assert j < self.num_rows
        return self.room_grid[j][i]

    def _gen_grid(self, width, height):
        # Create the grid
        self.grid = Grid(width, height)

        self.room_grid = []

        # For each row of rooms
        for j in range(0, self.num_rows):
            row = []

            # For each column of rooms
            for i in range(0, self.num_cols):
                room = Room(
                    (i * (self.room_size-1), j * (self.room_size-1)),
                    (self.room_size, self.room_size)
                )
                row.append(room)

                # Generate the walls for this room
                self.grid.wall_rect(*room.top, *room.size)

            self.room_grid.append(row)

        # For each row of rooms
        for j in range(0, self.num_rows):
            # For each column of rooms
            for i in range(0, self.num_cols):
                room = self.room_grid[j][i]

                x_l, y_l = (room.top[0] + 1, room.top[1] + 1)
                x_m, y_m = (room.top[0] + room.size[0] - 1, room.top[1] + room.size[1] - 1)

                # Door positions, order is right, down, left, up
                if i < self.num_cols - 1:
                    room.neighbors[0] = self.room_grid[j][i+1]
                    room.door_pos[0] = (x_m, self._rand_int(y_l, y_m))
                if j < self.num_rows - 1:
                    room.neighbors[1] = self.room_grid[j+1][i]
                    room.door_pos[1] = (self._rand_int(x_l, x_m), y_m)
                if i > 0:
                    room.neighbors[2] = self.room_grid[j][i-1]
                    room.door_pos[2] = room.neighbors[2].door_pos[0]
                if j > 0:
                    room.neighbors[3] = self.room_grid[j-1][i]
                    room.door_pos[3] = room.neighbors[3].door_pos[1]

        # The agent starts in the middle, facing right
        self.agent_pos = (
            (self.num_cols // 2) * (self.room_size-1) + (self.room_size // 2),
            (self.num_rows // 2) * (self.room_size-1) + (self.room_size // 2)
        )
        self.agent_dir = 0

    def place_in_room(self, i, j, obj):
        """
        Add an existing object to room (i, j)
        """

        room = self.get_room(i, j)

        pos = self.place_obj(
            obj,
            room.top,
            room.size,
            reject_fn=reject_next_to,
            max_tries=1000
        )

        room.objs.append(obj)

        return obj, pos

    def add_object(self, i, j, kind=None, color=None):
        """
        Add a new object to room (i, j)
        """

        if kind == None:
            kind = self._rand_elem(['key', 'ball', 'box'])

        if color == None:
            color = self._rand_color()

        # TODO: we probably want to add an Object.make helper function
        assert kind in ['key', 'ball', 'box']
        if kind == 'key':
            obj = Key(color)
        elif kind == 'ball':
            obj = Ball(color)
        elif kind == 'box':
            obj = Box(color)

        return self.place_in_room(i, j, obj)

    def add_door(self, i, j, door_idx=None, color=None, locked=None):
        """
        Add a door to a room, connecting it to a neighbor
        """

        room = self.get_room(i, j)

        if door_idx == None:
            # Need to make sure that there is a neighbor along this wall
            # and that there is not already a door
            while True:
                door_idx = self._rand_int(0, 4)
                if room.neighbors[door_idx] and room.doors[door_idx] is None:
                    break

        if color == None:
            color = self._rand_color()

        if locked is None:
            locked = self._rand_bool()

        assert room.doors[door_idx] is None, "door already exists"

        room.locked = locked
        door = Door(color, is_locked=locked)

        pos = room.door_pos[door_idx]
        self.grid.set(*pos, door)
        door.cur_pos = pos

        neighbor = room.neighbors[door_idx]
        room.doors[door_idx] = door
        neighbor.doors[(door_idx+2) % 4] = door

        return door, pos

    def remove_wall(self, i, j, wall_idx):
        """
        Remove a wall between two rooms
        """

        room = self.get_room(i, j)

        assert wall_idx >= 0 and wall_idx < 4
        assert room.doors[wall_idx] is None, "door exists on this wall"
        assert room.neighbors[wall_idx], "invalid wall"

        neighbor = room.neighbors[wall_idx]

        tx, ty = room.top
        w, h = room.size

        # Ordering of walls is right, down, left, up
        if wall_idx == 0:
            for i in range(1, h - 1):
                self.grid.set(tx + w - 1, ty + i, None)
        elif wall_idx == 1:
            for i in range(1, w - 1):
                self.grid.set(tx + i, ty + h - 1, None)
        elif wall_idx == 2:
            for i in range(1, h - 1):
                self.grid.set(tx, ty + i, None)
        elif wall_idx == 3:
            for i in range(1, w - 1):
                self.grid.set(tx + i, ty, None)
        else:
            assert False, "invalid wall index"

        # Mark the rooms as connected
        room.doors[wall_idx] = True
        neighbor.doors[(wall_idx+2) % 4] = True

    def place_agent(self, i=None, j=None, rand_dir=True):
        """
        Place the agent in a room
        """

        if i == None:
            i = self._rand_int(0, self.num_cols)
        if j == None:
            j = self._rand_int(0, self.num_rows)

        room = self.room_grid[j][i]

        # Find a position that is not right in front of an object
        while True:
            super().place_agent(room.top, room.size, rand_dir, max_tries=1000)
            front_cell = self.grid.get(*self.front_pos)
            if front_cell is None or front_cell.type == 'wall':
                break

        return self.agent_pos

    def connect_all(self, door_colors=COLOR_NAMES, max_itrs=5000):
        """
        Make sure that all rooms are reachable by the agent from its
        starting position
        """

        start_room = self.room_from_pos(*self.agent_pos)

        added_doors = []

        def find_reach():
            reach = set()
            stack = [start_room]
            while len(stack) > 0:
                room = stack.pop()
                if room in reach:
                    continue
                reach.add(room)
                for i in range(0, 4):
                    if room.doors[i]:
                        stack.append(room.neighbors[i])
            return reach

        num_itrs = 0

        while True:
            # This is to handle rare situations where random sampling produces
            # a level that cannot be connected, producing in an infinite loop
            if num_itrs > max_itrs:
                raise RecursionError('connect_all failed')
            num_itrs += 1

            # If all rooms are reachable, stop
            reach = find_reach()
            if len(reach) == self.num_rows * self.num_cols:
                break

            # Pick a random room and door position
            i = self._rand_int(0, self.num_cols)
            j = self._rand_int(0, self.num_rows)
            k = self._rand_int(0, 4)
            room = self.get_room(i, j)

            # If there is already a door there, skip
            if not room.door_pos[k] or room.doors[k]:
                continue

            if room.locked or room.neighbors[k].locked:
                continue

            color = self._rand_elem(door_colors)
            door, _ = self.add_door(i, j, k, color, False)
            added_doors.append(door)

        return added_doors

    def add_distractors(self, i=None, j=None, num_distractors=10, all_unique=True):
        """
        Add random objects that can potentially distract/confuse the agent.
        """

        # Collect a list of existing objects
        objs = []
        for row in self.room_grid:
            for room in row:
                for obj in room.objs:
                    objs.append((obj.type, obj.color))

        # List of distractors added
        dists = []

        while len(dists) < num_distractors:
            color = self._rand_elem(COLOR_NAMES)
            type = self._rand_elem(['key', 'ball', 'box'])
            obj = (type, color)

            if all_unique and obj in objs:
                continue

            # Add the object to a random room if no room specified
            room_i = i
            room_j = j
            if room_i == None:
                room_i = self._rand_int(0, self.num_cols)
            if room_j == None:
                room_j = self._rand_int(0, self.num_rows)

            dist, pos = self.add_object(room_i, room_j, *obj)

            objs.append(obj)
            dists.append(dist)

        return dists

```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/window.py`:

```py
import sys
import numpy as np

# Only ask users to install matplotlib if they actually need it
try:
    import matplotlib.pyplot as plt
except:
    print('To display the environment in a window, please install matplotlib, eg:')
    print('pip3 install --user matplotlib')
    sys.exit(-1)

class Window:
    """
    Window to draw a gridworld instance using Matplotlib
    """

    def __init__(self, title):
        self.fig = None

        self.imshow_obj = None

        # Create the figure and axes
        self.fig =plt.figure()
        self.ax = plt.subplot()

        # Show the env name in the window title
        #self.fig.canvas.set_window_title(title)

        # Turn off x/y axis numbering/ticks
        self.ax.xaxis.set_ticks_position('none')
        self.ax.yaxis.set_ticks_position('none')
        _ = self.ax.set_xticklabels([])
        _ = self.ax.set_yticklabels([])

        # Flag indicating the window was closed
        self.closed = False

        def close_handler(evt):
            self.closed = True

        self.fig.canvas.mpl_connect('close_event', close_handler)

    def show_img(self, img, pause = None):
        """
        Show an image or update the image being shown
        """

        # If no image has been shown yet,
        # show the first image of the environment
        if self.imshow_obj is None:
            self.imshow_obj = self.ax.imshow(img, interpolation='bilinear')

        # Update the image data
        self.imshow_obj.set_data(img)

        # Request the window be redrawn
        self.fig.canvas.draw_idle()
        self.fig.canvas.flush_events()

        # Let matplotlib process UI events
        if pause:
            plt.pause(pause)
        else:
            plt.pause(0.001)


    def set_caption(self, text):
        """
        Set/update the caption text below the image
        """

        plt.xlabel(text)

    def reg_key_handler(self, key_handler):
        """
        Register a keyboard event handler
        """

        # Keyboard handler
        self.fig.canvas.mpl_connect('key_press_event', key_handler)

    def show(self, block=True):
        """
        Show the window, and start an event loop
        """

        # If not blocking, trigger interactive mode
        if not block:
            plt.ion()

        # Show the plot
        # In non-interative mode, this enters the matplotlib event loop
        # In interactive mode, this call does not block
        plt.show()

    def close(self):
        """
        Close the window
        """

        plt.close()
        self.closed = True

```

`hierarchical-nav/gym-minigrid_minimal-1/gym_minigrid/wrappers.py`:

```py
import math
import operator
from functools import reduce

import numpy as np
import gym
from gym import error, spaces, utils
from .minigrid import OBJECT_TO_IDX, COLOR_TO_IDX, STATE_TO_IDX, Goal

class ReseedWrapper(gym.core.Wrapper):
    """
    Wrapper to always regenerate an environment with the same set of seeds.
    This can be used to force an environment to always keep the same
    configuration when reset.
    """

    def __init__(self, env, seeds=[0], seed_idx=0):
        self.seeds = list(seeds)
        self.seed_idx = seed_idx
        super().__init__(env)

    def reset(self, **kwargs):
        seed = self.seeds[self.seed_idx]
        self.seed_idx = (self.seed_idx + 1) % len(self.seeds)
        self.env.seed(seed)
        return self.env.reset(**kwargs)

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        return obs, reward, done, info

class ActionBonus(gym.core.Wrapper):
    """
    Wrapper which adds an exploration bonus.
    This is a reward to encourage exploration of less
    visited (state,action) pairs.
    """

    def __init__(self, env):
        super().__init__(env)
        self.counts = {}

    def step(self, action):
        obs, reward, done, info = self.env.step(action)

        env = self.unwrapped
        tup = (tuple(env.agent_pos), env.agent_dir, action)

        # Get the count for this (s,a) pair
        pre_count = 0
        if tup in self.counts:
            pre_count = self.counts[tup]

        # Update the count for this (s,a) pair
        new_count = pre_count + 1
        self.counts[tup] = new_count

        bonus = 1 / math.sqrt(new_count)
        reward += bonus

        return obs, reward, done, info

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

class StateBonus(gym.core.Wrapper):
    """
    Adds an exploration bonus based on which positions
    are visited on the grid.
    """

    def __init__(self, env):
        super().__init__(env)
        self.counts = {}

    def step(self, action):
        obs, reward, done, info = self.env.step(action)

        # Tuple based on which we index the counts
        # We use the position after an update
        env = self.unwrapped
        tup = (tuple(env.agent_pos))

        # Get the count for this key
        pre_count = 0
        if tup in self.counts:
            pre_count = self.counts[tup]

        # Update the count for this key
        new_count = pre_count + 1
        self.counts[tup] = new_count

        bonus = 1 / math.sqrt(new_count)
        reward += bonus

        return obs, reward, done, info

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

class ImgObsWrapper(gym.core.ObservationWrapper):
    """
    Use the image as the only observation output, no language/mission.
    """

    def __init__(self, env):
        super().__init__(env)
        self.observation_space = env.observation_space.spaces['image']

    def observation(self, obs):
        return obs['image']

class ImgActionObsWrapper(gym.core.ObservationWrapper):
    """
    Use the image abd the effectued action as observation output, no language/mission.
    """

    def __init__(self, env):
        super().__init__(env)
        self.observation_space = env.observation_space.spaces['image']

    def observation(self, obs):
        if 'mission' in obs:
            del obs['mission']
        return obs

class OneHotPartialObsWrapper(gym.core.ObservationWrapper):
    """
    Wrapper to get a one-hot encoding of a partially observable
    agent view as observation.
    """

    def __init__(self, env, tile_size=8):
        super().__init__(env)

        self.tile_size = tile_size

        obs_shape = env.observation_space['image'].shape

        # Number of bits per cell
        num_bits = len(OBJECT_TO_IDX) + len(COLOR_TO_IDX) + len(STATE_TO_IDX)

        self.observation_space.spaces["image"] = spaces.Box(
            low=0,
            high=255,
            shape=(obs_shape[0], obs_shape[1], num_bits),
            dtype='uint8'
        )

    def observation(self, obs):
        img = obs['image']
        out = np.zeros(self.observation_space.spaces['image'].shape, dtype='uint8')

        for i in range(img.shape[0]):
            for j in range(img.shape[1]):
                type = img[i, j, 0]
                color = img[i, j, 1]
                state = img[i, j, 2]

                out[i, j, type] = 1
                out[i, j, len(OBJECT_TO_IDX) + color] = 1
                out[i, j, len(OBJECT_TO_IDX) + len(COLOR_TO_IDX) + state] = 1

        return {
            'action': obs['action'],
            'vel_ob': obs['vel_ob'],
            'image': out
        }

class RGBImgObsWrapper(gym.core.ObservationWrapper):
    """
    Wrapper to use fully observable RGB image as observation,
    This can be used to have the agent to solve the gridworld in pixel space.
    """

    def __init__(self, env, tile_size=8):
        super().__init__(env)

        self.tile_size = tile_size

        self.observation_space.spaces['image'] = spaces.Box(
            low=0,
            high=255,
            shape=(self.env.width * tile_size, self.env.height * tile_size, 3),
            dtype='uint8'
        )

    def observation(self, obs):
        env = self.unwrapped

        rgb_img = env.render(
            mode='rgb_array',
            highlight=False,
            tile_size=self.tile_size
        )

        return {
            'mission': obs['mission'],
            'image': rgb_img
        }


class RGBImgPartialObsWrapper(gym.core.ObservationWrapper):
    """
    Wrapper to use partially observable RGB image as observation.
    This can be used to have the agent to solve the gridworld in pixel space.
    """

    def __init__(self, env, tile_size=8):
        super().__init__(env)

        self.tile_size = tile_size

        obs_shape = env.observation_space.spaces['image'].shape
        self.observation_space.spaces['image'] = spaces.Box(
            low=0,
            high=255,
            shape=(obs_shape[0] * tile_size, obs_shape[1] * tile_size, 3),
            dtype='uint8'
        )

    def observation(self, obs):
        env = self.unwrapped

        rgb_img_partial = env.get_obs_render(
            obs['image'],
            tile_size=self.tile_size
        )
        obs['image'] = rgb_img_partial
        return obs

class FullyObsWrapper(gym.core.ObservationWrapper):
    """
    Fully observable gridworld using a compact grid encoding
    """

    def __init__(self, env):
        super().__init__(env)

        self.observation_space.spaces["image"] = spaces.Box(
            low=0,
            high=255,
            shape=(self.env.width, self.env.height, 3),  # number of cells
            dtype='uint8'
        )

    def observation(self, obs):
        env = self.unwrapped
        full_grid = env.grid.encode()
        full_grid[env.agent_pos[0]][env.agent_pos[1]] = np.array([
            OBJECT_TO_IDX['agent'],
            COLOR_TO_IDX['red'],
            env.agent_dir
        ])

        return {
            'mission': obs['mission'],
            'image': full_grid
        }

class FlatObsWrapper(gym.core.ObservationWrapper):
    """
    Encode mission strings using a one-hot scheme,
    and combine these with observed images into one flat array
    """

    def __init__(self, env, maxStrLen=96):
        super().__init__(env)

        self.maxStrLen = maxStrLen
        self.numCharCodes = 27

        imgSpace = env.observation_space.spaces['image']
        imgSize = reduce(operator.mul, imgSpace.shape, 1)

        self.observation_space = spaces.Box(
            low=0,
            high=255,
            shape=(imgSize + self.numCharCodes * self.maxStrLen,),
            dtype='uint8'
        )

        self.cachedStr = None
        self.cachedArray = None

    def observation(self, obs):
        image = obs['image']
        mission = obs['mission']

        # Cache the last-encoded mission string
        if mission != self.cachedStr:
            assert len(mission) <= self.maxStrLen, 'mission string too long ({} chars)'.format(len(mission))
            mission = mission.lower()

            strArray = np.zeros(shape=(self.maxStrLen, self.numCharCodes), dtype='float32')

            for idx, ch in enumerate(mission):
                if ch >= 'a' and ch <= 'z':
                    chNo = ord(ch) - ord('a')
                elif ch == ' ':
                    chNo = ord('z') - ord('a') + 1
                assert chNo < self.numCharCodes, '%s : %d' % (ch, chNo)
                strArray[idx, chNo] = 1

            self.cachedStr = mission
            self.cachedArray = strArray

        obs = np.concatenate((image.flatten(), self.cachedArray.flatten()))

        return obs

class ViewSizeWrapper(gym.core.Wrapper):
    """
    Wrapper to customize the agent field of view size.
    This cannot be used with fully observable wrappers.
    """

    def __init__(self, env, agent_view_size=7):
        super().__init__(env)

        assert agent_view_size % 2 == 1
        assert agent_view_size >= 3

        # Override default view size
        env.unwrapped.agent_view_size = agent_view_size

        # Compute observation space with specified view size
        observation_space = gym.spaces.Box(
            low=0,
            high=255,
            shape=(agent_view_size, agent_view_size, 3),
            dtype='uint8'
        )

        # Override the environment's observation space
        self.observation_space = spaces.Dict({
            'image': observation_space
        })

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

    def step(self, action):
        return self.env.step(action)

class DirectionObsWrapper(gym.core.ObservationWrapper):
    """
    Provides the slope/angular direction to the goal with the observations as modeled by (y2 - y2 )/( x2 - x1)
    type = {slope , angle}
    """
    def __init__(self, env,type='slope'):
        super().__init__(env)
        self.goal_position = None
        self.type = type

    def reset(self):
        obs = self.env.reset()
        if not self.goal_position:
            self.goal_position = [x for x,y in enumerate(self.grid.grid) if isinstance(y,(Goal) ) ]
            if len(self.goal_position) >= 1: # in case there are multiple goals , needs to be handled for other env types
                self.goal_position = (int(self.goal_position[0]/self.height) , self.goal_position[0]%self.width)
        return obs

    def observation(self, obs):
        slope = np.divide( self.goal_position[1] - self.agent_pos[1] ,  self.goal_position[0] - self.agent_pos[0])
        obs['goal_direction'] = np.arctan( slope ) if self.type == 'angle' else slope
        return obs

class SymbolicObsWrapper(gym.core.ObservationWrapper):
    """
    Fully observable grid with a symbolic state representation.
    The symbol is a triple of (X, Y, IDX), where X and Y are
    the coordinates on the grid, and IDX is the id of the object.
    """

    def __init__(self, env):
        super().__init__(env)

        self.observation_space.spaces["image"] = spaces.Box(
            low=0,
            high=max(OBJECT_TO_IDX.values()),
            shape=(self.env.width, self.env.height, 3),  # number of cells
            dtype="uint8",
        )

    def observation(self, obs):
        objects = np.array(
            [OBJECT_TO_IDX[o.type] if o is not None else -1 for o in self.grid.grid]
        )
        w, h = self.width, self.height
        grid = np.mgrid[:w, :h]
        grid = np.concatenate([grid, objects.reshape(1, w, h)])
        grid = np.transpose(grid, (1, 2, 0))
        obs['image'] = grid
        return obs

```

`hierarchical-nav/gym-minigrid_minimal-1/manual_control.py`:

```py
#!/usr/bin/env python3

import os
import argparse
import numpy as np
import h5py
import gym
import gym_minigrid
from gym_minigrid.wrappers import *
from gym_minigrid.window import Window
from datetime import datetime

def redraw(img):
    #if not args.agent_view:
    img = env.render('rgb_array', tile_size=args.tile_size)
    window.show_img(img)

def reset():
    if args.seed != -1:
        env.seed(args.seed)

    obs = env.reset()
   
    if hasattr(env, 'mission'):
        print('Mission: %s' % env.mission)
        window.set_caption(env.mission)

    redraw(obs)

def step(action):
    print(action)
    obs, reward, done, info = env.step(action)
    print('step=%s' % (env.step_count))
    # print(obs) # action 6dof + image rgb 56,56,3
    # print(obs['image'].shape)
    data_dictionary['image'].append(obs['image'])
    data_dictionary['action'].append(obs['action'])
    data_dictionary['vel_ob'].append(obs['vel_ob'])
    #TODO: save the elements in a csv file or .h5 doc

    if done:
        print('done!')
        reset()
    else:
        redraw(obs)

def key_handler(event):
    print('pressed', event.key)

    if event.key == 'escape':
        window.close()
        return

    if event.key == 'backspace':
        reset()
        return

    if event.key == 'left':
        step(env.actions.left)
        return
    if event.key == 'right':
        step(env.actions.right)
        return
    if event.key == 'up':
        step(env.actions.forward)
        return

    # Spacebar
    if event.key == ' ':
        step(env.actions.toggle)
        return
    if event.key == 'pageup':
        step(env.actions.pickup)
        return
    if event.key == 'pagedown':
        step(env.actions.drop)
        return

    if event.key == 'enter':
        step(env.actions.done)
        return

parser = argparse.ArgumentParser()
parser.add_argument(
    "--env",
    help="gym environment to load",
    default='MiniGrid-MultiRoom-N6-v0'
)
parser.add_argument(
    "--seed",
    type=int,
    help="random seed to generate the environment with",
    default=-1
)
parser.add_argument(
    "--tile_size",
    type=int,
    help="size at which to render tiles",
    default=32
)
parser.add_argument(
    '--agent_view',
    default=True,
    help="draw the agent sees (partially observable view)",
    action='store_true'
)
try:
    data_dictionary = {'image':[],'vel_ob':[],'action':[]}
    now = datetime.now()
    current_time = now.strftime("%H-%M-%d-%m-%y")

    args = parser.parse_args()
    
    env = gym.make(args.env)

    if args.agent_view:
        env = RGBImgPartialObsWrapper(env)
        env = ImgActionObsWrapper(env)
    # else:
    #     env = OneHotPartialObsWrapper(env)    

    window = Window('gym_minigrid - ' + args.env)
    window.reg_key_handler(key_handler)

    reset()

    # Blocking event loop
    window.show(block=True)
finally:
    cwd = os.getcwd()
    if not os.path.exists(cwd + '/data'):
        os.makedirs(os.path.join(cwd, 'data'), exist_ok=True)
        
    h5file = h5py.File(cwd +'/data/data'+ current_time+ '.h5', 'w')
    for key in data_dictionary.keys():
        h5file.create_dataset(
                key, data=data_dictionary[key], compression='lzf')
        print(h5file.get(key))
    h5file.close()
    print("Saving data as: data" + current_time + '.h5 in data folder' )

```

`hierarchical-nav/gym-minigrid_minimal-1/setup.py`:

```py
from setuptools import setup

setup(
    name='gym_minigrid',
    version='1.0.2',
    keywords='memory, environment, agent, rl, openaigym, openai-gym, gym',
    url='https://github.com/maximecb/gym-minigrid',
    description='Minimalistic gridworld package for OpenAI Gym, reduced to one custom environment',
    packages=['gym_minigrid', 'gym_minigrid.envs'],
    install_requires=[
        'gym==0.17.0',
        'numpy>=1.21.6'
    ]
)

```

`hierarchical-nav/main.py`:

```py
#! /usr/bin/env python3
import json
import yaml
from pathlib import Path
import importlib
import os
import pipes
import sys
import datetime
import shutil
import glob 
import logging

from dommel_library.datastructs.dict import Dict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


"""
Main.py
-------
Entry point for training and evaluating experiments with scripts. This script
will check for the DISPLAY variable and switch between matplotlib backends
if necessary. It will also parse command line arguments of the type
`<key>=<value>` to a config dictionary ready to be merged in a config object.
Missing arguments will be filled in with defaults.
Note if the `config` argument is provided all defaults will be read from
a config yaml file.
"""
def get_model_parameters(log_dir, epoch=None):
    #check if we inputed a param file
    dir, ext = os.path.splitext(log_dir)
    if ext == '.pt':
        return log_dir

    model_dir = os.path.join(log_dir, "models")
    if epoch is None:
        model_files = glob.glob(os.path.join(model_dir, "*.pt"))
        model = max(model_files, key=os.path.getctime)
    else:
        model = os.path.join(model_dir, "model-{:04d}.pt".format(epoch))
    return model


def run_experiment(mode, experiment_file, args):
    logger.info("Run experiment %s in %s mode", experiment_file, mode)
    experiment_module, ext = os.path.splitext(experiment_file)
    experiment_name = os.path.basename(experiment_module)
    if ext == ".py":
        # path to .py file given
        config_file = experiment_file[:-3] + ".yml"
    elif ext == "":
        if experiment_name == "":
            # path to directory with trailing /
            experiment_name = os.path.basename(experiment_module[0:-1])
            experiment_module = os.path.join(experiment_module,
                                             experiment_name)
        else:
            # path to directory without trailing /
            experiment_module = os.path.join(experiment_module,
                                             experiment_name)
        experiment_file = experiment_module + ".py"
        config_file = experiment_module + ".yml"
    else:
        print("Invalid mode (", mode, ") or experiment (" + experiment_file + ")")
        print('Usage: ./main.py [run|evaluate] [experiment_file] [args]')
        exit(-1)

    # parse args that are key-value pairs
    overrides = Dict({})
    if args is not None:
        for arg in args:
            k, v = arg.split("=")

            # override config file
            if k == "config":
                if os.path.exists(v):
                    config_file = v
                else:
                    _, ext = os.path.splitext(v)
                    if ext != ".yml":
                        v = v + ".yml"
                    f = os.path.join(os.path.dirname(experiment_file), v)
                    if os.path.exists(f):
                        config_file = f
                    else:
                        raise Exception("Invalid configuration file: " + v)

                overrides["config"] = config_file
                continue
  

            # keys might be hierarchical with . notation
            overrides_dict = overrides
            keys = k.split(".")
            while len(keys) > 1:
                key = keys.pop(0)
                if key not in overrides_dict.keys():
                    overrides_dict[key] = {}
                overrides_dict = overrides_dict[key]
            k = keys[0]

            # check for json
            if v.startswith("{") or v.startswith("["):
                v = json.loads(v)

            # check for booleans
            elif v in ["True", "true", "False", "false"]:
                v = v in ["True", "true"]

            # check for None
            elif v == "None":
                v = None

            # check for numbers
            else:
                try:
                    v = int(v)
                except ValueError:
                    try:
                        v = float(v)
                    except ValueError:
                        pass

            overrides_dict[k] = v
    
    
    
    # set the log_dir for this experiment
    run_id = overrides.get("id", datetime.datetime.now().isoformat())
    run_id = run_id.replace(":", "")
    config_file_name = os.path.basename(config_file)
    config_id, _ = os.path.splitext(config_file_name)
    experiment_dir = overrides.get("experiment_dir",
                                os.path.join('runs', experiment_name))
    overrides["experiment_dir"] = experiment_dir
    log_dir = os.path.join(experiment_dir, config_id, run_id)
    overrides["log_dir"] = log_dir

    # create default paths
    if mode == 'run':
        # check if we are resuming, if not make sure we have a new log_dir
        resume = overrides.get(
            "resume", ("start_epoch" in overrides.keys()))
        if resume:
            # load the config file from the resuming experiment
            config_file = os.path.join(log_dir, experiment_name + ".yml")

            # if start_epoch not set, continue from last checkpoint
            if "start_epoch" not in overrides.keys():
                last_model = get_model_parameters(log_dir)
                last_epoch = int(last_model.split(".")[-2].split("-")[-1])
                overrides["start_epoch"] = last_epoch
        else:
            # don't overwrite existing experiment logs
            if os.path.exists(log_dir):
                count = 1
                log_dir = os.path.join(experiment_dir, config_id,
                                       run_id + '_' + str(count))

                while os.path.exists(log_dir):
                    count += 1
                    log_dir = os.path.join(experiment_dir, config_id,
                                           run_id + '_' + str(count))

                run_id + '_' + str(count)
                overrides["log_dir"] = log_dir

            overrides["start_epoch"] = 0
            os.makedirs(experiment_dir, exist_ok=True)
            os.makedirs(log_dir)

            src_destination_path = os.path.join(log_dir, 'src')
            src_source_path = os.path.dirname(os.path.abspath(__file__))

            def ignore_files(d, files):
                return set(f for f in files
                           if f in ["runs", ".git"] or  # ignore these dirs
                           (not f.endswith(".py") and  # and only include .py
                            not os.path.isdir(os.path.join(d, f))))
            try:
                shutil.copytree(
                    src_source_path,
                    src_destination_path,
                    ignore=ignore_files
                )  # ignore runs and git files
            except OSError:
                logger.error(
                    "Warning: failed to copy source files into the log path")

            # record the executed command
            arg_string = ' '.join(pipes.quote(a) for a in args)
            cmd = './main.py run' + experiment_file + ' ' + arg_string
            try:
                print(cmd, file=open(os.path.join(log_dir, 'cmd'), 'w'))
            except OSError:
                logger.error(
                    "Warning: failed to log the command into the log path")

    else:
        # use the config file from the previous experiment if id specified
        if "id" in overrides.keys():
            config_file = os.path.join(log_dir, experiment_name + ".yml")
    # read config_file
    try:
        logger.info("Read config %s", config_file)
        config = Dict(yaml.load(open(config_file, "r"),
                                Loader=yaml.FullLoader))
        
    except OSError as e:
        logger.error(
            "Error: configuration file %s not available", config_file)
        config = Dict({})
        
    if 'log_dir' in config:
        overrides["log_dir"] = config['log_dir']
    if 'experiment_dir' in config:
        overrides["experiment_dir"] = config['experiment_dir']
   
    # override with CLI args
    config.update(overrides)

    logger.info("Experiment ID " + overrides["experiment_dir"])
    logger.info("Log directory " + overrides["log_dir"])

    # dump the overriden config yaml
    if mode != "evaluate":
        
        try:
            config_destination_path = os.path.join(log_dir,
                                                   experiment_name + ".yml")
            count = 1
            while os.path.exists(config_destination_path):
                config_destination_path = os.path.join(log_dir,
                                                       experiment_name + "_" + str(count) + ".yml")
                count += 1

            yaml.dump(config.dict(), open(config_destination_path, "w"),
                      default_flow_style=False)
        except OSError:
            logger.error(
                "Warning: failed to write the config into the log path")
    # load module and run
    experiment_module = experiment_module.replace('/', '.')
    experiment_module = experiment_module.replace("\\", ".")
    if experiment_module[0:1] == "..":
        del experiment_module[0]
    experiment = importlib.import_module(experiment_module)
    
    func = None
    try:
        func = getattr(experiment, mode)
    except AttributeError:
        logger.error("Unsupported mode: %s", mode)

    if func is not None:
        func(config)


def main(args):
    if len(args) < 3:
        print('Usage: ./main.py [run|evaluate] [experiment_file] [args]')
        exit(-1)
    else:
        run_experiment(args[1], args[2], args[3:])


if __name__ == "__main__":
    try:
        main(sys.argv)
    except:
        logger.info("Exiting experiment due to exception", exc_info=True)
        raise
    finally:
        logger.info("Experiment done.")

        # flush the logs now, just to be sure all output is seen
        logging.shutdown()

        # Flush stdout and stderr
        sys.stdout.flush()
        sys.stderr.flush()

```

`hierarchical-nav/navigation_model/HierarchicalHMMBayesianObserver.py`:

```py
import numpy as np
from collections import defaultdict, deque
from typing import Dict, List, Tuple, Optional
import time, math
from scipy import stats
import time
import random
import numpy as np
from nltk import PCFG
import math
import heapq
import torch
from scipy.spatial.distance import euclidean
import warnings
warnings.filterwarnings('ignore', category=RuntimeWarning)
from collections import namedtuple
from nltk.parse.generate import generate
State = namedtuple("State", ["x","y","d"])
DIR_VECS = [(1,0),(0,1),(-1,0),(0,-1)]
ACTIONS   = ["forward","left","right"]

class HierarchicalHMMBayesianObserver:
    """
    A single HMM whose hidden states are (mode, submode) pairs,
    fused with Bayesian Online Changepoint Detection (BOCPD).
    """
    def __init__(self):
        # ── 1) Define your two‐level hierarchy ─────────────────────
        self.mode_submodes = {
            'EXPLORE':        ['ego_allo','ego_allo_lookahead','short_term_memory','astar_directed'],
            'NAVIGATE':       ['distant_node','unvisited_priority','plan_following'],
            'RECOVER':        ['solve_doubt','backtrack_safe'],
            'TASK_SOLVING':   ['goal_directed','systematic_search','task_completion']
        }
        # flatten into atomic states
        self.hierarchical_states = []
        self.state_to_idx = {}
        self.idx_to_state = {}
        idx = 0
        for mode,subs in self.mode_submodes.items():
            for sub in subs:
                self.hierarchical_states.append((mode,sub))
                self.state_to_idx[(mode,sub)] = idx
                self.idx_to_state[idx] = (mode,sub)
                idx += 1
        self.n_states = len(self.hierarchical_states)

        # ── 2) BOCPD + evidence as before ────────────────────────
        self.last_poses            = deque(maxlen=100)
        self.replay_buffer         = deque(maxlen=80)
        self.new_nodes_created     = deque(maxlen=50)
        self.plan_progress_history = deque(maxlen=30)
        self.last_significant_progress = time.time()
        self.stagnation_counter    = 0
        self.exploration_attempts  = 0
        self.navigation_attempts   = 0
        self.recovery_attempts     = 0

        self.params = {
            'loop_threshold':20,
            'stagnation_time_threshold':30.0,
            'stagnation_step_threshold':50,
            'max_exploration_cycles':3,
            'max_navigation_cycles':2,
            'max_recovery_cycles':2
        }

        self.max_run_length       = 50
        self.run_length_dist      = np.zeros(self.max_run_length+1)
        self.run_length_dist[0]   = 1.0
        self.hazard_rate          = 1.0/25.0
        self.changepoint_threshold= 0.1

        self.evidence_buffer      = deque(maxlen=100)
        self.evidence_history     = deque(maxlen=self.max_run_length)
        self.mode_history         = deque(maxlen=100)

        self.joint_states = self.hierarchical_states
        self.state_beliefs = np.ones(self.n_states) / self.n_states

        # ── 3) Build your hierarchical HMM parameters ────────────
        self.transition_matrix = self._build_hierarchical_transition_matrix()
        self.emission_params   = self._build_hierarchical_emission_params()

        # beliefs now over the joint states
        self.state_beliefs = np.ones(self.n_states)/self.n_states
        self.modes = list(self.mode_submodes.keys())

    # ──────────────────────────────────────────
    #  Transition matrix builder (TrueHHMM style)
    # ──────────────────────────────────────────
    def _build_hierarchical_transition_matrix(self) -> np.ndarray:
        T = np.zeros((self.n_states, self.n_states))
        for i, (from_m, from_s) in enumerate(self.hierarchical_states):
            row = np.zeros(self.n_states)
            for j, (to_m, to_s) in enumerate(self.hierarchical_states):
                if from_m == to_m:
                    if from_s == to_s:
                        # self‐persistence
                        row[j] = self._get_submode_persistence(from_m, from_s)
                    else:
                        # switch submode within same mode
                        row[j] = self._get_intra_mode_transition(from_m, from_s, to_s)
                else:
                    # jump mode
                    row[j] = self._get_inter_mode_transition(from_m, from_s, to_m, to_s)
            
            # Ensure proper normalization
            row_sum = row.sum()
            if row_sum > 1e-10:
                T[i, :] = row / row_sum
            else:
                # Fallback to uniform if all probabilities are zero
                T[i, :] = 1.0 / self.n_states
                
            # Double-check normalization (should sum to 1.0)
            assert abs(T[i, :].sum() - 1.0) < 1e-6, f"Row {i} doesn't sum to 1.0: {T[i, :].sum()}"
        
        return T


    def get_diagnostics(self) -> Dict:
        """
        Return diagnostic information about the HMM observer state.
        """
        current_mode_probs = self.get_mode_probabilities()
        dominant_mode = max(current_mode_probs.items(), key=lambda x: x[1])
        
        return {
            'current_mode_probabilities': current_mode_probs,
            'dominant_mode': dominant_mode[0],
            'dominant_confidence': dominant_mode[1],
            'current_run_length_dist': self.run_length_dist.copy(),
            'max_run_length_prob': np.max(self.run_length_dist),
            'evidence_buffer_size': len(self.evidence_buffer),
            'evidence_history_size': len(self.evidence_history),
            'buffer_stats': {
                'poses_tracked': len(self.last_poses),
                'replay_buffer_size': len(self.replay_buffer),
                'new_nodes_buffer': len(self.new_nodes_created),
                'plan_progress_history': len(self.plan_progress_history)
            },
            'state_beliefs': self.state_beliefs.copy(),
            'n_states': self.n_states
        }
    def _get_submode_persistence(self, mode:str, sub:str)->float:
        base = 0.7
        # tweak per submode
        if mode=='EXPLORE' and sub=='astar_directed': return 0.8
        if mode=='NAVIGATE' and sub=='plan_following': return 0.85
        if mode=='RECOVER': return 0.5
        if mode=='TASK_SOLVING': return 0.9
        return base
    def _compute_prior_likelihood(self, evidence: Dict) -> float:
        """
        Uniform prior over *all* joint (mode,submode) states.
        """
        # simply average the emission probability under each joint state
        likes = [
            self.compute_emission_likelihood(evidence, state)
            for state in self.hierarchical_states
        ]
        return float(np.mean(likes))
    
    def _compute_joint_weighted_likelihood(self, evidence: Dict, var: str) -> float:
        """
        Soft‐assign fallback: weight the PDF of `var` under each joint state
        by the current joint-state belief.
        """
        total = 0.0
        for idx, state in enumerate(self.hierarchical_states):
            p_state = self.state_beliefs[idx]
            prm = self.emission_params[state].get(var)
            if isinstance(prm, dict):
                total += p_state * stats.norm.pdf(
                    evidence[var],
                    prm['mean'],
                    prm['std']
                )
        return total
    
    def _get_intra_mode_transition(self, mode:str, fsub:str, tsub:str)->float:
        # small base for all submode‐to‐submode
        mapping = {
          'EXPLORE': {
            'ego_allo':{'ego_allo_lookahead':0.15,'short_term_memory':0.1,'astar_directed':0.05},
            'ego_allo_lookahead':{'ego_allo':0.1,'astar_directed':0.1},
            'short_term_memory':{'ego_allo':0.1,'astar_directed':0.15},
            'astar_directed':{'ego_allo':0.1,'short_term_memory':0.05},
          },
          'NAVIGATE': {
            'distant_node':{'unvisited_priority':0.1,'plan_following':0.05},
            'unvisited_priority':{'distant_node':0.1,'plan_following':0.1},
            'plan_following':{'distant_node':0.08,'unvisited_priority':0.05},
          },
          'RECOVER': {'solve_doubt':{'backtrack_safe':0.2}, 'backtrack_safe':{'solve_doubt':0.25}},
          'TASK_SOLVING': {
            'goal_directed':{'systematic_search':0.05,'task_completion':0.02},
            'systematic_search':{'goal_directed':0.08,'task_completion':0.05},
            'task_completion':{'goal_directed':0.03,'systematic_search':0.03},
          }
        }
        return mapping.get(mode,{}).get(fsub,{}).get(tsub, 0.05)

    def _get_inter_mode_transition(self, fm,fs, tm,ts)->float:
        base = {
          'EXPLORE':{'NAVIGATE':0.15,'RECOVER':0.05,'TASK_SOLVING':0.05},
          'NAVIGATE':{'EXPLORE':0.1,'RECOVER':0.15,'TASK_SOLVING':0.05},
          'RECOVER':{'EXPLORE':0.2,'NAVIGATE':0.2,'TASK_SOLVING':0.05},
          'TASK_SOLVING':{'EXPLORE':0.05,'NAVIGATE':0.05,'RECOVER':0.05},
        }[fm].get(tm,0.01)
        # split among target submodes by entry preference
        prefs = {
          'EXPLORE':{'ego_allo':0.4,'ego_allo_lookahead':0.3,'short_term_memory':0.2,'astar_directed':0.1},
          'NAVIGATE':{'distant_node':0.4,'unvisited_priority':0.3,'plan_following':0.3},
          'RECOVER':{'solve_doubt':0.6,'backtrack_safe':0.4},
          'TASK_SOLVING':{'goal_directed':0.5,'systematic_search':0.3,'task_completion':0.2},
        }[tm].get(ts,1.0/len(self.mode_submodes[tm]))
        return base * prefs

    # ──────────────────────────────────────────
    #  Emission parameters builder (per‐state)
    # ──────────────────────────────────────────
    def _build_hierarchical_emission_params(self) -> Dict[Tuple[str,str],Dict]:
        out = {}
        for mode,subs in self.mode_submodes.items():
            for sub in subs:
                out[(mode,sub)] = self._get_emission_params_for_state(mode,sub)
        return out

    def detect_loop_behavior(self, agent_state, env_state) -> bool:
        """
        Simple loop detection based on recent position history.
        Fixed to handle different position formats and edge cases.
        """
        # Extract position with multiple fallback methods
        current_pos = None
        if 'position' in agent_state:
            current_pos = agent_state['position']
        elif 'pos' in agent_state:
            current_pos = agent_state['pos']
        elif hasattr(agent_state, 'position'):
            current_pos = agent_state.position
        else:
            # Try to extract from state if it's a State namedtuple
            if hasattr(agent_state, 'x') and hasattr(agent_state, 'y'):
                current_pos = (agent_state.x, agent_state.y)
            else:
                # Default fallback
                current_pos = (0, 0)
        
        # Ensure position is a tuple
        if not isinstance(current_pos, (tuple, list)):
            current_pos = (float(current_pos), 0.0) if isinstance(current_pos, (int, float)) else (0, 0)
        elif len(current_pos) < 2:
            current_pos = (float(current_pos[0]), 0.0) if len(current_pos) == 1 else (0, 0)
        else:
            current_pos = (float(current_pos[0]), float(current_pos[1]))
        
        self.last_poses.append(current_pos)
        
        if len(self.last_poses) < self.params['loop_threshold']:
            return False
        
        # Check if current position appeared recently
        recent_poses = list(self.last_poses)[-self.params['loop_threshold']:]
        position_counts = {}
        for pos in recent_poses:
            # Use rounded positions to handle floating point precision issues
            rounded_pos = (round(pos[0], 1), round(pos[1], 1))
            position_counts[rounded_pos] = position_counts.get(rounded_pos, 0) + 1
        
        # If any position appears more than 30% of recent steps, consider it a loop
        if not position_counts:
            return False
            
        max_count = max(position_counts.values())
        threshold = len(recent_poses) * 0.3
        return max_count > threshold

    def _get_emission_params_for_state(self, mode:str, sub:str) -> Dict:
        # copy your old mode‐params and tweak
        base = {
            'EXPLORE': {
                'info_gain':{'mean':0.4,'std':0.2},
                'progress': {'mean':0.2,'std':0.15},
                'stagnation':{'mean':0.3,'std':0.2},
                'lost_prob':0.2,
                'loop_prob':0.3
            },
            'NAVIGATE': {
                'info_gain':{'mean':0.1,'std':0.1},
                'progress': {'mean':0.6,'std':0.2},
                'stagnation':{'mean':0.2,'std':0.15},
                'lost_prob':0.15,
                'loop_prob':0.2
            },
            'RECOVER': {
                'info_gain':{'mean':0.05,'std':0.05},
                'progress': {'mean':0.1,'std':0.1},
                'stagnation':{'mean':0.8,'std':0.2},
                'lost_prob':0.9,
                'loop_prob':0.7
            },
            'TASK_SOLVING': {
                'info_gain':{'mean':0.2,'std':0.1},
                'progress': {'mean':0.5,'std':0.2},
                'stagnation':{'mean':0.1,'std':0.1},
                'lost_prob':0.1,
                'loop_prob':0.1
            }
        }[mode].copy()

        # submode tweaks:
        if mode=='EXPLORE':
            if   sub=='ego_allo':           base['info_gain']['mean']+=0.1
            elif sub=='astar_directed':     base['progress']['mean']+=0.2
            elif sub=='short_term_memory':  base['loop_prob']=0.2
        if mode=='NAVIGATE' and sub=='plan_following':
            base['progress']['mean']+=0.15; base['stagnation']['mean']-=0.1
        if mode=='RECOVER' and sub=='solve_doubt':
            base['lost_prob']=0.7
        # TASK_SOLVING left as mode base

        return base

    # ──────────────────────────────────────────
    #  Emission likelihood for a joint state
    # ──────────────────────────────────────────
    def compute_emission_likelihood(self, evidence: Dict, state: Tuple[str,str]) -> float:
        """
        Compute emission likelihood for a joint (mode, submode) state.
        Returns probability (not log-probability) with improved numerical stability.
        """
        if state not in self.emission_params:
            return 1e-10
            
        params = self.emission_params[state]
        log_ll = 0.0
        
        # Continuous variables - use log probabilities and sum
        for var in ['info_gain', 'progress', 'stagnation']:
            if var in evidence and var in params and isinstance(params[var], dict):
                value = float(evidence[var])
                # Clamp value to reasonable range
                value = np.clip(value, -10, 10)
                
                mean = float(params[var]['mean'])
                std = max(float(params[var]['std']), 1e-3)  # Prevent division by zero
                
                # Use scipy.stats for better numerical stability
                try:
                    log_ll += stats.norm.logpdf(value, mean, std)
                except:
                    # Fallback calculation if scipy fails
                    log_ll += -0.5 * np.log(2 * np.pi * std**2) - 0.5 * ((value - mean) / std)**2
        
        # Binary variables with better handling
        if 'agent_lost' in evidence and 'lost_prob' in params:
            p_lost = np.clip(float(params['lost_prob']), 1e-10, 1-1e-10)
            if evidence['agent_lost']:
                log_ll += np.log(p_lost)
            else:
                log_ll += np.log(1 - p_lost)
        
        # Handle loop detection if present in evidence
        if 'loop_detected' in evidence and 'loop_prob' in params:
            p_loop = np.clip(float(params['loop_prob']), 1e-10, 1-1e-10)
            if evidence['loop_detected']:
                log_ll += np.log(p_loop)
            else:
                log_ll += np.log(1 - p_loop)
        
        # Convert back to probability with better numerical stability
        log_ll = np.clip(log_ll, -50, 50)  # Prevent extreme values
        return np.exp(log_ll)

    # ──────────────────────────────────────────
    #  BOCPD: exactly as your run‐length specific code
    # ──────────────────────────────────────────
    # ————— BOCPD helpers —————
    def compute_run_length_specific_likelihood(self,
                                               evidence: Dict,
                                               run_length: int) -> float:
        """
        For each candidate run_length, build an empirical norm from the
        last `run_length` observations of each var, and score current evidence.
        Fallback to joint-weighted static if not enough history.
        """
        # 1) changepoint = use uniform prior
        if run_length == 0:
            return self._compute_prior_likelihood(evidence)

        hist = list(self.evidence_history)
        relevant = hist[-run_length:] if len(hist) >= run_length else hist
        if not relevant:
            return self._compute_prior_likelihood(evidence)

        logl = 0.0
        for var in ['info_gain','progress','stagnation',
                    'exploration_productivity',
                    'navigation_progress',
                    'recovery_effectiveness']:
            if var in evidence:
                vals = [e[var] for e in relevant if var in e]
                if len(vals) >= 2:
                    μ = np.mean(vals)
                    σ = max(np.std(vals), 1e-2) * (1 + 1.0/run_length)
                    logl += stats.norm.logpdf(evidence[var], μ, σ)
                else:
                    # not enough history → fallback to static joint mixture
                    logl += np.log(self._compute_joint_weighted_likelihood(evidence, var) + 1e-10)

        # 2) combine with a static fallback on emission likelihood
        #    static_mix = Σ_b(state) · p(evidence | state)
        emis = np.array([
            self.compute_emission_likelihood(evidence, state)
            for state in self.hierarchical_states
        ])
        static_mix = float(np.dot(self.state_beliefs, emis))

        # 3) convex combine
        comb = 0.7 * np.exp(logl) + 0.3 * static_mix
        return max(comb, 1e-10)

    # ──────────────────────────────────────────
    #  HMM forward step (joint states)
    # ──────────────────────────────────────────
    def hmm_forward_step(self, evidence: Dict) -> np.ndarray:
        """
        Standard HMM forward (predict + update) over joint states.
        """
        # emission likelihood under each joint (mode,submode)
        e_likes = np.array([
            self.compute_emission_likelihood(evidence, state)
            for state in self.hierarchical_states
        ])

        # predict
        pred = self.transition_matrix.T @ self.state_beliefs

        # update
        newb = e_likes * pred
        if newb.sum() > 0:
            self.state_beliefs = newb / newb.sum()
        else:
            self.state_beliefs = np.ones(self.n_states) / self.n_states

        return self.state_beliefs
    
    def bocpd_update(self, evidence: Dict) -> Tuple[bool, float]:
        """
        Run‐length recursion using your run_length‐specific likelihoods.
        Fixed array handling and probability computation.
        """
        # Compute likelihoods for each run length
        likelihoods = np.zeros(self.max_run_length + 1)
        for r in range(len(self.run_length_dist)):
            if self.run_length_dist[r] > 1e-10:
                likelihoods[r] = self.compute_run_length_specific_likelihood(evidence, r)
        
        # Growth probabilities (no changepoint)
        growth_probs = np.zeros(self.max_run_length + 1)
        if len(self.run_length_dist) > 1:
            growth_probs[1:] = (self.run_length_dist[:-1] * 
                            (1 - self.hazard_rate) * 
                            likelihoods[:-1])
        
        # Changepoint probability 
        cp_mass = float(np.sum(self.run_length_dist * self.hazard_rate * likelihoods))
        
        # Update run length distribution
        new_dist = np.zeros_like(self.run_length_dist)
        new_dist[0] = cp_mass  # Changepoint mass goes to run length 0
        new_dist[1:] = growth_probs[1:]  # Growth probabilities
        
        # Normalize to ensure it's a proper probability distribution
        total_mass = new_dist.sum()
        if total_mass > 1e-10:
            new_dist /= total_mass
        else:
            # Fallback to uniform if numerical issues
            new_dist = np.ones_like(new_dist) / len(new_dist)
        
        self.run_length_dist = new_dist
        
        # Detect changepoint based on mass at run length 0
        changepoint_detected = cp_mass > self.changepoint_threshold
        
        return changepoint_detected, cp_mass

    # ──────────────────────────────────────────
    #  Full update: BOCPD + forward
    # ──────────────────────────────────────────
    def update(self,evidence:Dict)->Tuple[np.ndarray,bool]:
        # 1) accumulate evidence
        self.evidence_history.append(evidence.copy())
        self.evidence_buffer.append(evidence.copy())
        # 2) BOCPD
        cp_flag, cp_p = self.bocpd_update(evidence)
        if cp_flag:
            # reset joint‐belief “sticky”
            uniform = np.ones(self.n_states)/self.n_states
            self.state_beliefs = 0.5*self.state_beliefs + 0.5*uniform
        # 3) HMM forward
        self.hmm_forward_step(evidence)
        # 4) book-keeping
        self.mode_history.append((time.time(), self.state_beliefs.copy()))
        return self.state_beliefs, cp_flag

    # ──────────────────────────────────────────
    #  Diagnostics: marginalize back to modes
    # ──────────────────────────────────────────
    def get_mode_probabilities(self)->Dict[str,float]:
        marg = defaultdict(float)
        for idx,prob in enumerate(self.state_beliefs):
            mode, _ = self.idx_to_state[idx]
            marg[mode] += prob
        return dict(marg)

    def get_submode_probabilities(self)->Dict[str,Dict[str,float]]:
        mode_probs = self.get_mode_probabilities()
        out = {}
        for mode,subs in self.mode_submodes.items():
            out[mode]={}
            if mode_probs[mode]>1e-10:
                for sub in subs:
                    idx = self.state_to_idx[(mode,sub)]
                    out[mode][sub] = self.state_beliefs[idx]/mode_probs[mode]
            else:
                # uniform fallback
                for sub in subs:
                    out[mode][sub] = 1.0/len(subs)
        return out
# --------------------------------------------------
# PCFG Builder & Controller (unchanged from your skeleton)
# --------------------------------------------------
class EnhancedMixturePCFGBuilder:
    def __init__(self, key, hmm_observer: HierarchicalHMMBayesianObserver):
        self.key          = key
        self.hmm_observer = hmm_observer
        self.memory_graph = key.models_manager.memory_graph
        self.emap         = self.memory_graph.experience_map
        self.submode_beliefs = {
            'EXPLORE': {'ego_allo':0.4,'ego_allo_lookahead':0.3,'short_term_memory':0.2,'astar_directed':0.1},
            'NAVIGATE':{'distant_node':0.4,'unvisited_priority':0.3,'plan_following':0.3},
            'RECOVER': {'solve_doubt':0.6,'backtrack_safe':0.4},
            'TASK_SOLVING':{'goal_directed':0.5,'systematic_search':0.3,'task_completion':0.2}
        }

    def update_submode_beliefs(self,evidence:Dict):
        # your RL-style update from before...
        current = max(self.hmm_observer.get_mode_probabilities().items(),
                      key=lambda x:x[1])[0]
        perf = evidence.get('performance_score',0.5)
        sub = evidence.get('active_submode')
        if sub and sub in self.submode_beliefs[current]:
            lr=0.1; cb=self.submode_beliefs[current][sub]
            if perf>0.6:   self.submode_beliefs[current][sub]=min(0.9,cb+lr*(1-cb))
            elif perf<0.3: self.submode_beliefs[current][sub]=max(0.1,cb-lr*cb)
            # renormalize
            tot=sum(self.submode_beliefs[current].values())
            for k in self.submode_beliefs[current]:
                self.submode_beliefs[current][k]/=tot

    def build_mixture_pcfg(self,use_soft:bool=True)->PCFG:
        mode_probs=self.hmm_observer.get_mode_probabilities()
        if use_soft:
            return self._build_soft_mixture_pcfg(mode_probs)
        else:
            best=max(mode_probs,key=mode_probs.get)
            return self._build_single_mode_pcfg(best)

    def _build_soft_mixture_pcfg(self, mode_probs):
        """
        Build PCFG using current HMM submode beliefs instead of local ones.
        """
        rules = []
        s = sum(mode_probs.values())
        
        # Top-level mode rules
        if s > 0:
            for m, p in mode_probs.items():
                rules.append(f"START -> {m}_ROOT [{p/s:.4f}]")
        else:
            for m in mode_probs: 
                rules.append(f"START -> {m}_ROOT [0.25]")
        
        # Get current submode beliefs from HMM observer
        current_submode_beliefs = self.hmm_observer.get_submode_probabilities()
        
        # Build rules using HMM submode beliefs
        rules += self._build_explore_rules(current_submode_beliefs.get('EXPLORE', {}))
        rules += self._build_navigate_rules(current_submode_beliefs.get('NAVIGATE', {}))
        rules += self._build_recover_rules(current_submode_beliefs.get('RECOVER', {}))
        rules += self._build_task_solving_rules(current_submode_beliefs.get('TASK_SOLVING', {}))
        
        return PCFG.fromstring("\n".join(rules))

    def _build_explore_rules(self, submode_probs=None):
        """Fixed to use consistent terminal names and proper probability formatting."""
        if submode_probs is None:
            submode_probs = self.submode_beliefs['EXPLORE']
        
        p = submode_probs
        s = sum(p.values()) if p else 1.0
        
        if s > 0:
            # Fixed: Use consistent naming that matches the terminal definitions below
            mapping = {
                'ego_allo': 'EGOALLO',
                'ego_allo_lookahead': 'EGOALLOLOOKAHEAD', 
                'short_term_memory': 'SHORTTERMMEMORY',
                'astar_directed': 'ASTARDIRECTED'
            }
            r = [f"EXPLORE_ROOT -> EXPLORE_{mapping.get(k, k.upper())} [{(v/s):.4f}]" for k, v in p.items()]
        else:
            # Fallback to uniform distribution
            r = [
                "EXPLORE_ROOT -> EXPLORE_EGOALLO [0.25]",
                "EXPLORE_ROOT -> EXPLORE_EGOALLOLOOKAHEAD [0.25]", 
                "EXPLORE_ROOT -> EXPLORE_SHORTTERMMEMORY [0.25]",
                "EXPLORE_ROOT -> EXPLORE_ASTARDIRECTED [0.25]"
            ]
        
        r += [
            "EXPLORE_EGOALLO -> 'forward' [0.6] | 'left' [0.2] | 'right' [0.2]",
            "EXPLORE_EGOALLOLOOKAHEAD -> 'forward' 'forward' [0.4] | 'left' 'forward' [0.3] | 'right' 'forward' [0.3]", 
            "EXPLORE_SHORTTERMMEMORY -> 'scan' [0.3] | 'forward' [0.4] | 'backtrack' [0.3]",
            "EXPLORE_ASTARDIRECTED -> 'plan_to_frontier' [1.0]"
        ]
        return r

    def _build_navigate_rules(self, submode_probs=None):
        """Fixed to use consistent terminal names."""
        if submode_probs is None:
            submode_probs = self.submode_beliefs['NAVIGATE']
        
        p = submode_probs
        s = sum(p.values()) if p else 1.0
        
        if s > 0:
            mapping = {
                'distant_node': 'DISTANTNODE',
                'unvisited_priority': 'UNVISITEDPRIORITY',
                'plan_following': 'PLANFOLLOWING'
            }
            r = [f"NAVIGATE_ROOT -> NAVIGATE_{mapping.get(k, k.upper())} [{(v/s):.4f}]" for k, v in p.items()]
        else:
            r = [
                "NAVIGATE_ROOT -> NAVIGATE_DISTANTNODE [0.33]",
                "NAVIGATE_ROOT -> NAVIGATE_UNVISITEDPRIORITY [0.33]",
                "NAVIGATE_ROOT -> NAVIGATE_PLANFOLLOWING [0.34]"
            ]
        
        r += [
            "NAVIGATE_DISTANTNODE -> 'goto_distant' [1.0]",
            "NAVIGATE_UNVISITEDPRIORITY -> 'goto_unvisited' [1.0]", 
            "NAVIGATE_PLANFOLLOWING -> 'follow_plan' [0.8] | 'replan' [0.2]"
        ]
        return r

    def _build_recover_rules(self, submode_probs=None):
        """Fixed to use consistent terminal names."""
        if submode_probs is None:
            submode_probs = self.submode_beliefs['RECOVER']
        
        p = submode_probs  
        s = sum(p.values()) if p else 1.0
        
        if s > 0:
            mapping = {
                'solve_doubt': 'SOLVEDOUBT',
                'backtrack_safe': 'BACKTRACKSAFE'
            }
            r = [f"RECOVER_ROOT -> RECOVER_{mapping.get(k, k.upper())} [{(v/s):.4f}]" for k, v in p.items()]
        else:
            r = [
                "RECOVER_ROOT -> RECOVER_SOLVEDOUBT [0.5]",
                "RECOVER_ROOT -> RECOVER_BACKTRACKSAFE [0.5]"
            ]
        
        r += [
            "RECOVER_SOLVEDOUBT -> 'scan' [0.4] | 'relocalize' [0.6]",
            "RECOVER_BACKTRACKSAFE -> 'backtrack' [0.7] | 'return_to_known' [0.3]"
        ]
        return r

    def _build_task_solving_rules(self, submode_probs=None):
        """Fixed to use consistent terminal names."""
        if submode_probs is None:
            submode_probs = self.submode_beliefs['TASK_SOLVING']
        
        p = submode_probs
        s = sum(p.values()) if p else 1.0
        
        if s > 0:
            mapping = {
                'goal_directed': 'GOALDIRECTED',
                'systematic_search': 'SYSTEMATICSEARCH', 
                'task_completion': 'TASKCOMPLETION'
            }
            r = [f"TASK_SOLVING_ROOT -> TASK_{mapping.get(k, k.upper())} [{(v/s):.4f}]" for k, v in p.items()]
        else:
            r = [
                "TASK_SOLVING_ROOT -> TASK_GOALDIRECTED [0.33]",
                "TASK_SOLVING_ROOT -> TASK_SYSTEMATICSEARCH [0.33]",
                "TASK_SOLVING_ROOT -> TASK_TASKCOMPLETION [0.34]"
            ]
        
        r += [
            "TASK_GOALDIRECTED -> 'execute_task_plan' [0.8] | 'refine_task_plan' [0.2]",
            "TASK_SYSTEMATICSEARCH -> 'systematic_exploration' [0.6] | 'check_all_rooms' [0.4]", 
            "TASK_TASKCOMPLETION -> 'complete_objective' [0.9] | 'verify_completion' [0.1]"
        ]
        return r

class EnhancedHybridBayesianController:
    def __init__(self, key, buffer_size:int=50):
        self.key           = key
        self.hmm_observer  = HierarchicalHMMBayesianObserver()
        self.pcfg_builder  = EnhancedMixturePCFGBuilder(key,self.hmm_observer)
        self.performance_buffer = deque(maxlen=buffer_size)
        self.last_evidence     = {}
        self.use_soft_mixture  = True
        self.adaptation_enabled = True

    def extract_enhanced_evidence(self, agent_state, env_state, perf):
        """
        Extract evidence dictionary that matches what HMM expects.
        Improved error handling and type checking.
        """
        # Safely extract values with defaults
        def safe_get(obj, key, default=0.0):
            if obj is None:
                return default
            if isinstance(obj, dict):
                return obj.get(key, default)
            return getattr(obj, key, default)
        
        # Calculate derived metrics with bounds checking
        info_gain = float(safe_get(perf, 'info_gain', 0.0))
        if info_gain == 0.0:
            # Fallback: derive info_gain from new nodes created
            new_nodes = float(safe_get(env_state, 'new_nodes', 0))
            info_gain = min(1.0, max(0.0, new_nodes / 10.0))
        
        progress = float(safe_get(perf, 'plan_progress', 0.0))
        if progress == 0.0:
            has_goal = bool(safe_get(agent_state, 'has_navigation_goal', False))
            if has_goal:
                # Derive progress from goal proximity or path completion
                steps_since = float(safe_get(agent_state, 'steps_since_progress', 0))
                progress = max(0.0, 1.0 - min(1.0, steps_since / 20.0))
        
        # Calculate stagnation score
        doubt_count = float(safe_get(agent_state, 'place_doubt_step_count', 0))
        steps_since_progress = float(safe_get(agent_state, 'steps_since_progress', 0))
        stagnation = min(1.0, max(0.0, (doubt_count + steps_since_progress) / 30.0))
        
        # Compute additional metrics expected by BOCPD
        exploration_productivity = max(0.0, info_gain * (1.0 - stagnation))
        navigation_progress = progress if bool(safe_get(agent_state, 'has_navigation_goal', False)) else 0.0
        recovery_effectiveness = max(0.0, 1.0 - stagnation) if doubt_count > 5 else 0.0
        
        # Get reward with fallback
        reward = float(safe_get(perf, 'reward', 0.5))
        if reward == 0.5:  # Default fallback, try other keys
            reward = float(safe_get(perf, 'performance_score', 0.5))
        
        e = {
            'timestamp': time.time(),
            'performance_score': np.clip(reward, 0.0, 1.0),
            'active_submode': safe_get(agent_state, 'active_submode', None),
            
            # Core HMM evidence variables (all clipped to [0,1])
            'info_gain': np.clip(info_gain, 0.0, 1.0),
            'progress': np.clip(progress, 0.0, 1.0), 
            'stagnation': np.clip(stagnation, 0.0, 1.0),
            'agent_lost': bool(doubt_count > 6),
            
            # Additional BOCPD variables
            'exploration_productivity': np.clip(exploration_productivity, 0.0, 1.0),
            'navigation_progress': np.clip(navigation_progress, 0.0, 1.0),
            'recovery_effectiveness': np.clip(recovery_effectiveness, 0.0, 1.0),
            
            # Raw state information (keep as is for debugging)
            'place_doubt_step_count': int(doubt_count),
            'new_nodes': int(safe_get(env_state, 'new_nodes', 0)),
            'nodes_created': int(safe_get(env_state, 'nodes_created_total', 0)),
            'plan_progress': np.clip(progress, 0.0, 1.0),
            'has_navigation_goal': bool(safe_get(agent_state, 'has_navigation_goal', False)),
            'path_blocked': bool(safe_get(env_state, 'path_blocked', False)),
            'task_defined': bool(safe_get(agent_state, 'task_defined', False)),
            'exploration_completeness': np.clip(float(safe_get(env_state, 'exploration_completeness', 0.0)), 0.0, 1.0),
            'steps_since_progress': int(steps_since_progress)
        }
        return e

    def update(self, agent_state, env_state, perf):
        """
        Main update method for the controller.
        """
        # Add loop detection to evidence
        loop_detected = self.hmm_observer.detect_loop_behavior(agent_state, env_state)
        
        # Extract evidence with loop detection included
        evidence = self.extract_enhanced_evidence(agent_state, env_state, perf)
        evidence['loop_detected'] = loop_detected
        
        self.last_evidence = evidence
        self.performance_buffer.append(perf.get('reward', 0.0))

        # 1) HMM+BOCPD update
        beliefs, cp = self.hmm_observer.update(evidence)
        
        # 2) Submode adaptation
        if self.adaptation_enabled:
            self.pcfg_builder.update_submode_beliefs(evidence)
        
        # 3) Build PCFG
        pcfg = self.pcfg_builder.build_mixture_pcfg(self.use_soft_mixture)
        
        # 4) Compile diagnostics
        diag = {
            'mode_beliefs': self.hmm_observer.get_mode_probabilities(),
            'submode_beliefs': self.hmm_observer.get_submode_probabilities(),
            'changepoint': cp,
            'changepoint_probability': float(np.sum(self.hmm_observer.run_length_dist * self.hmm_observer.hazard_rate)),
            'last_evidence': evidence,
            'loop_detected': loop_detected,
            **self.hmm_observer.get_diagnostics()
        }
        
        return pcfg, diag

    def get_current_strategy(self)->Tuple[str,float]:
        mp = self.hmm_observer.get_mode_probabilities()
        best,conf = max(mp.items(),key=lambda x:x[1])
        return best,conf

    def toggle_mixture_mode(self,use_soft:Optional[bool]=None):
        if use_soft is not None: self.use_soft_mixture = use_soft
        else:                   self.use_soft_mixture = not self.use_soft_mixture
        print(f"[Controller] {'soft' if self.use_soft_mixture else 'hard'} mixture")

    def print_status(self):
        best,conf = self.get_current_strategy()
        stats = self.hmm_observer.get_diagnostics()['buffer_stats']
        print(f"Dominant: {best} ({conf:.2f}); poses={stats['poses_tracked']}, replay={stats['replay_buffer_size']}")

```

`hierarchical-nav/navigation_model/Processes/AIF_modules.py`:

```py
import torch
import numpy as np
from dommel_library.modules.dommel_modules import tensor_dict


# ------------ AIF FUNCTIONS ------------------# 
def calculate_FE_EFE(model: object, place:torch.Tensor, model_step_output:dict, preferred_state:torch.Tensor) -> tuple[float, float, float, np.ndarray]:
    ''' check FE at each step and predict one step EFE
    with current :
    model (Class)
    model one model_step_output output (TensorDict
    )
    preferred_state we want to favorise (MultivariateNormal)
    '''
    KL,ambiguity= KL_and_ambiguity(model_step_output, preferred_state = preferred_state)
    mse, state_prob_in_place, image_predicted_data = free_energy(model, place, model_step_output, preferred_state = preferred_state)

    return KL[0], mse, np.mean(state_prob_in_place), image_predicted_data

def free_energy( model:object, place:torch.Tensor, sequence:dict, preferred_state:torch.Tensor = None)-> tuple[float,list,np.ndarray]:
    mse,image_predicted_data = mse_observation(model, place,sequence) # only works with real observation given with pose
    state_prob_in_place = logprob_observation(model, sequence,preferred_state)

    return float(mse.detach().cpu().numpy()), state_prob_in_place.tolist(), image_predicted_data

def estimate_a_surprise(state:torch.Tensor, preferred_state:torch.Tensor, predicted_obs:torch.Tensor, preferred_ob:torch.Tensor):
    """
    Get KL:
    given a distribution : state
    a distribution of comparison : preferred_state
    Get ambiguity between predicted ob:
    images of format [sample, ...]: predicted_obs
    Get discrepency between those predicted obs and an image of reference: preferred_ob [sample,...]
    """
    ambiguity = observation_ambiguity(predicted_obs).detach().cpu().numpy().tolist()[0]
    kl = round(np.mean(calculate_KL(state, preferred_dists=preferred_state).detach().cpu().numpy()), 4)
    pred_expected_error = mse_elements(predicted_obs, preferred_ob).detach().cpu().numpy().tolist()
    return kl, ambiguity, pred_expected_error

def KL_and_ambiguity(imagined_sequence,preferred_state=None, ambiguity_beta=1):
    """ expected entry shape [batch,lookahead,dims], outputs shape [lookahead,1] """
    
    if 'place' in imagined_sequence:
        posterior = imagined_sequence.place
    elif 'posterior' in imagined_sequence:
        posterior = imagined_sequence.posterior
    elif 'prior' in imagined_sequence:
        posterior = imagined_sequence.prior

    
    #just for test
    if 'image_predicted' in imagined_sequence:
        keys = ["image_predicted"]
    if 'image_reconstructed' in imagined_sequence:
        keys = ["image_reconstructed"]
    
    elif 'image' in imagined_sequence:
        keys = ["image"]
    elif 'state' in imagined_sequence:
        keys = ['state']
        
    else:
        raise KeyError ('NO IDEA WHICH KEY TO EXTRACT FOR logprob_reconstruction')

    logprob_image = logprob_reconstruction(imagined_sequence, keys).detach().cpu().numpy().tolist()
    if not isinstance(logprob_image, list):
        logprob_image = [logprob_image]   
                                
    epistemic_term = calculate_KL(posterior, preferred_state).detach().cpu().numpy() 
    if isinstance(epistemic_term,float):
        epistemic_term = [epistemic_term]   
    epistemic_term = epistemic_term* ambiguity_beta
   
    return  epistemic_term.tolist(),logprob_image

def compute_std_dist(dist)->float:
    ''' 
    extract the std from either a distribution as an array or tensor.
    calculate the mean std of this distribution over dimensions and return it
    '''
    std_dist = dist[...,(dist.shape[-1] // 2):]
    mean_std_dist = torch.mean(1 / 2 * torch.log(2 * np.pi * np.e * std_dist ** 2), dim=[0,-1]).cpu().detach().numpy().tolist()[0]
    return mean_std_dist

def calculate_KL(state_dist, preferred_dists=None):
    KL = torch.tensor([0.0]) 
    #print('before state mean shape + preferred dist shape', state_dist.shape, preferred_dists.shape)
    if preferred_dists is not None:
        mean_state_dist = torch.mean(state_dist, dim=0)
        if len(mean_state_dist.shape) == 1:
            mean_state_dist = mean_state_dist.unsqueeze(0)
        if len(preferred_dists.shape) == 1:
            preferred_dists = preferred_dists.unsqueeze(0).to(state_dist.device)
        elif len(preferred_dists.shape) > 2:
            preferred_dists =  torch.mean(preferred_dists, dim=0).to(state_dist.device)
        #print('state mean shape + preferred dist shape', mean_state_dist.shape, preferred_dists.shape)
        KL = torch.distributions.kl_divergence(mean_state_dist, preferred_dists)
        KL = KL / preferred_dists.shape[-1]
    #print('KL CHECK',KL, type(KL))

    return KL

def logprob_reconstruction(sequence, key=[]): 
    ''' check  prior reconstructed ob ambiguity '''
    H = 0   
    for k in key: #not adapted for more than 1 key
        ob = sequence[k]
    #prior = torch.mean(prior, dim=0)
        if len(ob.shape) == 4 : #IMAGE only [sample,lookahead,3,64,64], if <5, no lookahead
            ob = ob.unsqueeze(1)
        sigmas = torch.std(ob, dim=0)
        Hs = 1 / 2 * torch.log(2 * np.pi * np.e * sigmas ** 2)
        #print('Hs', Hs.shape)

        #to consider images shape
        if len(Hs.shape) > 2:
            H = torch.mean(Hs, dim=list(range(1,len(Hs.shape))))
        else:
            H = torch.mean(Hs, dim=-1)
    return H

def observation_ambiguity(ob:torch.Tensor)->torch.Tensor:
    ''' check if the different predictions match each other or not '''
    H = 0   
    if len(ob.shape) == 4 : #IMAGE only [sample,lookahead,3,64,64], if <5, no lookahead
        ob = ob.unsqueeze(1)
    sigmas = torch.std(ob, dim=0)
    Hs = 1 / 2 * torch.log(2 * np.pi * np.e * sigmas ** 2)

    #to consider images shape
    if len(Hs.shape) > 2:
        H = torch.mean(Hs, dim=list(range(1,len(Hs.shape))))
    else:
        H = torch.mean(Hs, dim=-1)
    return H

def logprob_observation(model:object, sequence:dict, pref_post:torch.Tensor)->torch.Tensor: 
    ''' check  log prob of seq stste in preferred post distrib'''
    logprob_preferences = 0
    
    with torch.no_grad():
        model.reset()
        step = model.forward(sequence, place = None, reconstruct=False) 

    logprob_preferences = pref_post.log_prob(step['state']) / np.prod(pref_post.shape)
    
    return logprob_preferences

def mse_observation(model:object, place:torch.Tensor, sequence:dict)-> tuple[float,np.ndarray]: 
    ''' check  expected ob with current pose and post vs real ob '''
    
    tmp_seq = tensor_dict({'pose_query': sequence['pose'] })
    with torch.no_grad():
               
        step = model.forward(tmp_seq, place, reconstruct=True) 
    model_error = mse_elements(step['image_predicted'], sequence['image'])
    return model_error, step['image_predicted']

def mse_elements(prediction:torch.Tensor, target:torch.Tensor) -> torch.Tensor: 
    ''' check mse between 2 elements '''
    model_error = torch.nn.functional.mse_loss(prediction, target, reduction='mean') *10 #/ np.prod(sequence['image'].shape) *10
    return model_error


```

`hierarchical-nav/navigation_model/Processes/exploitative_behaviour.py`:

```py
import torch
import numpy as np
from navigation_model.Services.model_modules import torch_observations
from navigation_model.Processes.motion_path_modules import (
    action_to_pose, define_policies_objectives, create_policies, dijkstra_weigthed, 
    dijkstra_shortest_node_path)
from navigation_model.Processes.AIF_modules import mse_elements


from navigation_model.visualisation_tools import visualise_image, transform_policy_from_hot_encoded_to_str
from env_specifics.env_calls import find_preferred_features_in_img, call_env_place_range


class ExploitativeBehaviour():
    def __init__(self, verbose:bool=True) :
        self.verbose = verbose
        self.preferred_objective = {}

#========================= SET GOAL ================================================#
    def get_preferred_objective(self)->dict:
        return self.preferred_objective
    
    def set_objective(self, pose:list=[], exp_id:int =-1, image=None, decay:float=100)->dict:
        ''' save pose, exp_id and ob as torched objective to reach'''
        goal_info = {'pose': pose, 'image': image}
        goal_data = torch_observations(goal_info, ['pose', 'image'])
        goal_data['exp'] = exp_id
        goal_data['decay'] = decay
        return goal_data
    
    def set_current_pose_as_goal(self, manager:object, ob:np.ndarray=[])->dict:
        '''
        Given a goal set during run , determine where it is IN ROOM
        '''
        pose = manager.get_best_place_hypothesis()['pose']
        if len(ob) == 0:
            ob = manager.get_best_place_hypothesis()['image_predicted'][0][0]
        
        current_exp_id = manager.get_current_exp_id()
        goal = self.set_objective(pose, current_exp_id, ob)
        return goal
    
    def define_preferred_objective(self,manager:object, preferred_features:np.ndarray):
        """
        given what the objective should look like and search for it in ego then allo
        The decay set the progression at which this information is lost, 
        meaning that we should refresh our memory regularly
        """
        #check conidtions to do so
        pref_objective = self.search_goal_around(manager, preferred_features)
        if pref_objective :
            self.preferred_objective = self.set_objective(**pref_objective, decay=0.75)
            return self.preferred_objective
        
        pref_objective = self.search_goal_in_memory(manager, preferred_features)
        if pref_objective:
            self.preferred_objective = self.set_objective(**pref_objective, decay=0.75)
            return self.preferred_objective
        
        return {}

    def goal_memory_decay(self, decay_value:float=0.25):
        self.preferred_objective['decay'] -= decay_value

#========================= SEARCH GOAL ================================================#
    
    def search_goal_around(self,manager:object, preferred_features:np.ndarray):
        """ 
        egocentric model search in vicinity a pose predicting the preferred_features.
        It returns the best pose and nothing else.
        """
        
        pref_goal = self.ego_match_preferred_features_wt_pose(manager, preferred_features)
       
        return pref_goal
    
    def search_goal_in_memory(self, manager:object, preferred_features:np.ndarray) -> dict:
        """ memory graph + allo search"""
        preferrences_in_memory = self.allo_match_preferred_features_wt_pose(manager, preferred_features)
        if preferrences_in_memory:
            best_preferrence = max(preferrences_in_memory.values(), key=lambda x: x['pref_likelihood'])
            print('best_preferrence', best_preferrence['image'].shape, best_preferrence['exp_id'], best_preferrence['pose'], best_preferrence['pref_likelihood'])
            del best_preferrence['pref_likelihood']
        else:
            best_preferrence = {}
        return best_preferrence

#============================ MOTION PUSHED BY OB ======================================#
    #NOTE: WHAT IF ERROR? (GOAL ACTUALLy NOT IN PLACE)
    def go_to_observation_in_place(self, manager:object, goal:dict)->list:
        '''
        output the best policy to reach the observation objective in place 
        (no obstacle)
        #NOTE: THERE IS A RISK OG GOING BACK AND FORTH BETWEEN TWO ORIENTATIONS LEADING TO GOAL, 
        depending on which policy holds the best goal prediction. NOT A BIG ISSUE BUT COULD BE 
        Annoying
        '''
      
        goal['image'] = manager.sample_visual_ob(goal['image'].unsqueeze(0))
        current_pose = manager.get_best_place_hypothesis()['pose'].copy()
        if np.all(current_pose == goal['pose'].tolist()):
            print("we are not entering go_to_observation_in_place",current_pose, goal['pose'].tolist())
            return [], False
        policy = self.reach_objective_wt_ego_short_term_pred(manager, goal, current_pose)
        print("policies in go_to_observation_in_place, wt ego short term", policy)
        #If a path to the goal is predicted by the ego model, reach there
        if len(policy)> 0:
            return policy, False
        
        policies = create_policies(current_pose, [goal['pose'].tolist()], exploration=False)
        print("policies", policies)
        policies_to_goal = self.evaluate_go_to_goal_pose_policies(manager,policies)
        print("policies_to_goal", policies_to_goal)
        if len(max(policies, key=len)) == 0:
            print('We have probably reached the objective')
            return [], False

        if len(policies_to_goal)==0:
            #If all policies leads to a colision. We will consider the full policies
            print("ERROR in go_to_observation_in_place: The goal pose is predicted unreachable")
            print('We will still continue the approach')
            policies_to_goal = policies
        
        if len(min(policies_to_goal, key=len)) < 3:
            #If the shortest path to the goal < 3 ... the ego model should have predicted the goal observation.
            #We shouldn't be here. there is been an error in goal prediction
            print('ERROR in go_to_observation_in_place: less than 3 steps to reach goal, \
                  yet ego model not seeing the goal. We consider the allo model allucinating the goal!')
            return min(policies_to_goal, key=len), True
        
        #------ We process all the policies through the allocentric model to select best approach ---# 
        best_policy = self.reach_objective_wt_allo_pred(manager, goal, policies_to_goal, current_pose)
        print(best_policy)
        return best_policy, False
        
    def evaluate_go_to_goal_pose_policies(self, manager:object,policies:list)-> list:
        """ 
        given policies to evaluate, check if they are dynamically plausible
        return those that are
        """
        policies_to_goal = []
        for policy in policies:
            #Check if there is any collision in the polcicies
            #print('policy for ego pred', transform_policy_from_hot_encoded_to_str(policy))
            returned_policy, _ = manager.policy_egocentric_prediction(policy)
            #if no collision, then this is a good path to take.
            if len(returned_policy) == len(policy):
                policies_to_goal.append(policy)
        return policies_to_goal
    
    def reach_objective_wt_allo_pred(self, manager:object, goal:dict, policies:list, current_pose:list) -> list:
        """
        we check if any of the policies hold an allocentric prediction matching it. 
        The policy having the best mse for a pred matching the goal is considered for the trajectory
        NOTE: can lead to loops turning between 2 re-try depending on pred quality
        """
        place = manager.get_best_place_hypothesis()['post']
        print("place inside reach_objective_wt_allo_pred", place)
        goal_matching_mse_pose = {}
        for policy in policies:
            print("policy entering estimate_policy_allo_pred",policy)
            goal_matching_mse_pose = self.estimate_policy_allo_pred_matching_goal(manager, place, current_pose, policy, goal, goal_matching_mse_pose)
            print("policy out", goal_matching_mse_pose)
        best_policy = self.get_policy_with_min_mse_pose(goal_matching_mse_pose)
        print("best_policy", best_policy)
        return best_policy
    
    def estimate_policy_allo_pred_matching_goal(self,manager:object, place:np.ndarray, current_pose:list,
                                                 policy:list, goal:dict, goal_matching_mse_pose:dict={}):
        ''' given the place, current pose and policy as well as the goal to compare to, 
        predict the observation for each pose of policy and compare each of them to the goal
        '''
        seq_pose = current_pose.copy()
        pose_queries = []
        pose_queries.extend([list(seq_pose := action_to_pose(action, seq_pose)) for action in policy])
        allo_prediction = manager.several_poses_allocentric_prediction(pose_queries, place) 
        goal_matching_mse_pose = self.compare_policy_prediction_to_preferred_ob(manager, goal, current_pose, allo_prediction, policy,
                                                key='image_predicted', goal_matching_mse_pose=goal_matching_mse_pose)
        return goal_matching_mse_pose
    
    def reach_objective_wt_ego_short_term_pred(self, manager:object, goal:dict, current_pose:list)-> tuple[dict,list]:
        '''
        we check in the immediate vicinity with the ego model 
        (it has a short term memory, it is not biased by long past data)
        if the observation is found, we update the objective to that pose. 
        return the modified goal and po
        #warning, if the agent needs to turn back on its path to see goal, this policy won't be formed with create_policy
        thus, the short term planning won't find anything
        #NOTE: we have to make sure that we consider the correct observation 
        and are not mistaken with aliases.
        '''
        policies = self.define_explo_policies(manager, current_pose)
        print("define_explo_policies", policies, "how many policies", len(policies))
        
        goal_matching_mse_pose = {}
        for policy in policies:
            print("policy entering", policy)
            policy, ego_prediction = manager.policy_egocentric_prediction(policy)
            print("policies inside the ego predict",policy, ego_prediction.keys(),ego_prediction["collision_reconstructed"])
            if len(policy) == 0:
                continue
            goal_matching_mse_pose = self.compare_policy_prediction_to_preferred_ob(manager, goal, current_pose, ego_prediction,
                                                    policy, 'image_reconstructed', 
                                                    goal_matching_mse_pose)
        
        print('goal_matching_mse_pose', goal_matching_mse_pose)
        best_policy = self.get_policy_with_min_mse_pose(goal_matching_mse_pose)

        return best_policy

    def compare_policy_prediction_to_preferred_ob(self, manager:object, goal:dict, current_pose:list, pred:dict, 
                                                    policy:list, key:str='image_reconstructed', 
                                                    goal_matching_mse_pose:dict={})-> dict:
        """
        given the manager
        the goal as a sampled dict as the predictions
        the prediction containing the image (the keys is an option to compare with both ego and allo preds)
        the whole policy and wether we want to compare new poses to an already existing dict goal_matching_mse_pose

        For each action in the policy, check if the prediction matches the preferred observation. If it does
        save it if it's new and the best approximation of the goal at this pose.
        Also, to avoid aliasing, we consider the goal orientation, must be the same between the pose and goal.

        """
        motion_pose = current_pose.copy()  
        for action_idx in range(len(policy)): 
            motion_pose = action_to_pose(policy[action_idx], motion_pose)
            pose = list(motion_pose)
            #CHECK IF IMAGE OF CORRECT SHAPE FOR COMPARISON
            mse_ob = mse_elements(pred[key][:,action_idx,...], goal['image'])
            print("this is the mse_ob, motion and pose", mse_ob,pose,motion_pose)
            if manager.mse_under_threshold(mse_ob, sensitivity=0.76):
                #only consider poses having the same orientation. To avoid aliasing
                #+ We take the best mse under threshold of a given pose
                key_pose  =tuple(pose)
                #We consider the length of the policy, the shortest it is, the higher the weightaccount
                #Thus the less steps there is in policy, the higher the mse.
                weight = 1/len(policy[:action_idx+1])
                #We consider when we have no goal pose and search one
                #or if we want to validate an approximated pose goal
                if 'pose' not in goal or goal['pose'][2] == pose[2] \
                    and (key_pose not in goal_matching_mse_pose\
                    or mse_ob*weight < goal_matching_mse_pose[key_pose]['weighted_mse']):
                    goal_matching_mse_pose[key_pose] = {'weighted_mse':mse_ob*weight, 'policy': policy[:action_idx+1]}

                    print(transform_policy_from_hot_encoded_to_str(policy[:action_idx]))
                    # visualise_image(ego_prediction[key][:,action_idx,...], title="pred pose" + str(pose), fig_id=len(goal_matching_mse_pose)+11)
                    # visualise_image(goal['image'], title="goal pose" + str(goal['pose']), fig_id=10)
        return goal_matching_mse_pose
    
    def get_policy_with_min_mse_pose(self, goal_matching_mse_pose:dict):
        """ given the dict of the poses having the lowest mse """
        if goal_matching_mse_pose:
            pose_tuple, pose_info = min(goal_matching_mse_pose.items(), key=lambda item: item[1]['weighted_mse'])
            # goal['pose'] = torch.tensor(pose_tuple)
            return pose_info['policy']
        return []

    def define_explo_policies(self, manager:object, current_pose:list)-> list:
        ''' 
        get the lookahead and divise it by 2 to stay near agent
        then look all around agent to set goals
        and create policies to reach all those goals
        '''
        search_lookahead = int(manager.get_current_lookahead()/2) #/2 is a random choice
        goals_explo_pose = define_policies_objectives(current_pose, lookahead=search_lookahead, full_exploration=True)
        policies = create_policies(current_pose, goals_explo_pose)
        return policies
#========================= FIND FEATURE IN PRED OB =====================================#

    def allo_match_preferred_features_wt_pose(self,manager:object, preferred_features:np.ndarray)-> dict:
        """
        For each location in memory, check if we find matching pref features. 
        If we do, we stop going through this exp and redo the process on the next exp
        from cloest to furthest to agent pose.
        NOTE: This is highly computationally intensive as it consider all possible poses in each exp
        Could be reduced, it is not to limit code quantity and allow easy reading.
        """
        poses_options_in_this_env = call_env_place_range(self.env_type, reduction = 2)
        exps = manager.get_exps_organised_by_distance_from_exp()

        preferrences_in_memory = {}
        for exp in exps:
            place = manager.torch_sample_place(exp['observation'])
            allo_prediction = manager.several_poses_allocentric_prediction(poses_options_in_this_env, place)
            for pose_idx in range(len(poses_options_in_this_env)):
                pref_pose, pref_likelihood = find_preferred_features_in_img(self.env_type, allo_prediction['image_predicted'][:,pose_idx,...], poses_options_in_this_env[pose_idx], preferred_features)
                if pref_pose is not None:
                    pref_pose_allo_pred = manager.single_pose_allocentric_prediction(pref_pose, place)
                    preferrences_in_memory[exp['id']]={'exp_id': exp['id'], 'pose':pref_pose, 'image':pref_pose_allo_pred['image_predicted'], 'pref_likelihood': pref_likelihood}
                    if self.verbose:
                        print(exp['id'], 'has found a preferred ob at pose', pref_pose, 'that is near the desired feature by:', pref_likelihood, '%') 
                    break #Only 1 pref ob by exp

        return preferrences_in_memory

#========================= FIND PATH TO DESIRED EXP ====================================#

    def apply_djikstra_over_memory(self, manager:object, objective:dict, weigth_on_unconnected_exps:float=100)->list:
        """ 
        Search shortest path using Djikstra between current exp and objective exp
        The djikstra: return a path containing the exps ID from current o objective.

        """
        exps_list = manager.get_all_exps_in_memory(wt_links = True)
        current_exp_id = manager.get_current_exp_id()
        #Get all the exps and their 'distance' between each other
        djikstra_exps_list = dijkstra_weigthed(exps_list, current_exp_id, weigth_on_unconnected_exps)
        #Find the target_exp in this list 
        target_exp_index_in_list = [exp['id'] for exp in djikstra_exps_list].index(objective['exp'])
        target_exp = djikstra_exps_list[target_exp_index_in_list]
        if objective['exp'] != target_exp['id']:
            raise ValueError("goal_id != target id in apply_goal_strategy, IMPLEMENTATION ERROR")
        
        #The first value of path is current exp, the last is the desired exp
        path = dijkstra_shortest_node_path(target_exp, [target_exp['id']])[::-1]
        print('The shortest node path from current to goal exp:  %s' %(path))

        if len(path) <= 1:
            raise ValueError("path " + str(path)+ "is impossible. ")
        
        return path

#========================= FIND NEXT PLACE IN PATH CONNECTED TO CURRENT ================#
    def determine_location_to_reach_in_path(self,manager:object,path:list)->dict:
        """ Search door connection poses between this place and linked exps,
        Then check if any of those exps lead to an ep in desired path
        IF no then we return {}
        Else we return the place to go to in path
        """
        #get all the connected places of current through doors
        linked_exps_info = manager.connect_places_to_current_location()
        if not linked_exps_info:
            print('ERROR in determine_location_to_reach_in_path:The current exp'+ str(manager.get_current_exp_id()) + 'does not link to any other exp' )
            return {}
        
        if self.verbose:
            print('This place is connected to:', linked_exps_info)
        
        #find which linked exp is closest to objective 
        place_index = -1
        for item in path[::-1]:
            for idx, idx_dict in linked_exps_info.items():
                if item == idx_dict['linked_exp_id']:
                    place_index = idx
                    break
            if place_index != -1:
                break

        #This is to push agent toward the exp having closest value to objective...But it's really situational
        # if place_index == -1 and len(linked_exps_info) > 0:
        #     place_index = min(linked_exps_info.keys(), key=lambda i: abs(linked_exps_info[i]['linked_exp_id'] - path[-1]))

        if place_index != -1:
            return linked_exps_info[place_index]
        
        return {}

class Goal_seeking_Minigrid(ExploitativeBehaviour):
    def __init__(self, env_type:str, verbose:bool=True,) :
        ExploitativeBehaviour.__init__(self, verbose)
        self.env_type = env_type

    def go_to_given_place(self, manager:object, door_to_place_to_go:list):
        '''
        given place to go, search policy to reach this pose, 
        then add a bit of a push to pass the corridor up to next place
        '''
        objective_to_reach = self.set_door_pose_as_goal_in_place(manager, door_to_place_to_go)
        print("this is the objective_to_reach", objective_to_reach)
        policy, error = self.go_to_observation_in_place(manager,objective_to_reach)
        print("this is the final policy", policy)
        if error:
            print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
            print('ERROR: The goal allocentric prediction is incorrect, please implement a solution')
            print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
        #if we are near the objective, we can assume there is been no long 
        #range mis-prediction and go straight ahead of this door pose to pass the
        #corridor to the next place
        if len(policy) < 3 and not error :
            policy = policy + [[1, 0, 0] for _ in range(5)]
            p_idx = len(policy)
        else:
            p_idx = 1
        
        return policy, p_idx

    def set_door_pose_as_goal_in_place(self, manager:object, pose_to_go:list)->dict:
        '''
        Given a goal pose corresponding to a relevant_ob (door), 
        determine where it is IN ROOM
        '''
        door_dict = manager.get_env_relevant_ob()
        door_ob = torch.mean(door_dict['image'], dim=0).squeeze(0)
        current_exp_id = manager.get_current_exp_id()
        sub_goal = self.set_objective(pose_to_go, current_exp_id, door_ob)
        return sub_goal

    #========================= FIND FEATURE IN PRED OB ====================================#
    def ego_match_preferred_features_wt_pose(self,manager:object, preferred_features:np.ndarray):
        """ 
        Search in policies around agent if a predicted observation contains a detail matching
        preferred_features. If so return that pose and corresponding observation, else return None
        """
        pref_goal = {}
        place_description = manager.get_best_place_hypothesis()
        current_pose = place_description['pose'].copy()
        
        policy = [[-1,0,0]]*3
        policy = self.evaluate_go_to_goal_pose_policies(manager, [policy])[0]
        next_exp_ahead = self.passing_from_exp_to_exp(manager, place_description, policy)
        policies = self.define_explo_policies(manager, current_pose)
        for policy in policies:
            policy, ego_prediction = manager.policy_egocentric_prediction(policy)
            if len(policy) == 0:
                continue
            agent_pose = current_pose
            for action_idx in range(len(policy)):
                agent_pose = action_to_pose(policy[action_idx], agent_pose)
                pref_pose, _ = find_preferred_features_in_img(self.env_type, ego_prediction['image_reconstructed'][:,action_idx,...], agent_pose, preferred_features)
                if pref_pose is not None :
                    if not next_exp_ahead:
                        policies_to_pref = create_policies(current_pose, [pref_pose], exploration=False)
                        policies_to_pref = self.evaluate_go_to_goal_pose_policies(manager, policies_to_pref)
                        #We are not checking their plausibility, just if we see a relevant ob meaning a place limit
                        for policy_to_pref in policies_to_pref:
                            next_exp_ahead = self.passing_from_exp_to_exp(manager, place_description, policy_to_pref)
                            if next_exp_ahead:
                                break
                    if next_exp_ahead and next_exp_ahead['id'] >= 0:
                        #If the goal is in another place, then reaching this place is the priority
                        pref_goal = {'exp_id': next_exp_ahead['id']}
                    else:
                        allo_pred_ob = manager.single_pose_allocentric_prediction(pref_pose,place_description['post'])['image_predicted']
                        allo_pred_ob = torch.mean(allo_pred_ob, dim=[0,1])
                        pref_goal = {'pose': pref_pose, 'exp_id': manager.get_current_exp_id(), 'image':allo_pred_ob}
                    #TODO: CONSIDER ALL ORIENTATION SINCE WE SHOULDN'T CARE, currently we consider one at "random"
                    if self.verbose:
                        print("pref_pose",pref_pose," seen at action", action_idx, 'of policy', transform_policy_from_hot_encoded_to_str(policy))
                        print(', '.join([f'{key}: {value}' for key, value in pref_goal.items() if key != 'image']))
                    return pref_goal
        return {}

    def passing_from_exp_to_exp(self, manager, place_description, policy):
        """ we check if this policy leads to another place acording to relevant ob match
        if so we retrieve the next place, next exp id and the connecting door
        """
        
        current_pose = place_description['pose']
        place = place_description['post']
        door_dict = manager.get_env_relevant_ob()

        goal_matching_mse_pose = self.estimate_policy_allo_pred_matching_goal(manager, place, current_pose, policy, door_dict, {})
        print('passing_from_exp_to_exp', goal_matching_mse_pose, 'policy', transform_policy_from_hot_encoded_to_str(policy))
        next_exp_ahead =  {}
        if goal_matching_mse_pose:
            pose_tuple, _ = min(goal_matching_mse_pose.items(), key=lambda item: item[1]['weighted_mse'])
            limit_pose = list(pose_tuple)
            next_exp_ahead['current_exp_limit_pose'] = limit_pose
            next_exp_ahead['place'], next_exp_ahead['id'], next_exp_ahead['limit_pose'] = manager.get_connected_place_info(limit_pose)

        return next_exp_ahead


        
```

`hierarchical-nav/navigation_model/Processes/explorative_behaviour.py`:

```py
from itertools import product
import numpy as np
import torch
from torch.distributions.categorical import Categorical

from env_specifics.env_calls import call_process_limit_info
from env_specifics.minigrid_maze_wt_aisles_doors.minigrid_maze_modules import (
    from_door_view_to_door_pose, is_agent_at_door_given_ob)
from navigation_model.Processes.AIF_modules import (calculate_KL,
                                                    estimate_a_surprise)
from navigation_model.Processes.motion_path_modules import (
    action_to_pose, create_policies, define_policies_objectives)
from navigation_model.visualisation_tools import (
    transform_policy_from_hot_encoded_to_str, visualise_image)


class ExplorativeBehaviour():
    def __init__(self, possible_actions: list, verbose:bool=True) :
        self.possible_actions = possible_actions
        self.verbose = verbose
        self.place_info_gain = []
        self.latest_EFE = None

        self.agent_exploring = True
        self.rolling_info_gain = []

    def set_latest_policy_EFE(self, G:float)-> None:
        self.latest_EFE = G

    
#====================== Get Methods ===================================================================
    def define_is_agent_exploring(self, info_gain_coeff:float, policy_G:float, threshold:int=1)-> bool:
        """ 
        according to the info gain coeff, the policy_G and the threshold
        return whether the agent is stuck in a local minima 
        and doesn't explore anymore
        The agent explore if it is surprised enough in env or it has an idea where to explore next
        For teh agent to reach an info gain coeff < 0.001, it is expected to need about 10-30steps in a room
        """
        print("why no explore?",abs(info_gain_coeff),(threshold / 100) ,abs(info_gain_coeff) < (threshold / 100),policy_G < (threshold/ 3),policy_G ,(threshold/ 3)  )
        #if (abs(info_gain_coeff) < (threshold / 1000) and policy_G < (threshold/ 10)) :
        if (abs(info_gain_coeff) < (threshold / 300) and policy_G < (threshold/ 3)) :
            self.agent_exploring = False
        else:
            self.agent_exploring = True

        if self.verbose:
            print('is agent exploring', self.agent_exploring,' updated with info gain coeff', info_gain_coeff, \
                  'vs TH', threshold / 1000, 'policy', policy_G, 'vs TH', threshold/ 10)
        return self.agent_exploring
     
    def is_agent_exploring(self)-> bool:
        """ 
        return whether the agent is stuck in a local minima 
        and doesn't explore anymore
        """
        return self.agent_exploring 
         
    def get_latest_policy_EFE(self, reset_EFE:bool = False)->float:
        """
        Retrieve latest generated EFE, 
        if reset_EFE is True then we erase it upon return
        """
        EFE = self.latest_EFE
        if reset_EFE:
            self.latest_EFE = None
        return EFE
    
    def get_policies_info_gain(self, manager:object, policies:list)->tuple[list,list]:
        """ 
        info gain based motion.
        the lowest G based on KL gets it all, the OZ model cut action 
        sequences based on its collision detection, then each action 
        is passed through the allo model pred
        """
        #possible policies are returned as a list of Tensor
        policies = manager.get_plausible_policies(policies)
        paths_KL = []
        imagined_doors_info = self.initialise_place_doors_knowledge(manager)
        for policy in policies:
            path_steps_KL,imagined_doors_info = self.path_predicted_info_gain(manager,policy,imagined_doors_info)
            paths_KL.append(np.mean(path_steps_KL))
        return policies, paths_KL
    
    def moving_average_info_gain(self, window:int=5)->float:
        """
        Calculates the average of the info_gain over a fixed window of time.
        Not ideal with info_gain, would be more adapted to KL measuring instead
        """
        window = min(window, len(self.rolling_info_gain))
        if len(self.rolling_info_gain) < window:
            ma_info_gain = 1
        else:
            ma_info_gain = sum(self.rolling_info_gain) / window
        return ma_info_gain
    
    def rolling_coeff_info_gain(self, window:int=5)->float:
        """
        Calculates the coefficient over a rolling window of info_gain.
        only if the rolling_info_gain list is at least as big as the window.
        else returns a static high value s= 10 
        """
        window = min(window, len(self.rolling_info_gain))
        if len(self.rolling_info_gain) < window:
            rc_info_gain = 10
        else:
            rc_info_gain = (self.rolling_info_gain[-1] - self.rolling_info_gain[0]) / window
        return rc_info_gain
    def update_rolling_info_gain(self, info_gain:float, window:int = 10)->None: 
        ''' 
        we store the info gain in a memory list of size window.
        The info_gain is stored up to windows size (with newest as last and oldest as first)
        when window size reached, the 1st (oldest) value pop up 
        '''
        self.rolling_info_gain.append(info_gain)
        #we only want a sliding X steps info gain
        while len(self.rolling_info_gain) > window and len(self.rolling_info_gain)>0:
            del self.rolling_info_gain[0] 

        if self.verbose:
            print('updated rolling info gain', self.rolling_info_gain)

#====================== Create Policies =============================================================
    
    def create_policies(self, manager:object, lookahead:int)-> list:
        full_exploration = manager.get_confidence_about_place_description()
        current_pose = manager.get_best_place_hypothesis()['pose'].copy()
        print('In exploration create_policies, verifying if confident about place description', full_exploration)
        goals = define_policies_objectives(current_pose, lookahead, full_exploration)
        policies = create_policies(current_pose, goals)
        return policies

#====================== Exploration METHODS===============================================================

    def one_step_ego_allo_exploration(self,manager:object):
        """
        create policies, assess their info gain and select the policy 
        that would modify the believes the most if proven true by ob (worth checking)
        return the policy and how many consecutive actions we want to apply from it
        Since it's one step exploration. We only apply the first action of the policy
        """
        lookahead = manager.get_current_lookahead()
        policies = self.create_policies(manager, lookahead)
        policies, paths_KL = self.get_policies_info_gain(manager,policies)
        chosen_policy, policy_G = self.curiosity_driven_policy_selection(policies, paths_KL)
        self.set_latest_policy_EFE(policy_G)

        return chosen_policy, 1
    
    def curiosity_driven_policy_selection(self, policies:list, paths_KL:list) -> list:
        """
        select lowest KL policy amonst all the choices.

        """
        Gs = paths_KL
        #TODO: curiosity temp do not exist
        categorised_G = Categorical(logits= self.curiosity_temp * torch.tensor(Gs) )
        index = categorised_G.sample()
        policy_G =  paths_KL[index]
        policy = policies[index]
        if self.verbose :
            print('IN Exploration: curiosity_driven_policy_selection')
            print('stochastic curiosity policy, selected G:', policy_G , ',policy:', transform_policy_from_hot_encoded_to_str(policy.tolist()))
            print('The highest G was:',max(paths_KL))
            print('______________')
        return policy, policy_G
    #based on ego_step_decision
    def one_step_egocentric_exploration(self, manager:object, lookahead:int=5):
        """
        Use the egocentric model with a X steps lookahead to move the agent
        The policies considered not leading in a wall are considered for a G assessment
        then G is Cat and softmax and we consider a third of the selected policy to move
        the agent out of current pose
        """
        policies = self.create_policies(manager, lookahead)
        ego_posterior = manager.get_egocentric_posterior()
        Gs = []
        considered_policies = []
        print('IN one_step_egocentric_exploration')
        for policy in policies:
            policy, ego_prediction = manager.policy_egocentric_prediction(policy)
            if len(policy) > 0:
                ego_posterior_sampled = ego_posterior.unsqueeze(1).repeat(1,len(policy),1)
                kl, ambiguity, _ = estimate_a_surprise(ego_prediction['prior'], preferred_state= ego_posterior_sampled,
                                    predicted_obs=ego_prediction['image_reconstructed'], 
                                    preferred_ob=torch.zeros_like(ego_prediction['image_reconstructed']))
                
                if np.isinf(ambiguity):
                    ambiguity = -2*len(policy)
                #Kl is postive, ambiguity is negative
                Gs.append(-(kl + ambiguity))
                print(len(Gs)-1, Gs[-1],transform_policy_from_hot_encoded_to_str(policy))
                considered_policies.append(policy)

        categorised_G = Categorical(logits= -0.7 * torch.tensor(Gs)) 
        index_best_option = categorised_G.sample()
        print('Chosen policy and Gs', Gs[index_best_option], transform_policy_from_hot_encoded_to_str(considered_policies[index_best_option]))
        chosen_policy = considered_policies[index_best_option]
        num_action_to_apply = int(np.ceil(len(chosen_policy)/3))
        print('applying ', num_action_to_apply, ' 1st actions from this policy')
        #We didn't average the G per step before to favorise long policies. 
        self.set_latest_policy_EFE(Gs[index_best_option]/len(chosen_policy))
        return chosen_policy, num_action_to_apply
    
#====================== RESOLVING HESITATION OVER PLACE ==================================================
    def solve_doubt_over_place(self, manager:object) -> list:
        """ 
        Consider allo and ego models to converge to
        a single place description         
        """
        
        plausible_actions, ego_predicted_images = self.define_plausible_actions(manager)
   
        hypothesis_predictions, best_hypothesis = self.solve_hypothesis(manager, plausible_actions,ego_predicted_images)
        
        if len(best_hypothesis) == 0:  
            print('no hypothesis standing from the rest, considering them all')  
            best_hypothesis = list(hypothesis_predictions.keys())
        print('Best hypotheses', best_hypothesis)

        kl_per_action, summed_ambiguity_per_action, actions_weight = \
            self.consider_kl_ambiguity_per_action(hypothesis_predictions, best_hypothesis, len(plausible_actions))
        log_kl_per_action = self.log_kl(kl_per_action)
        G_policies = self.calculate_policies_EFE(actions_weight, log_kl_per_action, summed_ambiguity_per_action)

        best_action = np.argmin(G_policies)
        chosen_action = plausible_actions[best_action]
        if self.verbose:
            print('_')
            print('solving doubt over place')
            print(len(best_hypothesis),'hypothesis being considered',len(plausible_actions))
            print('kl_per_action:', kl_per_action)
            print('summed_ambiguity_per_action:', summed_ambiguity_per_action)
            print('actions_weight', actions_weight)
            print('For policies', plausible_actions, ', EFE:', G_policies)
            print('Chosen action', chosen_action)
            print('_')

        return [chosen_action], 1
    
    def define_plausible_actions(self, manager:object) -> tuple[list, list]:
        '''
        determine which actions are doable and what is the ego prediction for those
        '''
        plausible_actions = []
        ego_predicted_images = []
        for action in self.possible_actions:
            action, ego_prediction = manager.policy_egocentric_prediction([action])
            if any(isinstance(el, list) for el in action):
                action = action[0] #just remove extra list
            if len(action) >0:
                plausible_actions.append(action)
                ego_predicted_images.append(ego_prediction['image_reconstructed'].squeeze(1))
        return plausible_actions, ego_predicted_images
    
    def solve_hypothesis(self, manager:object, plausible_actions:list, ego_predicted_images:list) -> tuple[dict, list]:
        """  
        We want to know the prediction of each hypothesis place and pose for each policy
        If we estimate the prediction to be good:
        what is the expected change to the place's hypothesis? 
        How sure is the hypothesis about its output?
        it is stored ina  dict.

        We also return a list of the best hypothesis solving all policies.
        mean Time: 9.55s / 254hypothesis
        """
        
        best_hypothesis = []
        hypothesis_predictions = {}
        place_hypothesis = manager.get_all_places_hypothesis()
        print('How many hypothesis to process:', len(place_hypothesis))
        
        for place_description in place_hypothesis.values():
            hypo_idx = len(hypothesis_predictions)

            for i, action in enumerate(plausible_actions):
                starting_pose = place_description['pose'].copy()
                pose_queries = action_to_pose(action, starting_pose)
                hypo_prediction = manager.single_pose_allocentric_prediction(pose_queries, place_description['post'][:])
                kl, ambiguity, pred_expected_error = estimate_a_surprise(place_description['post'][:], hypo_prediction['place'], hypo_prediction['image_predicted'][:,0], ego_predicted_images[i])
                

                if manager.mse_under_threshold(pred_expected_error, sensitivity=1):
                    if not hypo_idx in hypothesis_predictions:
                        hypothesis_predictions[hypo_idx] = {}
                    if kl > 10:
                        kl = 10
                    if np.isinf(ambiguity):
                        ambiguity = -2
                    hypothesis_predictions[hypo_idx][i] = {'kl': kl, 'ambiguity': ambiguity}

            #if this hypothesis predicts all actions well
            if hypo_idx in hypothesis_predictions and len(hypothesis_predictions[hypo_idx].keys()) == len(plausible_actions):
                best_hypothesis.append(hypo_idx) 

        return hypothesis_predictions, best_hypothesis

    def solve_hypothesis_option2(self, manager:object, plausible_actions:list, ego_predicted_images:list) -> tuple[dict, list]:
        """  
        OPTION 2 OF THE CODE, REALISED TO SPEED COMPUTING. mean Time: 9.26s / 254hypothesis
        ONLY 0.3s faster but less readable. So not used (kept as ref)
        We want to know the prediction of each hypothesis place and pose for each policy
        If we estimate the prediction to be good:
        what is the expected change to the place's hypothesis? 
        How sure is the hypothesis about its output?
        it is stored ina  dict.

        We also return a list of the best hypothesis solving all policies.
        """
        best_hypothesis = []
        hypothesis_predictions = {}
        place_hypothesis = manager.get_all_places_hypothesis()
        print('How many hypothesis to process:', len(place_hypothesis))
        for place_description in place_hypothesis.values():
            starting_pose = place_description['pose'].copy()
            pose_queries = [list(action_to_pose(a, starting_pose)) for a in plausible_actions]
            hypo_prediction = manager.several_poses_allocentric_prediction(pose_queries, place_description['post'][:])
            kl_amb_pred_errors = [estimate_a_surprise(place_description['post'][:], hypo_prediction['place'][:,action_id], hypo_prediction['image_predicted'][:,action_id], ego_predicted_images[action_id]) \
                                for action_id in range(len(plausible_actions))]
            #get the index of the actions giving predictions under the mse threshold
            valid_actions = [i for i, (_, _, pred_err) in enumerate(kl_amb_pred_errors) \
                            if manager.mse_under_threshold(pred_err, sensitivity=1)]
            #if any actions are valid, we consider adding them to the dict
            if len(valid_actions) > 0:
                hypo_idx = len(hypothesis_predictions)
                hypothesis_predictions[hypo_idx] = {
                    id_action: {'kl': kl, 'ambiguity': -2 if np.isinf(ambiguity) else ambiguity}
                        for id_action, (kl, ambiguity, _) in zip(valid_actions, kl_amb_pred_errors)
                    }

            #if this hypothesis predicts all actions well
            if len(valid_actions) == len(plausible_actions):
                best_hypothesis.append(hypo_idx)
        return hypothesis_predictions, best_hypothesis
    
    def consider_kl_ambiguity_per_action(self,hypothesis_predictions:dict, best_hypothesis:list, len_plausible_actions:int) -> tuple[list,list,list]:
        """
        determine the Kl and ambiguity of each action of the considered hypothesis
        We want to know the weight of each action (how many time it resulted in 
        an estimated correct prediction) 
        """
        
        actions_in_each_hypo = [list(ids.keys()) for ids in hypothesis_predictions.values()] 
        kl_per_action = [[] for _ in range(len_plausible_actions)]
        summed_ambiguity_per_action = [0]*len_plausible_actions
        actions_weight = [0]*len_plausible_actions
        for idx_hypo in best_hypothesis:
            for id_action in actions_in_each_hypo[idx_hypo]:
                actions_weight[id_action]+=1
                kl_per_action[id_action].append(hypothesis_predictions[idx_hypo][id_action]['kl'])
                summed_ambiguity_per_action[id_action] += hypothesis_predictions[idx_hypo][id_action]['ambiguity']

        return kl_per_action, summed_ambiguity_per_action, actions_weight

    def log_kl(self, kl_per_action:list) -> list:
        ''' 
        in order to homogeneise the KL and to have comparable values we log them 
        If any value is nan, we convert it to 0.
        '''
        #if no hypothesis valorise an action, we create a 0 value
        #so the list is not empty or full of nan
        kl_per_action = [sublist if (sublist and not np.isnan(sublist).all()) else [0] for sublist in kl_per_action]
        min_kl = np.min(list(map(np.nanmin, kl_per_action)))
        #we want the min value to be 0, not under, 
        # thus before doing a log we make sure no value is under 1
        if min_kl < 1:
            log_kl_per_action = [[np.log(kl + 1 - min_kl) if not np.isnan(kl) else 0 for kl in sub_list] for sub_list in kl_per_action]
        #we are also converting any np.nan value to 0 if there is any
        else:
            log_kl_per_action = [[np.log(kl) if not np.isnan(kl) else 0 for kl in sub_list] for sub_list in kl_per_action]
        return log_kl_per_action

    def calculate_policies_EFE(self,actions_weight:list, log_kl_per_action:list, summed_ambiguity_per_action:list)-> list:
        """
        For each action we compute the sum of KL multiplied by the weight of the action + 
        the ambiguity reduced by the total number of action 
        Thus to armonise the two values and have a total value that consider both arguments
        """
        G_policies = []
        tot_num_actions_considered = np.sum(actions_weight)
        for i in range(len(actions_weight)):
            if actions_weight[i] == 0:
                G_policies.append(0)
                continue
            percentage_action = actions_weight[i] / tot_num_actions_considered

            G = - np.sum(log_kl_per_action[i]) * percentage_action + summed_ambiguity_per_action[i]/ tot_num_actions_considered
            G_policies.append(G)
        return G_policies

class Exploration_Minigrid(ExplorativeBehaviour):
    def __init__(self, env_type:str, possible_actions:list, curiosity_temp:float=100, verbose:bool=True,) :
        ExplorativeBehaviour.__init__(self, possible_actions, verbose)
        self.env_type = env_type
        
        self.curiosity_temp = curiosity_temp
    
        #TEST MODULE
        #set to True to visualise generated policies
        self.visualise_policy = False
        self.test_id = 11

    def setting_prediction_and_comparison_places(self, manager:object, \
                                                 changing_place:int, imagined_doors_info:dict)\
                                                    -> tuple[torch.Tensor, torch.Tensor] :
        '''
        Are we passing a door? 
        if so we want to use other place to predict observations : prediction_post
        is the other place new? 
        if so we want to use current place to measure the surprise of new room 
        We are not passing a door:
        well then prediction and comparison place are the same
        '''
        #If we are passing a door, we use the predicted place as posterior for prediction and info gain 
        if changing_place >= 0:
            posterior_for_kl_ref = imagined_doors_info[changing_place]['connected_place']
            if posterior_for_kl_ref is None:
                posterior_for_kl_ref = manager.get_best_place_hypothesis()['post'][:]
                normal_dist_place = np.concatenate((np.zeros(32), np.ones(32)), axis=0).astype(int)
                prediction_post = manager.torch_sample_place(normal_dist_place)  #  multivariate_distribution(torch.zeros(1, 1, 32), torch.ones(1, 1, 32))
            else:
                prediction_post = posterior_for_kl_ref
            return posterior_for_kl_ref, prediction_post

        posterior_for_kl_ref = manager.get_best_place_hypothesis()['post'][:]
        prediction_post = posterior_for_kl_ref
            
        return posterior_for_kl_ref, prediction_post

#====================== Door Limit logic ==========================================================
    def initialise_place_doors_knowledge(self, manager:object) -> dict:
        """
        We retrieve all the doors present in memory for this place 
        and organise all the details to know if we crossed this door 
        Each entry of the dict contains:
        door_pose
        motion_axis
        direction
        origin_place
        connected_place
        exp_connected_place
        connected_place_door_pose
        """
        imagined_doors_info = {}
        '''current_exp_door_poses = manager.get_location_limits()
        for pose in current_exp_door_poses:
            imagined_doors_info = self.new_entry_in_imagined_doors_info(manager, imagined_doors_info, pose)'''
        return imagined_doors_info
    
    def new_entry_in_imagined_doors_info(self, manager:object, imagined_doors_info:dict, pose:list)-> dict:
        ''' create a door info entry in dict given a door pose, add it to dict'''
        pose_info = self.create_pose_limit_info_entry(manager,pose)
        new_door_entry = len(imagined_doors_info)
        imagined_doors_info[new_door_entry] = pose_info
        return imagined_doors_info
    
    def create_pose_limit_info_entry(self, manager:object, pose:list) -> dict:
        ''' Get all the infos necessary for place change given the door pose '''
        #initialise the entry dict
        door_info = call_process_limit_info(self.env_type,pose)
        #complete the entry dict
        door_info = self.door_info_update(manager, door_info)
        return door_info
    
    def door_info_update(self, manager, door_info):
        ''' 
        Add all we know about this door 
        (which place it leads to and through which door)
        '''
        #If we have yet to imagine what to expect passed this particular door
        posterior, expected_exp_id, place_door_pose = manager.get_connected_place_info(door_info['door_pose'])
        door_info['connected_place'] = posterior
        door_info['origin_place'] = manager.get_best_place_hypothesis()['post']
        door_info['exp_connected_place'] = expected_exp_id
        door_info['connected_place_door_pose'] = place_door_pose #not usefull to keep, but may be useful in later code changes
        return door_info

#====================== STEP BY STEP PREDICTION =====================================================

    def path_predicted_info_gain(self, manager:object, policy:list, imagined_doors_info:dict):
        """
        Adapted to Minigrid maze environment with doors and such,
        Get the KL of the prediction considering passing from place to place
        """

        #VISUALISATION TEST
        # predicted_img_seq = None

        #==== INIT ====#
        changing_place = -1
        path_steps_kl = []
        sequential_pose = manager.get_best_place_hypothesis()['pose'].copy()
        saved_current_place_pose = []
        info_printed_once = False

        for action in policy:
            sequential_pose = action_to_pose(action, sequential_pose)
            if existing_an_alternative_pose(saved_current_place_pose):
                saved_current_place_pose = action_to_pose(action, saved_current_place_pose)
            
            #Verify if the action leads to another place
            changing_place, sequential_pose, saved_current_place_pose = \
                self.verify_changing_place(manager, imagined_doors_info, \
                                           changing_place, sequential_pose, saved_current_place_pose)
            
            if self.verbose and changing_place >= 0 and not info_printed_once:
                print('changing place with policy:', transform_policy_from_hot_encoded_to_str(policy.tolist()))
                info_printed_once = True
            
            #Setting the posterior we are getting the observation from and the posterior we compare to
            posterior_for_kl_ref, prediction_post = \
                self.setting_prediction_and_comparison_places(manager, changing_place, imagined_doors_info)

            predicted_step = manager.single_pose_allocentric_prediction(sequential_pose, prediction_post)
                                           
            #--- Verify if we predict a door_view at this pose and if it's a new door  ---#
            imagined_doors_info = self.recognising_place_limit(manager, imagined_doors_info, predicted_step['image_predicted'], sequential_pose)

            #NOTE: reconstruction error has no meaning here, since it's pure prediction
            kl = float(calculate_KL(predicted_step.place, posterior_for_kl_ref).cpu().detach().numpy())   
            path_steps_kl.append(kl)
            
        #     if self.visualise_policy:
        #         #Do we want to see each policy predicted steps
        #         if predicted_img_seq is None:
        #             predicted_img_seq = predicted_step['image_predicted']
        #         else:
        #             predicted_img_seq = torch.cat((predicted_img_seq,predicted_step['image_predicted']),dim=1)
                         

        # if self.visualise_policy:
        #     str_policy = transform_policy_from_hot_encoded_to_str(policy)
        #     #if str_policy[0] == 'F':
        #         #Only watch the policies starting by going forward, to adapt at wish
        #     print('for policy', self.test_id, str_policy)
        #     print('G:', np.mean(path_steps_kl), 'steps KL', path_steps_kl)
        #     print('_')
        #     visualise_image(predicted_img_seq,title= self.test_id, fig_id=self.test_id)
        #     self.test_id += 1
            

        return path_steps_kl, imagined_doors_info

#====================== Place Changing logic ========================================================
    def recognising_place_limit(self, manager:object, imagined_doors_info:dict, \
                                predicted_image:torch.Tensor, sequential_pose:list) -> dict:
        '''
        Verify if we predict a door_view at this pose and if it's a new door. If it is we add it to the door dict

        '''
        pose = list(sequential_pose)
        pose_at_door = is_agent_at_door_given_ob(manager,predicted_image, pose, sensitivity=0.18)
        if pose_at_door :
            known_door_poses = [list(info['door_pose']) for info in imagined_doors_info.values()]
            if list(pose) not in known_door_poses:
                print('new door pose entry in memory:', pose, 'searching corresponding place')
                imagined_doors_info = self.new_entry_in_imagined_doors_info(manager,imagined_doors_info, pose)

        return imagined_doors_info
    
    def verify_changing_place(self,manager:object, imagined_doors_info:dict, \
                              changing_place:int, sequential_pose:list, saved_current_place_pose:list) -> tuple[int, list, list]:
        '''
        For each known door position we check if we are (or have passed) the door step. 
        If we are changing place (we are at door step) we mark it an verify which pose to replace it with (known place: place POV pose;
        new place: reset pose)
        Below a detailed description of the whole code:
        '''
        """
        # for each door in place we have to check if we passed that door (thus changing pov) (line 409)
        # FIRST OF ALL If we are in the process of passing a door and we are not considering the correct door.
        # Pass until we are at that door
        # NOW, to check if we are indeed passing a door
        # For that we consider the direction the agent is taking vs the direction it needs
        #       to take to pass that door, as well as which axis the agent needs to move on 
        #       (basically X or Y and - or + motion)
        #       the linear_motion is the value holding the direction of the correct axis
        # Thus we check if the sign of the linear motion is the exact same as the motion needed to pass the door
        # IF IT IS 
        # Have we passed the door and not facing back toward origin place? 
        # For that we check if we have passed the door AND we should not face backward 
        #          to it (face original place instead)
        # IFEVERYTHING HOLD TRUE up to now 
        # then we know we are on the verge of passing a door and which one. so we mark it
        # doors are not linked to exp id in memory, as such we previosuly inferred how both places are connected
        # so we need to know for the prediction: the other place and the door that links the 2 
        # (to get relative pose pov of other room)
        # THEN, have we passed the door for the 1st time ? 
        # we replace the current place pose with other place pose
        # New place: pose reset
        # known place: other room pose
        # we save the current room pose as well in case we go back to it
        # NOTE: since the door pose is the pose where we can see the closed door... 
        # we know the actual doorstep is 2 steps away.
        # NOTE: Since we change room reference the orientation we had in previous reference does not holds true anymore
        # thus we consider the global reference to get the position in the new room coordinate and have the orientation
        #//COMMENTED PART we check if we are not facing the door
        #if we don't, then we are not passing the door
        #//
        # Finally we check if, for this door, we are not facing opposite to the door after having switched pose pov
        #If so then we switch back to prev/current room pov again and mark that we are NOT passing a door 
        # if we are passing a door, then don't check the other doors, the work is done

        # with this we finished the door passing logic
        
        """
        for key, door_info in imagined_doors_info.items():
            #speed up process. we assume we can only pass 1 door by policy 
            # + it avoids issues if we reset the pose and it matches another door pose
            if changing_place >=0 and key != changing_place :
                continue
            index = door_info['motion_axis']
            linear_motion = sequential_pose[index] - door_info['door_pose'][index]
                            
            #If we are in front of door or at/passed door step 
            # //passed door view pose on the correct axis and direction
            if np.sign(linear_motion) == door_info['direction']:
                # If we are at least at door step AND we are not facing the current room
                #NOTE: when we are just facing open door, we do not change place of reference 
                # (that is a change from test phase)
                if passed_limit(linear_motion) and not backfacing_limit(sequential_pose,door_info['door_pose']):
                    changing_place = key
                    print('In imagination, changing place at pose:', sequential_pose, ', door info:', \
                        door_info['door_pose'], 'exp_connected_place:', door_info['exp_connected_place'],\
                        'connected_place_door_pose:', door_info['connected_place_door_pose'])
                    #print('\n'.join(f"{key}: {value}" for key, value in door_info.items() if key not in \
                    #               ['connected_place', 'origin_place', 'motion_axis', 'direction']))
                    #is_it_first_step_out_of_place ?
                    if not existing_an_alternative_pose(saved_current_place_pose):
                        #saving original place pose pov, in case we turn back to it
                        saved_current_place_pose = sequential_pose.copy()
                        sequential_pose = self.creating_new_pov_pose(manager, door_info, sequential_pose)
                        print('Changing place, thus switching policy pose from', saved_current_place_pose, 'to ', sequential_pose)
                
                #If we have changed place yet not passed front door or facing toward current room 
                # elif changing_place >= 0:
                #     changing_place = -1      
            
            #IF we have changed poc from original place accumulated_pose to new place pose
            # with THIS door view 
            # and we are turning back toward known room (180deg from door pose view)
            if key == changing_place and existing_an_alternative_pose(saved_current_place_pose) and backfacing_limit(saved_current_place_pose,door_info['door_pose']):
                #print('we are turning back on known room')
                sequential_pose = saved_current_place_pose
                saved_current_place_pose = []
                changing_place = -1
                break

            if changing_place != -1:
                break #we don't need to search through other door poses
        return changing_place, sequential_pose, saved_current_place_pose

    def creating_new_pov_pose(self,manager:object, door_info:dict, sequential_pose:list):
        """
        If we change place, we have to change local frame, 
        thus we have to adapt the pose used for place prediction
        """
        #if we have a door pose reference to set a new pose at door
        if len(door_info['connected_place_door_pose']) > 0:
            #We get the door pose from the pose of the door view
            forward_pose = from_door_view_to_door_pose(door_info['connected_place_door_pose'].copy())
            #We convert the current place orientation to the new place ref frame orientation
            forward_pose[2] = manager.convert_orientation_between_two_places_ref_frame(sequential_pose, goal_exp_id = door_info['exp_connected_place'])
            sequential_pose = forward_pose
        else:
            sequential_pose = np.array([0,0,0])
        
        return sequential_pose


#====================== CONDITIONS METHODS ==========================================================
def passed_limit(linear_motion:int)->bool:
    return abs(linear_motion) > 1
    
def existing_an_alternative_pose(saved_current_place_pose:list)-> bool:
    return len(saved_current_place_pose) > 0

def facing_limit(sequential_pose:list, door_pose:list)-> bool:
    return sequential_pose[2] == door_pose[2]

def backfacing_limit(sequential_pose:list, door_pose:list)-> bool:
    return sequential_pose[2] == (door_pose[2]+2)%4

def known_connected_place(exp_connected_place):
    return exp_connected_place is not None


```

`hierarchical-nav/navigation_model/Processes/manager.py`:

```py
import copy

import numpy as np
import torch
import cv2


from env_specifics.env_calls import (call_env_entry_poses_assessment,
                                     call_env_number_of_entry_points,
                                     call_env_remove_double_poses,
                                     call_get_place_behind_limit)
from navigation_model.Services.allocentric_model import \
    init_allocentric_process
from dommel_library.datastructs import cat as cat_dict
from navigation_model.Services.egocentric_model import init_egocentric_process
from navigation_model.Services.memory_service.memory_graph import MemoryGraph
from navigation_model.Services.model_modules import torch_observations, sample_ob
from navigation_model.visualisation_tools import convert_tensor_to_matplolib_list, visualise_image

egocentric_process = None
class Manager():
    def __init__(self, allo_model_config:dict, memory_graph_config:dict, env_actions:list, env:str, lookahead:int=5, replay_buffer=None) :
        global egocentric_process
        self.empty_memory = True
        self.num_samples = 5
        self.env_relevant_ob = {}

        self.env_specific = env

        #DICT CONFIG
        #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        device = torch.device('cpu')
        #ego config //STATIC//
        self.egocentric_process = init_egocentric_process(env_actions, device)
        egocentric_process = self.egocentric_process
        #ALLO config 
        self.allocentric_process, self.std_place_th = init_allocentric_process(allo_model_config, self.env_specific, device)
        self.replay_buffer = replay_buffer
        #MemoryGraph initialisation
        self.memory_graph = MemoryGraph(**memory_graph_config) 
        self.memory_graph.experience_map.manager = self
        self.memory_graph.experience_map.replay_buffer = self.replay_buffer

        self.observations_keys = list(set(self.allocentric_process.get_observations_keys()) | \
                                set(self.egocentric_process.get_observations_keys()))
        

        #NAVIGATION PARAMETERS
        self.default_lookahead = lookahead
        self.variable_lookahead = self.default_lookahead

  

        #TODO: change all that    
        # THE GQN + OZ PROCESS
        #self.place_model = PlaceGQN(self.allocentric_process, self.egocentric_process, self.observation_keys, std_place_th = self.std_place_th)
            
        #To be sure not to remove the ghost node realised behind oneself when entering a room
        #self.dist_margin = memory_graph_config['delta_exp_threshold'] + 1


        #goal_to reach variables
        #TODO: Goal process not in manager but in goal python file.
        #Goal will be a class instead of a dict
        #SO ALL THE BELOW ELEMENTS WILL BE ELSEWHERE
        # self.goal_data = {}
        # #How much proba we want to put on unconnected links from 1 to inf 
        # # (high number means highly improbable that they are chosen, 
        # # low number means higher choice proba)
        # self.weigth_on_unconnected_exps = 10 
        # self.wt_lower_range = np.array([235,235,235])
        
        
        # #TODO: put all that in exploration
        # #curiosity driven variables
        # #self.turn_motion_perceived = False
        # self.original_lookahead = self.place_model.lookahead 
        # self.agent_exploring = True
        # self.curiosity_temp = 100

        
        # #TESTs
        # self.n_step = 0
        # self.close_exploration_plot = True
        # self.GP_data = []
        #self.last_chosen_step_policy_EFE = None

        #self.lock = multiprocess.Lock()

#================= SETUP METHODS ==============================================================================#
   
    #TODO: add this call in test setup, not in manager
    def load_memory(self, load_memory: str) -> None:
        '''
        We re-use previously memorised experiences
        '''
        self.memory_graph.load(load_memory)
        self.empty_memory = False
    
    def save_memorised_map(self,file:str) -> None:
        self.memory_graph.save(file)
    
    def set_env_relevant_ob(self, significant_ob: dict) -> None:
        ''' save the given step observatrion as relevant ob for future use, 
        of input format [batch,...]'''

        self.env_relevant_ob = significant_ob
    
    def reset_variable_lookahead_to_default(self) -> None:
        self.variable_lookahead = self.default_lookahead

#================= GET METHODS ==============================================================================#
   
    def get_observations_keys(self) -> list :
        return self.observations_keys
    
    def get_manager_sampling(self) -> int:
        ''' return manager set sample lenght'''
        return self.num_samples
    
    def get_env_relevant_ob(self) -> dict:
        return self.env_relevant_ob  
    
    def get_allocentric_model_mse_theshold(self) -> float:
        return self.allocentric_process.get_mse_threshold()
    
    def get_confidence_about_place_description(self) -> bool:
        return self.allocentric_process.confident_about_place_description()
    
    def get_current_lookahead(self) -> int:
        return self.variable_lookahead
    
    def mse_under_threshold(self, mse: float, sensitivity:float = 1) -> bool:
        ''' 
        is the mse under the allocentric mse threshold * sensitivity?
        the sensitivity is used to adapt the threshold to our need
        '''
        return mse < self.get_allocentric_model_mse_theshold() * sensitivity

    def get_best_place_hypothesis(self) -> dict:
        return self.allocentric_process.get_best_place_hypothesis()
    def get_all_places_hypothesis(self)->dict:
        return self.allocentric_process.get_all_place_hypothesis()
    
    def agent_lost(self) -> bool:
        ''' are we lost? '''
        return self.allocentric_process.place_doubt_step_count() > 6

    def get_location_limits(self, exp_id:int=None)-> list:
        return self.memory_graph.get_exp_relevant_poses(exp_id)

    def get_connected_place_info(self, pose:list)-> tuple[any, int, list]:
        ''' 
        If we imagine going to a known place, we retrieve the place,
        If we don't know what to imagine, we create an empty place
        '''
        expected_exp_id, limit_jonction_from_other_place = call_get_place_behind_limit(self.env_specific, self, pose)
        #If known place
        if expected_exp_id >= 0 :
            expected_place = self.memory_graph.get_exp_place(expected_exp_id)
            expected_place = self.allocentric_process.place_as_sampled_Normal_dist(expected_place,self.num_samples)
        else:
            expected_place = None 
            print('no expected known experience behind limit', pose)
        
        return expected_place, expected_exp_id, limit_jonction_from_other_place

    def torch_sample_place(self, place:np.ndarray) -> torch.Tensor:
        return self.allocentric_process.place_as_sampled_Normal_dist(place,self.num_samples)
    
    def get_current_exp_id(self)->int:
        return self.memory_graph.get_current_exp_id()
    
    def get_exps_organised_by_distance_from_exp(self, exp_id:int=None)->dict:
        return self.memory_graph.get_exps_organised(exp_id)
    
    def get_all_exps_in_memory(self, wt_links:bool=True):
        return self.memory_graph.get_exps(wt_links)
    
#=================  FORMAT METHODS ==========================================================================
    def save_pose_in_memory(self, pose:list)-> None:
        ''' save pose in memory graph exp memory'''
        #TODO: this is too particular for minigrid, to generalise
        relevant_poses = list(self.memory_graph.get_exp_relevant_poses())
        if pose not in relevant_poses:
            relevant_poses.append(pose)
            print('XXXXXXX relevant_poses',relevant_poses)
            self.memory_graph.memorise_poses(relevant_poses)

    def increase_lookahead(self, max:int=6)->bool:
        '''
        Increase the lookahead if it's under the max lookahead threshold.
        return wether it increased the lookahead or not
        '''
        if self.variable_lookahead < max:
            self.variable_lookahead+=1
            return True
        return False

    def sample_visual_ob(self, observation:torch.Tensor) -> torch.Tensor:
        return sample_ob(observation, self.num_samples, len(observation.shape))
    
#================= STEP OBSERVATION UPDATE ==========================================================================
    def digest(self, sensor_data:dict) -> None:
        ''' 
        for new observations,
        update the egocentric model
        update the allocentric model --> implies possible change of state
        update the memory graph --> implies possible change location
        '''

        #=====Observations treatment process======#
        torch_sensor_data = torch_observations(sensor_data, self.observations_keys)
        print(torch_sensor_data["pose"],torch_sensor_data["image"])
        #=====ALLO+EGOCENTRIC MODELS process (update model with latest motion observations)======#
        self.egocentric_process.digest(torch_sensor_data, self.num_samples) #
        self.process_place_believes(torch_sensor_data)
        
        #======Update the map======# 
        # localize on map, update map
        self.memory_update(torch_sensor_data, self.get_best_place_hypothesis())
  
        return 
    
    def process_place_believes(self, torch_sensor_data: dict):
        ''' 
        Update the allocentric model with new data.
        It implies the creation of parallel place hypothesis if
        the model predicts the observation incorrectly
        '''
        #TODO: transform the hypothesis in classes, 1 class by info
        self.allocentric_process.update_place_believes(torch_sensor_data, self.num_samples)
        #If we are lost and there are memory graph as experiences in memory
        if self.agent_lost() and not self.empty_memory: 
            #Verify if some prev place door poseS match current ob. If yes, add them to the hypothesis
            print("adding memories to hypothesis")
            self.add_memories_to_hypothesis(torch_sensor_data)
                    
        self.allocentric_process.assess_believes()
    
    def memory_update(self, observations: dict, place_descriptor: dict) -> None:
        """ update the topo map with new step update """
        #Save previous step state
        prev_exp_id = self.memory_graph.get_current_exp_id()
        self.memory_digest(observations, place_descriptor)
        current_exp_id, current_view_cell_id = self.memory_graph.get_current_exp_and_view_id()

        print('CURRENT ID and view cell ID',current_exp_id, current_view_cell_id)
        
        #If no memory formed
        if current_exp_id == -1:
            print("there was no memory")
            return
        else:
            self.empty_memory = False
        
        #NOTE: IS THIS PART REALLY USEFULL? DON'T THINK SO, TO CHECK
        # current_view_cell_place = multivariate_distribution(self.memory_graph.view_cells.templates[current_view_cell_id])  
        #If the memory recognised an experience that do NOT match the current belief
        #if slam_obs["place"] is not None and not (current_view_cell_place == slam_obs["place"]).all():
             #self.place_model.update_place(observations, current_view_cell_place)
        
        #If we change experience, we do a recap on previous exp main features before storing it in memory
        #if current_exp_id != prev_exp_id and prev_exp_id != -1:
            #self.handle_exp_change(prev_exp_id)

    def rgb56_to_template64(self,
    img,
    eps: float = 1e-6,
    device: torch.device = torch.device("cpu"),
) -> torch.Tensor:
        """
        56×56×3 RGB  →  64-D descriptor
            • 48 dims = 16-bin histograms of L*, a*, b*
            • 16 dims = 4×4 block-mean edge magnitudes (Sobel)
        """

        # ------------------------------------------------------------------ #
        # 1. Make sure we have HWC uint8                                     #
        # ------------------------------------------------------------------ #


        if torch.is_tensor(img):
            if img.shape == (3, 56, 56):                      # CHW tensor
                img = img.permute(1, 2, 0).contiguous().cpu().numpy()
                
            else:
                img = img.cpu().numpy()
                
        else:
            if img.shape == (3, 56, 56):                      # CHW numpy
                img = np.transpose(img, (1, 2, 0))
                

        assert img.shape == (56, 56, 3), f"expected 56×56×3, got {img.shape}"

        if img.dtype != np.uint8:
            img = (img * 255.0).round().astype(np.uint8)
            

        # ------------------------------------------------------------------ #
        # 2. 16-bin Lab histograms (48 dims)                                 #
        # ------------------------------------------------------------------ #
        lab = cv2.cvtColor(img, cv2.COLOR_RGB2Lab).astype(np.float32)
        L   = (lab[:, :, 0] * 255.0 / 100.0).clip(0, 255)
        a   = lab[:, :, 1] + 128.0
        b   = lab[:, :, 2] + 128.0

        bins = np.linspace(0, 256, 17, dtype=np.float32)
        h_L, _ = np.histogram(L, bins=bins)
        h_a, _ = np.histogram(a, bins=bins)
        h_b, _ = np.histogram(b, bins=bins)

        h48 = np.concatenate([h_L, h_a, h_b]).astype(np.float32)
        h48 /= h48.sum() + eps

        # ------------------------------------------------------------------ #
        # 3. 4×4 Sobel-edge energy (16 dims)                                 #
        # ------------------------------------------------------------------ #
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mag  = cv2.magnitude(
            cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3),
            cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3),
        )
        edge16 = [
            mag[y : y + 14, x : x + 14].mean()
            for y in range(0, 56, 14)
            for x in range(0, 56, 14)
        ]
        edge16 = np.asarray(edge16, np.float32)
        edge16 /= edge16.sum() + eps

        # ------------------------------------------------------------------ #
        # 4. Concatenate → 64-D  & return torch tensor                       #
        # ------------------------------------------------------------------ #
        vec64 = np.concatenate([h48, edge16])
        print("[DBG] vec64  shape", vec64.shape, " first5", vec64[:5])

        return torch.from_numpy(vec64).to(device)
        

    
    def memory_digest(self, observations: dict, place_descriptor: dict) -> None:
        """ We update the memory graph with current:
        - believed place (dist)
        - believed pose ([x,y,th])
        - action ([F, R, L])
        with current believed place + pose not NONE only if we are confident about the place
        We also update the belief over the place if the memory graph believes we are
        in another experience.
        """
        #=== UPDATE MAP WITH CURRENT BELIEF ===#
        slam_obs = {"place": None, 'pose': None, "HEaction": observations['action'].cpu().detach().numpy(), "image":None}
        print("This is the place descriptor [0] that updates the memory graph", place_descriptor["pose"])
        print("this is the stable distribution in std place th compared to the place descriptor std ", place_descriptor['std'] < self.std_place_th,place_descriptor['std'], self.std_place_th)
        #if we have only 1 place hypothesis and we have a stable distribution
        if (not self.agent_lost() and place_descriptor['std'] < self.std_place_th):
            print("slam before changes",place_descriptor['post'].shape )
            slam_obs["place"] = torch.mean(place_descriptor['post'],dim=0).squeeze(0)
            print("slam obs place", slam_obs["place"].shape)
            slam_obs["pose"] = place_descriptor['pose']
            print("slam obs image shape", observations["image"],observations["image"].shape)
            slam_obs["image"] = self.rgb56_to_template64(
                                observations["image"]) 
            print("slam obs image",slam_obs["image"], slam_obs["image"].shape)
            
        else :
            self.memory_graph.memorise_poses([]) #if we are lost, we reset door memories
            
        #process state in memory_graph model
        print("maybe slam",slam_obs)
        self.memory_graph.digest(slam_obs, dt=1, adjust_map = False)

    def handle_exp_change(self, prev_exp_id:int)-> None:
        """ we change experience, thus we save the important info relative to this place"""

        #prev_place_id = self.memory_graph.get_exp_view_cell_id(prev_exp_id)
        print('update previous place limit poses')
        self.update_place_limits(prev_exp_id)
        
        if self.memory_graph.ghost_node_process :
            #We create ghost nodes at memorised poses + margin: memory_graph_config['delta_exp_threshold']+ 1 
            self.memory_graph.create_ghost_exps(exp_id = prev_exp_id) 

#================= Memories Methods ==========================================================================#
    def update_place_limits(self, exp_id:int=-1):
        '''
        Given exp id, get the place and find if any relevant obs found there
        If yes, memorise them in long term memory (not essential, but easier on process)
        '''
        prev_place = self.memory_graph.get_exp_place(exp_id)
        #We serach for poses matching the desired observation
        ob_poses = self.identify_observation_in_place(prev_place, self.env_relevant_ob)
        
        #We save the previous experience door poses in memory
        
        self.memory_graph.memorise_poses(ob_poses, exp_id)
        #do we create ghost experiences behind identified doors?

   
    def ascii_hist(self,data, bins=20, width=40):
        """
        Create an ASCII histogram for a 1D numpy array.
        """
        
        hist, bin_edges = np.histogram(data, bins=bins)
        max_val = hist.max()
        for i in range(len(hist)):
            edge = bin_edges[i]
            count = hist[i]
            # Normalize the count to the desired width
            bar = '#' * int((count / max_val) * width) if max_val > 0 else ''
            print(f"{edge:8.2f} | {bar} ({count})")
    
    def ascii_histogram_adaptive(self,samples, bins=20):
        """
        Adaptively create ASCII histograms based on the shape of the samples.
        If samples is 1D, prints one histogram.
        If samples is 2D (num_samples x d), prints one histogram per column.
        If samples has more dimensions, it flattens the data.
        """
        if isinstance(samples, torch.Tensor):
            samples = samples.detach().cpu().numpy()
        
        # Remove singleton dimensions (e.g., if shape is (num_samples, 1, d))
        samples = np.squeeze(samples)
        
        if samples.ndim == 1:
            print("Histogram for all values:")
            self.ascii_hist(samples, bins=bins)
        elif samples.ndim == 2:
            num_dims = samples.shape[1]
            for i in range(num_dims):
                print(f"Histogram for dimension {i}:")
                self.ascii_hist(samples[:, i], bins=bins)
                print()  # Blank line for separation
        else:
            print("Data has more than 2 dimensions; flattening:")
            self.ascii_hist(samples.flatten(), bins=bins)


    #TODO: change how we check for exp plausibility for competition
    def add_memories_to_hypothesis(self, observations):
        '''
        we search among the experiences of the memory graph if any door entry fits current observation.
        if yes then we add this observation in the competition of the fitest hypothesis to describe the place
        '''
        #WE want the exps organised by current GP
        exps = self.memory_graph.get_exps_organised(from_current_pose=True)
        #we go through all memorised experiences
        for exp in exps:
            if exp['id'] < 0:
                continue
            print('exp', exp['id'], ' distance to current GP', exp['delta_goal'], 'MAX TH:', self.memory_graph.get_delta_exp_threshold() * 2)
            #If the exp is too far from last
            if exp['delta_goal'] > self.memory_graph.get_delta_exp_threshold() * 2: #NOTE:THIS NUMBER IS PURELY ARBITRARY
                print("we got breaken")
                break
            #we recall the experience entry positions
            #TODO:change this logic of door poses in memory graph exp as such
            entry_poses = copy.deepcopy(exp['observation_entry_poses'])
            print("ENTRY_POSES", entry_poses)
            if len(entry_poses) == 0:
                print("we continued")
                continue
            entry_poses = call_env_entry_poses_assessment(self.env_specific, entry_poses)
            place = self.allocentric_process.place_as_sampled_Normal_dist(exp['observation'], self.num_samples)
            #print("Visualizing the sampled place distribution:")
            #self.ascii_histogram_adaptive(place, bins=10)
            observations = self.allocentric_process.sample_observations(observations, self.num_samples)
            #print("observations", observations)
            pose_observations = self.allocentric_process.create_pose_observations(entry_poses, self.num_samples)
            #print("this pose_observations",pose_observations)
            plausible_poses, poses_mse = self.allocentric_process.assess_poses_plausibility_in_place(place, observations, pose_observations)
            
            print('exp', exp['id'], 'selected_poses', plausible_poses, 'associated mse:', poses_mse)
            if len(plausible_poses)>0:
                self.allocentric_process.add_hypothesis_to_competition(place, poses = plausible_poses, exp= exp['id'], mse = poses_mse)

    def identify_observation_in_place(self, place:np.ndarray, env_relevant_ob:dict) -> list:
        ''' search for a particular observaion in place '''
        door_poses = []
        if place is None:
            place = self.get_best_place_hypothesis()['post']
        else:
            place = self.allocentric_process.place_as_sampled_Normal_dist(place,self.num_samples)
        #How many entry points do we want to remember
        num_return_poses = call_env_number_of_entry_points(self.env_specific)
        print("this should be 4", num_return_poses)
        #best poses matching ob
        best_poses_wt_mse = self.allocentric_process.best_matching_poses_with_ob(place, env_relevant_ob, num_return_poses)
        print(best_poses_wt_mse)
        #remove duplicate pose (in minigrid means removing double orientation)
        ob_poses = call_env_remove_double_poses(self.env_specific, best_poses_wt_mse)
        print(ob_poses)
        #Only remember pose if it is under a mse threshold
        for mse, p in ob_poses:
            if self.mse_under_threshold(mse, sensitivity=0.8) :
                door_poses.append(p)
        print('predicted door poses 222', door_poses)
        print('corresponding MSE ', ob_poses[0:len(door_poses)])
        return door_poses
    
    def convert_orientation_between_two_places_ref_frame(self,pose:list,  start_exp_id:int= None, goal_exp_id:int=None)-> int:
        return self.memory_graph.convert_pose_orientation_from_start_ref_frame_to_another(pose,  start_exp_id, goal_exp_id)

    def connect_places_to_current_location(self)->dict:
        ''' 
        Return all the exps linked to current exp
        '''
        linked_exps_info = {}
        current_exp_door_poses = self.memory_graph.get_exp_relevant_poses()
        #NOTE: This means no update of door poses with new seen door poses.
        #We consider that they are all imagined during exploration
        if len(current_exp_door_poses) == 0:
            self.update_place_limits(exp_id=-1)
            current_exp_door_poses = self.memory_graph.get_exp_relevant_poses()
        
        for current_exp_door_pose in current_exp_door_poses:
            linked_exp_id, door_pose_from_new_place = self.memory_graph.linked_exp_behind_door(current_exp_door_pose)

            if linked_exp_id >= 0 :
                view_cell = self.memory_graph.get_exp_view_cell(linked_exp_id)
                #the decay increase at each step in place and decrease over time as we move in other places
                view_cell_place_decay = view_cell.decay
                linked_exps_info[len(linked_exps_info)] = {
                                    'linked_exp_id' : linked_exp_id, \
                                    'current_exp_door_pose':current_exp_door_pose, \
                                    'door_pose_from_new_place': door_pose_from_new_place,\
                                    'view_cell_place_decay':view_cell_place_decay}
                #linked_exps_info.append([current_exp_door_pose, linked_exp_id, door_pose_from_new_place, view_cell_place_decay])
        print('linked_exps_info', linked_exps_info)

        return linked_exps_info
    
    def connected_place_to_visit(self,)-> dict:
        ''' 
        Return the most decayed place connected to current exp, if None, it returns an empty dict
        '''
        linked_exps_info = self.connect_places_to_current_location()
        if linked_exps_info:
            place_to_go = min(linked_exps_info.values(), key=lambda x: x['view_cell_place_decay'])
            
        else:
            place_to_go = {}
            print('No connected exp to current exp')
    
        print('place_to_go (most decayed place)', place_to_go)
        
        return place_to_go

    def get_egocentric_posterior(self)-> torch.Tensor:
        return self.egocentric_process.get_egocentric_posterior()
#================= Predictions Methods =================================================================#
    def single_pose_allocentric_prediction(self, pose:list, place) -> dict:
        """
        given a place and pose, what does the allo-model predicts for that step
        """

        return self.allocentric_process.allocentric_pose_prediction(pose, place , self.num_samples)
    
    def several_poses_allocentric_prediction(self, poses:list, place) -> dict:
        """
        given a place and list of pose (as list), what does the allo-model predicts for that step
        This is so the place updating resulting from the step prediction is unique for each pose of the list
        """
        results = None
        for p in poses:
            step_pred = self.single_pose_allocentric_prediction(p, place)

            if results is None:
                results = step_pred
            else:
                results = cat_dict(results, step_pred)
        return results
    
    def policy_egocentric_prediction(self, policy:list) -> tuple[list, dict]:
        if len(policy) == 0:
            return [], {}
        policy = torch.as_tensor(policy)
        policy, ego_prediction = self.egocentric_process.egocentric_policy_assessment(policy, self.num_samples, prediction_wanted=True)
        return policy.tolist(), ego_prediction
#================ Policies ====================================================================#

    def get_plausible_policies(self,policies: list) -> list:
        """  
        return only one set of all the policies 
        that seem dynamically plausible to effectuate to the egocentric model
        """
        plausible_policies_dict = {}
        for policy in policies:
            policy = torch.as_tensor(policy)
            # ==== egocentric model PROCESS ==== #
            #assess policy and cut off any colision move
            policy = self.egocentric_process.egocentric_policy_assessment(policy, self.num_samples)
            #if no action to apply, we don't need to consider it
            if policy.shape[0] != 0:
                #We want only one set of each policy, thus avoiding any double occurence
                #if we have only 1 action (list)
                if len(policy.shape) == 1:
                    policy_tuple = tuple(policy.tolist())
                #if we have a sery of actions (list of list)
                else:
                    policy_tuple = tuple(map(tuple, policy.tolist()))
                plausible_policies_dict[policy_tuple] = policy
        
        plausible_policies = list(plausible_policies_dict.values())
        return plausible_policies


#====== Data TEST related functions ======#

    def exp_visualisation_proba(self, place_descriptors):
        '''
        This is for testing purposes
        
        '''   
        hypo_weights = {}
        new_exp = -1
   
        for id, content in place_descriptors.items():
            
            if 'exp' in content :
                exp = content['exp']
                new_exp = exp
            else:
                new_exp-=1
            if new_exp in hypo_weights:
                if hypo_weights[new_exp] < content['hypothesis_weight']:
                    hypo_weights[new_exp] = content['hypothesis_weight']
            else:
                hypo_weights[new_exp] = content['hypothesis_weight']
        
        
        ids = list(hypo_weights.keys())
        weights = np.array(list(hypo_weights.values())) + 0.01 #to avoid 0

        
        # prob = Categorical(logits= torch.from_numpy(weights)).probs.numpy()
        # if len(place_descriptors) >1:
        #     fig, ax = plt.subplots(1 ,figsize=(9, 3), layout='constrained')
        #     plt.title(f"Hypothesis weight")
        #     ax.bar(ids, prob)
        #     plt.show()


        #new_exps_prob = []

        # for i in range(len(ids)):
        #     if ids[i] < 0:
        #         new_exps_prob.append([prob[i], ids[i]])
        # print(prob)
        # print(new_exps_prob)
        # print('mean prob of all new exps', np.mean(new_exps_prob, axis=0)[0])
        # max_prob = np.amax(new_exps_prob, axis=0)[0]
        # ids_index = np.where(np.array(new_exps_prob)[:,0] == max_prob)[0][0]

        # real_max_prob = np.max(prob)
        # print('max exp prob', real_max_prob, ids.index[real_max_prob] )
      
        # max_ids = ids[ids_index]
        # print('MAX prob of new exps', max_prob,max_ids)
        
        return ids, list(weights)
          
    def get_setting_variables(self):
        ''' for visualisation purposes'''
        dict_var = {'lookahead': self.place_model.lookahead, 'x_range': self.place_model.x_range, 'y_range': self.place_model.y_range, \
            'mse_err_th': self.place_model.mse_err_th , 'temp_GQN': self.curiosity_temp, 'weigth_exp': self.weigth_on_unconnected_exps, \
            'match_th': self.memory_graph.view_cells.MATCH_THRESHOLD, 'delta_exp': self.memory_graph.experience_map.DELTA_EXP_THRESHOLD, \
            'ghost_node': self.memory_graph.ghost_node_process, 'wt_lower_range': self.wt_lower_range}
        
        return dict_var

    def set_setting_variables(self, dict_var):
        ''' to modify the variables during a test '''
        self.place_model.lookahead = dict_var.get('lookahead', self.place_model.lookahead)
        x = dict_var.get('x_range', self.place_model.x_range)
        y = dict_var.get('y_range', self.place_model.y_range)
        self.place_model.pose_options = self.place_model.pose_option_setup(x,y)
        self.place_model.x_range, self.place_model.y_range = x,y
        self.place_model.mse_err_th = dict_var.get('mse_err_th', self.place_model.mse_err_th)
        self.curiosity_temp = dict_var.get('temp_GQN', self.curiosity_temp)
        self.memory_graph.view_cells.MATCH_THRESHOLD = dict_var.get('match_th', self.memory_graph.view_cells.MATCH_THRESHOLD )
        self.memory_graph.experience_map.DELTA_EXP_THRESHOLD = dict_var.get('delta_exp',self.memory_graph.experience_map.DELTA_EXP_THRESHOLD)
        self.memory_graph.ghost_node_process = dict_var.get('ghost_node',self.memory_graph.ghost_node_process)
        self.weigth_on_unconnected_exps = dict_var.get('weigth_exp',self.weigth_on_unconnected_exps)
        self.wt_lower_range =  np.array(dict_var.get('wt_lower_range', self.wt_lower_range ))
    
        #plt.show()

    # def modify_place_model_variables(self,**kwargs):
    #     for key, value in kwargs.items():
    #         setattr(self.place_model, key, value)

    #     if len(set(kwargs.keys()).intersection(('x_range', 'y_range')))>0:
    #         pose_opt = self.place_model.pose_option_setup(self.place_model.x_range, self.place_model.y_range)
    #         self.place_model.pose_options = pose_opt

    #NOTE: IS THIS METHOD USEFULL?
    def get_nodes_info(self):
        ''' return the number of exp created + how many links each have.
        Node and not exp as its from the env perspective'''
        exps_list = self.memory_graph.get_exps(wt_links = True)

        n_nodes = len(exps_list)
        links_per_node = []
        for exp in exps_list:
            links_per_node.append(len(exp['links']))

        return n_nodes, links_per_node
    

    def given_pose_prediction_visualisation(self, pose):
        """ visuliasing tool to see what the model predicts for a particular pose
        """
        place_info = self.get_best_place_hypothesis()
        step = self.allocentric_process.allocentric_pose_prediction(pose = pose, place = place_info['post'], num_samples=self.num_samples)
        
        visualise_image(torch.mean(step['image_predicted'], dim=0), 'pose:'+ str(pose), fig_id = 100, )

        return step['image_predicted']
        
```

`hierarchical-nav/navigation_model/Processes/motion_path_modules.py`:

```py

import sys
from operator import itemgetter

import numpy as np
import torch

#--------------- ACTION PROCESSES ------------#
def action_to_pose(action,pose):
    current_pose = pose.copy()
    if isinstance(action, torch.Tensor) and len(action.shape)>1:
        action = action[0]
    # #GQN increment action whatever the real situation, so we check if no vel [0,0]
    # if vel_ob[0] ==vel_ob[1] :
    #     action = [0,0,0]
    DIR_TO_VEC = [
        # Pointing right (positive X)
        [1, 0],
        # Down (positive Y)
        [0, 1],
        # Pointing left (negative X)
        [-1, 0],
        # Up (negative Y)
        [0, -1],
    ]
    if action[0] == 1:
        current_pose[:2] = DIR_TO_VEC[current_pose[2]] + current_pose[:2]
    elif action[0] == -1:
        current_pose[:2] = [ -i for i in DIR_TO_VEC[current_pose[2]] ] + current_pose[:2]
    elif action[1] == 1:
        current_pose[2] = (current_pose[2]+1)%4
    elif action[2] == 1:
        current_pose[2] = (current_pose[2]-1)%4
    return current_pose

#--------------- POLICY PROCESSES ------------#
def define_policies_objectives(current_pose:list, lookahead:int, full_exploration:bool) ->list:
    """ 
    we determine if we want the agent to just reach forward or a full 2D exploration
    around the agent. 
    If full_exploration is True : all corners of square (dist to agent:lookahead) 
    perimeters around agent set as goal
    """
    goal_poses = []

    goal_poses.append([current_pose[0]+lookahead,  current_pose[1]-lookahead, current_pose[2]])
    goal_poses.append([current_pose[0]+lookahead,  current_pose[1]+lookahead, current_pose[2]])
    
    # if we have a stable place modelised, then we can explore the negative positions of the newly created place
    if full_exploration :
        goal_poses.append([current_pose[0]-lookahead,  current_pose[1]-lookahead, current_pose[2]])
        goal_poses.append([current_pose[0]-lookahead,  current_pose[1]+lookahead, current_pose[2]])
    
    return goal_poses

def create_policies(current_pose:list, goal_poses:list, exploration:bool=True)-> list:
    ''' Given current pose, and the goals poses
    we want to explore or reach those goals, 
    generate policies going around in a square perimeter. 
    Either just forward (goals), or all around (explore)'''

    policies_lists = []
    #get all the actions leading to the endpoints
    for endpoint in goal_poses:
        print('end point and current pose', endpoint, current_pose)
        action_seq_options = define_policies_to_goal(current_pose, endpoint, exploration)
        policies_lists.extend(action_seq_options)

    return policies_lists

def exploration_goal_square(dx, dy):
    '''
    Create 1 path going to given dx for each y latitude and vice versa for dy. 
    This limit the path generation to half (opposing sides of the starting agent position) 
    the number of tiles on the outline of the rectangle formed by dx and dy.
    LIMITATION: WORKS WITH squarred areas
    '''
    paths = [[[0,0]]]
    paths.append([[0,0]])
    while paths[-1][-1] != [dx,dy] :
        new_paths = []
        #print('last path step', paths[-1][-1])
        for p in range(0,len(paths),2):
            #print(p,paths)
            #print('p',p, paths[p][-1], paths[p+1][-1], dx, dy)
            try:
                if paths[p+1][-1][1] < dy:
                    new_step_y = paths[p+1][-1].copy()
                    new_step_y[1] += 1
                    paths[p+1].append(new_step_y)
                    if p == 0:
                        new_paths.append(paths[p+1].copy())
            #this means we need to change axis more on x than y
            except IndexError:
                pass
            try:
                if paths[p][-1][0] < dx:
                    new_step_x = paths[p][-1].copy()
                    new_step_x[0] += 1
                    paths[p].append(new_step_x)
                    if p == 0:
                        new_paths.append(paths[p].copy())
            #this means we need to change axis more on y than x
            except IndexError:
                pass
            #print('end for ', p, ' paths', paths)
        paths.extend(new_paths)

    return paths

def two_paths_to_goal(dx,dy):
    '''
    Output the 2 shortest paths leading to goal, 
    so it's the rectangle outline between (0,0) and (dx,dy)
    '''
    paths = [[[0,0]]]
    paths.append([[0,0]])
    for p in range(2):
        while paths[p][-1][0] < dx:
            new_step_x = paths[p][-1].copy()
            new_step_x[0] += 1
            paths[p].append(new_step_x)

        while paths[-1-p][-1][1] < dy:
            new_step_y = paths[-1-p][-1].copy()
            new_step_y[1] += 1
            paths[-1-p].append(new_step_y)
        #print('p paths', p, paths)
    return paths

#TODO: SIMPLIFY STP
def define_policies_to_goal(start_pose:list, end_pose:list, exploration:bool= True)->list:
    '''
    MINIRID ADAPTED AS ORIENTATION ARE ONLY 4 (90deg turns)
    Given the current pose and goal pose establish all the sequence of actions 
    leading TOWARD the objective. Do not propose all turn options if needed to turn 
    180deg at start and end. 
    This code is only valid in a zone without obstacles in room. If there are, consider
    expanding the area of possible paths.
    '''
    #NOTE: THIS IS CONSIDERING POSES WITH 2 POSES IN SAME ROOM
    #we shift from the 0-3 orientation representation to 1-4
    
    start_theta, end_theta = start_pose[2]+1,  end_pose[2]+1
    dx,dy = abs(int(start_pose[0] - end_pose[0])), abs(int(start_pose[1] - end_pose[1])) # destination cell

    #If we want to reach a goal, we just need the 2 shortest paths leading there
    if not exploration:
        paths = two_paths_to_goal(dx, dy)  
    #If we want to explore, we want a grid path coverage (squared)
    else:    
        paths = exploration_goal_square(dx, dy)
        
    action_seq_options = []
    orientations= np.array([1,2,3,4] * 3)
   
    for path in paths:
        path = np.array(path)
        if start_pose[0] > end_pose[0]:
            path[:,0]= -path[:,0]
        if start_pose[1] > end_pose[1]:
            path[:,1]= -path[:,1]
            
        path = path.tolist()
        action_seq = []
        action_seq_alt = []
        for step in range(1,len(path)+1):
            # print('step', step)
            continue_path = True
            #first step init, add orientation
            if step == 1:
                path[step-1].append(start_theta)
            
            #last step final orientation check
            if step == len(path):
                path.append(path[-1])
                path[step].append(end_theta)
                desired_orientation = end_theta
                continue_path = False
            
            if path[step][0] - path[step-1][0] > 0 : #go forward x 
                desired_orientation = 1
            elif path[step][0] - path[step-1][0] < 0 : #go backward x
                desired_orientation = 3
            elif path[step][1] - path[step-1][1] > 0 : #go forward y
                desired_orientation = 2
            elif path[step][1] - path[step-1][1] < 0 : #go backward y
                desired_orientation = 4
            
            path[step].append(desired_orientation)

            # --- check if we need to turn and apply action --- #
            id_current_orientation = np.where(orientations == path[step-1][2])[0][1]
            ids_desired_orientation = np.where(orientations == desired_orientation)[0]
            closest_indexes = np.array(ids_desired_orientation)-id_current_orientation

            best_ids = np.where(abs(closest_indexes) == np.min(abs(closest_indexes)))[0]

            #NOTE: we can save alternative paths by uncommenting the part below, 
            # commented because of computation issue (only 1 cpu core used)
            for option, id in enumerate(best_ids):
                seq = []
                # print('best ids',best_ids, closest_indexes)
                n_turn = closest_indexes[id]
                # print('number of turn', n_turn)
                if n_turn < 0:
                    turn = [0,0,1] #turn left
                elif n_turn > 0:
                    turn = [0,1,0] #turn right
                for i in range(abs(n_turn)):
                    seq.append(turn) # turn 
                
                if continue_path:
                    seq.append([1,0,0])

                #NOTE: there is only 1 alternative list, so won't consider ALL 180deg turns. 
                # Meaning that if 2 are necessary at start and end, you won't 
                # have RR--RR  / RR--LL / LL--LL / LL--RR 
                # but only RR--RR / LL--LL 
                
                #--- Save action in action_seq or alternative_action_seq --#
                if option ==0 :
                    ##If we have no alternative action seq but 2 possible motions, get all prev actions 
                    if action_seq_alt == [] and len(best_ids) > 1:
                            action_seq_alt = action_seq.copy()
                    ##If we have an alternative action seq and we are on an unique path 
                    elif action_seq_alt != [] and len(best_ids) == 1:
                         action_seq_alt.extend(seq)
                    action_seq.extend(seq)
                #update the second motion option in the alternative seq 
                else:
                     action_seq_alt.extend(seq)
        #path_lengths.append(len(action_seq))
        action_seq_options.append(action_seq)
        if action_seq_alt != []:
             
        #     # print('added', action_seq)
        #     # print('added', action_seq_alt)
                action_seq_options.append(action_seq_alt)
        #path_lengths.append(len(action_seq_alt))
    
    return action_seq_options

#--------------- DIJKSTRA PROCESSES ------------#
def dijkstra_weigthed(exps_list:list, start_id:int, proba_impact:int = 100)->list:
        '''
        Dijkstra's shortest path, considering node dist + how many node to pass from
        The number of node to pass by is more important than the distance between nodes
        Djikstra check through all exps. A High proba_impact means that unlinked exps will
        have a low proba of being considered, A low proba_impact means that ghost nodes and
        unconnected nodes gets considered.
        '''
        # Put tuple pair into the priority queue
        unvisited_queue = []
        
        
        for exp in exps_list:
            init_setting = {'weight':sys.maxsize, 'num_nodes': sys.maxsize, 'previous_exp':None, 
                        'visited_exp':False, 'linked_exps': [], 'num_nodes':0} #IN FOR to avoid python shared memory issues with nested elements
            exp.update(init_setting)
            # Set the distance for the start node to zero 
            if exp['id'] == start_id:
                exp['weight'] = 0
            
            for link in exp['links']:
                exp['linked_exps'].append(link.target.id)

            unvisited_queue.append((exp['weight'],exp))

        while len(unvisited_queue) > 0:
            # Pops exp with the smallest distance 
            unvisited_queue = sorted(unvisited_queue, key=itemgetter(0))
            uv = unvisited_queue.pop(0)
            current = uv[1]
            current['visited_exp'] = True

            #for next linked exp (only go through links)
            # for next_exp_id in current['linked_exps']:
            #     exp_index_in_list = [exp['id'] for exp in exps_list].index(next_exp_id)
            #     next = exps_list[exp_index_in_list]
            
            #for next among ALL nodes:
            for next in exps_list :
                if next['id'] == current['id']:
                    continue
                next_exp_id = next['id']

                if next_exp_id in current['linked_exps']:
                    node_dist, ghost_exp = get_linked_exps_dist(current['links'],next_exp_id)
                    if node_dist is None and ghost_exp is None:
                        continue
                    if not ghost_exp:
                        exps_linked_proba = 1
                    else:
                        exps_linked_proba = 1 * 4 / (proba_impact)
                
                else:
                    node_dist = get_exps_euclidian_dist(current,next)
                    exps_linked_proba = 1/ proba_impact
                
                # if visited, skip
                if next['visited_exp']:
                    continue
                new_weight = current['weight'] + node_dist + (node_dist * ((1-exps_linked_proba) * proba_impact))
                
                #If new weight < to the next node currently calculated weigth 
                # OR if there are less nodes to go there
                #Then we replace next weigth by current and link exps
                if new_weight < next['weight'] or (current['num_nodes'] + 1 < next['num_nodes']):
                    next['weight'] = new_weight
                    next['num_nodes'] = current['num_nodes'] + 1
                    next['previous_exp'] = current
                    print('updated : current = %s next = %s new_weight = %s num_nodes = %s' \
                            %(current['id'], next['id'], next['weight'], next['num_nodes']))
                else:
                    print('not updated : current = %s next = %s next weight = %s , new_weight = %s , num_nodes = %s' \
                            %((current['id'], next['id'], next['weight'], new_weight, next['num_nodes'])))
                # print()
            # Clear used unvisited queue
            unvisited_queue.clear()
            # Fill the queue with all non visited nodes for next iteration
            for exp in exps_list:
                if not exp['visited_exp']: 
                    unvisited_queue.append((exp['weight'],exp))
            
        return exps_list

def dijkstra_shortest_node_path(target_exp:dict, path:list)->list:
    ''' make shortest path from linked previous_exp.
        Start from the target exp.
    '''
    if target_exp['previous_exp'] is not None:
        path.append(target_exp['previous_exp']['id'])
        path = dijkstra_shortest_node_path(target_exp['previous_exp'], path)
    return path

def get_linked_exps_dist(links:list, neighbor_id:int):
    for link in links:
        if link.target.id == neighbor_id:
            return link.d, link.target.ghost_exp  
    return None, None

#--------------- EUCLIDIAN DIST PROCESSES ------------#
def get_exps_euclidian_dist(exp1:dict, exp2:dict) -> float:
    delta_exps = np.sqrt(
        np.abs(exp1['x'] - exp2['x'])**2 +
        np.abs(exp1['y'] - exp2['y'])**2 
    )
    return delta_exps

def get_pose_euclidian_dist(p1:list, p2:list)->float:
    delta_poses = np.sqrt(
        np.abs(p1[0] - p2[0])**2 +
        np.abs(p1[1] - p2[1])**2 
    )
    return delta_poses

```

`hierarchical-nav/navigation_model/Services/__init__.py`:

```py
from .base_perception_model import PerceptionModel
from .allocentric_model import init_allocentric_process, AllocentricModel
from .egocentric_model import init_egocentric_process, EgocentricModel
from .model_modules import *



```

`hierarchical-nav/navigation_model/Services/allocentric_model.py`:

```py
import os.path
import sys
from itertools import product
from operator import itemgetter
from typing import Tuple

import numpy as np
import torch

from env_specifics.env_calls import call_env_place_range
from experiments.GQN_v2.models import GQNModel
from navigation_model.Services.model_modules import delete_object_from_memory
from navigation_model.Processes.AIF_modules import (
    calculate_FE_EFE, compute_std_dist,
    mse_observation)
from navigation_model.Processes.motion_path_modules import action_to_pose
from navigation_model.Services.base_perception_model import PerceptionModel
from dommel_library.modules.dommel_modules import (
    multivariate_distribution, tensor_dict)
from navigation_model.Services.model_modules import (get_model_parameters,
                                                     sample_observations,
                                                     torch_observations,
                                                     torch_pose)
# from navigation_model.visualisation_tools import visualise_image


def init_allocentric_process(allo_model_config:dict, env:str, device:str = 'cpu'):
    allo_model_config['device'] = device
    
    std_place_th = allo_model_config['model']['SceneEncoder']['clip_variance'] #0.25
    allocentric_process = AllocentricProcess(allo_model_config, env)
    allocentric_process.reset()
    
    return allocentric_process, std_place_th

class AllocentricProcess():
    def __init__(self, config:dict, env:str) -> None:
        self.allocentric_model = AllocentricModel(config)
        self.env_specific = env
        default_content = {'post': None, 'pose': np.array([0,0,0]), 'hypothesis_weight': 0.5, 'std': 1, 'info_gain': 0, 'exp':-1 }
        self.place_descriptors = { 0: default_content} #, 1: default_content }

        self.prev_step_place_std = sys.maxsize
        self.step_lost_counter = 0
        self.mse_threhsold = 0.5
    
    def reset(self) -> None:
        self.allocentric_model.reset()

#================= GET METHODS ==============================================================================#
   
    def get_mse_threshold(self) -> float:
        return self.mse_threhsold
    
    def get_best_place_hypothesis(self) -> dict:
        return self.place_descriptors[0]
    
    def get_all_place_hypothesis(self) -> dict:
        return self.place_descriptors
    
    def get_observations_keys(self) -> list:
        return self.allocentric_model.get_observations_keys()
    
    def get_place_range(self) -> list:
        return call_env_place_range(self.env_specific)
         
    def extract_mse_from_hypothesis(self, default_value= np.nan) -> list:
        return [item.get('mse', default_value) for item in self.place_descriptors.values()]
    
    def extract_weight_from_hypothesis(self, default_value= np.nan) -> list:
        return [item.get('hypothesis_weight', default_value) for item in self.place_descriptors.values()]
    
    def place_doubt_step_count(self) -> int:
        '''
        get how many steps the model has been hesitating on the best 
        place description 
        '''
        return self.step_lost_counter
    
    def get_place_descriptions(self) -> dict:
        ''' get all place descriptions hypothesis'''
        return self.place_descriptors
    
    def get_best_place_description(self) -> dict:
        ''' get the place description having the highest probability'''
        return self.place_descriptors[0]
    
    def get_sampled_data(self,data_dict:dict, num_samples:int):
        ''' input a TensorDict or dict of observations 
         returns it with the batch sample added'''
        return self.allocentric_model.sample_observations(data_dict, sample = num_samples)

    def confident_about_place_description(self) -> bool:
        ''' Does the place distribution have a small standard deviation?'''
        place_std = self.get_best_place_hypothesis()['std']
        # if we have a stable place modelised (std < th and not moving anymore) --> we have a strong belief
        return round(self.prev_step_place_std - place_std, 4) == 0

#=================  FORMAT METHODS ==========================================================================
    def reset_step_lost_counter(self):
        print("REEEEEEEESEEEEEEEET")
        self.step_lost_counter = 0
    
    def lost_steps_increase(self) -> None:
        '''
        increase lost steps count, only if we are already lost
        '''
        print("place_doubt_step before",self.step_lost_counter)
        if self.step_lost_counter > 0:
            #self.step_lost_counter + 1
            self.step_lost_counter += 1
            print("place_doubt_step after",self.step_lost_counter)

    def sample_observations(self, observations:dict, num_samples:int)-> dict:
        return self.allocentric_model.sample_observations(observations, sample = num_samples)
    
    def place_as_sampled_Normal_dist(self, place, num_samples:int)-> dict:
        ''' transform a place in a distribution and sample it '''

        #print(' place as sampled Normal dist shape and type', place.shape, type(place))
        if not isinstance(place, torch.Tensor):
            place = multivariate_distribution(torch.from_numpy(place))
        place = place.unsqueeze(0).unsqueeze(0).repeat(num_samples,1,1) 
        return place

    def create_pose_observations(self,poses:list, num_samples:int)-> dict:
        ''' torch and sample poses and save them as dict observations '''
        pose_options_dict = {'pose': list(poses)}
        pose_observations = self.allocentric_model.torch_observations(pose_options_dict)
        pose_observations = self.allocentric_model.sample_observations(pose_observations, sample = num_samples)
        return pose_observations

    def one_action_ob_update(self,observations:dict, prev_pose:list) -> Tuple[dict,list] :
        ''' update observations given hypo current pose and action'''
        pose  = action_to_pose(observations['action'],prev_pose)
        observations['pose'] = torch_pose(pose.copy())
        #observations = self.allocentric_model.sample_observations(observations,sample)
        return observations, pose  

#================= STEP UPDATE METHODS START==============================================================================#

    def update_place_believes(self, observations: dict, sample:int) -> None:
        """  
        for all models in dict, we update them and predict their mse/kl, hypothesis_weight and predicted_img
        """
        #step = [None]*len(self.place_descriptors)
        mse_list = []
        
        print('pose of best hypothesis before action ', self.place_descriptors[0]['pose'])
        #Update the step count for the parallel models run
        self.lost_steps_increase()

        self.prev_step_place_std = self.get_best_place_hypothesis()['std']
        
        #copy nested dictionary to be able to simultaneously:
        # - access and modify directly place_descriptors  (it's a copy, not a deepcopy)
        # - add dicts to place_descriptors 
        tmp_model_properties = self.place_descriptors.copy()
        print("XXXXXXXX", len(self.place_descriptors))
        for place_idx, place_description in tmp_model_properties.items():
            #---- 1. Observations incorporation ----#
            observations, place_description['pose'] = self.one_action_ob_update(observations, place_description['pose'])
            
            #---- 2. Update Believes of the generative models ----#
            self.allocentric_model.reset() #safety net to be sure to start anew
            step = self.allocentric_model.digest(observations, place_description['post'],sample)

            #---- 3. Process believes and update the model properties----#
            
            #If we have a prior to our place
            place_description['mse'], place_description['kl'], place_description['image_predicted'] = self.calculate_hypo_likelihood(place_description, step)
            mse_list.append(place_description['mse'])                          
            #print('min(mse_list) ',min(mse_list), 'place_description[mse]', place_description['mse'] , "place_doubt_step",self.place_doubt_step_count()) 
  
            #if we have no prior for this place  OR  If our place explain our image
            if np.isnan(place_description['mse']) \
                or place_description['mse'] < self.mse_threhsold : 
                print("THIS HAS BEEN EXPLAINAAAAAD","place_doubt_step",self.place_doubt_step_count())
                place_description = self.increase_hypo_likelihood(place_description, step)

            
            #We don't want to create thousands hypo, let's restrict the parallel creation on the best looking positions
            #If we already have an mse < th, then we know it's not worth creating new models since all will be erased. 
            #If the mse is too high and there is a lot of models, don't consider this hypo at all
            elif not min(mse_list) < self.mse_threhsold and \
                (place_description['mse'] <= 1.4 * self.mse_threhsold \
                or len(self.place_descriptors) < len(self.get_place_range())):

                place_description = self.decrease_hypo_likelihood(place_description)
                print("WHY",place_description['mse'],place_description['mse'] <= 1.4 * self.mse_threhsold,len(self.place_descriptors) < len(self.get_place_range()))
                print("CREATING NEW ALTERNATIVE place""place_doubt_step",self.place_doubt_step_count())
                self.add_new_place_hypothesis(step['place'],place_idx,observations['image'], sample)
        
        if self.place_doubt_step_count() > 0 :
            self.prev_step_place_std = sys.maxsize
        return
    
    def calculate_hypo_likelihood(self, place_description:dict, step_update:dict)->Tuple[float,float, torch.Tensor]:
        if place_description['post'] != None :
            kl ,mse, _, img_pred = \
            calculate_FE_EFE(self.allocentric_model.model.fork(), step_update['place'], step_update, place_description['post'])
        else:
            mse = np.nan
            kl = np.nan
            img_pred = None
        return mse, kl, img_pred
    
    def increase_hypo_likelihood(self,place_description:dict, step_update:dict) ->dict:
        new_place = step_update['place']
        place_description['post'] = new_place
        place_description['hypothesis_weight'] += 0.5
        place_description['std'] = compute_std_dist(new_place)
        if not np.isnan(place_description['kl']):
            place_description['info_gain'] += place_description['kl']
        return place_description
    
    def decrease_hypo_likelihood(self,place_description:dict) -> dict:
        
        #hypothesis not likely anymore
        place_description['hypothesis_weight'] = 0
        if self.place_doubt_step_count() == 0: #If we just started doubting our position
            place_description['exp'] = -1
        
        #we don't keep current hypo post
        self.allocentric_model.reset()
        return place_description
    
    def lowest_mse_hypo_increase_likelihood(self, mse:list) -> int:
        """
        The hypothesis having the lowest MSE will have an 
        increased probability of being correct. If all mse are nan
        then hypothesis 0 will automatically get the precedence
        """
        best_mse_idx = self.best_model_according_mse(mse) 
        print('number of parallel options running: ', len(mse) ,' and lowest values of mse:', mse[best_mse_idx])#, kl[index_min_err])
        #print('place explaining observation best considering the recon err:' +str(best_mse_idx))#+ ', considering kl:'+ str(index_min_kl) )
        self.add_hypo_weight_mse_under_threshold(mse, best_mse_idx)
        return best_mse_idx
    
    def assess_believes(self) -> None:
        """ Using MSE and Weight confirmation we select the model that seems to best describe the environment """
        
        #---- 1. Search for the best matching MSE and add weight to it if it's a plausible place descriptor----#
        mse = self.extract_mse_from_hypothesis(default_value= np.nan)
        self.lowest_mse_hypo_increase_likelihood(mse)
        
        #---- 2. Search for the best place descriptor having a sufficient weight to be plausible ----#
        self.highest_weight_hypo_convergence()        
        print('agent best hypothesis lp pose:', self.place_descriptors[0]['pose'])

    def best_model_according_mse(self, mse:list) -> int:
        ''' extract all mse from the place descriptions and return it with the idx of the lowest mse'''
        try:
            best_mse_idx = np.nanargmin(mse)
            #index_min_kl = np.nanargmin(kl)
        except ValueError: #all NAN
            print('value error mse', mse)
            best_mse_idx = 0 #index_min_kl = 0,0
        return best_mse_idx
    
    def add_hypo_weight_mse_under_threshold(self, mse:list, best_mse_idx:int) -> None:
        ''' if the mse is under threshold, hypo proba gain +1'''
        if mse[best_mse_idx] < self.mse_threhsold:
            print('according to MSE:')
            print('place ', best_mse_idx ,' is the best place to explain observations')
            print('pose:', self.place_descriptors[best_mse_idx]['pose'])
            #best model preferably always takes index 0
            if self.place_doubt_step_count() > 0 :
                print(self.place_doubt_step_count())
                temp = self.place_descriptors[0].copy()
                self.place_descriptors[0] = self.place_descriptors[best_mse_idx]
                self.place_descriptors[best_mse_idx] = temp
            self.place_descriptors[0]['hypothesis_weight'] += 1 #add more weight to this option  
 
    def best_model_according_to_weights(self, hypothesis_weight:list, index_max_weight:int)->int:
        """
        check if we have several hypothesis with equivalent weight, 
        return the index one having the highest weight and lowest mse 
        """
        mse = self.extract_mse_from_hypothesis(default_value= np.nan)
        list_indexes_max_weight = [idx for idx,val in enumerate(hypothesis_weight) if val==hypothesis_weight[index_max_weight]]
        #print('highest weight models indexes', list_indexes_max_weight)
        best_model_idx = index_max_weight
        for idx in list_indexes_max_weight:
            if idx == 0:
                best_model_idx = idx
                break
            if mse[idx] < mse[best_model_idx]: 
                best_model_idx = idx   
            
        print('according to model weigths:')
        print('place ' + str(best_model_idx) + ' is the best place to explain observations')
        print('pose:', self.place_descriptors[best_model_idx]['pose'], 'mse:', mse[best_model_idx], 'weight:', self.place_descriptors[best_model_idx]['hypothesis_weight'])
        return best_model_idx
    
    def converge_to_one_hypothesis(self, best_model_idx:int)-> None:
        ''' The best hypothesis is considered enough to describe the environment'''
        tmp_best_hypothesis = self.place_descriptors[best_model_idx]
        prev_exp = self.place_descriptors[best_model_idx].get('exp', -1)
            
        if len(self.place_descriptors)> 1 :
            #condition added for computational memory sake
            print('we select best matching model and delete the other parallel models')
            delete_object_from_memory([self.place_descriptors])
        self.place_descriptors = { 0: {**tmp_best_hypothesis}}
        self.place_descriptors[0]['exp'] = prev_exp
    
#================= MODEL LOST MULTIPLES HYPOTHESES PROCESS ==============================================================================#
    def remove_worst_hypothesis(self, portion:float = 2 )->None:
        """
        remove a portion of the hypothesis having the worst mse. 
        The ones having none are not considered.
        """
        mse = self.extract_mse_from_hypothesis(default_value= 0)
        
        n_remove = len(mse) // portion # Calculate number of models to remove
        print('we erase the last ', n_remove, ' worst models')
        
        # Sort indices by mse, with their original index
        mse_sorted = sorted(enumerate(mse), key=lambda x: x[1], reverse=True)[:n_remove] 
        for idx, _ in mse_sorted:
            del self.place_descriptors[idx] # Remove models with highest mse
        self.place_descriptors = {idx: model_props.copy() for idx, model_props in enumerate(self.place_descriptors.values())} # Update keys
    
    def highest_weight_hypo_convergence(self)-> None:
        """
        We check the weight proba of the hypothesis, if one is above the threshold 
        we converge to the hypotheses having this weight and the lowest mse
        we erase all other hypothesis
        else, every 2 steps we erase a set portion of the worst hypothesis
        """
        hypothesis_weight = self.extract_weight_from_hypothesis(default_value=0)
        index_max_weight = np.nanargmax(hypothesis_weight)
        print('highest weight index and its value in list and dict',  index_max_weight, self.place_descriptors[index_max_weight]['hypothesis_weight'])
        print("place_doubt_step",self.place_doubt_step_count(),hypothesis_weight[index_max_weight] >= 2.5,self.place_descriptors[index_max_weight]['hypothesis_weight'],hypothesis_weight[index_max_weight])
        #if we have likely hypo, we converge to the best hypo among them  
        if hypothesis_weight[index_max_weight] >= 2.5:
            best_model_idx = self.best_model_according_to_weights(hypothesis_weight, index_max_weight)
            self.converge_to_one_hypothesis(best_model_idx)
            self.reset_step_lost_counter()
        
        #Every 2 steps, we erase a part of the worst hypothesis
        #elif self.place_doubt_step_count() >= 2:
        elif self.place_doubt_step_count() >= 3:
            print("________________")
            print("_______________")
            print("________________")
            print("_______________")
            print("we erased", self.place_doubt_step_count)
            self.remove_worst_hypothesis(portion = 3)
    
    def add_new_place_hypothesis(self, current_updated_post, place_idx: int, current_ob: torch.Tensor, num_samples:int) -> None:
        """ we create parallel hypothesis with poses encompasing -most- possible agent position """
        #If we are here, we are doubting our belief.
        if self.step_lost_counter == 0:
            self.step_lost_counter += 1
     #We re-add the current wrong hypo place in the loop with the previous pose.
        self.add_parallel_hypothesis(pose=self.place_descriptors[place_idx]['pose'].copy(), post=current_updated_post[:])
        
        pose_options = self.get_place_range()
        pose_options_dict = {'pose': pose_options}
        observations = self.allocentric_model.torch_observations(pose_options_dict).squeeze(0)
           
        #we only create post:Normal hypothesis around all ranged poses given current ob 
        #when we start doubting or have been wondering for long enough to have erased enough worst hypothesis
        if (len(self.place_descriptors) < len(pose_options)) and self.step_lost_counter > 1: 
        #if (len(self.place_descriptors) < len(pose_options)):
            #print("why are we entering here?", len(self.place_descriptors), len(pose_options),range(observations['pose'].shape[0]))
            ob = {'image': current_ob.clone()}
            for pose_idx in range(observations['pose'].shape[0]):
                pose = observations['pose'][pose_idx,...].unsqueeze(0)
                ob.update(pose= pose) 
                
                alternative_step = self.allocentric_model.digest(ob, None, sample=num_samples)
                if 'place' in alternative_step:
                    alt_post = alternative_step.place
                else:
                    alt_post = alternative_step.posterior

                self.add_parallel_hypothesis(pose= np.array(pose_options[pose_idx]).copy(), post=alt_post)
        
    def add_parallel_hypothesis(self, pose:list, post=None) -> None:
        """ We create new model property batch at the end of current dict """
        new_place_idx = len(self.place_descriptors)
        self.place_descriptors[new_place_idx] = {}
        self.place_descriptors[new_place_idx]['post'] = post
        self.place_descriptors[new_place_idx]['pose'] = pose
        self.place_descriptors[new_place_idx]['hypothesis_weight']= 0
        self.place_descriptors[new_place_idx]['std']= 1
        self.place_descriptors[new_place_idx]['info_gain']= 0
        self.place_descriptors[new_place_idx]['exp']= -len(self.place_descriptors)
        #Those do not need to be init 
        self.place_descriptors[new_place_idx]['image_predicted'] = None
        self.place_descriptors[new_place_idx]['kl'] = np.nan
        self.place_descriptors[new_place_idx]['mse'] = np.nan
        #print('new para model inside the allocentric process, add parallel hypothesis',new_place_idx )
    
    def add_hypothesis_to_competition(self,place, poses, mse=None, exp=None):
        """
        we add the given place and its poses to the hypothesis
        """
        for p_idx in range(len(poses)):
            self.add_parallel_hypothesis(np.array(poses[p_idx]), post=place)
            if mse[p_idx] is None:
                #If the place do not have an associated MSE
                #Then we add the starting weight of a promising hypothesis
                weight = self.place_descriptors[len(self.place_descriptors)-1]['hypothesis_weight'] + 0.5
                self.place_descriptors[len(self.place_descriptors)-1]['mse'] = np.nan
            else:
                #If the place has an associated MSE
                #Then we add the starting weight corresponding to the MSE, with the weight
                #ranging necessarily between 0.1 and 0.55 (the min is there to avoid any sad 0)
                try:
                    weight = np.max([np.min([np.log(7*(self.mse_threhsold-mse[p_idx])),0.1]), 0.55])
                except RuntimeWarning:
                    weight = 0.1

                self.place_descriptors[len(self.place_descriptors)-1]['mse'] = mse[p_idx]
            self.place_descriptors[len(self.place_descriptors)-1]['hypothesis_weight'] = weight
                       
            if isinstance(exp, int):
                self.place_descriptors[len(self.place_descriptors)-1]['exp'] = exp

#================= STEP UPDATE METHODS ==============================================================================#

    def update_place(self, observations: dict, view_cell_place) -> None:

        print('The experience view do not match the current place')
        view_cell_place = view_cell_place.unsqueeze(0).unsqueeze(0).repeat(self.num_samples,1,1)
        pose_options = self.get_place_range()
        pose_options_dict = {'pose': pose_options}
        pose_observations = self.allocentric_model.torch_observations(pose_options_dict,self.get_observations_keys()).squeeze(0)
        pose_observations = self.allocentric_model.sample_observations(pose_observations, sample = self.num_samples)
        if 'pose' in observations:
            del observations['pose']
        find_pose_observations = self.allocentric_model.sample_observations(observations, sample = self.num_samples)
        
        mse_list = []
        for pose_idx in range(pose_observations['pose'].shape[1]):
            pose = pose_observations['pose'][:,pose_idx,...].unsqueeze(1)
            find_pose_observations.update(pose= pose) 
            #alternative_step = self.allocentric_model.digest(find_pose_observations, view_cell_place, sample=self.num_samples)
        
            mse_pose, image_predicted = mse_observation(self.allocentric_model.fork(), view_cell_place, find_pose_observations)
            mse_list.append([mse_pose, pose_idx])
        best_pose_options = sorted(mse_list, key=itemgetter(0))[:5]
        
        print('show me the 5 best pose options')
        for (mse, idx) in best_pose_options:
            print('mse, idx and pose',mse, idx, pose_options[idx] )
        
        self.place_descriptors[0]['post'] = view_cell_place
        self.place_descriptors[0]['pose'] = np.array(pose_options[best_pose_options[0][1]])

    def reset_parallel_models(self, idx: int= None) -> None:
        """ We erase selected model property based on its id """
        if idx == None:
            memory_saved = self.place_descriptors[0]
            delete_object_from_memory([self.place_descriptors])
            self.place_descriptors = {0: {**memory_saved}}
        else:
            del self.place_descriptors[idx]
            print('reset models, check model properties keys before erasing', self.place_descriptors.keys())
            for i in range (idx+1, len(self.place_descriptors)):
                self.place_descriptors[i] = self.place_descriptors[i-1]
            print('reset models, check model properties keys AFTER erasing', self.place_descriptors.keys())
    
    def allocentric_pose_prediction(self, pose: np.ndarray, place , num_samples:int) -> dict:
        ''' Given a pose and a place, what does the model predicts'''
        self.allocentric_model.reset() #safety to be sure to start anew
        pose_query = self.allocentric_model.torch_observations({'pose': list(pose)})
        pose_query = self.sample_observations(pose_query, num_samples)
        pose_query = self.allocentric_model.create_query(pose_query, keys=['pose'])
        
        predicted_step = self.allocentric_model.predict(place, pose_query, sample=num_samples)
        return predicted_step
    
    def assess_poses_plausibility_in_place(self, place:torch.Tensor, observations:dict, pose_observations:dict)-> tuple[list, list]:
        """
        for all the poses we check their predicted image mse compared to ob, 
        and we keep only the poses and mse under set threshold as valid. 
        """
        mse_pose_list = self.predicted_mse_of_poses(place, observations, pose_observations)
        mse_pose_list = sorted(mse_pose_list, key=itemgetter(0))
        selected_poses, selected_poses_mse = [], []
  
        for (mse,pose) in mse_pose_list:
            if mse < self.mse_threhsold - self.mse_threhsold/2: 
                #we are stricter with memory assumption than with pure extrapolation 
                selected_poses.append(pose[0])
                selected_poses_mse.append(mse)
                
            else:
                break
        
        return selected_poses, selected_poses_mse
    
    def predicted_mse_of_poses(self, place:torch.Tensor, observations:dict, pose_observations:dict)-> list:
        """ for each pose in the pose_ob' dict and the ob' place,
        we evaluate the predicted image 
        returns a list of list containing mse and pose"""
        mse_pose_list = []
        try:
            allocentric_model = self.allocentric_model.fork()
        except AttributeError:
            allocentric_model = self.allocentric_model.model

        for pose_idx in range(pose_observations['pose'].shape[1]):
            print("we are inside the mse of poses, what are the number of poses?", range(pose_observations['pose'].shape[1]))
            allocentric_model.reset()
            pose = pose_observations['pose'][:,pose_idx,...].unsqueeze(1)
            observations.update(pose= pose)
            mse_pose, image_predicted = mse_observation(allocentric_model, place, observations)
            mse_pose_list.append([float(mse_pose.numpy()), pose.squeeze(1).type(torch.int64).tolist()])
        return mse_pose_list
    
    def best_matching_poses_with_ob(self, place:torch.Tensor, relevant_ob:dict, num_return_poses:int=4) -> list:
        """ 
        return matching poses and their mse with the observation.
        Limited to 'num_return_poses' set to 4.
        """
        
        if 'pose' in relevant_ob:
            #IN CASE WE DON'T TRUST THE PLACE POSE ESTIMATION
            #re-estimate the goal position in this place
            print("is pose in relevant ob?")
            x = list(range(int(relevant_ob['pose'][:,0])-1, int(relevant_ob['pose'][:,0])+2))
            y = list(range(int(relevant_ob['pose'][:,1])-1,int(relevant_ob['pose'][:,1])+2))
            theta = [int(relevant_ob['pose'][:,2])]
            pose_options = list(product(*[x,y,theta]))
            pose_options = list(map(list, pose_options)) #36 poses in total
        else:
            pose_options = self.get_place_range()
        
        best_pose_options_wt_mse = []
        for p in pose_options:
            pose_observations = self.allocentric_model.torch_observations({'pose': p}).squeeze(0)
            pose_observations.update(image = relevant_ob['image'].unsqueeze(1)) 
          
            #--- MSE EVALUATION ---#
            pose_observations = self.allocentric_model.sample_observations(pose_observations, sample= place.shape[0])
            #print('show me shape of pose and placein best matching poses with ob', pose_observations['pose'].shape, place.shape)
            model_error, image_predicted = mse_observation(self.allocentric_model.model, place, pose_observations)
            best_pose_options_wt_mse.append([float(model_error.cpu().detach().numpy()), p])

        best_pose_options_wt_mse = np.array(sorted(best_pose_options_wt_mse, key=itemgetter(0))[:num_return_poses], dtype=object)
        return best_pose_options_wt_mse

    

class AllocentricModel(PerceptionModel):
    def __init__(self, config: dict) -> None:
        PerceptionModel.__init__(self)
        model_config = config['model']
    
        model_type = model_config.get("type", None)
        if model_type == "GQN":
            self.model = GQNModel(**model_config)
            
        epoch = config.get('model_epoch', None)
        if os.path.isdir(config['params']):
            model_param = get_model_parameters(config['params'], epoch)
        else: #we consider the file as a .pt without checking
            model_param = config['params']
        
        self.model.load(model_param,map_location=config['device'])
        self.model.to(config['device'])
    
        self.space_post = None
        self.live_training = True
        self.observations_keys = config['dataset']['keys'] 
        self.observations_shape_length = {'image':5, 'action':2, 'pose':3}
     
    def get_observations_keys(self) -> list :
        return self.observations_keys
    
    def torch_observations(self,observations:dict):
        observations = torch_observations(observations, self.observations_keys)
        if 'image' in observations:       
                observations.update(image = self.resize_image(observations['image']))
        return observations

    def sample_observations(self, observations:dict, sample:int)-> dict:
        ''' all the observations are sampled to have the desired batch size'''
        observations_keys = self.observations_keys + ['action']
        obs = sample_observations(observations, observations_keys, sample, self.observations_shape_length)
        for key in obs:
            self.move_to_model_device(obs[key])   
        return obs
    
    def move_to_model_device(self, tensor):
        return tensor.to(self.model.device) 
        
    def digest(self, observations:dict, space_posterior:torch, sample:int =1, reconstruct=False): 
        #TODO: adapt for several sensors
        # returns dict with current prior/posterior
        obs = self.sample_observations(observations, sample = sample)
        #NOTE: IF TRAINING continues while moving
        if not self.live_training:
            with torch.no_grad():
                step = self.model.forward(obs, place = space_posterior, reconstruct=reconstruct)
        else:
            #print('uncomment process in digest pytorchslammodel')
            step = self.model.forward(obs, place = space_posterior, reconstruct=False)
        return step

    def create_query(self, obs, keys=[]):
        observations = tensor_dict({})
        for k in keys:
            ob = obs.get(k,None)
            observations[k+'_query'] = ob
        
        return observations


    def predict(self, place, next_pose, sample=1):
        '''
        predict image given new pose and place
        predict new place given the expected image of that pose
        '''
        # returns dict with imagined observations for given query pose
        ob_shapes = {'pose_query':3, 'image_query':5}
        observations_keys = ['pose_query', 'image_query']

        next_pose = sample_observations(next_pose, observations_keys, sample, ob_shapes)
       
        with torch.no_grad():
            #predict pose or image given ob and current belief
            future_step = self.model.forward(next_pose, place, reconstruct=True)

            if 'image_predicted' in future_step:
                predicted_ob = 'image'
                expected_ob = 'pose'

            if 'pose_predicted' in future_step:
                predicted_ob = 'pose'
                expected_ob = 'image'

            future_step[predicted_ob] = future_step[predicted_ob+'_predicted']
            future_step[expected_ob] = future_step.get(expected_ob+'_predicted', \
                                        future_step.get(expected_ob+'_query', future_step.get(expected_ob)))
            
            #predict believe update given predicted ob 
            future_step = self.model.forward(future_step, place, reconstruct=False)
    
        return future_step


    def reset(self, state:torch.Tensor=None, post:torch.Tensor=None)-> None:
        if state is not None:
            state = state.to(self.model.device)
            post = post.to(self.model.device)
        self.model.reset(state,post)

    def reconstruct_ob(self, state:torch.Tensor, key:str='image') -> torch.Tensor:
        ''' 
        reconstruct the observation given a state (posterior or post sample)
        the key allows you to determine which reconstruction you want.
        '''
        if isinstance(state, dict):
            state = state['dist']
        state = state.to(self.model.device)
        if type(state)== type(multivariate_distribution(torch.zeros(1),torch.ones(1))):
            state = state.sample()
        with torch.no_grad():
            return self.model.likelihood(state, key)[key]

    def fork(self, batch_size:int = None) -> object:
        return self.model.fork(batch_size)

    def resize_image(self, image:torch.Tensor)-> torch.Tensor:
        ''' Transforn image into model desired shape'''
        model_width = self.model.observation_size("image")[-1]
        model_height = self.model.observation_size("image")[-2]
        if model_width != image.shape[-1] or model_height != image.shape[-2]:
            image = interpolate(image, size=(model_height, model_width), mode="nearest")
    
        return image
    

```

`hierarchical-nav/navigation_model/Services/base_perception_model.py`:

```py
class PerceptionModel():

    def digest(self, observations):
        # returns dict with current prior/posterior
        pass

    def lookahead(self, actions, reconstruct=False):
        # returns dict with imagined actions/observations
        pass

    def reconstruct(self, state):
        pass

    def reset(self, state=None):
        pass

    
```

`hierarchical-nav/navigation_model/Services/egocentric_model.py`:

```py
import os.path
import torch
import numpy as np
from pathlib import Path
import yaml
from torch.distributions import kl_divergence
from torch.nn.functional import interpolate

from dommel_library.datastructs import cat as cat_dict
from dommel_library.nn import module_factory

from experiments.OZ.models import  ConvModel
from navigation_model.Services.base_perception_model import PerceptionModel
from navigation_model.Services.model_modules import (get_model_parameters, torch_observations, sample_observations)
from dommel_library.modules.dommel_modules import  (multivariate_distribution)

def init_egocentric_process(env_actions:list, device:str = 'cpu') -> object:
    try:
        oz_config = yaml.safe_load(Path('runs/OZ/OZ_AD_Col16_8_id/OZ.yml').read_text())
    except FileNotFoundError:
        raise ' ddddddddegocentric yaml config path file not found in runs/OZ/OZ_AD_Col16_8_id/OZ.yml.'
    if 'params_dir' in oz_config:
        oz_config['params'] = oz_config['params_dir'] 
    if 'device' in oz_config:
        oz_config['device'] = device
        return EgocentricProcess(oz_config, env_actions)
    
class EgocentricProcess():
    def __init__(self, config: dict, possible_actions: list) -> None:
        self.egocentric_model = EgocentricModel(config)
        # left = [0, 0, 1] 
        # right = [0,1,0]
        # forward = [1,0,0]
        self.possible_actions = possible_actions #[forward, right, left]

    def get_observations_keys(self) -> list:
        return self.egocentric_model.get_observations_keys()
    
    def get_egocentric_posterior(self)-> torch.Tensor:
        return self.egocentric_model.get_post()
    
    def digest(self, observations: dict, sample:int = 1) -> None:
        '''
        update egocentric model state with new data
        '''
        self.egocentric_model.digest(observations, sample=sample, reconstruct=True)

    def one_step_egomodel_prediction(self, num_samples):
    # get the list of possible actions
        list_of_action = self.possible_actions.copy()
        oz_predicted_ob = None

        #== SAVE THE EGOCENTRIC MODEL 1 STEP PREDICTIONS FOR THE POSSIBLE ACTIONS WTHOUT COLISIONS ==#
        for action in list_of_action[:]: #F/R/L
            collision_step, oz_prediction =  self.ego_model.predict(torch.tensor([action]), sample= num_samples, reconstruct=True, collision_condition=True)
            #if we have a collision 
            if collision_step == 0: 
                list_of_action.remove(action)
                continue
            if oz_predicted_ob is None:
                oz_predicted_ob = oz_prediction['image_reconstructed']
            else:
                oz_predicted_ob = torch.cat((oz_predicted_ob, oz_prediction['image_reconstructed']), dim=1)
        return list_of_action, oz_predicted_ob
    
    
    def egocentric_policy_assessment(self, policy:list, num_samples: int, prediction_wanted:bool=False):

        # Define if no collision in action_seq, and if so, truncate action seq up to there
        collision_step, oz_prediction =  self.egocentric_model.predict(policy, sample= num_samples, reconstruct=True, collision_condition=True)
        #consider the policy up to collision (collision not considered)
        policy = policy[:collision_step]
        if prediction_wanted == True:
            return policy, oz_prediction
        
        return policy

    
class EgocentricModel(PerceptionModel):

    def __init__(self, config):
        PerceptionModel.__init__(self)

        #=== Create Model structure ===#
        model_config = config['model']

        model_type = model_config.get("type", None)
        if model_type == "Conv":
            self.model = ConvModel(**model_config)
        else:
            self.model = module_factory(**model_config)
        
        #=== Load Model params ===#
        epoch = config.get('model_epoch', None)
        
        if os.path.isdir(config['params']):
            model_param = get_model_parameters(config['params'], epoch)
        else: #we consider the file as a .pt without checking
            model_param = config['params']
        try:
            self.model.load(model_param,map_location=config['device'])
        except: 
            self.model.load(model_param)
            

        self.model.to(config['device'])
        self.observations_keys = config['dataset']['keys'] 
        self.observations_shape_length = {'image':4, 'action':2, 'vel_ob':3}
     
    def get_observations_keys(self) -> list :
        ''' return ego model observations keys used as input'''
        return self.observations_keys
    
    def torch_observations(self,sensor_data: dict) -> dict:
        ''' 
        Extract from sensor_data the observations the egocentric model uses
        and adapt them for pytorch
        '''
        observations = torch_observations(sensor_data, self.observations_keys)
        if 'image' in observations:       
                observations.update(image = self.resize_image(observations['image']))
        
        return observations
    
    def sample_observations(self, observations:dict, sample:int) -> tuple[torch.Tensor,dict]:
        ''' all the observations are sampled to have the desired batch size'''
        obs = sample_observations(observations, self.observations_keys, sample, self.observations_shape_length)
        for key in obs:
            self.move_to_model_device(obs[key])   
          
            if key == 'action':
                action = obs[key] 
        del obs['action']
            
        return action, obs

    def move_to_model_device(self, tensor):
        return tensor.to(self.model.device) 
    
    def digest(self, observations:dict,  sample:int=1, reconstruct:bool= True) -> dict: 
        '''
        returns the model forward outputs containing current prior/posterior and expected surprise
        if reconstruct is True, then the output will also contains predicted obs
        '''
        #TODO: adapt for several sensors
        action, obs = self.sample_observations(observations, sample)

        with torch.no_grad():
            step = self.model.forward(action, obs, reconstruct=reconstruct)
            #print("we are inside the egocentric digest, just to see if it is updating", step)
            step.surprise = kl_divergence(step.posterior, step.prior)
            print("what kldivergence?", step.surprise)
            
           
        return step.squeeze(0)

    def predict(self, actions:torch.Tensor, sample:int=1, reconstruct:bool=False, collision_condition:bool=False) -> dict:
        '''
        predict an action sequence. params
        reconstruct: wether we want the reconstruction or not
        collision_condition: wether we consider a collision as an action sequence abortion (return the step at which there is collision)
        '''
        # returns dict with imagined actions/observations
        actions = actions.repeat(sample,1,1).to(self.model.device)
        lookahead = actions.shape[1]
        print("lookahead?",lookahead)
        fork = self.model.fork(sample)
        result = []
        # lookahead
        with torch.no_grad():
            for step in range(lookahead):
                future_step = fork.forward(
                    actions[:, step, :], reconstruct=reconstruct)
                if collision_condition == True :
                    if 'collision_reconstructed'in future_step:
                        collision = future_step['collision_reconstructed']
                        collision = int(np.round(np.mean(collision.cpu().detach().numpy())))
                    else:
                        collision = 0
                    #here we consider the collsion detection near perfect
                    if collision == 1:
                        print("we collide")
                        try:
                            print("we tried")
                            predictions = cat_dict(*result)
                        except IndexError:
                            print("we tried but failed")
                            predictions = future_step
                        return step, predictions
                result.append(future_step.unsqueeze(1))
        predictions = cat_dict(*result)
        return lookahead, predictions


    def reset(self, state:torch.Tensor) -> None:
        if state is not None:
            state = state.to(self.model.device)
        self.model.reset(state)

    def reconstruct_ob(self, state:torch.Tensor, key:str='image') -> torch.Tensor:
        ''' 
        reconstruct the observation given a state (posterior or post sample)
        the key allows you to determine which reconstruction you want.
        '''
        if isinstance(state, dict):
            state = state['dist']
        state = state.to(self.model.device)
        if type(state)== type(multivariate_distribution(torch.zeros(1),torch.ones(1))):
            state = state.sample()
        with torch.no_grad():
            return self.model.likelihood(state, key)[key]


    def fork(self, batch_size:int = None) -> object:
        return self.model.fork(batch_size)

    def get_post(self) -> torch.Tensor:
        ''' return model posterior'''
        return self.model.get_post()

    def resize_image(self, image:torch.Tensor)-> torch.Tensor:
        ''' Transforn image into model desired shape'''
        model_width = self.model.observation_size("image")[-1]
        model_height = self.model.observation_size("image")[-2]
        if model_width != image.shape[-1] or model_height != image.shape[-2]:
            image = interpolate(image, size=(model_height, model_width), mode="nearest")
    
        return image
```

`hierarchical-nav/navigation_model/Services/memory_service/__init__.py`:

```py
from navigation_model.Services.memory_service.experience_map import *
from navigation_model.Services.memory_service.memory_graph import *
from navigation_model.Services.memory_service.view_cells import *
from navigation_model.Services.memory_service.odometry import *
from navigation_model.Services.memory_service.pose_cells import *
from navigation_model.Services.memory_service.modules import *
```

`hierarchical-nav/navigation_model/Services/memory_service/experience_map.py`:

```py
# Update 2022
# =============================================================================
# Ghent University 
# IDLAB of IMEC
# Daria de Tinguy - daria.detinguy at ugent.be
# =============================================================================

# Original Source
# =============================================================================
# Federal University of Rio Grande do Sul (UFRGS)
# Connectionist Artificial Intelligence Laboratory (LIAC)
# Renato de Pontes Pereira - rppereira@inf.ufrgs.br
# =============================================================================
# Copyright (c) 2013 Renato de Pontes Pereira, renato.ppontes at gmail dot com
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# =============================================================================

import numpy as np
import math
from operator import itemgetter
from .modules import *
from sys import maxsize
import math
import heapq
import torch
import cv2
# ---------- SIMPLE THETA DEBUG (toggle on/off here) ----------
EM_THETA_DEBUG = True     # set False to silence
EPS_TH = 1e-6
NO_DRIFT_MODE = True  

def _wrap_pi(a: float) -> float:
    return (a + np.pi) % (2*np.pi) - np.pi

def theta_snap(em, tag=""):
    """One-liner snapshot of angles and deltas."""
    if not EM_THETA_DEBUG: return
    if getattr(em, "current_exp", None) is not None:
        ax, ay, aθ = em.current_exp.x_m, em.current_exp.y_m, em.current_exp.facing_rad
        gx = ax + em.accum_delta_x
        gy = ay + em.accum_delta_y
    else:
        ax = ay = 0.0; aθ = 0.0
        gx = em.accum_delta_x; gy = em.accum_delta_y
    dθ  = em.accum_delta_facing
    θw  = _wrap_pi(aθ + dθ)
    print(f"[θ][{tag}] a={aθ:+.3f} Δ={dθ:+.3f} w={θw:+.3f} "
          f"Δxy=({em.accum_delta_x:+.3f},{em.accum_delta_y:+.3f}) "
          f"GP=({gx:+.3f},{gy:+.3f})")

def theta_reanchor_check(em, tag=""):
    """After (re)anchor, deltas must be exactly zero; world θ must equal anchor θ."""
    if not EM_THETA_DEBUG: return
    ok_xy = abs(em.accum_delta_x) <= EPS_TH and abs(em.accum_delta_y) <= EPS_TH
    ok_dθ = abs(_wrap_pi(em.accum_delta_facing)) <= EPS_TH
    if getattr(em, "current_exp", None) is not None:
        θw = _wrap_pi(em.current_exp.facing_rad + em.accum_delta_facing)
        ok_θ = abs(_wrap_pi(θw - em.current_exp.facing_rad)) <= EPS_TH
    else:
        ok_θ = True
    ok = ok_xy and ok_dθ and ok_θ
    print(f"[θ][{tag}] REANCHOR {'OK' if ok else 'PROBLEM'} "
          f"Δxy=({em.accum_delta_x:+.3f},{em.accum_delta_y:+.3f}) Δθ={em.accum_delta_facing:+.3f}")

def theta_forward_check(em, vtrans: float, prev_dx: float, prev_dy: float, tag=""):
    """For a pure translation step, moved direction must align with world θ."""
    if not EM_THETA_DEBUG or abs(vtrans) < EPS_TH: return
    aθ = em.current_exp.facing_rad if getattr(em, "current_exp", None) is not None else 0.0
    θw = _wrap_pi(aθ + em.accum_delta_facing)
    dx = em.accum_delta_x - prev_dx
    dy = em.accum_delta_y - prev_dy
    # project onto heading and lateral
    fwd =  dx*np.cos(θw) + dy*np.sin(θw)
    lat = -dx*np.sin(θw) + dy*np.cos(θw)
    ok  = (fwd > 0.5*vtrans) and (abs(lat) < 1e-3)
    print(f"[θ][{tag}] MOVE {'OK' if ok else 'PROBLEM'} "
          f"stepΔ=({dx:+.3f},{dy:+.3f}) proj_fwd={fwd:+.3f} lateral={lat:+.3f}")

def theta_link_check(u, v, link, tag=""):
    """Right after creating a link: check heading_rad & facing_rad geometry."""
    if not EM_THETA_DEBUG: return
    dx, dy = (v.x_m - u.x_m), (v.y_m - u.y_m)
    abs_head = np.arctan2(dy, dx)
    head_exp = _wrap_pi(abs_head - u.facing_rad)      # expected link.heading_rad
    face_exp = _wrap_pi(v.facing_rad - u.facing_rad)  # expected link.facing_rad
    dh = abs(_wrap_pi(link.heading_rad - head_exp))
    df = abs(_wrap_pi(link.facing_rad  - face_exp))
    ok = (dh <= 1e-6 and df <= 1e-6)
    print(f"[θ][{tag}] LINK {u.id}->{v.id} {'OK' if ok else 'PROBLEM'} "
          f"h(exp={head_exp:+.3f},link={link.heading_rad:+.3f},Δ={dh:.1e}) "
          f"f(exp={face_exp:+.3f},link={link.facing_rad:+.3f},Δ={df:.1e})")

def theta_gp_check(em, tag=""):
    """Compare get_global_position() vs anchor+deltas recompute."""
    if not EM_THETA_DEBUG: return
    gx, gy, gθ = em.get_global_position()
    if getattr(em, "current_exp", None) is not None:
        ax, ay, aθ = em.current_exp.x_m, em.current_exp.y_m, em.current_exp.facing_rad
    else:
        ax = ay = aθ = 0.0
    x2 = ax + em.accum_delta_x
    y2 = ay + em.accum_delta_y
    θ2 = _wrap_pi(aθ + em.accum_delta_facing)
    ok = (abs(gx - x2) <= 1e-6 and abs(gy - y2) <= 1e-6 and abs(_wrap_pi(gθ - θ2)) <= 1e-6)
    print(f"[θ][{tag}] GP {'OK' if ok else 'PROBLEM'} "
          f"get=({gx:+.3f},{gy:+.3f},{gθ:+.3f}) "
          f"re=({x2:+.3f},{y2:+.3f},{θ2:+.3f})")
# ---------- END SIMPLE THETA DEBUG ----------

from collections import namedtuple
State = namedtuple("State", ["x","y","d"])
DIR_VECS = [(1,0),(0,1),(-1,0),(0,-1)]
ACTIONS   = ["forward","left","right"]

class Experience(object):
    '''A single experience.'''

    _ID = 0
    _ghost_ID = 0

    def __init__(self,
                 x_pc, y_pc, th_pc,
                 x_m, y_m, facing_rad,
                 view_cell, local_pose, ghost_exp,
                 real_pose=None,
                 pose_cell_pose=None):
        '''Initializes the Experience.'''
        # Basic pose-cell and map coordinates
        self.x_pc = x_pc
        self.y_pc = y_pc
        self.th_pc = th_pc
        self.x_m = x_m
        self.y_m = y_m
        self.facing_rad = facing_rad

        # Cell references
        self.view_cell = view_cell
        self.init_local_position = local_pose

        # New pose attributes
        #self.imagined_pose= imagined_pose
        self.real_pose= real_pose
        self.pose_cell_pose= [x_pc, y_pc, th_pc]

        self.confidence=1
        # Link list
        self.links = []

        # ID assignment
        self.ghost_exp = ghost_exp
        if self.ghost_exp:
            self.id = Experience._ghost_ID
            Experience._ghost_ID += 1
        else:
            self.id = Experience._ID
            Experience._ID += 1
        self.place_kind = None     # "ROOM", "CORRIDOR", or "UNKNOWN"
        self.room_color = None     # e.g., "red", "purple", or None
        self.grid_xy    = None     # (gx, gy) in env grid coord space


        print(f"[DEBUG][Experience __init__] Created {'ghost' if ghost_exp else ''}Exp{self.id}: ")
        print(f"   pose_cell=({self.x_pc},{self.y_pc},{self.th_pc}), ")
        print(f"   map=({self.x_m:.3f},{self.y_m:.3f},{self.facing_rad:.3f}), ")
        print(f"   real_pose={self.real_pose}, pose_cell_pose={self.pose_cell_pose}")

    def link_to(self, target,
                accum_delta_x, accum_delta_y, accum_delta_facing,
                active_link,
                confidence=1):
        """
        Create a directed link self → target and append it to self.links.
        Debug prints included.
        """
        print(f"\n[DEBUG][link_to] from Exp{self.id} to Exp{target.id}")
        print(f"   deltas: dx={accum_delta_x:.3f}, dy={accum_delta_y:.3f}, dtheta={accum_delta_facing:.3f}")
        print(f"   self.map=({self.x_m:.3f},{self.y_m:.3f},{self.facing_rad:.3f}), target.map=({target.x_m:.3f},{target.y_m:.3f},{target.facing_rad:.3f})")
        if target is self:
            print(f"[WARN][link_to] ignoring self-link request {self.id}->{target.id}")
            return None
        # geometry
        d = np.hypot(accum_delta_x, accum_delta_y)
        abs_heading = np.arctan2(accum_delta_y, accum_delta_x)
        heading_rad = signed_delta_rad(self.facing_rad, abs_heading)
        facing_rad  = signed_delta_rad(self.facing_rad, accum_delta_facing)

        print(f"   computed abs_heading={abs_heading:.3f}, heading_rad={heading_rad:.3f}, d={d:.3f}, facing_rad={facing_rad:.3f}")

        link = ExperienceLink(
            parent=self,
            target=target,
            facing_rad=facing_rad,
            d=d,
            heading_rad=heading_rad,
            active_link=active_link,
            confidence=int(confidence),
            
        )
        self.links.append(link)
        print(f"   [DEBUG] Stored link: {link}")

    def __repr__(self):
        return f"[Exp {self.id}]"

    def update_link(self, link, e1):
        print(f"[DEBUG][update_link] Exp{self.id} updating link to Exp{e1.id}")
        if link.active_link:
            delta_x = e1.x_m - self.x_m
            delta_y = e1.y_m - self.y_m
        else:
            delta_x = self.x_m - e1.x_m
            delta_y = self.y_m - e1.y_m
        delta_facing = signed_delta_rad(self.facing_rad, e1.facing_rad)
        print(f"   deltas: dx={delta_x:.3f}, dy={delta_y:.3f}, dtheta={delta_facing:.3f}")

        link.d = np.hypot(delta_x, delta_y)
        link.heading_rad = signed_delta_rad(self.facing_rad, np.arctan2(delta_y, delta_x))
        link.facing_rad = delta_facing
        print(f"   updated link: {link}")
        return link


class ExperienceLink(object):
    '''Connection between two experiences, with debug prints.'''

    def __init__(self, parent, target,
                 facing_rad, d, heading_rad,
                 active_link,
                 confidence=1):
        self.parent       = parent
        self.target       = target
        self.facing_rad   = facing_rad
        self.d            = d
        self.heading_rad  = heading_rad
        self.active_link  = active_link
        self.confidence   = int(confidence) 
        # Store primitive paths
        

        
        print(f"[DEBUG][ExperienceLink __init__] Created link {self.parent.id}->{self.target.id}", self.confidence)
   

    def __repr__(self):
        return (f"ExperienceLink({self.parent.id}->{self.target.id}, "
                f"conf={self.confidence}, d={self.d:.2f}, "
                f"heading={self.heading_rad:.2f}, active={self.active_link})")


class ExperienceMap(object):
    '''Experience Map module with debug prints.'''

    def __init__(self, dim_xy=61, dim_th=36,
                 delta_exp_threshold=1.0, delta_pc_threshold=1.0,
                 correction=0.5, loops=100,
                 constant_adjust=False,
                 replay_buffer=None,
                 
                 **kwargs):
        print("[DEBUG][ExperienceMap __init__] initializing with replay_buffer=", bool(replay_buffer))
        
        self.replay_buffer = replay_buffer
        self._recent_prims: list[str] = []
        self.env = kwargs.get("env", None)
        self.area_room_threshold = kwargs.get("area_room_threshold", 12)
        self.last_link_action= None
        self.last_real_pose= None
        self.last_imagined_pose= None
        # existing fields
        self.DIM_XY = dim_xy
        self.DIM_TH = dim_th
        self.DELTA_EXP_THRESHOLD = delta_exp_threshold
        self.DELTA_PC_THRESHOLD = delta_pc_threshold
        self.CORRECTION = correction
        self.LOOPS = loops
        print("past constant adjust", constant_adjust)
        self.constant_adjust = constant_adjust

        self.size = 0
        self.exps = []
        self.ghost_exps = []
        self.current_exp = None
        self.current_view_cell = None
        self.accum_delta_x = 0
        self.accum_delta_y = 0
        self.accum_delta_facing = 0.0
    def set_env(self, env):
        """Optionally attach the MiniGrid environment so new exps get annotated."""
        self.env = env
    
    def rgb_to_template64(self,img, eps: float = 1e-6, device="cpu"):
        """
        56×56×3 or 64×64×3 RGB  →  64-D descriptor
        • 48 dims = 16-bin histograms of L*, a*, b*
        • 16 dims = 4×4 block-mean edge magnitudes (Sobel)
        Returns: torch.float32 (64,)
        """
        import numpy as np, torch, cv2
        print(img)
        print(type(img))
        # --- 0) to HWC uint8 ---
        if torch.is_tensor(img):
            arr = img.detach().cpu().numpy()
            if arr.ndim == 3 and arr.shape[0] in (1,3):  # CHW -> HWC
                arr = arr.transpose(1,2,0)
        else:
            arr = np.asarray(img)

        assert arr.ndim == 3 and arr.shape[-1] == 3, f"expected HxWx3, got {arr.shape}"

        # --- 1) resize to 56×56 (the old pipeline assumes 56 for 14×14 blocks) ---
        if arr.shape[0] != 56 or arr.shape[1] != 56:
            arr = cv2.resize(arr, (56,56), interpolation=cv2.INTER_AREA)

        # --- 2) uint8 range ---
        if arr.dtype != np.uint8:
            # assume 0..1 floats or other; clamp to [0,255]
            arr = np.clip(arr, 0, 255)
            if arr.max() <= 1.0 + 1e-6:
                arr = (arr * 255.0).round()
            arr = arr.astype(np.uint8)

        # --- 3) 16-bin Lab histograms (48 dims) ---
        lab = cv2.cvtColor(arr, cv2.COLOR_RGB2Lab).astype(np.float32)
        L   = (lab[:, :, 0] * 255.0 / 100.0).clip(0, 255)
        a   = lab[:, :, 1] + 128.0
        b   = lab[:, :, 2] + 128.0

        bins = np.linspace(0, 256, 17, dtype=np.float32)
        h_L, _ = np.histogram(L, bins=bins)
        h_a, _ = np.histogram(a, bins=bins)
        h_b, _ = np.histogram(b, bins=bins)

        h48 = np.concatenate([h_L, h_a, h_b]).astype(np.float32)
        h48 /= h48.sum() + eps

        # --- 4) 4×4 Sobel-edge energy (16 dims) ---
        gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)
        mag  = cv2.magnitude(
            cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3),
            cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3),
        )
        edge16 = [
            mag[y:y+14, x:x+14].mean()
            for y in range(0, 56, 14)
            for x in range(0, 56, 14)
        ]
        edge16 = np.asarray(edge16, np.float32)
        edge16 /= edge16.sum() + eps

        # --- 5) concat → 64-D torch.float32 ---
        vec64 = np.concatenate([h48, edge16]).astype(np.float32)
        return torch.from_numpy(vec64).to(device)

    def rgb56_to_template64(self,
        img,
        eps: float = 1e-6,
        device: torch.device = torch.device("cpu"),
    ) -> torch.Tensor:
        """
        56×56×3 RGB  →  64-D descriptor
            • 48 dims = 16-bin histograms of L*, a*, b*
            • 16 dims = 4×4 block-mean edge magnitudes (Sobel)
        """

        # ------------------------------------------------------------------ #
        # 1. Make sure we have HWC uint8                                     #
        # ------------------------------------------------------------------ #


        if torch.is_tensor(img):
            if img.shape == (3, 56, 56):                      # CHW tensor
                img = img.permute(1, 2, 0).contiguous().cpu().numpy()
                
            else:
                img = img.cpu().numpy()
                
        else:
            if img.shape == (3, 56, 56):                      # CHW numpy
                img = np.transpose(img, (1, 2, 0))
                

        assert img.shape == (56, 56, 3), f"expected 56×56×3, got {img.shape}"

        if img.dtype != np.uint8:
            img = (img * 255.0).round().astype(np.uint8)
            

        # ------------------------------------------------------------------ #
        # 2. 16-bin Lab histograms (48 dims)                                 #
        # ------------------------------------------------------------------ #
        lab = cv2.cvtColor(img, cv2.COLOR_RGB2Lab).astype(np.float32)
        L   = (lab[:, :, 0] * 255.0 / 100.0).clip(0, 255)
        a   = lab[:, :, 1] + 128.0
        b   = lab[:, :, 2] + 128.0

        bins = np.linspace(0, 256, 17, dtype=np.float32)
        h_L, _ = np.histogram(L, bins=bins)
        h_a, _ = np.histogram(a, bins=bins)
        h_b, _ = np.histogram(b, bins=bins)

        h48 = np.concatenate([h_L, h_a, h_b]).astype(np.float32)
        h48 /= h48.sum() + eps

        # ------------------------------------------------------------------ #
        # 3. 4×4 Sobel-edge energy (16 dims)                                 #
        # ------------------------------------------------------------------ #
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        mag  = cv2.magnitude(
            cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3),
            cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3),
        )
        edge16 = [
            mag[y : y + 14, x : x + 14].mean()
            for y in range(0, 56, 14)
            for x in range(0, 56, 14)
        ]
        edge16 = np.asarray(edge16, np.float32)
        edge16 /= edge16.sum() + eps

        # ------------------------------------------------------------------ #
        # 4. Concatenate → 64-D  & return torch tensor                       #
        # ------------------------------------------------------------------ #
        vec64 = np.concatenate([h48, edge16])
        print("[DBG] vec64  shape", vec64.shape, " first5", vec64[:5])

        return torch.from_numpy(vec64).to(device)


    def _create_exp(self, x_pc, y_pc, th_pc,
                    view_cell, local_pose):
        real     = tuple(self.last_real_pose)

        print("\n[DEBUG][_create_exp] creating new experience")
        self.size += 1
        if self.current_exp is not None:
            gx, gy, gth = self.get_global_position()   # (x_cur + Δx, y_cur + Δy, wrap(f_cur + Δθ))
            x_m, y_m, facing_rad = gx, gy, gth
        else:
            x_m = self.accum_delta_x
            y_m = self.accum_delta_y
            facing_rad = clip_rad_180(self.accum_delta_facing)
        
        facing_rad = self._quantize_cardinal(facing_rad)
        theta_reanchor_check(self, "after-anchor")
        print(f"   posed at cell=({x_pc},{y_pc},{th_pc}), map=({x_m:.3f},{y_m:.3f},{facing_rad:.3f})")
        print(view_cell.id)
        exp = Experience(
            x_pc, y_pc, th_pc,
            x_m, y_m, facing_rad,
            view_cell, local_pose, ghost_exp=False,
            real_pose=real,
            pose_cell_pose=[x_pc,y_pc,th_pc]
        )
        if getattr(self, "env", None) is not None:
            self._annotate_place_from_env(exp, self.env)
        if hasattr(self, "env") and self.env is not None:
            self._annotate_place_from_env(exp, self.env)
        
        e = exp
        print(f"[PLACE] Expppppppp{e.id} at {e.grid_xy}: {e.place_kind}"
            + (f" ({e.room_color})" if e.room_color else ""))
        print(self.get_global_position())
            # ------------------------------------------------------------------
            # Handle *empty* A* result safely
            # ------------------------------------------------------------------
        if self.current_exp is not None:
            self.current_exp.link_to(
                exp,
                self.accum_delta_x,
                self.accum_delta_y,
                exp.facing_rad,
                active_link=True,
                confidence=1
            )
            # backward link
            exp.link_to(
                self.current_exp,
                -self.accum_delta_x,
                -self.accum_delta_y,
                self.current_exp.facing_rad,
                active_link=False,
                confidence=1
            )
            theta_link_check(self.current_exp, exp, self.current_exp.links[-1], "create->fwd")
            theta_link_check(exp, self.current_exp, exp.links[-1], "create->back")
        self.exps.append(exp)
        if view_cell:
            view_cell.exps.append(exp)
        print(f"[DEBUG][_create_exp] total experiences = {len(self.exps)}")
        return exp
    
    
    def get_pose(self, exp_id: int) -> tuple[float, float, float]:
        """
        Return the (x_m, y_m, facing_rad) of the Experience with id==exp_id
        """
        for exp in self.exps:
            if exp.id == exp_id:
                return exp.real_pose
        raise KeyError(f"No experience with id={exp_id}")
    
    def _map_raw_action(self, raw):
        # unchanged
        if isinstance(raw, (list, tuple)) and len(raw)==3:
            idx = max(range(3), key=lambda i: raw[i])
            return ['forward','right','left'][idx]
        if isinstance(raw, int):
            return {0:'left',1:'right',2:'forward'}.get(raw,'forward')
        if isinstance(raw,str):
            low = raw.lower()
            if 'forw' in low: return 'forward'
            if 'left' in low: return 'left'
            if 'right' in low: return 'right'
        return 'forward'

    def _get_recent_prims(self, u_id: int, v_id: int) -> list[str]:
        """
        1) Try to find the exact span where we went u_id → v_id (as before).
        2) If we’ve not yet recorded v_id in the buffer, fall back to
           “all actions performed while in u_id,” so that we capture
           the entire drift inside u_id before the final leave.
        """
        rb = self.replay_buffer or []
        L  = len(rb)
        print(f"[DEBUG3][_get_recent_prims] buffer length = {L}, looking for transition {u_id}->{v_id}")

        # ——— Attempt the exact u→v slice ———
        entry_idx = None
        for i in range(L-1, -1, -1):
            if rb[i].get('node_id') == v_id:
                entry_idx = i
                break

        if entry_idx is not None:
            # (existing logic: scan back to find start_idx, u_idx, etc.)
            start_idx = entry_idx
            while start_idx > 0 and rb[start_idx - 1].get('node_id') == v_id:
                start_idx -= 1

            # find last time we were at u_id
            u_idx = None
            for j in range(start_idx - 1, -1, -1):
                if rb[j].get('node_id') == u_id:
                    u_idx = j
                    break

            action_seq = []
            if u_idx is not None:
                for k in range(u_idx + 1, start_idx + 1):
                    raw  = rb[k].get('action')
                    prim = self._map_raw_action(raw)
                    action_seq.append(prim)
                    print(f"  [DEBUG3] idx {k-1}->{k}: nodes {rb[k-1]['node_id']}->{rb[k]['node_id']},"
                          f" raw_action={raw} -> prim={prim}")
            print(f"  [DEBUG3] collected buffer‐based prims = {action_seq}")
            return action_seq

        # ——— Fallback: we haven’t even seen v_id yet ———
        print(f"  [DEBUG3] never saw v_id={v_id} in buffer; collecting actions while in u_id={u_id}")
        seq = []
        # walk backwards while still in u_id
        idx = L - 1
        while idx >= 0 and rb[idx].get('node_id') == u_id:
            raw  = rb[idx].get('action')
            prim = self._map_raw_action(raw)
            # prepend to build chronological order
            seq.insert(0, prim)
            print(f"  [DEBUG3] idx {idx-1}->{idx}: nodes {rb[idx-1]['node_id'] if idx>0 else '??'}->{u_id},"
                  f" raw_action={raw} -> prim={prim}")
            idx -= 1

        print(f"  [DEBUG3] collected same‐node prims = {seq}")
        return seq


    def update_exp_wt_view_cell(self, updated_exp, x_pc, y_pc, th_pc,
                            new_view_cell=None, local_pose=None):
        # anchor + deltas
        x_m = updated_exp.x_m + self.accum_delta_x
        y_m = updated_exp.y_m + self.accum_delta_y
        facing_rad = clip_rad_180(updated_exp.facing_rad + self.accum_delta_facing)
        facing_rad = self._quantize_cardinal(facing_rad)  # 4 orientations

        updated_exp.x_pc = x_pc
        updated_exp.y_pc = y_pc
        updated_exp.th_pc = th_pc
        updated_exp.x_m = x_m
        updated_exp.y_m = y_m
        updated_exp.facing_rad = facing_rad

        if local_pose is not None:
            updated_exp.init_local_position = local_pose

        if new_view_cell is not None:
            for e in updated_exp.view_cell.exps:
                if e.id == updated_exp.id:
                    updated_exp.view_cell.exps.remove(e)
                    break
            updated_exp.view_cell = new_view_cell
            new_view_cell.exps.append(updated_exp)

        # IMPORTANT: re-anchored → zero the odom deltas
        self.accum_delta_x = 0.0
        self.accum_delta_y = 0.0
        self.accum_delta_facing = 0.0
        
    def _quantize_cardinal(self, theta):
        # nearest multiple of pi/2, normalized to [-pi, pi)
        k = int(np.round(theta / (np.pi/2.0)))
        return clip_rad_180(k * (np.pi/2.0))
    def get_exp(self, exp_id):
        '''
        Experience lookup by id - assumes ids are monotonically rising
        '''
        index = exp_id
        if len(self.exps) >= index:
            index = len(self.exps) - 1

        while self.exps[index].id != exp_id:
            if self.exps[index].id > exp_id:
                index -= 1
            else:
                index += 1
                if index == len(self.exps):
                    index = 0

        return self.exps[index]

    def get_current_exp_id(self):
        ''' where are we?'''
        if self.current_exp is not None:
            return self.current_exp.id
        else:
            return -1

    def get_current_exp_view_cell_id(self):
        ''' where are we?'''
        if self.current_exp is not None:
            return self.current_exp.view_cell.id
        else:
            return -1

    def get_exp_view_cell_content(self,exp_id:int):
        if self.current_exp is not None and exp_id>=0:
            exp = self.get_exp(exp_id)
            return exp.view_cell
        else:
            return None

    def get_exps(self, wt_links = False):
        ''' get all exp in a dict'''
        map_experiences = []
        for exp in self.exps:
            experience = {}
            experience['id'] = exp.id
            experience['x'] = exp.x_m
            experience['y'] = exp.y_m
            experience['facing'] = exp.facing_rad
            experience['init_local_position'] = exp.init_local_position
            experience['observation'] = exp.view_cell.template
            experience['observation_entry_poses'] = exp.view_cell.relevant_poses
            experience['place_kind'] = getattr(exp, 'place_kind', None)
            experience['room_color'] = getattr(exp, 'room_color', None)
            experience['grid_xy']    = getattr(exp, 'grid_xy', None)

            #experience['ob_info'] = exp.view_cell.template_info
            if wt_links == True:
                experience['links'] = exp.links

            map_experiences.append(experience)
        return map_experiences
    def _reanchor_to_current(self):
        """We just committed to self.current_exp as the anchor → zero deltas."""
        self.accum_delta_x = 0.0
        self.accum_delta_y = 0.0
        self.accum_delta_facing = 0.0
    def get_exp_as_dict(self, id=None):
        experience = {}
        if isinstance(id, int):
            exp = self.get(id)
        else:
            exp = self.current_exp
        if exp is not None:
            experience['id'] = exp.id
            experience['x'] = exp.x_m
            experience['y'] = exp.y_m
            experience['facing'] = exp.facing_rad
            experience['observation'] = exp.view_cell.template
            experience['place_kind'] = getattr(exp, 'place_kind', None)
            experience['room_color'] = getattr(exp, 'room_color', None)
            experience['grid_xy']    = getattr(exp, 'grid_xy', None)
            #experience['ob_info'] = exp.view_cell.template_info
            #print('exp ' + str(self.current_exp.id) +' map position x: ' + str(self.current_exp.x_m) + ' y: '+str(self.current_exp.y_m) +' facing: ' +str(self.current_exp.facing_rad))
        return experience

    def get_delta_exp(self, x,y, delta_x, delta_y):
        ''' return euclidian distance between two points'''
        if self.current_exp is None:
            delta_exp = np.inf
        else:
            delta_exp = euclidian_distance([x,y], [delta_x, delta_y]) 
            
        
        return delta_exp   

    def accumulated_delta_location(self, vtrans, vrot):
        # Update delta heading first
        new_delta_facing = clip_rad_180(self.accum_delta_facing + vrot)

        # Heading to move along in MAP/world coordinates
        if self.current_exp is not None:
            theta_world = clip_rad_180(self.current_exp.facing_rad + new_delta_facing)
        else:
            theta_world = new_delta_facing  # no anchor yet

        new_delta_x = self.accum_delta_x + vtrans * np.cos(theta_world)
        new_delta_y = self.accum_delta_y + vtrans * np.sin(theta_world)

        return new_delta_facing, new_delta_x, new_delta_y

    def delta_exp_above_thresold(self, delta_exp:float)->bool:
        print('delta exp and delta threshold', delta_exp, self.DELTA_EXP_THRESHOLD)
        return delta_exp > self.DELTA_EXP_THRESHOLD
    
    def delta_pc_above_thresold(self, delta_pc):
        print('delta exp and delta threshold', delta_pc, self.DELTA_PC_THRESHOLD)
        return delta_pc > self.DELTA_PC_THRESHOLD

    def get_global_position(self):
        if self.current_exp is None:
            return (
                self.accum_delta_x,
                self.accum_delta_y,
                clip_rad_180(self.accum_delta_facing),
            )
        return (
            self.current_exp.x_m + self.accum_delta_x,
            self.current_exp.y_m + self.accum_delta_y,
            clip_rad_180(self.current_exp.facing_rad + self.accum_delta_facing),
        )
        
    
    def get_exp_global_position(self, exp:object=-1)->list:
        if isinstance(exp, int):
            if self.current_exp is not None:
                return [self.current_exp.x_m, self.current_exp.y_m, self.current_exp.facing_rad] 
            else:
                raise "get_exp_global_position can't accept element" + str(exp) +'of type' + str(type(exp))
        elif exp is None:
            raise "get_exp_global_position can't accept element" + str(exp) +'of type' + str(type(exp))
        return [exp.x_m, exp.y_m, exp.facing_rad] 
    # ────────────────────────────── MiniGrid tagging helpers ──────────────────────
    def _env_unwrapped(self, env):
        # Some wrappers hide grid; unwrapped always has .grid and .agent_pos
        return getattr(env, "unwrapped", env)

    def _get_agent_floor_cell(self, env):
        """
        Return (cell, (gx,gy)) where cell is the WorldObj under the agent,
        or None if out of bounds.
        """
        e = self._env_unwrapped(env)
        gx, gy = map(int, map(float, e.agent_pos))   # ensure ints
        if gx < 0 or gy < 0 or gx >= e.width or gy >= e.height:
            return None, (gx, gy)
        return e.grid.get(gx, gy), (gx, gy)

       # ------------------ Place/Color classification helpers ------------------

    def _env_unwrapped(self, env):
        return getattr(env, "unwrapped", env)

    def _neighbors4(self, x, y):
        return ((x+1,y), (x-1,y), (x,y+1), (x,y-1))

    def _is_floor(self, obj):
        # MiniGrid WorldObj: .type ∈ {'floor', 'wall', 'door', ...}
        return getattr(obj, "type", None) == "floor"

    def _is_corridor_color_name(self, color_name: str | None) -> bool:
        # In many ADRooms/Minigrid variants corridors are 'black', 'grey', or 'gray'
        if not color_name:
            return False
        c = color_name.lower()
        return c in ("black", "grey", "gray")

    def _flood_fill_floor(self, env, start_xy, max_tiles=400):
        """BFS over contiguous floor tiles; returns area, color histogram, and bounds."""
        from collections import deque, Counter
        e = self._env_unwrapped(env)
        W, H = e.width, e.height
        sx, sy = start_xy
        if not (0 <= sx < W and 0 <= sy < H):
            return {"area": 0, "colors": Counter(), "bounds": (sx, sx, sy, sy)}

        seen = set()
        q = deque([(sx, sy)])
        colors = Counter()
        xmin = xmax = sx
        ymin = ymax = sy

        while q and len(seen) < max_tiles:
            x, y = q.popleft()
            if (x, y) in seen:
                continue
            seen.add((x, y))
            cell = e.grid.get(x, y)
            if not self._is_floor(cell):
                continue

            colors[getattr(cell, "color", None)] += 1
            if x < xmin: xmin = x
            if x > xmax: xmax = x
            if y < ymin: ymin = y
            if y > ymax: ymax = y

            for nx, ny in self._neighbors4(x, y):
                if 0 <= nx < W and 0 <= ny < H and (nx, ny) not in seen:
                    ncell = e.grid.get(nx, ny)
                    if self._is_floor(ncell):
                        q.append((nx, ny))

        return {"area": len(seen), "colors": colors, "bounds": (xmin, xmax, ymin, ymax)}

    def _classify_place_from_xy(self, env, gx, gy, *, area_room_threshold=None):
        """Classify (ROOM/CORRIDOR/UNKNOWN, color) at a specific grid cell."""
        e = self._env_unwrapped(env)
        if not (0 <= gx < e.width and 0 <= gy < e.height):
            return "UNKNOWN", None, (gx, gy)

        cell = e.grid.get(gx, gy)
        if not self._is_floor(cell):
            return "UNKNOWN", None, (gx, gy)

        color = getattr(cell, "color", None)
        # If it is an explicitly colored floor (not corridor palette), it's a ROOM
        if color and not self._is_corridor_color_name(color):
            return "ROOM", color, (gx, gy)

        # Otherwise use region size and dominant non-corridor color
        region = self._flood_fill_floor(env, (gx, gy))
        thr = self.area_room_threshold if area_room_threshold is None else area_room_threshold
        kind = "ROOM" if region["area"] >= thr else "CORRIDOR"

        # For ROOMs on neutral/black tiles, infer dominant non-corridor color in region
        dom = None
        if kind == "ROOM":
            # Filter out corridor palette; keep any true colors including 'purple'
            candidates = [(c, n) for c, n in region["colors"].items()
                          if c and not self._is_corridor_color_name(c)]
            if candidates:
                dom = max(candidates, key=lambda t: t[1])[0]
        return kind, dom, (gx, gy)

    def _classify_place_from_xy(self, env, gx, gy, *, area_room_threshold=None):
        """
        Classify an arbitrary (gx,gy). Uses metadata first, then door/corridor,
        then a robust bounds-based room fallback, then floor color as last resort.
        """
        try:
            e = self._env_unwrapped(env)
        except Exception:
            e = getattr(env, "unwrapped", env)

        gx = int(gx); gy = int(gy)

        # 1) Exact mapping via xy_to_room
        rid = None
        if hasattr(e, "get_room_id_at"):
            rid = e.get_room_id_at(gx, gy)
        elif hasattr(e, "xy_to_room"):
            rid = e.xy_to_room.get((gx, gy))
        if rid is not None:
            color = None
            if hasattr(e, "get_room_color_by_room"):
                color = e.get_room_color_by_room(rid)
            elif hasattr(e, "room_meta"):
                color = (e.room_meta.get(rid) or {}).get("color")
            return "ROOM", color, rid

        # 2) Door?
        is_door, door_color = False, None
        try:
            cell = e.grid.get(gx, gy) if (0 <= gx < e.width and 0 <= gy < e.height) else None
        except Exception:
            cell = None
        if cell is not None and getattr(cell, "type", None) == "door":
            is_door, door_color = True, getattr(cell, "color", None)
        if not is_door and hasattr(e, "door_xy"):
            is_door = (gx, gy) in set(e.door_xy)
        if is_door:
            return "DOOR", door_color, (gx, gy)

        # 3) Corridor?
        if hasattr(e, "corridor_xy") and (gx, gy) in e.corridor_xy:
            return "CORRIDOR", None, (gx, gy)

        # 4) Robust bounds-based room fallback (fills any mapping holes)
        if hasattr(e, "room_meta"):
            for rid2, meta in e.room_meta.items():
                b = meta.get("bounds")
                if not b or len(b) != 4:
                    continue
                x1, y1, x2, y2 = map(int, b)
                # inclusive bounds
                if x1 <= gx <= x2 and y1 <= gy <= y2:
                    return "ROOM", meta.get("color", None), rid2

        # 5) Last resort: tile-inspection (may classify black floor as ROOM — but only if we
        #    failed to identify corridor, which should be rare now that we register full stripes)
        if cell is not None and getattr(cell, "type", None) == "floor":
            return "ROOM", getattr(cell, "color", None), (gx, gy)

        return "UNKNOWN", None, (gx, gy)
    
    def _classify_place_at_agent(self, env, *, area_room_threshold=None):
        """
        Returns (place_kind, color, grid_xy)
        - place_kind ∈ {"ROOM","CORRIDOR","DOOR","UNKNOWN"}
        - color: room color (or door color) if known
        - grid_xy: (col,row) for ROOM (preferred) else (gx,gy)
        """
        try:
            e = self._env_unwrapped(env)
        except Exception:
            e = getattr(env, "unwrapped", env)

        try:
            gx, gy = map(int, map(float, getattr(e, "agent_pos", (None, None))))
        except Exception:
            gx = gy = None
        if gx is None or gy is None:
            return "UNKNOWN", None, None

        return self._classify_place_from_xy(env, gx, gy, area_room_threshold=area_room_threshold)


    def _annotate_place_from_env(self, exp, env):
        """
        Fill exp.place_kind, exp.room_color, exp.grid_xy using env's room/corridor map
        created at generation time. Falls back to tile-inspection if metadata is absent.
        """
        try:
            kind, color, gxy = self._classify_place_at_agent(env)
        except Exception:
            kind, color, gxy = "UNKNOWN", None, None

        exp.place_kind = kind
        exp.room_color = color
        exp.grid_xy    = gxy
        return exp
        
            

    def __call__(self, view_cell, vtrans, vrot, x_pc, y_pc, th_pc, adjust, local_pose=None, view_cell_copy=None):
        '''Run an interaction of the experience map.

        :param view_cell: the last most activated view cell.
        :param x_pc: index x of the current pose cell.
        :param y_pc: index y of the current pose cell.
        :param th_pc: index th of the current pose cell.
        :param vtrans: the translation of the robot given by odometry.
        :param vrot: the rotation of the robot given by odometry.
        :param adjust: run the map adjustment procedure.
        '''
        prev_dx, prev_dy = self.accum_delta_x, self.accum_delta_y
        self.accum_delta_facing, self.accum_delta_x, self.accum_delta_y = self.accumulated_delta_location(vtrans, vrot)
        theta_snap(self, "post-integrate")
        theta_forward_check(self, vtrans, prev_dx, prev_dy, "post-integrate")
        
        #delta_prev_exp = self.get_delta_exp(0,0,self.accum_delta_x, self.accum_delta_y)
        #print('accumulated dist from prev exp: ' + str(self.accum_delta_x) + ' y: '+str(self.accum_delta_y) )
        
        #delta_exp_above_thresold = self.delta_exp_above_thresold(delta_prev_exp)
        adjust_map = False

        if self.current_exp != None:
        
            #approximate curent position
            current_GP = self.get_global_position()
            
            print('CHECK CURRENT GP X,Y,TH', current_GP )
            print("EXPS IN VIEW CELL", view_cell.exps, "CURRENT CELL",self.current_exp )
            print("VIEW CELL ID",view_cell.id, "CURRENT EXP VIEW CELL ID",self.current_exp.view_cell.id)
        
            delta_exps = []
        
            for e in self.exps:
                print("this are the experiences in self.exps",e,e.x_m,e.y_m,"id  ",e.view_cell.id)
                print(e.view_cell.id,e.view_cell.exps)
                print(e.view_cell.x_pc, e.view_cell.y_pc)
                for (i, e) in enumerate(e.view_cell.exps):
                    print('exp ' + str(i) +' view position x: ' + str(e.x_m) + ' y: '+str(e.y_m) )
                delta_exp = self.get_delta_exp(e.x_m,e.y_m, current_GP[0], current_GP[1])
                delta_exps.append(delta_exp)
            
            min_delta_GP_id = np.argmin(delta_exps)
            min_delta_GP_val = delta_exps[min_delta_GP_id]
            print('delta_exps',delta_exps,min_delta_GP_val)
            delta_pc = np.sqrt(
                min_delta(self.current_exp.x_pc, x_pc, self.DIM_XY)**2 +
                min_delta(self.current_exp.y_pc, y_pc, self.DIM_XY)**2 
                #+ min_delta(self.current_exp.th_pc, th_pc, self.DIM_TH)**2
                )
            print('current experience x: ',self.current_exp.x_m, ', y:',self.current_exp.y_m,' and th:', self.current_exp.facing_rad)
            print('current position x_pc: ',x_pc, ', y_pc:',y_pc,' and th_pc:', th_pc)
            print("current exp pose cells",self.current_exp.x_pc,self.current_exp.y_pc)
            print('delta_pc between exp and currenttty pc',delta_pc)

            close_exp_link_exists = False
            if min_delta_GP_id != self.current_exp.id:
                for linked_exp in [l.target for l in self.current_exp.links]:
                    if linked_exp.id == min_delta_GP_id:
                        close_exp_link_exists = True
                        break
            else:
               #If the closest exp is the current exp, then a link between the two does exist
               close_exp_link_exists = True 
        else:
            min_delta_GP_val = np.inf
            min_delta_GP_id= 0
            delta_pc = np.inf

        print('accumulated dist in exp map', self.accum_delta_x, self.accum_delta_y, self.accum_delta_facing,'is delta_exp_above_thresold?',min_delta_GP_val<self.DELTA_EXP_THRESHOLD, min_delta_GP_val,self.DELTA_EXP_THRESHOLD, 'closest exp dist', min_delta_GP_val, min_delta_GP_id)
        print('len exps =' ,len(view_cell.exps))
        # if current exp is None, just select first matching?
        if self.current_exp is None and Experience._ID == 0 :
            print('first experience created',local_pose)
            exp = self._create_exp(x_pc, y_pc, th_pc, view_cell, local_pose)

            self.current_exp = exp
            self.accum_delta_x = 0
            self.accum_delta_y = 0
            self.accum_delta_facing = 0
            #self.current_exp = view_cell.exps[0]
        #if we loaded a memory map, then we need to get experience matching view cell
        elif self.current_exp is None:
            self.current_exp = view_cell.exps[0] #NOTE: this works considering that 1 exp has 1 ob
            print('we are initialising position to',self.current_exp.id, 'extracted from , based on observation', view_cell.id)

            self.accum_delta_x = 0
            self.accum_delta_y = 0
            self.accum_delta_facing = self.current_exp.facing_rad
                   

        #We have a new view but it's close to a previous experience
        elif len(view_cell.exps) == 0 and min_delta_GP_val < (self.DELTA_EXP_THRESHOLD):
            print('too close from exp', min_delta_GP_id, ', dist between exp and here', min_delta_GP_val)
            print("current exp", self.current_exp)
            exp = self.get_exp(min_delta_GP_id)

            delta_exp_pc = np.sqrt(
                min_delta(self.current_exp.x_pc, exp.x_pc, self.DIM_XY)**2 +
                min_delta(self.current_exp.y_pc, exp.y_pc, self.DIM_XY)**2
                # + min_delta(self.current_exp.th_pc, exp.th_pc, self.DIM_TH)**2
            )
            print('checking motion between two considered experiences', delta_exp_pc)

            # Only perform loop‐closure if it's a different experience
            if min_delta_GP_id != self.current_exp.id:
                print('we are close looping with exp', min_delta_GP_id, 'discarding newly generated view_cell', view_cell.id)
                adjust_map = True
                close_loop_exp = self.exps[min_delta_GP_id]

                # see if the exp nearby already has a link to the current exp
                link_exists = any(l.target == close_loop_exp for l in self.current_exp.links)

                if not link_exists:
                    # link both ways
                    self.current_exp.link_to(
                        close_loop_exp,
                        self.accum_delta_x,
                        self.accum_delta_y,
                        close_loop_exp.facing_rad,
                        active_link=True
                    )
                    close_loop_exp.link_to(
                        self.current_exp,
                        -self.accum_delta_x,
                        -self.accum_delta_y,
                        self.current_exp.facing_rad,
                        active_link=False
                    )
                    theta_link_check(self.current_exp, close_loop_exp, self.current_exp.links[-1], "match->fwd")
                    theta_link_check(close_loop_exp, self.current_exp, close_loop_exp.links[-1], "match->back")


                # Remember current GP BEFORE changing anchor
                prev_gx, prev_gy, prev_gth = self.get_global_position()

                # commit to new anchor
                self.current_exp = close_loop_exp

                # set residuals so GP stays the same for x, y, *and* θ
                self.accum_delta_x = prev_gx  - close_loop_exp.x_m
                self.accum_delta_y = prev_gy  - close_loop_exp.y_m
                self.accum_delta_facing = clip_rad_180(prev_gth - close_loop_exp.facing_rad)

                print("Global Position:", self.get_global_position(),
                    self.current_exp.x_m, self.current_exp.y_m, self.current_exp.facing_rad)
                print('We keep current GP facing rad:', 
                    self.accum_delta_x, self.accum_delta_y, self.accum_delta_facing)
            
        # if the vt is new AND the global pose x,y,th is far enough from any prev experience create a new experience
        elif len(view_cell.exps) == 0:
            #if current location is far enough from prev one, else, view cells are considered as in conflict
            print('no exp in view, len =' ,len(view_cell.exps), 'closest exp dist', min_delta_GP_val )
            exp = self._create_exp(x_pc, y_pc, th_pc, view_cell,local_pose)

                            
            self.current_exp = exp
            self.accum_delta_x = 0
            self.accum_delta_y = 0
            self.accum_delta_facing = 0
            
        

        # if the vt has changed (but isn't new) search for the matching exp
        elif view_cell != self.current_exp.view_cell:
            # find the exp associated with the current vt and that is under the
            # threshold distance to the centre of pose cell activity
            # if multiple exps are under the threshold then don't match (to reduce
            # hash collisions)
            print('new selected view_cell is different from previous one but has exp (known view)')
            print('view_cell ID and current exp view cell id', view_cell.id, self.current_exp.view_cell.id)
            adjust_map = True
            matched_exp = None

            delta_view_exps = []
            n_candidate_matches = 0
   
            for (i, e) in enumerate(view_cell.exps):
                print('exp ' + str(i) +' view position x: ' + str(e.x_m) + ' y: '+str(e.y_m) )
                
                delta_view_exp = self.get_delta_exp(e.x_m,e.y_m,current_GP[0], current_GP[1])
                delta_view_exps.append(delta_view_exp)

                if delta_view_exp < self.DELTA_EXP_THRESHOLD:
                    n_candidate_matches += 1

            if n_candidate_matches > 1:
                print('more than 1 exp could correspond, candidate match > 1', n_candidate_matches)
                pass

            else:
                print('delta_view_exps of all exp of this view', delta_view_exps)
                
                min_delta_GP_id = np.argmin(delta_view_exps)
                min_delta_GP_val = delta_view_exps[min_delta_GP_id]
                if min_delta_GP_val < self.DELTA_EXP_THRESHOLD :
                    print('the delta between exp' , view_cell.exps[min_delta_GP_id].id ,' and ' ,self.current_exp.id, ' allow for a close-loop')
                    matched_exp = view_cell.exps[min_delta_GP_id]

                    # see if the prev exp already has a link to the current exp
                    link_exists = False
                    for linked_exp in [l.target for l in self.current_exp.links]:
                        if linked_exp == matched_exp:
                            link_exists = True
                            break

                    if not link_exists:
                        print(current_GP)
                        print("we are linking things",matched_exp, self.accum_delta_x, self.accum_delta_y, self.accum_delta_facing)
                        self.current_exp.link_to(
                            matched_exp,
                            self.accum_delta_x,
                            self.accum_delta_y,
                            matched_exp.facing_rad,        # <— target abs θ
                            active_link=True
                        )
                        matched_exp.link_to(
                            self.current_exp,
                            -self.accum_delta_x,           # <— negate
                            -self.accum_delta_y,
                            self.current_exp.facing_rad,   # <— target abs θ
                            active_link=False
                        )
                        # then snap and zero deltas
                        # Remember current GP BEFORE changing anchor
                        prev_gx, prev_gy, prev_gth = self.get_global_position()

                        # Commit to the new anchor
                        self.current_exp = matched_exp

                        # Re-carry residuals so GP stays EXACTLY the same
                        self.accum_delta_x      = prev_gx  - matched_exp.x_m
                        self.accum_delta_y      = prev_gy  - matched_exp.y_m
                        self.accum_delta_facing = clip_rad_180(prev_gth - matched_exp.facing_rad)
                if matched_exp is None:
                    if min_delta_GP_val > self.DELTA_EXP_THRESHOLD:
                        nearest_exp   = None
                        nearest_dist  = float("inf")
                        for e in self.exps:
                            d = self.get_delta_exp(e.x_m, e.y_m, current_GP[0], current_GP[1])
                            if d < nearest_dist:
                                nearest_dist, nearest_exp = d, e

                        if nearest_dist < self.DELTA_EXP_THRESHOLD:
                            print(f"[EM] SNAP to exp {nearest_exp.id}  (d={nearest_dist:.2f})")

                            # --- (A) Remember the old anchor and current GP before re-anchoring
                            prev_gx, prev_gy, prev_gth = self.get_global_position()

                            # --- (B) Optionally add a link old_exp -> matched_exp if it doesn't exist
                            
                            # before creating any link, right after computing nearest_exp / matched_exp
                            old_exp = self.current_exp
                            matched_exp = nearest_exp

                            if matched_exp is old_exp:
                                # We are already anchored on this node → no new link, no re-anchor churn.
                                # Optionally: keep residuals as-is, or zero them if you want strict snap.
                                # Here I keep residuals (continuous GP policy).
                                print(f"[EM] SNAP resolved to current exp {matched_exp.id} → no link/no-op")
                                # If you had planned to re-anchor math here, skip it and just return to the flow.
                            else:
                                # link-exists check (forward direction)
                                link_exists = any(l.target is matched_exp for l in old_exp.links)

                                # amount of *translational* odometry we just accumulated
                                d_trans = (self.accum_delta_x**2 + self.accum_delta_y**2) ** 0.5

                                # Gate: only add link if (1) not already present AND (2) we actually moved a bit
                                # You can tune eps; 1e-6 is numerically safe, 1e-3..1e-2 is pragmatic in grid envs
                                if (matched_exp is not old_exp) and (not link_exists) and (d_trans > 1e-6):
                                    # Forward link: old_exp -> matched_exp uses the ODOMETRY deltas
                                    old_exp.link_to(
                                        matched_exp,
                                        self.accum_delta_x,
                                        self.accum_delta_y,
                                        matched_exp.facing_rad,   # target absolute θ (see link_to implementation)
                                        active_link=True
                                    )

                                    # Backward link: matched_exp -> old_exp with negated deltas
                                    matched_exp.link_to(
                                        old_exp,
                                        -self.accum_delta_x,
                                        -self.accum_delta_y,
                                        old_exp.facing_rad,       # target absolute θ
                                        active_link=False
                                    )

                                # (Optional) tiny debug to verify internal consistency of the just-created links
                                # theta_link_check(old_exp, matched_exp, old_exp.links[-1], "snap->fwd")
                                # theta_link_check(matched_exp, old_exp, matched_exp.links[-1], "snap->back")

                                # --- (C) Re-anchor using your *continuous* policy so GP does not jump
                                self.current_exp = matched_exp
                                self.accum_delta_x       = prev_gx  - matched_exp.x_m
                                self.accum_delta_y       = prev_gy  - matched_exp.y_m
                                self.accum_delta_facing  = clip_rad_180(prev_gth - matched_exp.facing_rad)

                        else:
                            print("Creating new experience because no exp within metric gate")    
                            matched_exp = self._create_exp(x_pc, y_pc, th_pc,
                                                        view_cell_copy, local_pose)
                            self.current_exp   = matched_exp
                            self.accum_delta_x = self.accum_delta_y = 0
                            self.accum_delta_facing = 0.0 
                            print("warning, no matched experience although matching view cell ", min_delta_GP_val, "did we enter though?",nearest_dist > self.DELTA_EXP_THRESHOLD)
                    '''#View_cell_copy contains the same ob as view_cell but without experience attached and with a new ID
                    #TODO: add view_cell_copy instead of view_cell
                    matched_exp = self._create_exp(
                        x_pc, y_pc, th_pc, view_cell_copy,local_pose)
                    self.current_exp = matched_exp
                    self.accum_delta_x = 0
                    self.accum_delta_y = 0'''

                #self.current_exp = matched_exp
                #self.accum_delta_x = 0
                #self.accum_delta_y = 0
    
                #commented line below as we don't narrow close loop around position but place, this means no strict position correction in odom
                #self.accum_delta_facing = self.current_exp.facing_rad
        # print('current experience id and matching view',self.current_exp.id, view_cell.id)
        # print('exp ' + str(self.current_exp.id) +' map position x: ' + str(self.current_exp.x_m) + ' y: '+str(self.current_exp.y_m) +' facing: ' +str(self.current_exp.facing_rad))
        
        #If nothing particular and we are close from a previous experience, we link them if not already
        elif min_delta_GP_val < (0.3*self.DELTA_EXP_THRESHOLD) and close_exp_link_exists != True : 
            # if the closest exp has no link to the current exp
            print(current_GP)
            print('linking current exp ', self.current_exp.id, ' to closest exp ', min_delta_GP_id )
            min_delta_GP_exp = self.get_exp(min_delta_GP_id)

            delta_x = min_delta_GP_exp.x_m - self.current_exp.x_m
            delta_y = min_delta_GP_exp.y_m - self.current_exp.y_m

            self.current_exp.link_to(
                min_delta_GP_exp, delta_x, delta_y, min_delta_GP_exp.facing_rad, active_link=True
            )
            min_delta_GP_exp.link_to(
                self.current_exp, -delta_x, -delta_y, self.current_exp.facing_rad, active_link=False
            )

            

        #If we replaced the view cell in same exp, we need to update the global position to match init_local_position
        if self.current_exp.view_cell.to_update:
            print('Updating global location of the exp ', self.current_exp.id , 'to match the new init_local_position of view cell:',local_pose )

            print('OLD current exp position x: ' + str(self.current_exp.x_m) + ' y: '+str(self.current_exp.y_m) , ' facing: '+str(self.current_exp.facing_rad) \
                      , ' local pose: '+str(self.current_exp.init_local_position), 'view cell id', self.current_exp.view_cell.id ) 
            self.update_exp_wt_view_cell(self.current_exp, x_pc, y_pc, th_pc, new_view_cell= None, local_pose= local_pose)
            self.accum_delta_x=0
            self.accum_delta_y=0
            #self.accum_delta_facing = self.current_exp.facing_rad
            print('NEW current exp position x: ' + str(self.current_exp.x_m) + ' y: '+str(self.current_exp.y_m) , ' facing: '+str(self.current_exp.facing_rad) \
                , ' local pose: '+str(self.current_exp.init_local_position), 'view cell id', self.current_exp.view_cell.id) 
            self.current_exp.view_cell.to_update = False
            for link in self.current_exp.links:
                e1 = link.target
                print(current_GP)
                print('original link of current exp', self.current_exp.id, link.facing_rad, link.heading_rad, link.d)
                #change link from source to linked exp
                self.current_exp.update_link(link, e1)
                print('updated link of current exp', self.current_exp.id, link.facing_rad, link.heading_rad, link.d)
                
                #change link from linked exp to source
                for l in e1.links:
                    if l.target == self.current_exp:
                        print('original link of linked exp', e1.id, l.facing_rad, l.heading_rad, l.d)
                        e1.update_link(l, self.current_exp)
                        print('updated link of linked exp', e1.id, l.facing_rad, l.heading_rad, l.d)
                        break
        
      
        print('self.current_exp.init_local_position',  self.current_exp.init_local_position)
        print("\n=== LINK DUMP (after step) ===")
        for e in self.exps:
            for l in e.links:
                print(f"{e.id:3}  → {l.target.id:3} | d={l.d:5.2f}  "
                    f"hdg={l.heading_rad:+5.2f}  fac={l.facing_rad:+5.2f} "
                    f"{'ACTIVE' if l.active_link else 'inv'}")
        print("=== END LINK DUMP ===\n")

        print('adjust map?', self.constant_adjust, adjust, adjust_map)
        
        do_adjust = (self.constant_adjust or (adjust and adjust_map))
        if not do_adjust:
            return
        if NO_DRIFT_MODE or not do_adjust:
            # If you want GP continuity across re-anchoring (what you already do),
            # keep the residuals as set above (prev_gx/gy/gth logic you already run).
            # If you prefer hard snap in a perfect world, uncomment the next 3 lines:
            # self.accum_delta_x = 0.0
            # self.accum_delta_y = 0.0
            # self.accum_delta_facing = 0.0

            theta_gp_check(self, "after-step")
            return
        prev_gx, prev_gy, prev_gth = self.get_global_position()
        self.relax_graph(fixed_exp=self.current_exp, loops=self.LOOPS)
        ax, ay, ath = self.current_exp.x_m, self.current_exp.y_m, self.current_exp.facing_rad
        self.accum_delta_x      = prev_gx  - ax
        self.accum_delta_y      = prev_gy  - ay
        self.accum_delta_facing = clip_rad_180(prev_gth - ath)
        for e0 in self.exps:           
            print('aFTER CORRECTIONS  exp ' + str(e0.id) +' map position x: ' + str(e0.x_m) + ' y: '+str(e0.y_m) +' facing: ' +str(e0.facing_rad))
          

        #print('AFTER CORRECTION  exp ' + str(self.current_exp.id) +' map position x: ' + str(self.current_exp.x_m) + ' y: '+str(self.current_exp.y_m) +' facing: ' +str(self.current_exp.facing_rad))
        # print( )
        # print('____')
        theta_gp_check(self, "after-step")
        return
```

`hierarchical-nav/navigation_model/Services/memory_service/memory_graph.py`:

```py
# Update 2022
# =============================================================================
# Ghent University 
# IDLAB of IMEC
# Daria de Tinguy - daria.detinguy at ugent.be
# =============================================================================

# Original Source
# =============================================================================
# Federal University of Rio Grande do Sul (UFRGS)
# Connectionist Artificial Intelligence Laboratory (LIAC)
# Renato de Pontes Pereira - rppereira@inf.ufrgs.br
# =============================================================================
# Copyright (c) 2013 Renato de Pontes Pereira, renato.ppontes at gmail dot com
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# =============================================================================

'''
This is a full MemoryGraph implementation in python heavily based on the Ratslam implementation.
This implementation is based on Milford's original implementation [1]_ 
in matlab, and Christine Lee's python 
implementation [2]_. 

.. [2] https://github.com/coxlab/ratslam-python
.. [3] http://www.numpy.org/

'''
import torch
import dill
import cv2
import numpy as np
from .odometry import VisualOdometry,\
    PoseOdometry, ActionOdometry, Odometry, HotEncodedActionOdometry
from .view_cells import  TorchedViewCells, TorchedViewCell
from .pose_cells import PoseCells
from .experience_map import Experience, ExperienceMap
from .modules import *
# from matplotlib import pyplot as plt
# import mpl_toolkits.mplot3d.axes3d as p3


class MemoryGraph(object):
    '''MemoryGraph implementation.

    The memory_graph is divided into 4 modules: visual odometry, view cells, pose
    cells, and experience map. This class also store the odometry and pose
    cells activation in order to plot them.
    '''

    def __init__(self, observation="camera", odom="action", 
                 **kwargs):
        '''Initializes the memory_graph modules.
        :param key: which key to use as observation
        :param odom: which key to use for odometry estimation
        '''
        self.observation_key = observation
        self.device = torch.device('cpu')

        self.local_position = kwargs.get('local_position', None)
        self.ghost_node_process = kwargs.get('ghost_node', False)
        self.odometry_key = odom
       
        if odom == "odom":
            self.odom = Odometry()
        elif odom == "action":
            self.odom = ActionOdometry()
        elif odom == "HEaction":
            self.odom = HotEncodedActionOdometry()
        elif odom == "pose":
            self.odom = PoseOdometry()
        else:
            self.odom = VisualOdometry(observation)

        self.view_cells = TorchedViewCells(observation, **kwargs)

        self.pose_cells = PoseCells(**kwargs)
        self.experience_map = ExperienceMap(**kwargs)
        
        Experience._ID = 0
        TorchedViewCell._ID = 0
        self.observation = None
        self.odometry = [[], [], []]
        self.pc = [[], [], []]

        

        #TODO: TEMPO fixed odom (no pose_cell in imp1):
        #self.odometry = [0.,0.,np.pi/2]
        
    def digest(self, observations, dt=0.1, adjust_map=True):
        # transform from torch format to numpy format
        # after migration this should no longer be required
        if "TensorDict" in str(type(observations)):
            observations = {key: val.numpy()
                            for key, val in observations.items()}
            if 'camera' in observations:
                observations["camera"] = np.transpose(
                observations["camera"], (1, 2, 0))
            if 'image' in observations:
                observations["image"] = np.transpose(
                observations["image"], (1, 2, 0))

        '''Execute a step of memory_graph algorithm for observations
        at the current time step.

        :param observations: a key-value dict of observations
        '''

        
                                                  
        self.observation = observations[self.observation_key]
        print("wtf is happening here",self.observation,observations[self.odometry_key])
        action_ob = observations[self.odometry_key]
        
        kwargs = {}
        kwargs['KL'] = observations.get('KL',None)
        if self.local_position:
            local_pose = observations.get(self.local_position,None)
            kwargs['local_position'] = local_pose

        

        # t1 = time.time()
        # get active pose and view cells
        # x_pc, y_pc, th_pc = self.pose_cells.active
        #TODO: clean later odom param (here we consider global odom directly from odom)
        x_pc, y_pc, th_pc = self.pose_cells.active
        print(x_pc, y_pc, th_pc)
        
        #if no observation, we consider previous one
        # if self.observation is None:
        #     self.observation = self.view_cells.prev_cell.template
        # get odom estimations
        vtrans, vrot = self.odom(action_ob, dt)
        x, y, th = self.odom.odometry
        print("odom", x, y, th)

        #TODO: TEMPO SIMPLIFICQTION
        #We want to create a view_cell ONLY if distance is far enough from prev one.
        #_,accum_delta_x, accum_delta_y  = self.experience_map.accumulated_delta_location(vtrans,vrot)
        #delta_exp = self.experience_map.get_delta_exp(0,0,accum_delta_x, accum_delta_y)
        print(f"[DEBUG] odometry → vtrans={vtrans:.6f}, vrot={vrot:.6f}, odom pose=({x:.2f}, {y:.2f}, {th:.2f})")

        
        rotation_only = abs(vtrans) < 1e-4 and abs(vrot) > 0
        print(f"[DEBUG] rotation_only? {rotation_only}")

        if rotation_only and self.experience_map.current_exp is not None:
            # override: don’t create a new cell if we only turned
            print(f"[DEBUG] Pure rotation detected. Forcing no new experience creation.")
            delta_pc_above_thresold = False
            kwargs['current_exp_id']= self.experience_map.current_exp.id
            kwargs['delta_exp_above_thresold'] = False
        else:
            if self.experience_map.current_exp is not None:
                x_exp = self.experience_map.current_exp.x_pc
                y_exp = self.experience_map.current_exp.y_pc
                print(f"[DEBUG] Translational step. last exp at (x_pc={x_exp}, y_pc={y_exp}), now at (x_pc={x_pc}, y_pc={y_pc})")
                delta_pc = self.experience_map.get_delta_exp(x_exp, y_exp, x_pc, y_pc)
                delta_pc_above_thresold = self.experience_map.delta_pc_above_thresold(delta_pc)
                print(f"[DEBUG] computed delta_pc={delta_pc:.4f}, above_threshold={delta_pc_above_thresold}")
                kwargs['current_exp_id'] = self.experience_map.current_exp.id
            else:
                delta_pc_above_thresold = 100
                kwargs['current_exp_id'] = None
        print(delta_pc_above_thresold)
        kwargs['delta_exp_above_thresold'] = delta_pc_above_thresold
        
        view_cell, view_cell_copy = self.view_cells(self.observation, x_pc, y_pc, th_pc, **kwargs)
        #view_cell_copy: same as view cell but with a different id
       
        print(self.view_cells.cells,len(self.view_cells.cells))
        if view_cell is None:
            #this only happens if we start memory_graph with a memory 
            # and have yet to define the place (no observation yet)
            return

        # update pose cells
        print("this goes into a pose cell",self.observation, view_cell,vtrans,vrot,adjust_map,"local pose",local_pose)
        x_pc, y_pc, th_pc = self.pose_cells(view_cell, vtrans, vrot)
        print("this goes into a pose cell",x_pc, y_pc, th_pc)           
        if self.experience_map.current_exp is None:
            # reset the pose cells to this activity
            print("reset the pose cell")
            self.pose_cells.reset(view_cell.x_pc,
                                  view_cell.y_pc,
                                  view_cell.th_pc)
            

       
        print("this are the view_cell pre exp:",view_cell.exps,view_cell,view_cell.id)
        # update experience map
        self.experience_map(view_cell, vtrans, vrot,
                            x_pc, y_pc, th_pc, adjust_map, local_pose, view_cell_copy)
        if self.experience_map.current_exp is not None:
            self.view_cells.update_prev_cell(self.experience_map.current_exp.view_cell)
        #TEST
        if self.experience_map.current_exp is not None:
            pose = observations.get(self.local_position,None)
            print('memory_graph pose', pose)
            try:
                pose_GP_facing = local_encoded_orientation_to_global_facing(pose[2], self.experience_map.current_exp.init_local_position[2], self.experience_map.current_exp.facing_rad)
                test = encoded_orientation_given_facing(pose_GP_facing, self.experience_map.current_exp.init_local_position[2], self.experience_map.current_exp.facing_rad)
                print('must be True', test == pose[2], test, pose[2])
            except TypeError:
                 print('type error')

        # for tracking and plotting
        self.odometry[0].append(x)
        self.odometry[1].append(y)
        self.odometry[2].append(th)
        self.pc[0].append(x_pc)
        self.pc[1].append(y_pc)
        self.pc[2].append(th_pc)

#============= Ghost node creation ================#
    def create_ghost_exps(self, exp_id:int = None, steps_margin:int = 3)-> None:
        ''' Params:
        exp_id: the experience id we want to create ghost nodes for
        steps_margin: how further from the limits of prev place do we want to create the ghost node at
        Ghost nodes are created at the GP of this exp door poses + some margin'''
        
        if not self.ghost_node_process :
            return
        
        if exp_id is None:
            ref_exp = self.current_exp
        else:
            ref_exp = self.get_exp(exp_id)
        #No exp? then don't do anything
        if ref_exp is None:
            return
        #TODO: this is specific to ghost created AT ALL relevant poses, 
        # might not be pertinent in a non minigrid env
        exp_local_pose = ref_exp.init_local_position
        exp_GP = [ref_exp.x_m, ref_exp.y_m, ref_exp.facing_rad]

        relevant_poses = self.get_exp_relevant_poses(ref_exp)
        GP_relevant_poses = []
        for pose in relevant_poses:
            GP_pose = convert_LP_to_GP(exp_GP, exp_local_pose, pose)
            for margin in range(self.get_delta_exp_threshold() + 1):
                GP_pose = self.odom.position_applying_motion(GP_pose, [1,0,0])
            GP_relevant_poses.append(GP_pose)
        self.experience_map.create_ghost_exps(ref_exp, GP_relevant_poses)
          
#============== GET METHODS ================#
    def get_delta_exp_threshold(self):
        """ min distance between two exps"""
        return self.experience_map.DELTA_EXP_THRESHOLD
    
    def get_exp_relevant_poses(self, exp_id= None):
        ''' Extract  exp view cell relevant_poses'''
        if exp_id == None:
            exp = self.experience_map.current_exp
        else:
            exp = self.experience_map.get_exp(exp_id)
        if exp == None:
            return []
        print("relevant poses", len(exp.view_cell.relevant_poses.copy()),exp.view_cell.relevant_poses.copy())
        return exp.view_cell.relevant_poses.copy()

    def get_next_place_view_id(self, door_pose):
        ''' old strategy to link exp through doors, but it is dependant on correct imagination (door imagined + ~correct position)'''
        GP_door_pose = [0,0,0]
        current_exp = self.experience_map.current_exp
        global_position = [current_exp.x_m, current_exp.y_m, current_exp.facing_rad]
        exp_local_pose = current_exp.init_local_position

        door_dist_to_start_pose = [int(door_pose[0] - exp_local_pose[0]), int(door_pose[1] - exp_local_pose[1])]
        #GP localisation of the given door in current exp
        entry_door_facing_rad = local_encoded_orientation_to_global_facing(0, exp_local_pose[2], global_position[2])
        
        print('current exp Lp and GP',exp_local_pose, global_position)
        GP_door_pose[0] = global_position[0] +  (door_dist_to_start_pose[0] * np.cos(entry_door_facing_rad) - door_dist_to_start_pose[1] * np.sin(entry_door_facing_rad))
        GP_door_pose[1] = global_position[1] +  (door_dist_to_start_pose[0] * np.sin(entry_door_facing_rad) + door_dist_to_start_pose[1] * np.cos(entry_door_facing_rad))
        
        #get the GP orientation of the door when facing the room (given the view when facing it)
        door_room_facing = (door_pose[2]+2) %4
        door_facing_room_rad = local_encoded_orientation_to_global_facing(door_room_facing, exp_local_pose[2], global_position[2])
        
        print('CURRENT PLACE Door ', door_pose,' global pose', GP_door_pose[:2], 'orientation facing room', door_facing_room_rad,' door lp dist to init pose', door_dist_to_start_pose, )
        print('exp xpc, ypc tph + init lp', [current_exp.x_pc, current_exp.y_pc, current_exp.th_pc], current_exp.init_local_position)

        for link in current_exp.links:
            print()
            linked_exp = link.target
            if linked_exp.ghost_exp == True:
                print('IMPLEMENT WHAT HAPPENS IF THERE IS A GHOST NODE')
                continue
            
            linked_exp_global_position =   [linked_exp.x_m, linked_exp.y_m, linked_exp.facing_rad] 
            linked_exp_view = self.experience_map.get_exp_view_cell_content(linked_exp.id)
            
            linked_exp_local_pose = linked_exp.init_local_position
            linked_exp_door_poses = linked_exp_view.relevant_poses

            linked_exp_entry_door_facing_rad = local_encoded_orientation_to_global_facing(0, linked_exp_local_pose[2], linked_exp_global_position[2])
            print('exp',linked_exp.id ,' Lp and GP',linked_exp_local_pose, linked_exp_global_position)
            print('exp xpc, ypc, tph + init lp', [linked_exp.x_pc, linked_exp.y_pc, linked_exp.th_pc], linked_exp.init_local_position)
            #get the GP orientation of the entry doors of the linked exp
            for link_exp_door_pose in linked_exp_door_poses:
                linked_exp_GP_door_pose = [0,0,0]
               
                door_dist_to_start_pose = [int(link_exp_door_pose[0] - linked_exp_local_pose[0]), int(link_exp_door_pose[1] - linked_exp_local_pose[1])]
                linked_exp_GP_door_pose[0] = linked_exp_global_position[0] +  (door_dist_to_start_pose[0] * np.cos(linked_exp_entry_door_facing_rad) - door_dist_to_start_pose[1] * np.sin(linked_exp_entry_door_facing_rad))
                linked_exp_GP_door_pose[1] = linked_exp_global_position[1] +  (door_dist_to_start_pose[0] * np.sin(linked_exp_entry_door_facing_rad) + door_dist_to_start_pose[1] * np.cos(linked_exp_entry_door_facing_rad))
                
                linked_exp_GP_door_pose[2] = local_encoded_orientation_to_global_facing(link_exp_door_pose[2], linked_exp_local_pose[2], linked_exp_global_position[2])
                print('linked exp ', linked_exp.id,'considered door ',link_exp_door_pose,' GP door', linked_exp_GP_door_pose, 'dist to this exp init lp', door_dist_to_start_pose)
                
                delta_gp = [np.abs(linked_exp_GP_door_pose[0] - GP_door_pose[0]), np.abs(linked_exp_GP_door_pose[1] - GP_door_pose[1])]
                print('delta GP in x, y between this door and current exp door', delta_gp)
                #If inversed orientation match the current door entry, then it means we are facing the door leading to this exp
                if np.abs(linked_exp_GP_door_pose[2] - door_facing_room_rad) < np.pi/3 and delta_gp[0] < 7 and delta_gp[1] < 7:
                    relevant_poses = self.get_exp_relevant_poses(linked_exp.id)
                    door_pose_from_new_place = [pose for pose in relevant_poses if pose[2] == link_exp_door_pose[2]][0]

                    print('returning linked exp ', linked_exp.id, ' view id ',linked_exp_view.id, 'door pose from this view perspective', door_pose_from_new_place)
                    return  linked_exp.id, door_pose_from_new_place

        return -1, []
    
    def get_exps(self, wt_links:bool = False)->dict:
        ''' 
        give back all the experiences as dict organised in a list with id, map position, 
        view cell template and infos (if any). They will be organised by exp id
        '''

        return self.experience_map.get_exps(wt_links = wt_links)

    def get_exps_organised(self, exp_id:int = None, from_current_pose:bool=False) -> dict:
        '''
        organise the exps from closest to further from given exp, in term of distance
        '''
        
        exps = self.get_exps()
        if from_current_pose:
            #current Global Position
            agent_gp = self.get_global_position()
            goal_exp = {'id':-1, 'x':agent_gp[0], 'y':agent_gp[1], 'facing':agent_gp[2] }
        elif exp_id is None:
            #current exp
            goal_exp = self.experience_map.get_exp_as_dict()
        elif isinstance(exp_id, int) and exp_id >= 0:
            #given exp
            goal_exp = exps[exp_id]
            
            #print('goal exp',goal_exp['id'])
        if not goal_exp :
            return exps
        exps.sort(key=lambda i: sort_by_distance(i, goal_exp))
        return exps
        
    def get_current_exp_and_view_id(self) -> tuple[int,int]:
        exp_id = self.experience_map.get_current_exp_id()
        view_cell_id = self.get_exp_view_cell_id(exp_id)

        return exp_id, view_cell_id

    def get_current_exp_id(self) ->int:
        exp_id = self.experience_map.get_current_exp_id()
        return exp_id
        
    def get_exp_view_cell_id(self, id:int)-> int:
        ''' given location id, get view cell id'''
        view_cell = self.get_exp_view_cell(id)
        if view_cell is not None:
            return view_cell.id
        else:
            return -1
    
    def get_view_cell_template(self, id:int)-> torch.Tensor: 
        return self.view_cells.templates[id]
    
    def get_exp_view_cell(self, id:int) -> object:   
        ''' given location id, extract view id '''
        return self.experience_map.get_exp_view_cell_content(id)
    
    def get_exp_place(self, id:int) -> np.ndarray:
        ''' given exp id return the observation.
        If exp_id = -1 return current exp observation 
        if possible (else observation is None)'''
        if id < 0:
            if self.experience_map.current_exp is not None:
                id = self.experience_map.current_exp.id
            else:
                id = -1
    
        view_cell = self.get_exp_view_cell(id)
        observation = self.extract_observation(view_cell)
        return observation
    
    def get_global_position(self) -> list:
        """ get agent current global position"""
        return self.experience_map.get_global_position()

    def get_exp_global_position(self, exp:object=-1):
        return self.experience_map.get_exp_global_position(exp)
#============== ? ================#
    def memorise_poses(self,poses:list, exp_id:int =-1):
        ''' Given exp id and list of door poses, save them in exp view cell'''
        
        if exp_id < 0:
            exp = self.experience_map.current_exp
        else:
            exp = self.experience_map.get_exp(exp_id)
        if exp is None:
            return
        exp.view_cell.relevant_poses = [list(pose) for pose in poses]
        print("poses_memorized",exp.view_cell.relevant_poses)

    def extract_observation(self, view_cell:object) -> np.ndarray:
        if view_cell is not None:
            return view_cell.template
        else:
            return None
      
    def current_distance_to_exp(self, exp_id:int=None) -> float:
        """ get global distance between two locations"""
        if exp_id == None:
            exp = self.experience_map.current_exp
        else:
            exp = self.experience_map.get_exp(exp_id)
        if exp == None:
            return []
        current_GP = self.get_global_position()
        exp_GP = [exp.x_m, exp.y_m, exp.facing_rad] 
        #print('give me the exp id, the exp GP and current GP', exp_id, exp_GP, current_GP)
        dist_exp = euclidian_distance(exp_GP, current_GP)

        return dist_exp
        
    def find_goal_in_memory(self):
        ''' We have a memory of a specific observation we want to go to, 
        we want to know where it corresponds to'''
        pass
        
    def find_door_linking_exps(self, goal_exp_id:int, start_exp_id:int = None)->list:
        '''
        Given goal exp and start exp (option) find the connecting door from start exp 
        '''
        if start_exp_id == None:
            start_exp = self.experience_map.current_exp
        else:
            start_exp = self.experience_map.get_exp(start_exp_id)

        start_exp_global_position = self.experience_map.get_exp_global_position(start_exp)
        start_exp_local_pose = start_exp.init_local_position
        start_exp_door_poses = self.get_exp_relevant_poses(start_exp.id)

        goal_exp = self.experience_map.get_exp(goal_exp_id)
        goal_exp_global_position = self.experience_map.get_exp_global_position(goal_exp)


        for door_pose in start_exp_door_poses:
            GP_door_pose = convert_LP_to_GP(start_exp_global_position, start_exp_local_pose, door_pose)
            angle_door_goal_exp = angle_between_two_poses(goal_exp_global_position, GP_door_pose)
             
            
            print('Between door', door_pose,' GP:', GP_door_pose , 'and exp', goal_exp.id, 'GP:', goal_exp_global_position, 'the angle is:', angle_door_goal_exp)
            print('the angular difference considering door orientation is:', np.rad2deg(np.round(clip_rad_180(GP_door_pose[2] - angle_door_goal_exp),2)))
            #The angle between the door and the linked exp can't be above 90 def considering door orientation. Else not correct direction
            rad_diff = clip_rad_180(GP_door_pose[2] - angle_door_goal_exp)
            if rad_distance_under_threshold(np.round(rad_diff,2), np.pi/2) :
                print('door',door_pose, ' links experience', start_exp.id, 'and', goal_exp.id )
                return door_pose
            
        return []

    def linked_exp_behind_door(self, door_pose:list):
        '''
        Confirm if door connects to another experience.
        The angular distance is considered (and not door matching) in case the memory 
        isn't correct and the connected exp is missing doors or they are not estimated at the correct place

        Thus this function check the angular distance one way, and just the angle matching between door pose the other way, 
        as missing connecting door pose is not really a problem
        
        '''
        #---- Get experience Reference frame and convert it to Global Frame ----#
        
        current_exp_id = self.get_current_exp_id()
        print("this is the current_exp_id",current_exp_id)
        if current_exp_id < 0:
            print("we returned nothing")
            return -1, []
        current_exp = self.experience_map.get_exp(current_exp_id)
        print("this is the current_exp", current_exp)
        exp_global_position = self.experience_map.get_exp_global_position(current_exp)
        exp_local_pose = current_exp.init_local_position
        

        GP_door_pose = convert_LP_to_GP(exp_global_position, exp_local_pose, door_pose)
        
        print('current exp Lp and GP',exp_local_pose, exp_global_position)
        print('Considering door pose', door_pose, GP_door_pose)
        
        
        exp_options = []
        #Check if exp door goes toward connected exp
        for link in current_exp.links:
            print("a",link)
            linked_exp = link.target
            if linked_exp.ghost_exp == True:
                print('IMPLEMENT WHAT HAPPENS IF THERE IS A GHOST NODE')
                continue

            linked_exp_global_position = self.experience_map.get_exp_global_position(linked_exp)
            print("this is the global position", linked_exp_global_position)
            '''
            We consider that a door leading to an exp cannot have an angular displacement between the 2
            of more than 90'.
            If they do have such a displacement, the door likely doesn't lead toward this exp.
            '''
            #the angular distance between the two poses. 
            angle_between_door_exp = angle_between_two_poses(linked_exp_global_position, GP_door_pose)
            print("angle_between_door",angle_between_door_exp)
            rad_diff = clip_rad_180(GP_door_pose[2] - angle_between_door_exp)
            print("rad_diff",rad_diff)
            print('Between door', door_pose,' GP:', GP_door_pose , 'and exp', linked_exp.id, 'GP:', linked_exp_global_position, 'the angular distance is:', angle_between_door_exp)
            print('the angular difference considering door orientation is:', np.rad2deg(rad_diff))
            #The angle between the door and the linked exp can't be above 90 def considering door orientation. Else not correct direction
            if rad_distance_under_threshold(rad_diff, threshold=np.pi/2) :
                print('linked exp', linked_exp.id, 'is likely connected through door pose', door_pose)

                #then we search which door connect to current exp from the next room 
                #NOTE: the implementation was chosen in 2 steps like this to paliate model imagination errors (imagined room doesn't have doors on this wall for instance)
                                
                linked_exp_door_poses = self.get_exp_relevant_poses(linked_exp.id)
                linked_exp_local_pose = linked_exp.init_local_position
                linked_exp_connecting_door = self.find_connecting_door_to_given_door(GP_door_pose, linked_exp_door_poses,\
                                                                                    linked_exp_local_pose,linked_exp_global_position)
                                
                print('door pose', door_pose, 'connected to', linked_exp.id ,' via door pose', linked_exp_connecting_door)
                dist_exp = euclidian_distance(linked_exp_global_position, exp_global_position)
                exp_options.append([linked_exp.id, linked_exp_connecting_door, dist_exp])
        
        #We consider the further connected exp in that direction to avoid exp too close (likely same room)
        if len(exp_options) > 0 :
            linked_exp = max(exp_options, key=lambda x: x[-1])[:2]
            return linked_exp[0], linked_exp[1]
        
        else:
            print('no known exp behind', door_pose)
            return -1, []
        
    def find_connecting_door_to_given_door(self,GP_destination_pose:list, linked_exp_door_poses:list,\
                                            linked_exp_local_pose:list, linked_exp_global_position:list):
        """
        We want to find a connection to the given door from another experience 
        GP_destination_pose: objective door
        linked_exp_door_poses: list of options to connect doors
        linked_exp_local_pose: exp local position
        linked_exp_global_position: exp global position
        """
        linked_exp_connecting_door = []
        for link_exp_door_pose in linked_exp_door_poses:
            linked_exp_door_inverse_facing = local_encoded_orientation_to_global_facing((link_exp_door_pose[2]+2) %4, linked_exp_local_pose[2], linked_exp_global_position[2])
            if rad_distance_under_threshold(linked_exp_door_inverse_facing - GP_destination_pose[2], threshold=np.pi/3):
                #print('connecting doors', link_exp_door_pose, 'and', door_pose)
                linked_exp_connecting_door = link_exp_door_pose
                break
        return linked_exp_connecting_door
    
    def convert_pose_orientation_from_start_ref_frame_to_another(self,pose:list,  start_exp_id:int= None, goal_exp_id:int=None)-> int:
        '''
        given a pose and the starting and goal experience we want to use the reference frame of
        convert pose orientation from a place reference frame to another place reference frame
        return the orientation in the goal ref frame
        '''
        if goal_exp_id == None:
            g_exp = self.experience_map.current_exp
        else:
            g_exp = self.experience_map.get_exp(goal_exp_id)

        if start_exp_id == None:
            s_exp = self.experience_map.current_exp
        else:
            s_exp = self.experience_map.get_exp(start_exp_id)
        
        #we get globalframe orientation of a pose using starting exp local reference frame
        pose_GP_facing = local_encoded_orientation_to_global_facing(pose[2], s_exp.init_local_position[2], s_exp.facing_rad)
        #we obtain the corresponding local orientation in the goal exp local reference frame
        pose_place_orientation = encoded_orientation_given_facing(pose_GP_facing, g_exp.init_local_position[2], g_exp.facing_rad)
        # pose_place_oriented = pose
        # pose_place_oriented[2]= pose_place_orientation
        return pose_place_orientation

#============== CLASS CREATION METHODS ================#
    def idify(self):
        # break circular dependencies by storing ids instead
        for vc in self.view_cells.cells:
            exp_ids = []
            for exp in vc.exps:
                exp_ids.append(exp.id)
            vc.exps = exp_ids

        for exp in self.experience_map.exps:
            exp.view_cell = exp.view_cell.id
            for link in exp.links:
                link.parent = link.parent.id
                link.target = link.target.id

    def objectify(self):
        # reestablish the object links
        for vc in self.view_cells.cells:
            exps = []
            for exp_id in vc.exps:
                exps.append(self.experience_map.exps[exp_id])
            vc.exps = exps

        for exp in self.experience_map.exps:
            exp.view_cell = self.view_cells.cells[exp.view_cell]
            for link in exp.links:
                link.parent = self.experience_map.exps[link.parent]
                link.target = self.experience_map.exps[link.target]

    def save(self, file:str):
        f = open(file, 'wb')
        self.idify()
        d = {
            "view_cells": self.view_cells.cells,
            "experience_map": self.experience_map.exps,
        }
        dill.dump(d, f)
        f.close()
        self.objectify()

    def load(self, file:str):
        f = open(file, 'rb')
        d = dill.load(f)
        self.view_cells.cells = d["view_cells"]
        self.view_cells.load_memorised_templates()

        self.experience_map.exps = d["experience_map"]
        self.experience_map.size = len(self.experience_map.exps)
        Experience._ID = self.experience_map.size
        TorchedViewCell._ID = len(self.view_cells.cells)

        self.observation = None
        self.odometry = [[], [], []]
        self.pc = [[], [], []]
        f.close()

        self.objectify()

```

`hierarchical-nav/navigation_model/Services/memory_service/memory_graph_config.yml`:

```yml

#rgb ob 10 states
observation: "image"
local_position: "pose"
odom: "HEaction"


#torched: True
posecell_vtrans_scalling: 1
dim_xy: 14
dim_th: 10
constant_adjust : False
loops: 1

#normal match: ~0.35, 0.1 means we don't want close loop based on visual but just based on distance
match_threshold: 0.10 #cos sim and KL have a similar tendency, with KL~=*10cos_sim 
delta_exp_threshold: 3
delta_pc_threshold: 3

ghost_node: False
```

`hierarchical-nav/navigation_model/Services/memory_service/modules.py`:

```py
import numpy as np

def euclidian_distance(d1:list, d2:list)->int:
    delta_exp = np.sqrt(
        min_delta(d1[0],d2[0], np.inf)**2 +
        min_delta(d1[1],d2[1], np.inf)**2 
        #+ min_delta(self.current_exp.th_pc, th_pc, self.DIM_TH)**2
        )
    return delta_exp
    # vector_exps = np.subtract(d1, d2)
    # return np.sqrt(vector_exps[0]**2 + vector_exps[1]**2)
    
def min_delta(d1, d2, max_):
    delta = np.min([np.abs(d1 - d2), max_ - np.abs(d1 - d2)])
    return delta

def clip_rad_90(angle):
    while angle > np.pi/2:
        angle -=  np.pi
    while angle < -np.pi/2:
        angle +=  np.pi
    return angle

def clip_rad_0_90(angle):
    while angle > np.pi/2:
        angle -=  np.pi
    while angle < 0:
        angle +=  np.pi
    return angle

def clip_rad_180(angle):
    while angle > np.pi:
        angle -= 2 * np.pi
    while angle <= -np.pi:
        angle += 2 * np.pi
    return angle

def clip_rad_360(angle):
    while angle < 0:
        angle += 2 * np.pi
    while angle >= 2 * np.pi:
        angle -= 2 * np.pi
    return angle

def convert_LP_to_GP(exp_global_position, exp_local_pose, door_pose):

    door_dist_to_start_pose = [int(door_pose[0] - exp_local_pose[0]), int(door_pose[1] - exp_local_pose[1])]
    entry_door_facing_rad = local_encoded_orientation_to_global_facing(0, exp_local_pose[2], exp_global_position[2])

    GP_door_pose = [0,0,0]
    GP_door_pose[0] = exp_global_position[0] +  (door_dist_to_start_pose[0] * np.cos(entry_door_facing_rad) - door_dist_to_start_pose[1] * np.sin(entry_door_facing_rad))
    GP_door_pose[1] = exp_global_position[1] +  (door_dist_to_start_pose[0] * np.sin(entry_door_facing_rad) + door_dist_to_start_pose[1] * np.cos(entry_door_facing_rad))
    GP_door_pose[2] = local_encoded_orientation_to_global_facing(door_pose[2], exp_local_pose[2], exp_global_position[2])

    return GP_door_pose
def local_encoded_orientation_to_global_facing(goal_lp_orientation:int,LP_start_orientation:int, GP_start_orientation:int)-> float:
    '''
    we have 4 orientations possible from 0 to 3 for the local pose in minigrid, 
    given the place local frame origin and corresponding global frame pose 
    we convert the orientation from one place local frame to global orientation in rad 
    '''
    #-- Option 2 --#
    orientations = [LP_start_orientation]
    GP_orientations = [GP_start_orientation]
    

    shifting_orientation = GP_start_orientation
    for i in range(1,4):
        orientations.append((LP_start_orientation+i)%4)
        shifting_orientation = shifting_orientation+np.pi/2
        GP_orientations.append(clip_rad_180(shifting_orientation))
    print('orientation and GP orientations', GP_orientations, orientations)
    goal_orientation_index = orientations.index(goal_lp_orientation)
    
    facing_rad = GP_orientations[goal_orientation_index]

    # -- OPTION1 -- #
    '''
    orientations= np.array([0,1,2,3] * 3)
    id_init_orientation = np.where(orientations == LP_start_orientation)[0][1]


    ids_desired_orientation = np.where(orientations == goal_lp_orientation)[0]
    closest_indexes = np.array(ids_desired_orientation)-id_init_orientation
    best_ids = np.where(abs(closest_indexes) == np.min(abs(closest_indexes)))[0]
    turn = closest_indexes[best_ids[0]]
    
    #print(best_ids)
    
    #180deg turn
    if len(best_ids) > 1 :
        facing_rad = clip_rad_180(GP_start_orientation  - (np.sign(GP_start_orientation) + int(GP_start_orientation == 0)) * np.pi)
    #forward
    elif turn == 0 :
        facing_rad = GP_start_orientation 
    #turning right
    elif turn > 0:
        #print('turning right')
        facing_rad = clip_rad_180(GP_start_orientation + np.pi / 2)
    #turning left
    else:
        #print('turning left')
        facing_rad = clip_rad_180(GP_start_orientation - np.pi / 2)
    '''
    return facing_rad

def encoded_orientation_given_facing(goal_GP_orientation, LP_start_orientation, GP_start_orientation):
    delta_angle = signed_delta_rad(GP_start_orientation, goal_GP_orientation)
    orientations = [LP_start_orientation]
    
    for i in range(1,4):
        orientations.append((LP_start_orientation+i)%4)

    orientation_shift = delta_angle/(np.pi/2)
    goal_lp_orientation = orientations[int(orientation_shift)]

    return goal_lp_orientation

def signed_delta_rad(angle1, angle2):
    dir = clip_rad_180(angle2 - angle1)

    delta_angle = abs(clip_rad_360(angle1) - clip_rad_360(angle2))

    if (delta_angle < (2 * np.pi - delta_angle)):
        if (dir > 0):
            angle = delta_angle
        else:
            angle = -delta_angle
    else:
        if (dir > 0):
            angle = 2 * np.pi - delta_angle
        else:
            angle = -(2 * np.pi - delta_angle)
    return angle

def sort_by_distance(exp, goal):
    #TODO: consider angle in a next iteration
    x,y,facing = exp['x'], exp['y'], exp['facing']
    xg,yg,facingg = goal['x'],goal['y'], goal['facing']
    delta_goal = np.sqrt(
            np.abs(x- xg)**2 +
            np.abs(y- yg)**2 
        )

    dist_angle = abs(signed_delta_rad(facing,facingg))
    delta_goal += dist_angle *1/(2*np.pi)  #to have the add_up < 1       
    exp['delta_goal'] = delta_goal #debug usage
    return delta_goal


def angle_between_two_poses(GP1:list, GP2:list) ->float:
    '''
    we calculate the angle between two vectors in a two-dimensional space defined 
    by their global positions (GP1 and GP2). It does this by finding the vector that 
    connects these two points and then determining the angle that this vector makes 
    with the horizontal axis. The result is the angular distance between the two vectors.
    '''
    vector_between_two_poses = np.subtract(GP1, GP2)
    angle_between_two_vectors = np.arctan2(vector_between_two_poses[1], vector_between_two_poses[0])
    return angle_between_two_vectors

def rad_distance_under_threshold(rad_diff:float, threshold:float=np.pi/2):
    return abs(rad_diff) < threshold
```

`hierarchical-nav/navigation_model/Services/memory_service/odometry.py`:

```py
# Update 2022
# =============================================================================
# Ghent University 
# IDLAB of IMEC
# Daria de Tinguy - daria.detinguy at ugent.be
# =============================================================================

# Original Source
# =============================================================================
# Federal University of Rio Grande do Sul (UFRGS)
# Connectionist Artificial Intelligence Laboratory (LIAC)
# Renato de Pontes Pereira - rppereira@inf.ufrgs.br
# =============================================================================
# Copyright (c) 2013 Renato de Pontes Pereira, renato.ppontes at gmail dot com
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# =============================================================================

import numpy as np
from .view_cells import compare_image_templates
from .modules import clip_rad_360
from pyquaternion import Quaternion
# from robosalad.runtime.datastructs import Pose, transform, inverse


class Odometry(object):
    def __init__(self):
        self.odometry = [0., 0., np.pi / 2]

    def __call__(self, observations, dt=0.1):
        odom = observations["odom"]
        vtrans = odom[-6] * dt
        vrot = odom[-1] * dt

        odom_angle = Quaternion(odom[6], odom[3], odom[4], odom[5])
        self.odometry[0] = odom[0]
        self.odometry[1] = odom[1]
        self.odometry[2] = odom_angle.angle
        return vtrans, vrot, self.odometry


class ActionOdometry(object):
    ''' Use cmd_vel for Odometry '''

    def __init__(self):
        self.odometry = [0., 0., np.pi / 2]

    def __call__(self, observations, dt=0.1):
        action = observations["action"]

        vtrans = action[0] * dt * 20
        vrot = action[-1] * dt * 0.85

        self.odometry[2] += vrot
        self.odometry[0] += vtrans * np.cos(self.odometry[2])
        self.odometry[1] += vtrans * np.sin(self.odometry[2])
        return vtrans, vrot, self.odometry

class HotEncodedActionOdometry(object):
    ''' Use HotEncoded Action for Odometry, gridworld adapted'''

    def __init__(self):
        self.odometry = [0., 0., np.pi/2]

    def __call__(self, action:list, dt:float=1)-> tuple[float, float]:
        #action = observations["HEaction"] #hot encoded action : [F,R,L]
        # print('action', action)
        vtrans = action[0] 
        vrot = 0
        if action[1] != 0: #right
            vrot = np.pi / 2 
        elif action[2] != 0: #left
            vrot = -np.pi / 2 

        self.odometry[2] += vrot
        self.odometry[2] = clip_rad_360(self.odometry[2])
        self.odometry[0] += vtrans * round(np.cos(self.odometry[2]),4) 
        self.odometry[1] += vtrans * round(np.sin(self.odometry[2]),4) # the round is there to correct the error on pi that would accumulate sin(2pi) != sin(0)
        #print('in action, vtrans, vrot, odom:', vtrans, vrot, self.odometry)
        return vtrans, vrot

    def position_applying_motion(self,odometry:list,action:list,dt:int=1)-> list:
        #hot encoded action : [F,R,L]
        vtrans = action[0] 
        vrot = 0
        if action[1] != 0: #right
            vrot = np.pi / 2 
        elif action[2] != 0: #left
            vrot = -np.pi / 2 

        odometry[2] += vrot
        odometry[2] = clip_rad_360(odometry[2])
        odometry[0] += vtrans * round(np.cos(odometry[2]),4) 
        odometry[1] += vtrans * round(np.sin(odometry[2]),4) 
        return odometry

class PoseOdometry(object):
    ''' Use pose for Odometry '''

    def __init__(self):
        self.latest = None
        self.odometry = None
        self.odometry = [0., 0., np.pi/2]
    def clip_rad_360(self,angle): #easierto debug
        while angle < 0:
            angle += 2 * np.pi
        while angle >= 2 * np.pi:
            angle -= 2 * np.pi
        return angle

    def __call__(self, odometry, observations, dt=0.1):
        p = observations["pose"]
        if len(p) == 3: #(xyth)
            print('pose', p)
            if 0 < p[2] < 4 and p[2].is_integer:
                p[2] = np.pi * p[2]
            
            if self.latest is None:
                self.latest = p
                diff = p- odometry
                #NOTE: the following only works when translation on only 1 axe
                vtrans,vrot = np.sum(np.sign(diff[:2])) * np.sqrt(pow(diff[0],2) + pow(diff[1],2)), p[2]
            else:
                vtrans = np.sqrt(pow(p[0] - odometry[0],2) + pow(p[1] - odometry[1],2))
                vrot = self.clip_rad_360(p[2] - odometry[2])                
                
            odometry = p
            print('in pose odom, vtrans, vrot, odom:', vtrans, vrot, odometry)
            return vtrans , vrot, odometry

        if len(p) == 7: #(xyzquat)
            pose = Pose.from_numpy(p)
            yaw, _, _ = pose.orientation.pyquaternion().yaw_pitch_roll
            # drone has Z pointing down
            yaw = -yaw
            self.odometry = [pose.position.x, pose.position.y, yaw]

            if self.latest is None:
                self.latest = pose
                return 0, 0
            else:
                diff = transform(pose, inverse(self.latest))

                vtrans = diff.position.x
                yaw, _, _ = diff.orientation.pyquaternion().yaw_pitch_roll
                vrot = -yaw
                self.latest = pose
                return vtrans / 100, vrot, self.odometry

        print('ERROR in memory_graph PoseOdometry')
class PoseOdometry(object):
    ''' Use pose for Odometry '''

    def __init__(self):
        self.latest = None
        self.odometry = None
        self.odometry = [0., 0., np.pi/2]
    def clip_rad_360(self,angle): #easierto debug
        while angle < 0:
            angle += 2 * np.pi
        while angle >= 2 * np.pi:
            angle -= 2 * np.pi
        return angle

    def __call__(self, observations, dt=0.1):
        p = observations["pose"]
        if len(p) == 3: #(xyth)
            print('we are inside the odometry pose', p)
            if 0 < p[2] < 4 and p[2].is_integer:
                p[2] = np.pi * p[2]
            
            if self.latest is None:
                self.latest = p
                diff = p- self.odometry
                #NOTE: the following only works when translation on only 1 axe
                vtrans,vrot = np.sum(np.sign(diff[:2])) * np.sqrt(pow(diff[0],2) + pow(diff[1],2)), p[2]
            else:
                vtrans = np.sqrt(pow(p[0] - self.odometry[0],2) + pow(p[1] - self.odometry[1],2))
                vrot = self.clip_rad_360(p[2] - self.odometry[2])                
                
            self.odometry = p
            print('in pose odom, vtrans, vrot, odom:', vtrans, vrot, self.odometry)
            return vtrans , vrot

        if len(p) == 7: #(xyzquat)
            pose = Pose.from_numpy(p)
            yaw, _, _ = pose.orientation.pyquaternion().yaw_pitch_roll
            # drone has Z pointing down
            yaw = -yaw
            self.odometry = [pose.position.x, pose.position.y, yaw]

            if self.latest is None:
                self.latest = pose
                return 0, 0
            else:
                diff = transform(pose, inverse(self.latest))

                vtrans = diff.position.x
                yaw, _, _ = diff.orientation.pyquaternion().yaw_pitch_roll
                vrot = -yaw
                self.latest = pose
                return vtrans / 100, vrot

        print('ERROR in memory_graph PoseOdometry')
        
            

        


class VisualOdometry(object):
    '''Visual Odometry Module.'''

    def __init__(self, key, width=160, height=120):
        self.key = key
        '''Initializes the visual odometry module.'''

        # TODO this is pretty hard-coded for 640x480 image input
        self.IMAGE_Y_SIZE = width
        self.IMAGE_X_SIZE = height
        # self.IMAGE_VTRANS_Y_RANGE = slice(270, 430)
        self.IMAGE_VTRANS_Y_RANGE = slice(height // 2, height)
        # self.IMAGE_VROT_Y_RANGE = slice(75, 240)
        self.IMAGE_VROT_Y_RANGE = slice(height // 6, height // 2)
        # self.IMAGE_ODO_X_RANGE = slice(180 + 15, 460 + 15)
        self.IMAGE_ODO_X_RANGE = slice(width // 4, width - width // 4)
        # TODO what with these numbers?
        self.VTRANS_SCALE = 100
        self.VISUAL_ODO_SHIFT_MATCH = width // 4
        self.ODO_ROT_SCALING = np.pi / 180. / 7.

        template_size = (self.IMAGE_ODO_X_RANGE.stop -
                         self.IMAGE_ODO_X_RANGE.start)
        self.old_vtrans_template = np.zeros(template_size)
        self.old_vrot_template = np.zeros(template_size)

        self.odometry = [0., 0., np.pi / 2]

    def _create_template(self, subimg):
        '''Compute the sum of columns in subimg and normalize it.

        :param subimg: a sub-image as a 2D numpy array.
        :return: the view template as a 1D numpy array.
        '''
        x_sums = np.sum(subimg, 0)
        avint = np.sum(x_sums, dtype=np.float32) / x_sums.size
        return x_sums / avint

    def __call__(self, observations, dt=None):
        '''Execute an interation of visual odometry.

        :param img: the full gray-scaled image as a 2D numpy array.
        :return: the deslocation and rotation of the image from the previous 
                 frame as a 2D tuple of floats.
        '''
        img = observations[self.key]
        if len(img.shape) == 3:
            # convert to numpy image format
            if img.shape[0] == 3:
                img = np.transpose(img, (1, 2, 0))
            # make grayscale
            img = np.average(img, axis=2)

        subimg = img[self.IMAGE_VTRANS_Y_RANGE, self.IMAGE_ODO_X_RANGE]
        template = self._create_template(subimg)

        # VTRANS
        offset, diff = compare_image_templates(
            template,
            self.old_vtrans_template,
            self.VISUAL_ODO_SHIFT_MATCH
        )
        vtrans = diff * self.VTRANS_SCALE

        if vtrans > 10:
            vtrans = 0

        self.old_vtrans_template = template

        # VROT
        subimg = img[self.IMAGE_VROT_Y_RANGE, self.IMAGE_ODO_X_RANGE]
        template = self._create_template(subimg)

        offset, diff = compare_image_templates(
            template,
            self.old_vrot_template,
            self.VISUAL_ODO_SHIFT_MATCH
        )
        vrot = offset * (50. / img.shape[1]) * np.pi / 180
        self.old_vrot_template = template

        # Update raw odometry
        self.odometry[2] += vrot
        self.odometry[0] += vtrans * np.cos(self.odometry[2])
        self.odometry[1] += vtrans * np.sin(self.odometry[2])

        return vtrans, vrot

```

`hierarchical-nav/navigation_model/Services/memory_service/pose_cells.py`:

```py
# Update 2022
# =============================================================================
# Ghent University 
# IDLAB of IMEC
# Daria de Tinguy - daria.detinguy at ugent.be
# =============================================================================

# Original Source
# =============================================================================
# Federal University of Rio Grande do Sul (UFRGS)
# Connectionist Artificial Intelligence Laboratory (LIAC)
# Renato de Pontes Pereira - rppereira@inf.ufrgs.br
# =============================================================================
# Copyright (c) 2013 Renato de Pontes Pereira, renato.ppontes at gmail dot com
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# =============================================================================

import itertools
import numpy as np
import torch

def create_pc_weights(dim, var):
    dim_center = int(np.floor(dim / 2.))

    weight = np.zeros([dim, dim, dim])
    for x, y, z in itertools.product(range(dim), range(dim), range(dim)):
        dx = -(x - dim_center)**2
        dy = -(y - dim_center)**2
        dz = -(z - dim_center)**2
        weight[x, y, z] = 1.0 / (var * np.sqrt(2 * np.pi)) * \
            np.exp((dx + dy + dz) / (2. * var**2))

    weight = weight / np.sum(weight)
    return weight


class PoseCells(object):
    '''Pose Cell module.'''

    def __init__(self, dim_xy=61, dim_th=36, inject_energy=0.1,
                 global_inhibition=0.00002, torched=False, device="cpu", posecell_vtrans_scalling = 1./10. , **kwargs):
        '''Initializes the Pose Cell module.'''

        self.DIM_XY = dim_xy
        self.DIM_TH = dim_th
        self.W_E_VAR = 1
        self.W_E_DIM = 7
        self.W_I_VAR = 2
        self.W_I_DIM = 5
        self.INJECT_ENERGY = inject_energy
        self.GLOBAL_INHIB = global_inhibition
        self.CELLS_TO_AVG = 3
        self.POSECELL_VTRANS_SCALING = posecell_vtrans_scalling

        self.W_EXCITE = create_pc_weights(self.W_E_DIM, self.W_E_VAR)
        self.W_INHIB = create_pc_weights(self.W_I_DIM, self.W_I_VAR)

        self.W_E_DIM_HALF = int(np.floor(self.W_E_DIM / 2.))
        self.W_I_DIM_HALF = int(np.floor(self.W_I_DIM / 2.))

        self.torched = False
        self.device = device
        if torched:
            self.torched = True
            self.W_EXCITE = torch.from_numpy(self.W_EXCITE).unsqueeze(0).unsqueeze(0).float()
            self.W_EXCITE = torch.nn.Parameter(self.W_EXCITE, requires_grad=False)
            self.W_INHIB = torch.from_numpy(self.W_INHIB).unsqueeze(0).unsqueeze(0).float()
            self.W_INHIB = torch.nn.Parameter(self.W_INHIB, requires_grad=False)
            self.excite_op = torch.nn.Conv3d(1, 1, self.W_E_DIM, 1, self.W_E_DIM_HALF, 1, 1, False, "circular")
            self.excite_op.weight = self.W_EXCITE
            self.excite_op = self.excite_op.to(device)
            self.inhibit_op = torch.nn.Conv3d(1, 1, self.W_I_DIM, 1, self.W_I_DIM_HALF, 1, 1, False, "circular")
            self.inhibit_op.weight = self.W_INHIB
            self.inhibit_op = self.inhibit_op.to(device)

        self.C_SIZE_TH = (2. * np.pi) / self.DIM_TH
        self.E_XY_WRAP = list(range(self.DIM_XY - self.W_E_DIM_HALF, self.DIM_XY)) + \
            list(range(self.DIM_XY)) + list(range(self.W_E_DIM_HALF))
        self.E_TH_WRAP = list(range(self.DIM_TH - self.W_E_DIM_HALF, self.DIM_TH)) + \
            list(range(self.DIM_TH)) + list(range(self.W_E_DIM_HALF))
        self.I_XY_WRAP = list(range(self.DIM_XY - self.W_I_DIM_HALF, self.DIM_XY)) + \
            list(range(self.DIM_XY)) + list(range(self.W_I_DIM_HALF))
        self.I_TH_WRAP = list(range(self.DIM_TH - self.W_I_DIM_HALF, self.DIM_TH)) + \
            list(range(self.DIM_TH)) + list(range(self.W_I_DIM_HALF))
        self.XY_SUM_SIN_LOOKUP = np.sin(np.multiply(
            list(range(1, self.DIM_XY + 1)), (2 * np.pi) / self.DIM_XY))
        self.XY_SUM_COS_LOOKUP = np.cos(np.multiply(
            list(range(1, self.DIM_XY + 1)), (2 * np.pi) / self.DIM_XY))
        self.TH_SUM_SIN_LOOKUP = np.sin(np.multiply(
            list(range(1, self.DIM_TH + 1)), (2 * np.pi) / self.DIM_TH))
        self.TH_SUM_COS_LOOKUP = np.cos(np.multiply(
            list(range(1, self.DIM_TH + 1)), (2 * np.pi) / self.DIM_TH))
        self.AVG_XY_WRAP = list(range(self.DIM_XY - self.CELLS_TO_AVG, self.DIM_XY)) + \
            list(range(self.DIM_XY)) + list(range(self.CELLS_TO_AVG))
        self.AVG_TH_WRAP = list(range(self.DIM_TH - self.CELLS_TO_AVG, self.DIM_TH)) + \
            list(range(self.DIM_TH)) + list(range(self.CELLS_TO_AVG))

        self.cells = np.zeros([self.DIM_XY, self.DIM_XY, self.DIM_TH])
        self.active = a, b, c = [self.DIM_XY //
                                 2, self.DIM_XY // 2, self.DIM_TH // 2]
        self.cells[a, b, c] = 1

    def reset(self, x_pc, y_pc, th_pc):
        a = np.min([np.max([int(np.floor(x_pc)), 1]), self.DIM_XY])
        b = np.min([np.max([int(np.floor(y_pc)), 1]), self.DIM_XY])
        c = np.min([np.max([int(np.floor(th_pc)), 1]), self.DIM_TH])

        self.cells = np.zeros([self.DIM_XY, self.DIM_XY, self.DIM_TH])
        self.cells[a, b, c] = 1
        self.active = a, b, c
        print(a,b,c)

    def compute_activity_matrix(self, xywrap, thwrap, wdim, pcw, excite=True):
        '''Compute the activation of pose cells.'''
        if self.torched:
            cell_tensor = torch.from_numpy(self.cells).unsqueeze(0).unsqueeze(0).float()
            cell_tensor = cell_tensor.to(self.device)
            with torch.no_grad():
                ret = None
                if excite:
                    ret = self.excite_op(cell_tensor)
                else:
                    ret = self.inhibit_op(cell_tensor)
            return ret.cpu().numpy()[0,0]
        else:
             # The goal is to return an update matrix that can be added/subtracted
            # from the posecell matrix
            pca_new = np.zeros([self.DIM_XY, self.DIM_XY, self.DIM_TH])
            # for nonzero posecell values
            indices = np.nonzero(self.cells)
            for i, j, k in zip(*indices):
                pca_new[np.ix_(xywrap[i:i + wdim],
                            xywrap[j:j + wdim],
                            thwrap[k:k + wdim])] += self.cells[i, j, k] * pcw
            return pca_new

    def get_pc_max(self, xywrap, thwrap):
        '''Find the x, y, th center of the activity in the network.'''

        x, y, z = np.unravel_index(np.argmax(self.cells), self.cells.shape)
        if self.torched:
            return x, y, z
        z_posecells = np.zeros([self.DIM_XY, self.DIM_XY, self.DIM_TH])

        zval = self.cells[np.ix_(
            xywrap[x:x + self.CELLS_TO_AVG * 2],
            xywrap[y:y + self.CELLS_TO_AVG * 2],
            thwrap[z:z + self.CELLS_TO_AVG * 2]
        )]
        z_posecells[np.ix_(
            self.AVG_XY_WRAP[x:x + self.CELLS_TO_AVG * 2],
            self.AVG_XY_WRAP[y:y + self.CELLS_TO_AVG * 2],
            self.AVG_TH_WRAP[z:z + self.CELLS_TO_AVG * 2]
        )] = zval

        # get the sums for each axis
        x_sums = np.sum(np.sum(z_posecells, 2), 1)
        y_sums = np.sum(np.sum(z_posecells, 2), 0)
        th_sums = np.sum(np.sum(z_posecells, 1), 0)
        th_sums = th_sums[:]

        # now find the (x, y, th) using population vector decoding to handle
        # the wrap around
        x = (np.arctan2(np.sum(self.XY_SUM_SIN_LOOKUP * x_sums),
                        np.sum(self.XY_SUM_COS_LOOKUP * x_sums)) *
             self.DIM_XY / (2 * np.pi)) % (self.DIM_XY)

        y = (np.arctan2(np.sum(self.XY_SUM_SIN_LOOKUP * y_sums),
                        np.sum(self.XY_SUM_COS_LOOKUP * y_sums)) *
             self.DIM_XY / (2 * np.pi)) % (self.DIM_XY)

        th = (np.arctan2(np.sum(self.TH_SUM_SIN_LOOKUP * th_sums),
                         np.sum(self.TH_SUM_COS_LOOKUP * th_sums)) *
              self.DIM_TH / (2 * np.pi)) % (self.DIM_TH)

        # print x, y, th
        return (x, y, th)

    def __call__(self, view_cell, vtrans, vrot):
        '''Execute an interation of pose cells.

        :param view_cell: the last most activated view cell.
        :param vtrans: the translation of the robot given by odometry.
        :param vrot: the rotation of the robot given by odometry.
        :return: a 3D-tuple with the (x, y, th) index of most active pose cell.
        '''
        #print('in posecell,trans and rot', vtrans, vrot)
        vtrans = vtrans * self.POSECELL_VTRANS_SCALING
        #print('in posecell,trans scalled', vtrans)

        # if this isn't a new vt then add the energy at its associated posecell
        # location
        if not view_cell.first:
            act_x = np.min(
                [np.max([int(np.floor(view_cell.x_pc)), 1]), self.DIM_XY])
            act_y = np.min(
                [np.max([int(np.floor(view_cell.y_pc)), 1]), self.DIM_XY])
            act_th = np.min(
                [np.max([int(np.floor(view_cell.th_pc)), 1]), self.DIM_TH])

            print ("ptm",act_x, act_y, act_th)
            # this decays the amount of energy that is injected at the vt's
            # posecell location
            # this is important as the posecell Posecells will errounously snap
            # for bad vt matches that occur over long periods (eg a bad matches that
            # occur while the agent is stationary). This means that multiple vt's
            # need to be recognised for a snap to happen
            energy = self.INJECT_ENERGY * \
                (1. / 30.) * (30 - np.exp(1.2 * np.min([view_cell.decay, 100])))
            if energy > 0:
                self.cells[act_x, act_y, act_th] += energy
        #===============================

        # local excitation - self.le = PC elements * PC weights
        self.cells = self.compute_activity_matrix(self.E_XY_WRAP,
                                                  self.E_TH_WRAP,
                                                  self.W_E_DIM,
                                                  self.W_EXCITE,
                                                  excite=True)
        # print np.max(self.cells)
        #raw_input()

        # local inhibition - self.li = self.le - self.le elements * PC weights
        self.cells = self.cells - self.compute_activity_matrix(self.I_XY_WRAP,
                                                               self.I_TH_WRAP,
                                                               self.W_I_DIM,
                                                               self.W_INHIB,
                                                               excite=False)

        # local global inhibition - self.gi = self.li elements - inhibition
        self.cells[self.cells < self.GLOBAL_INHIB] = 0
        self.cells[self.cells >= self.GLOBAL_INHIB] -= self.GLOBAL_INHIB

        # normalization
        total = np.sum(self.cells)
        self.cells = self.cells / total

        # Path Integration
        # vtrans affects xy direction
        # shift in each th given by the th
        for dir_pc in range(self.DIM_TH):
            direction = np.float64(dir_pc - 1) * self.C_SIZE_TH
            # N,E,S,W are straightforward
            if (direction == 0):
                self.cells[:, :, dir_pc] = \
                    self.cells[:, :, dir_pc] * (1.0 - vtrans) + \
                    np.roll(self.cells[:, :, dir_pc], 1, 1) * vtrans

            elif direction == np.pi / 2:
                self.cells[:, :, dir_pc] = \
                    self.cells[:, :, dir_pc] * (1.0 - vtrans) + \
                    np.roll(self.cells[:, :, dir_pc], 1, 0) * vtrans

            elif direction == np.pi:
                self.cells[:, :, dir_pc] = \
                    self.cells[:, :, dir_pc] * (1.0 - vtrans) + \
                    np.roll(self.cells[:, :, dir_pc], -1, 1) * vtrans

            elif direction == 3 * np.pi / 2:
                self.cells[:, :, dir_pc] = \
                    self.cells[:, :, dir_pc] * (1.0 - vtrans) + \
                    np.roll(self.cells[:, :, dir_pc], -1, 0) * vtrans

            else:
                pca90 = np.rot90(self.cells[:, :, dir_pc],
                                 int(np.floor(direction * 2 / np.pi)))
                dir90 = direction - \
                    int(np.floor(direction * 2 / np.pi)) * np.pi / 2

                # extend the Posecells one unit in each direction (max supported at the moment)
                # work out the weight contribution to the NE cell from the SW, NW, SE cells
                # given vtrans and the direction
                # weight_sw = v * cos(th) * v * sin(th)
                # weight_se = (1 - v * cos(th)) * v * sin(th)
                # weight_nw = (1 - v * sin(th)) * v * sin(th)
                # weight_ne = 1 - weight_sw - weight_se - weight_nw
                # think in terms of NE divided into 4 rectangles with the sides
                # given by vtrans and the angle
                pca_new = np.zeros([self.DIM_XY + 2, self.DIM_XY + 2])
                pca_new[1:-1, 1:-1] = pca90

                weight_sw = (vtrans**2) * np.cos(dir90) * np.sin(dir90)
                weight_se = vtrans * np.sin(dir90) - \
                    (vtrans**2) * np.cos(dir90) * np.sin(dir90)
                weight_nw = vtrans * np.cos(dir90) - \
                    (vtrans**2) * np.cos(dir90) * np.sin(dir90)
                weight_ne = 1.0 - weight_sw - weight_se - weight_nw

                pca_new = pca_new * weight_ne + \
                    np.roll(pca_new, 1, 1) * weight_nw + \
                    np.roll(pca_new, 1, 0) * weight_se + \
                    np.roll(np.roll(pca_new, 1, 1), 1, 0) * weight_sw

                pca90 = pca_new[1:-1, 1:-1]
                pca90[1:, 0] = pca90[1:, 0] + pca_new[2:-1, -1]
                pca90[1, 1:] = pca90[1, 1:] + pca_new[-1, 2:-1]
                pca90[0, 0] = pca90[0, 0] + pca_new[-1, -1]

                # unrotate the pose cell xy layer
                self.cells[:, :, dir_pc] = np.rot90(pca90,
                                                    4 - int(np.floor(direction * 2 / np.pi)))

        # Path Integration - Theta
        # Shift the pose cells +/- theta given by vrot
        if vrot != 0:
            weight = (np.abs(vrot) / self.C_SIZE_TH) % 1
            if weight == 0:
                weight = 1.0

            shift1 = int(np.sign(vrot) *
                         int(np.floor(abs(vrot) / self.C_SIZE_TH)))
            shift2 = int(np.sign(vrot) *
                         int(np.ceil(abs(vrot) / self.C_SIZE_TH)))
            self.cells = np.roll(self.cells, shift1, 2) * (1.0 - weight) + \
                np.roll(self.cells, shift2, 2) * (weight)

        self.active = self.get_pc_max(self.AVG_XY_WRAP, self.AVG_TH_WRAP)

        print('in posecell,active cell',self.active)
        return self.active

```

`hierarchical-nav/navigation_model/Services/memory_service/view_cells.py`:

```py
# Update 2022
# =============================================================================
# Ghent University 
# IDLAB of IMEC
# Daria de Tinguy - daria.detinguy at ugent.be
# =============================================================================

# Original Source
# =============================================================================
# Federal University of Rio Grande do Sul (UFRGS)
# Connectionist Artificial Intelligence Laboratory (LIAC)
# Renato de Pontes Pereira - rppereira@inf.ufrgs.br
# =============================================================================
# Copyright (c) 2013 Renato de Pontes Pereira, renato.ppontes at gmail dot com
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# =============================================================================
try:
    import torch
finally:
    pass


import numpy as np
# import time

#from dommel_library.distributions.multivariate_normal import MultivariateNormal
# import re
#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device = torch.device('cpu')
K_MAX          = 10      # max exemplars stored per view-cell
NOVELTY_TAU    = 1.2    # accumulator threshold to force a spawn
NOVELTY_GAMMA  = 0.97 
class TorchedViewCell(object):

    _ID = 0

    def __init__(self, cells, x_pc, y_pc, th_pc, init_decay=1.0):
        self.id = TorchedViewCell._ID
        self.cells = cells  #why???!!!!
        self.x_pc = x_pc
        self.y_pc = y_pc
        self.th_pc = th_pc
        self.decay = init_decay
        self.first = True
        self.exps = []#contains exp id
        self.to_update = False
        #self.init_local_position = local_position
        self.relevant_poses = []
        self.exemplars   = []   # NEW
        self.delta_accum = 0.0  # NEW
        #self.template_info = {}

        TorchedViewCell._ID += 1

    @property
    def template(self):
        #return self.cells.templates[self.id, :].cpu().numpy()
        return self.cells.templates[self.id, :].detach().cpu().numpy()

    @property
    def score(self):
        #return self.cells.scores[self.id, :].cpu().numpy()
        return self.cells.scores[self.id, :].detach().cpu().numpy()

    


class TorchedViewCells(object):
    '''View Cell module.'''

    def __init__(self, key, global_decay=0.1, active_decay=1.0,
                 match_threshold=0.09, **kwargs):
        '''Initializes the View Cell module.'''
        self.cells = []
        self.templates = None
        #self.cumulated_cell_score = None
        #self.scores = None
        self.prev_cell = None
       

        self.GLOBAL_DECAY = global_decay
        self.ACTIVE_DECAY = active_decay
        self.MATCH_THRESHOLD = 0.01
        #self.activated_cells = []

        self.key = key
        
    def load_memorised_templates(self):
        initial_size = 200

        self.templates = torch.zeros(
            (initial_size, self.cells[0].template.shape[-1]), dtype=torch.from_numpy(self.cells[0].template).dtype).to(device)
        
        for cell in self.cells:
            temp = torch.from_numpy(cell.template).to(device)
            if self.templates.shape[0] == cell.id:
                
                new_templates = torch.zeros(int(self.templates.shape[0] * 1.5), temp.shape[-1],
                                            dtype=temp.dtype).to(device)
                new_templates[:self.templates.shape[0], :] = self.templates
                self.templates = new_templates
    
            self.templates[cell.id, :] = temp
        

    def create_cell(self, template, x_pc, y_pc, th_pc):
        cell = TorchedViewCell(self, x_pc,
                               y_pc, th_pc, self.ACTIVE_DECAY)
        self.cells.append(cell)
        # add template to the array  and grow the array if required
        if self.templates is None:
            # intialize templates TODO: NOT SET A FINITE AMOUNT OF VIEWS
            initial_size = 200
            
            self.templates = torch.zeros(
                (initial_size, template.shape[-1]), dtype=template.dtype).to(device)
           # self.cumulated_cell_score = [0]*initial_size

        elif self.templates.shape[0] == cell.id:
            # we need to expand
            
                
            new_templates = torch.zeros(int(self.templates.shape[0] * 1.5), template.shape[-1],
                                        dtype=template.dtype).to(device)
            new_templates[:self.templates.shape[0], :] = self.templates
            self.templates = new_templates

            # cumulated_cell_score = [0]* len(self.cumulated_cell_score) * 1.5
            # cumulated_cell_score[:len(self.cumulated_cell_score)] = self.cumulated_cell_score.copy()
            # self.cumulated_cell_score = cumulated_cell_score
        
    
        self.templates[cell.id, :] = template
        cell.exemplars.append(template.clone())      # NEW
        print(f"[VC]  new cell {cell.id}  |  total={TorchedViewCell._ID}")
        return cell
    def update_prev_cell(self,exp_view_cell):
        self.prev_cell = exp_view_cell

    '''def _score(self, template):
        # TODO this only implements state cosine distance
    
        a = torch.matmul(self.templates, template.unsqueeze(-1))
        score = 1 - torch.abs(a / (
            torch.norm(torch.Tensor(template), dim=-1) * torch.norm(torch.Tensor(self.templates), dim=-1).unsqueeze(-1)))

        s = [[x,i] for i,x in enumerate(score)]
        print('show me the cos similiarity score', s[:TorchedViewCell._ID])
        return score'''
    def _score(self, template):
        dists = []
        for cid, cell in enumerate(self.cells[:TorchedViewCell._ID]):
            ex = cell.exemplars or [self.templates[cid]]
            E  = torch.stack(ex)                              # (k,d)
            cos = 1 - torch.abs(E @ template /
                (E.norm(dim=-1) * template.norm() + 1e-8))  # (k,)
            best = cos.min()
            dists.append(best)
            # ─ debug ────────────────────────
            print(f"[VC]  cid={cid:3d}  best_cos_dist={best.item():.3f}")
        return torch.stack(dists)[:, None]

    def _compare_templates_kl_alternative_fct(self, t1, t2):
        split = t1.shape[-1] // 2
        mu1 = t1[..., 0:split]
        sigma1 = t1[..., split:]
        mu2 = t2[..., 0:split]
        sigma2 = t2[..., split:]

        # s = - KL
        sigma_ratio_squared = (sigma1 / (sigma2 + 1e-12)) ** 2
        kl = 0.5 * (
            ((mu1 - mu2) / (mu2 + 1e-12)) ** 2
            + sigma_ratio_squared
            - np.log(sigma_ratio_squared)
            - 1
        )
        return torch.mean(kl)
    
    def kl_alternative_fct(self, template):
        kl_list = []
        for id in range(TorchedViewCell._ID):
            view_cell_id_template = self.templates[id]
            kl = self._compare_templates_kl_alternative_fct(template,view_cell_id_template )
            kl_list.append([kl, id])
        print('alternative fct KL, MUST be the same as other fct', kl_list)

    def _kl_divergence_score(self,template):
        templates = self.templates[:TorchedViewCell._ID]
        templates = MultivariateNormal(templates[:,:int(templates.shape[-1]/2)], templates[:,int(templates.shape[-1]/2):])
        
        if not type(template) == type(MultivariateNormal(torch.zeros(1),torch.ones(1))):  
            template =  MultivariateNormal(template[:,:int(template.shape[-1]/2)], template[:,int(template.shape[-1]/2):])
        temp= template.unsqueeze(0)
        #print('show me the current template and templates new shapes:',temp.shape, templates.shape)
        kl = torch.distributions.kl_divergence(temp,templates)
        kl = kl / temp.shape[-1]
        test_kl = [[x,i] for i,x in enumerate(kl)]
        print('show me the kl divergence',test_kl)
        return kl

    def remove_views_without_exp(self):
        print('How many view cells:', len(self.cells))
        
        # Identify indices of view cells that have no experiences.
        indices_to_remove = []
        for i, cell in enumerate(self.cells):
            if len(cell.exps) == 0:
                print(f'{cell.id} has no exp')
                indices_to_remove.append(i)
        
        # Remove cells in descending order to avoid index shifts.
        if indices_to_remove:
            for idx in sorted(indices_to_remove, reverse=True):
                self.cells.pop(idx)
                TorchedViewCell._ID -= 1
            
            # After removal, reassign IDs and update the templates accordingly.
            for new_id, cell in enumerate(self.cells):
                print(f'cell id {cell.id} will become {new_id}')
                # Use clone() to create a copy of the tensor row.
                cell_template = self.templates[cell.id, :].clone()
                cell.id = new_id
                self.templates[new_id, :] = cell_template

            
               
    def get_closest_template(self,template):
        scores = self._score(template)
               
        value, indice = torch.min(
            scores[:TorchedViewCell._ID, :], dim=0)
        #NOTE: there should be only 1 min value, unless there is been a copy of a view_cell, then we want the newly created one to avoid imp issue
        indices = (scores[:TorchedViewCell._ID, :] <= value + 0.001).nonzero().squeeze()
        
        min_score = value[-1].item()
        if len(indices.shape) == 1:
            indices = [indices]
        i = indices[-1][0].item()
        return min_score, i
    
    '''def __call__(self, observations, x_pc, y_pc, th_pc, **kwargs):
        Execute an iteration of visual template.

        :param observation: the observation as a numpy array.
        :param x_pc: index x of the current pose cell.
        :param y_pc: index y of the current pose cell.
        :param th_pc: index th of the current pose cell.
        :return: the active view cell.
        
        cell_bis = None
        delta_exp_above_thresold = kwargs.get('delta_exp_above_thresold', True)
        #local_position = kwargs.get('local_position', None)
        current_exp_id = kwargs.get('current_exp_id', None)

        self.remove_views_without_exp()
        # TODO vectorize decay as well?
        for cell in self.cells:
            cell.decay -= self.GLOBAL_DECAY
            if cell.decay < 0:
                cell.decay = 0

        
        #The agent holds a single belief, else Multiple models running in parallel, agent lost
        if observations is not None:

            if isinstance(observations, np.ndarray):
                template = torch.from_numpy(observations).to(device)
                print("template if ndarray", template, template.shape )
            else:
                template = observations
                print("template if not ndarray,if",template, template.shape )
            
            #if TorchedViewCell._ID :
                
                with torch.no_grad():

                    min_score, i = self.get_closest_template(template)
                    # kl_score = self._kl_divergence_score(template)
                    
                    print('min scores and indices', min_score, i)
                    print('NO KL: closest looking exp ' + str(i)+ ' view match score and th ' + str(min_score) +' '+ str(self.MATCH_THRESHOLD))
                    
            if not TorchedViewCell._ID or min_score > self.MATCH_THRESHOLD:
                print("NO MATCH")
                #TODO: REMOVE THIS IF, THIS IS A SIMPLIFICATION NOT ADAPTED FOR REAL WORLD
                if delta_exp_above_thresold:
                    print('creating view cell')
                    cell = self.create_cell(template, x_pc, y_pc, th_pc)
                    self.prev_cell = cell
                    return cell, None
                
                #if not far enough from prev view we replace prev cell ob by the new cell ob
                else: 
                    print('replacing view cell', self.prev_cell.id,'template of exp', current_exp_id,', LP and GP to update')
                    self.templates[self.prev_cell.id, :] = template
                    if len(self.view_cells) > 1:
                        
                        self.prev_cell.to_update = True
                # for exp in self.prev_cell.exps:
                #     if exp.id == current_exp_id:
                #         exp.init_local_position = local_position
                #         break
                    return self.prev_cell, None
            if TorchedViewCell._ID :
                with torch.no_grad():
                    min_score, i = self.get_closest_template(template)
                    print(f"[VC]  min_score={min_score:.3f}  cell_id={i}")

                    # ─ accumulate novelty ─────────────────────────────
                    self.prev_cell.delta_accum = \
                        self.prev_cell.delta_accum * NOVELTY_GAMMA + float(min_score)
                    force_spawn = self.prev_cell.delta_accum > NOVELTY_TAU
                    print(f"[VC]  Δacc={self.prev_cell.delta_accum:.3f}  "
                        f"force={force_spawn}")

                # ─ NO MATCH or forced spawn ──────────────────────────
            if (not TorchedViewCell._ID) or (
                    (min_score > self.MATCH_THRESHOLD or force_spawn)
                    and motion_far_enough):

                print("[VC]  CREATE   (novel visual OR accumulated)")
                cell = self.create_cell(template, x_pc, y_pc, th_pc)
                self.prev_cell = cell
                return cell, None

                # ─ visually new but motion small: add exemplar ───────
            elif min_score > self.MATCH_THRESHOLD and not motion_far_enough:
                print("[VC]  EXEMPLAR (visual drift, small motion)")
                ex = self.prev_cell.exemplars
                if len(ex) < K_MAX:
                    ex.append(template.clone())
                else:
                    ex.pop(0); ex.append(template.clone())
                    # refresh mean prototype
                self.templates[self.prev_cell.id, :] = torch.stack(ex).mean(0)
                return self.prev_cell, None

                # ─ confident match; update & maybe copy cell ─────────
            else:
                print("[VC]  UPDATE   (matched existing cell)")
                    # soft-update exemplar cache
                ex = self.prev_cell.exemplars
                if len(ex) < K_MAX:
                    ex.append(template.clone())
                else:
                    ex.pop(0); ex.append(template.clone())
                # refresh prototype
                self.templates[self.prev_cell.id, :] = torch.stack(ex).mean(0)
                    
            elif self.prev_cell is not None and self.prev_cell.id == i :

                print('updating view cell', i)
                #if we are still at the same cell, update content 
                self.templates[self.prev_cell.id, :] = template
                 
            elif :
                print('selecting old view cell', i)

            print("MATCHED BABY")
            cell = self.cells[i]
            cell.decay += self.ACTIVE_DECAY

            if self.prev_cell != cell:
                cell.first = False
                #We copy the cell in case we are not close looping but the view is the same.
                cell_bis = self.create_cell(self.templates[cell.id, :], cell.x_pc, cell.y_pc, cell.th_pc)
                

            self.prev_cell = cell
        elif self.prev_cell is not None: 
            # in the specific situation if we close loop observations, but dist to other exps > th, 
            # so we create a new exp with a copy of view cell
            # In such a case we need to update to the view_cell copy template (exact same template but higher id) 
            print("we close looped in view cell")
            min_score, i = self.get_closest_template(self.templates[self.prev_cell.id, :])
            if i != self.prev_cell.id :
                self.prev_cell = self.cells[i]

        return self.prev_cell, cell_bis'''


    def __call__(self, observations, x_pc, y_pc, th_pc, **kwargs):
        """
        Run one view‑cell iteration.

        Returns
        -------
        (active_cell, duplicate_or_None)
        """
        # -----------------------------------------------------------------
        cell_bis = None                                      # duplicate handle
        motion_far_enough = kwargs.get('delta_exp_above_thresold', True)
        current_exp_id    = kwargs.get('current_exp_id', None)
        self.remove_views_without_exp()
        # ---- decay all cells ----------------------------------------------------
        for c in self.cells:
            c.decay = max(0.0, c.decay - self.GLOBAL_DECAY)

        # ------------------------------------------------------------------------
        # 1.  IF NO IMAGE: keep last cell or switch via template copy
        # ------------------------------------------------------------------------
        if observations is None:
            if self.prev_cell is None:
                return None, None

            print("[VC]  Fallback – no observation; stay on prev_cell")
            min_score, i = self.get_closest_template(
                self.templates[self.prev_cell.id, :])

            matched_cell = self.cells[i]
            cell_bis = None

            if matched_cell != self.prev_cell:
                print("[VC]  Fallback match is different → optional duplicate for ambiguity handling")
                matched_cell.decay += self.ACTIVE_DECAY
                cell_bis = self.create_cell(self.templates[matched_cell.id, :],
                                            matched_cell.x_pc,
                                            matched_cell.y_pc,
                                            matched_cell.th_pc)
                self.prev_cell = matched_cell

            return self.prev_cell, cell_bis

        # ------------------------------------------------------------------------
        # 2.  Convert observation to tensor
        # ------------------------------------------------------------------------
        template = (torch.from_numpy(observations).to(device)
                    if isinstance(observations, np.ndarray)
                    else observations)
        print(f"[VC]  template shape={tuple(template.shape)}")

        # ------------------------------------------------------------------------
        # 3.  FAST EXIT WHEN NO CELLS YET
        # ------------------------------------------------------------------------
        if TorchedViewCell._ID == 0:
            print("[VC]  CREATE first view‑cell")
            cell = self.create_cell(template, x_pc, y_pc, th_pc)
            self.prev_cell = cell
            return cell, None

        # ------------------------------------------------------------------------
        # 4.  MATCH AGAINST EXISTING TEMPLATES
        # ------------------------------------------------------------------------
        with torch.no_grad():
            min_score, i = self.get_closest_template(template)
        print(f"[VC]  min_score={min_score:.3f}  match_id={i}")

        # ---- novelty accumulator (on *prev* cell) ------------------------------
        self.prev_cell.delta_accum = \
            self.prev_cell.delta_accum * NOVELTY_GAMMA + float(min_score)
        force_spawn = self.prev_cell.delta_accum > NOVELTY_TAU
        print(f"[VC]  Δacc={self.prev_cell.delta_accum:.3f}  force={force_spawn}")

        # ------------------------------------------------------------------------
        # 5.  DECISION TREE
        # ------------------------------------------------------------------------

        # 5‑A  CREATE  (novel visual OR accumulated)  +  enough motion
        if (min_score > self.MATCH_THRESHOLD or force_spawn) and motion_far_enough:
            print("[VC]  CREATE   (visual novelty or Δacc) ")
            cell = self.create_cell(template, x_pc, y_pc, th_pc)
            self.prev_cell = cell
            return cell, None

        # 5‑B  EXEMPLAR ADD  (visual drift but robot almost static)
        if min_score > self.MATCH_THRESHOLD and not motion_far_enough:
            print("[VC]  EXEMPLAR (drift, small motion)")
            ex = self.prev_cell.exemplars
            (ex.append if len(ex) < K_MAX else (lambda t: (ex.pop(0), ex.append(t))))(template.clone())
            # refresh prototype
            self.templates[self.prev_cell.id, :] = torch.stack(ex).mean(0)
            return self.prev_cell, None

        # 5‑C  MATCHED  (min_score ≤ threshold)  → same cell OR switch
        matched_cell = self.cells[i]

        # (i) SAME cell  ----------------------------------------------------------
        if matched_cell is self.prev_cell:
            print("[VC]  UPDATE   (same cell)")
            # soft exemplar update
            ex = matched_cell.exemplars
            (ex.append if len(ex) < K_MAX else (lambda t: (ex.pop(0), ex.append(t))))(template.clone())
            self.templates[matched_cell.id, :] = torch.stack(ex).mean(0)
            matched_cell.decay += self.ACTIVE_DECAY
            return matched_cell, None

        # (ii) DIFFERENT cell  ----------------------------------------------------
        print("[VC]  SWITCH    (matched different cell)")
        matched_cell.decay += self.ACTIVE_DECAY

        # optional: duplicate so map can store loop‑closure
        cell_bis = self.create_cell(self.templates[matched_cell.id, :],
                                    matched_cell.x_pc,
                                    matched_cell.y_pc,
                                    matched_cell.th_pc)

        self.prev_cell = matched_cell
        return matched_cell, cell_bis

def compare_image_templates(t1, t2, slen):
    cwl = t1.size

    mindiff = 1e10
    minoffset = 0

    for offset in range(slen + 1):
        e = (cwl - offset)

        cdiff = np.abs(t1[offset:cwl] - t2[:e])
        cdiff = np.sum(cdiff) / e

        if cdiff < mindiff:
            mindiff = cdiff
            minoffset = offset

        cdiff = np.abs(t1[:e] - t2[offset:cwl])
        cdiff = np.sum(cdiff) / e

        if cdiff < mindiff:
            mindiff = cdiff
            minoffset = -offset

    return minoffset, mindiff


'''
class GaussianLatentViewCells(ViewCells):
    #View Cell module that uses latent gaussian distributions as template

    def __init__(self, key, global_decay=0.1, active_decay=1.0,
                 match_threshold=1.0, **kwargs):
        ViewCells.__init__(self, global_decay, active_decay,
                           match_threshold, **kwargs)
        self.key = key

    def _create_template(self, observations):
        return observations[self.key]

    def _compare_templates(self, t1, t2):
        split = t1.shape[-1] // 2
        mu1 = t1[..., 0:split]
        sigma1 = t1[..., split:]
        mu2 = t2[..., 0:split]
        sigma2 = t2[..., split:]

        # s = - KL
        sigma_ratio_squared = (sigma1 / (sigma2 + 1e-12)) ** 2
        kl = 0.5 * (
            ((mu1 - mu2) / (mu2 + 1e-12)) ** 2
            + sigma_ratio_squared
            - np.log(sigma_ratio_squared)
            - 1
        )
        return min(np.mean(kl), 10.0)
'''
```

`hierarchical-nav/navigation_model/Services/model_modules.py`:

```py
import gc
import os.path
import glob
import torch
import numpy as np
from dommel_library.modules.dommel_modules import tensor_dict

def get_model_parameters(log_dir:str, epoch:int=None)->str:
    model_dir = os.path.join(log_dir, "models")
    if epoch is None:
        model_files = glob.glob(os.path.join(model_dir, "*.pt"))
        model = max(model_files, key=os.path.getctime)
    else:
        model = os.path.join(model_dir, "model-{:04d}.pt".format(epoch))
    return model

def no_vel_no_action(sensor_data: dict) -> dict:
    """ If we don't measure a velocity, the action effectuated is Null"""
    if sum(sensor_data['vel_ob']) == 0 :
        sensor_data['action'] = np.array([0,0,0])
    return sensor_data


def torch_and_sample_observations(data: dict, observations_keys:list, sample:int, \
                    lenghts_shape_by_ob:dict = {'pose':3, 'image':4, 'action':2, 'vel_ob':3})-> dict:
    
    ob = torch_observations(data, observations_keys)
    ob = sample_observations(ob, observations_keys, sample, lenghts_shape_by_ob)
    print("you just used the door view thingy")
    return ob

def sample_observations(data:dict, observations_keys: list, sample:int, \
                    lenghts_shape_by_ob:dict = {'pose':3, 'image':4, 'action':2, 'vel_ob':3}):
    ''' sample observations with static length shape adapted to observations'''
    matching_keys = list(set(data.keys()) & set(observations_keys))
    observations = tensor_dict({}) 

    for key in matching_keys:
        length_shape = lenghts_shape_by_ob[key]
        observations[key] = sample_ob(data[key].clone(), sample, length_shape=length_shape)
        
    return observations
        
def torch_observations(sensor_data:dict, observations_keys: list) -> dict:
    ''' Input numpy data and return torch data with all possible relevant info'''
    matching_keys = list(set(sensor_data.keys()) & set(observations_keys))
    observations = tensor_dict({}) 
    for key in matching_keys:
        if 'pose' in key :
            observations[key] = torch_pose(sensor_data[key])

        if 'image' in key:
            observations[key] = torch_image(sensor_data[key])

        if 'action' in key:
            observations[key] = torch_action(sensor_data[key])
        
        if 'vel_ob' in key:
            observations[key] = torch_vel_ob(sensor_data[key])
    return observations

def torch_image(image) -> torch.Tensor: 
    if image is None:
        return image 
    if not isinstance(image, torch.Tensor):
        image = torch.from_numpy(image)
    if image.shape[-1] in [1, 3]:
        #we make sure to change the shape from [..,x,y,z] to [..,z,x,y]
        image = torch.transpose(image, -1, -3)
        image = torch.transpose(image, -1, -2)
    if image.dtype == torch.uint8:
        image = image.float()
    if torch.any(image > 1):
        image /= 255
    return image

def torch_pose(pose) -> torch.Tensor:
    if not isinstance(pose, torch.Tensor):
        pose = torch.tensor(pose).float()
    return pose

def torch_action(action) -> torch.Tensor:
    return torch.tensor(action).float()

def torch_vel_ob(vel_ob) -> torch.Tensor:
    return torch.tensor(vel_ob)
 
def sample_ob(ob:torch.Tensor, sample:int, length_shape:int = 4) -> torch.Tensor:
    while len(ob.shape)<length_shape:
        ob = ob.unsqueeze(0)
    if ob.shape[0] != 1: 
        #If image is already sampled... we squish that prior sample
        ob = torch.mean(ob, dim=0).unsqueeze(0)
    dimensions = [sample] + [1]*(length_shape-1)
    ob = ob.repeat(*dimensions)
    return ob
    
        
def delete_object_from_memory(objects):
    for object in objects:
        del object
    gc.collect()

```

`hierarchical-nav/navigation_model/__init__.py`:

```py
from .visualisation_tools import *



```

`hierarchical-nav/navigation_model/pcfg_spatial_planner.py`:

```py
# pcfg_spatial_planner.py
# A self-contained demo of PCFG-based hierarchical spatial planning using NLTK

import random
from nltk import PCFG
from nltk.parse.generate import generate

# ------------------------
# Placeholder for MemoryGraph interaction
# ------------------------
class MockMemoryGraph:
    def __init__(self):
        self.experiences = {
            17: {'links': [19, 5]},
            19: {'links': [18]},
            5: {'links': [4]},
            4: {'links': [3]},
            3: {'links': []},
            18: {'links': []},
        }
        self.current_exp = 17

    def get_current_exp_id(self):
        return self.current_exp

    def get_exps_organised_by_distance_from_exp(self, exp_id=None):
        if exp_id is None:
            exp_id = self.current_exp
        # For simplicity, return a flat dict with dummy distances
        return {
            18: 2,
            3: 3
        }

    def get_all_exps_in_memory(self, wt_links=True):
        return self.experiences

# ------------------------
# Grammar Generator from MemoryGraph
# ------------------------
def build_exploration_grammar(memory_graph):
    current_exp = memory_graph.get_current_exp_id()
    distances = memory_graph.get_exps_organised_by_distance_from_exp(current_exp)

    # Assign probabilities inversely proportional to distance
    total = sum(1.0 / (dist + 1e-5) for dist in distances.values())
    rules = []
    for target, dist in distances.items():
        prob = (1.0 / (dist + 1e-5)) / total
        rules.append(f"NAVPLAN -> GOTO_{target} [{prob:.4f}]")

    # Terminal expansions
    terminal_rules = []
    for src, data in memory_graph.get_all_exps_in_memory().items():
        for dst in data['links']:
            terminal_rules.append(f"STEP_{src}_{dst} -> 'step({src},{dst})' [1.0]")

    # Compose the grammar
    grammar_string = [
        "EXPLORE -> NAVPLAN [1.0]"
    ] + rules + [
        f"GOTO_{target} -> MOVESEQ_{current_exp}_{target} [1.0]" for target in distances
    ] + [
        f"MOVESEQ_{current_exp}_{18} -> STEP_{current_exp}_19 STEP_19_18 [1.0]",
        f"MOVESEQ_{current_exp}_{3} -> STEP_{current_exp}_5 STEP_5_4 STEP_4_3 [1.0]"
    ] + terminal_rules

    return PCFG.fromstring("\n".join(grammar_string))

# ------------------------
# Sample and Display Plans
# ------------------------
def sample_plan(grammar, n=3):
    print("\nSampled Exploration Plans:")
    for plan in generate(grammar, n=n, start=grammar.start()):
        print("  ->", plan)

# ------------------------
# Run a Simple Demo
# ------------------------
if __name__ == "__main__":
    print("Creating PCFG from MemoryGraph...")
    memory_graph = MockMemoryGraph()
    grammar = build_exploration_grammar(memory_graph)

    print("Productions:")
    for prod in grammar.productions():
        print(" ", prod)

    sample_plan(grammar, n=5)  # Generate some plans

```

`hierarchical-nav/navigation_model/pcfgtry.py`:

```py
from nltk import PCFG, Nonterminal
from nltk.parse.generate import generate
import random
grammar = PCFG.fromstring("""
  EXPLORE -> NAVPLAN [1.0]

  # now a softmax over 10 choices
  NAVPLAN -> GOTO_0 [0.25]
  NAVPLAN -> GOTO_1 [0.15]
  NAVPLAN -> GOTO_2 [0.15]
  NAVPLAN -> GOTO_3 [0.10]
  NAVPLAN -> GOTO_4 [0.10]
  NAVPLAN -> GOTO_5 [0.05]
  NAVPLAN -> GOTO_6 [0.05]
  NAVPLAN -> GOTO_7 [0.05]
  NAVPLAN -> GOTO_8 [0.05]
  NAVPLAN -> GOTO_9 [0.05]

  GOTO_0 -> MOVESEQ_9_0 [1.0]
  GOTO_1 -> MOVESEQ_9_1 [1.0]
  GOTO_2 -> MOVESEQ_9_2 [1.0]
  GOTO_3 -> MOVESEQ_9_3 [1.0]
  GOTO_4 -> MOVESEQ_9_4 [1.0]
  GOTO_5 -> MOVESEQ_9_5 [1.0]
  GOTO_6 -> MOVESEQ_9_6 [1.0]
  GOTO_7 -> MOVESEQ_9_7 [1.0]
  GOTO_8 -> MOVESEQ_9_8 [1.0]
  GOTO_9 -> MOVESEQ_9_9 [1.0]

  MOVESEQ_9_0 -> HOPSEQ_9_0 [1.0]
  MOVESEQ_9_1 -> HOPSEQ_9_1 [1.0]
  MOVESEQ_9_2 -> HOPSEQ_9_2 [1.0]
  MOVESEQ_9_3 -> HOPSEQ_9_3 [1.0]
  MOVESEQ_9_4 -> HOPSEQ_9_4 [1.0]
  MOVESEQ_9_5 -> HOPSEQ_9_5 [1.0]
  MOVESEQ_9_6 -> HOPSEQ_9_6 [1.0]
  MOVESEQ_9_7 -> HOPSEQ_9_7 [1.0]
  MOVESEQ_9_8 -> HOPSEQ_9_8 [1.0]
  MOVESEQ_9_9 -> HOPSEQ_9_9 [1.0]

  HOPSEQ_9_0 -> STEP_9_9 STEP_9_8 STEP_8_5 STEP_5_4 STEP_4_3 STEP_3_2 STEP_2_1 STEP_1_0 [1.0]
  HOPSEQ_9_1 -> STEP_9_9 STEP_9_8 STEP_8_5 STEP_5_4 STEP_4_3 STEP_3_2 STEP_2_1 [1.0]
  HOPSEQ_9_2 -> STEP_9_9 STEP_9_8 STEP_8_5 STEP_5_4 STEP_4_3 STEP_3_2 [1.0]
  HOPSEQ_9_3 -> STEP_9_9 STEP_9_8 STEP_8_5 STEP_5_4 STEP_4_3 [1.0]
  HOPSEQ_9_4 -> STEP_9_9 STEP_9_8 STEP_8_5 STEP_5_4 [1.0]
  HOPSEQ_9_5 -> STEP_9_9 STEP_9_8 STEP_8_5 [1.0]
  HOPSEQ_9_6 -> STEP_9_9 STEP_9_8 STEP_8_5 STEP_5_4 STEP_4_6 [1.0]
  HOPSEQ_9_7 -> STEP_9_9 STEP_9_8 STEP_8_5 STEP_5_4 STEP_4_7 [1.0]
  HOPSEQ_9_8 -> STEP_9_9 STEP_9_8 [1.0]
  HOPSEQ_9_9 -> STEP_9_9 [1.0]

  STEP_9_9 -> 'right' 'right' 'forward' 'forward' 'forward' 'forward' 'forward' 'left' [1.0]
  STEP_9_8 -> 'right' 'right' 'forward' 'forward' 'forward' 'forward' 'left' 'left' [1.0]
  STEP_8_5 -> 'right' 'right' 'forward' 'forward' 'forward' 'forward' 'forward' 'left' 'left' [1.0]
  STEP_5_4 -> 'forward' 'forward' 'forward' 'right' 'forward' 'left' 'forward' 'left' 'left' [1.0]
  STEP_4_3 -> 'right' 'right' 'forward' 'forward' 'forward' 'forward' 'forward' 'forward' 'forward' 'left' 'forward' 'left' [1.0]
  STEP_3_2 -> 'right' 'forward' 'right' 'forward' 'forward' 'forward' 'left' 'forward' 'forward' 'left' 'left' [1.0]
  STEP_2_1 -> 'right' 'right' 'forward' 'forward' 'forward' 'forward' 'forward' 'forward' 'forward' 'forward' 'left' 'left' [1.0]
  STEP_1_0 -> 'left' 'left' 'forward' 'forward' 'forward' 'forward' 'forward' [1.0]
""")
'''def sample_plans(grammar, n=50):
    return list(generate(grammar, n=n, depth=100))

plans = sample_plans(grammar)
print("Sampled plans:")
if not plans:
    print("  (No plans generated)")
else:
    for i, plan in enumerate(plans):
        print(f"  Plan {i+1}: {' '.join(plan)}")

def mutate_plan(plan):
    if not plan:
        return plan
    mutated = plan[:]
    swap_map = {"step(0,1)": "step(1,2)", "step(1,2)": "step(0,1)"}
    idx = random.randint(0, len(plan) - 1)
    if mutated[idx] in swap_map:
        mutated[idx] = swap_map[mutated[idx]]
    return mutated

print("\nMutated versions:")
for i, plan in enumerate(plans):
    mplan = mutate_plan(plan)
    print(f"  Plan {i+1} mutated: {' '.join(mplan)}")'''


from collections import deque


from collections import deque, namedtuple
import heapq
import math

def beam_enumerate(grammar, beam_width=5, max_steps=1000):
    """
    Enumerate up to `beam_width` complete derivations,
    filtering out zero-probability productions.
    Returns a list of (joint_prob, terminal_list).
    """
    # each beam entry is (–log_prob, derivation_list)
    beam = [(0.0, [grammar.start()])]
    completed = []

    for _ in range(max_steps):
        if not beam or len(completed) >= beam_width:
            break
        next_beam = []

        # Expand each partial derivation in the beam
        for score, deriv in beam:
            # If fully expanded, collect as complete
            if all(not isinstance(sym, Nonterminal) for sym in deriv):
                completed.append((math.exp(-score), deriv))
                continue

            # find leftmost nonterminal
            idx, nt = next((i,s) for i,s in enumerate(deriv)
                           if isinstance(s, Nonterminal))

            # expand only positive-prob productions
            for prod in grammar.productions(lhs=nt):
                p = prod.prob()
                if p <= 0.0:
                    continue
                new_score = score - math.log(p)
                new_der   = deriv[:idx] + list(prod.rhs()) + deriv[idx+1:]
                next_beam.append((new_score, new_der))

        # keep top beam_width by score
        next_beam.sort(key=lambda x: x[0])
        beam = next_beam[:beam_width]

    # if not enough completed, drain any fully-expanded from the beam
    for score, deriv in beam:
        if all(not isinstance(sym, Nonterminal) for sym in deriv):
            completed.append((math.exp(-score), deriv))
    return completed[:beam_width]
# 3) Sample one plan from those top-K
def sample_from_beam(candidates):
    probs, plans = zip(*candidates)

    total   = sum(probs)
    weights = [p/total for p in probs]
    choice  = random.choices(plans, weights=weights, k=1)[0]
    print("Sampled plan:", " ".join(choice))
    return random.choices(plans, weights)[0]


# 2) Pure PCFG sampler (for proposals)
def sample_from_pcfg(grammar, symbol=None):
    from nltk import Nonterminal
    if symbol is None:
        symbol = grammar.start()
    if not isinstance(symbol, Nonterminal):
        return [symbol]
    prods = grammar.productions(lhs=symbol)
    probs = [p.prob() for p in prods]
    chosen = random.choices(prods, weights=probs)[0]
    result = []
    for sym in chosen.rhs():
        result += sample_from_pcfg(grammar, sym)
    return result

# 3) Compute plan probability under grammar
def plan_prob(plan):
    # For a full tree, the joint prob is ∏ rule_probs, but reconstructing
    # that tree is complex. Here, since each unique plan maps to exactly
    # one derivation, we approximate by counting rules used.
    # *In your case all rules are prob=1, so every plan has equal mass.*
    return 1.0

# 4) Single-site subtree re-sampling proposal
def propose(current):
    # pick a random nonterminal position in the derivation tree;
    # here, we cheat and re-sample the *entire* plan
    return sample_from_pcfg(grammar)

# 5) MCMC loop
def mcmc_plans(grammar, burn_in=50, sample_gap=10, n_samples=5):
    current = sample_from_pcfg(grammar)
    samples = []
    # burn-in
    for _ in range(burn_in):
        prop = propose(current)
        # MH acceptance (here trivially always accept since plan_prob is constant)
        current = prop
    # then collect
    for _ in range(n_samples):
        for __ in range(sample_gap):
            prop = propose(current)
            if random.random() < plan_prob(prop)/plan_prob(current):
                current = prop
        samples.append(current)
    return samples
# 2) Temperature-aware choice
def tempered_choice(prods, T=1.0):
    ps = [p.prob() for p in prods]
    # raise to 1/T
    adj = [p**(1.0/T) for p in ps]
    tot = sum(adj)
    weights = [a/tot for a in adj]
    return random.choices(prods, weights=weights, k=1)[0]

# 3) Top-down sampler with temperature
def sample_with_temperature(grammar, symbol=None, T=1.0, max_depth=50):
    from nltk import Nonterminal
    if symbol is None:
        symbol = grammar.start()
    if not isinstance(symbol, Nonterminal) or max_depth<=0:
        return [symbol] if not isinstance(symbol, Nonterminal) else []
    prods = grammar.productions(lhs=symbol)
    chosen = tempered_choice(prods, T)
    result = []
    for sym in chosen.rhs():
        result += sample_with_temperature(grammar, sym, T, max_depth-1)
    return result
# 4) Run an example
if __name__ == "__main__":
    candidates = beam_enumerate(grammar, beam_width=3)
    print("Top-3 candidates (prob, plan):")
    for p, plan in candidates:
        print(f"  {p:.4f} -> {' '.join(plan)}")

    print("\nSampled plan:", ' '.join(sample_from_beam(candidates)))

    plans = mcmc_plans(grammar, burn_in=100, sample_gap=20, n_samples=5)
    for i, p in enumerate(plans,1):
        print(f"Sample {i}: {' '.join(p)}")

    for T in [0.5, 1.0, 2.0]:
        print(f"\n--- Temperature T={T} ---")
        for i in range(3):
            plan = sample_with_temperature(grammar, T=T)
            print(f"Plan {i+1}:", ' '.join(plan))


    
```

`hierarchical-nav/navigation_model/visualisation_tools.py`:

```py
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import numpy as np
import torch
from dommel_library.datastructs import TensorDict
from dommel_library.modules.visualize import (vis_image, vis_images)
from mpl_toolkits.axes_grid1 import make_axes_locatable
from PIL import Image

#=============================== IMAGES METHODS ===========================================

def visualise_image(images: torch.Tensor, title: str = '', fig_id: int = np.random.randint(1)) -> np.ndarray:
    images_list= convert_tensor_to_matplolib_list(images)
    plt.figure(fig_id)
    plt.imshow(images_list)
    plt.title(title)
    plt.axis('off')
    return images_list

def convert_tensor_to_matplolib_list(images: torch.Tensor) -> list:
    images_list = vis_images(images,show=False, fmt="numpy").tolist()#.cpu().detach().numpy()
    #image_array = np.transpose(image_array, (1, 2, 0)).tolist()
    return images_list

def convert_tensor_image_to_array(image:torch.Tensor)->np.ndarray:
    img = vis_image(image, show=False, fmt="torch").cpu().detach().numpy()
    img = np.transpose(img, (1, 2, 0))
    img = img*255
    return img

#=============================== POLICY VISU =============================================

def transform_policy_from_hot_encoded_to_str(policy:list) -> list:
        ''' given a policy of hot encoded actions transform it 
        to the corresponding string direction
        '''
        str_policy = []
        for action in policy:
            if action[0] == 1:
                str_policy.append('F')
            elif action[1] == 1:
                str_policy.append('R')
            elif action[2] == 1:
                str_policy.append('L')
            else:
                str_policy.append('X')
        return str_policy

import numpy as np, torch

def prepare_for_imshow(img):
    import numpy as np, torch

    # --- to numpy --------------------------------------------------
    if torch.is_tensor(img):
        img = img.detach().cpu().numpy()
    if isinstance(img, (list, tuple)):
        img = np.asarray(img)

    # ---- squeeze sequence / batch dims ---------------------------
    if img.ndim == 5:                    # (T,B,3,H,W) or (B,T,3,H,W)
        img = img[0]                    # take first imagined frame
    if img.ndim == 4 and img.shape[0] == 1:
        img = img[0]                    # (1,3,H,W) -> (3,H,W)

    # ---- to H×W×3 -------------------------------------------------
    if img.ndim == 3 and img.shape[0] == 3:
        img = img.transpose(1, 2, 0)    # (3,H,W) -> (H,W,3)

    # ---- scale range ---------------------------------------------
    raw_min, raw_max = img.min(), img.max()
    print(f"[DBG prepare] raw  min={raw_min:.3f}  max={raw_max:.3f}")

    if raw_max > 1.5:                   # 0-255
        img = img / 255.0
    elif raw_min < -0.9:                # -1…1 tanh
        img = (img + 1.0) / 2.0
    elif raw_min < -0.1:                # -0.5…0.5
        img = img + 0.5

    img = np.clip(img, 0.0, 1.0)
    print(f"[DBG prepare] scaled min={img.min():.3f}  max={img.max():.3f}")

    return img.astype(np.float32)
#=============================== VIDEO METHODS ===========================================
def dbg_stats(name, arr):
        import numpy as np, torch
        if arr is None:                      # skip empty placeholders
            print(f"[DBG] {name:<15} is  None")
            return

        if isinstance(arr, (list, tuple)):
            arr = np.asarray(arr)
        if torch.is_tensor(arr):
            arr = arr.detach().cpu().numpy()

        print(f"[DBGGGGGG] {name:<15} dtype={arr.dtype}  shape={arr.shape}  "
            f"min={arr.min():.3f}  max={arr.max():.3f}  mean={arr.mean():.3f}")


def record_video_frames(data:dict, env_definition:dict, agent_lost:bool, visited_rooms:list, memory_map_data:dict, step_count:int) -> np.ndarray:
    ''' we create a full frame ready to be added to a video or displayed'''
    fig = plt.figure(3, figsize = (12, 6))
    plt.clf()
    #s = plt.GridSpec(5, 6, wspace =0.3, hspace = 0.3) #
    s = fig.add_gridspec(4, 6, width_ratios = [3, 3, 3, 3, 3,3], height_ratios = [3, 3, 3 , 3],wspace =0.8, hspace = 0.4)

    #-- WORLD--#
    ax1 = fig.add_subplot(s[:2, :2])
    dbg_stats("env_image", data['env_image']) 
    ax1 = get_image_plot(ax1, data['env_image'], 'World')

    #-- GT OBSERVATION--#
    ax2 = fig.add_subplot(s[:1, 2:3])
    dbg_stats("ground_truth_ob", data['ground_truth_ob'])  
    ax2 = get_image_plot(ax2, data['ground_truth_ob'], 'GT OB')
   
    # PREDICTED OBSERVATIONS -------------------------------------------
    if 'image_predicted' in data and data['image_predicted'] is not None:
        pred_img = prepare_for_imshow(data['image_predicted'])
        ax3 = fig.add_subplot(s[:2, 3])
        ax3 = get_image_plot(ax3, pred_img, 'Predicted ob')

    # DECODED IMAGE -----------------------------------------------------
    if 'decoded' in data and data['decoded'] is not None:
        dec_img = prepare_for_imshow(data['decoded'])
        ax_dec = fig.add_subplot(s[1:2, 2:3])
        ax_dec = get_image_plot(ax_dec, dec_img, 'Decoded ob')
    
    #-- MAPS OF VISITED ROOMS --#
    ax4 = fig.add_subplot(s[2:4, :2])
    ax4 = plot_visited_rooms(ax4,visited_rooms, env_definition)

    #-- DESIRED OBJECTIVE --#
    # --  HIGH-LEVEL STATE  (previously "goal") --------------------------
    if 'recommended_mode' in data:
        mode_line = f"mode={data['recommended_mode']}   " \
                f"submode={data['recommended_submode']}"
        ax_state = fig.add_subplot(s[2:4, 2:4])
        txt = []

        # single-line headline
        txt.append(f"► MODE :  {data['recommended_mode']} "
                f"({data.get('mode_confidence',0):.2f})")
        txt.append(f"► SUB  :  {data['recommended_submode']} "
                f"({data.get('submode_confidence',0):.2f})")

        # optional extras
        if 'uncertainty' in data:
            txt.append(f"entropy={data['uncertainty']:.2f}   "
                    f"changepoint P={data.get('changepoint_mass',0):.2f}")

        ax_state.text(0.0, 1.0, "\n".join(txt),
                    fontsize=10, va="top", ha="left",
                    family="monospace")
        ax_state.set_axis_off()

    #-- PRED/OB MSE ---#
    ax6 = fig.add_subplot(s[:2, 4])
    ax6 = plot_MSE_bar(ax6, data['mse'], agent_lost)

    ax7 = fig.add_subplot(s[2:4, 4:])
    ax7= plot_memory_map(ax7, memory_map_data)

    ax8 = fig.add_subplot(s[:2, 5])
    ax8 = print_positions(ax8, data['GP'], data['pose'], step_count, mode_str=mode_line)
    

    fig = plt.gcf()
    canvas_width, canvas_height = fig.canvas.get_width_height()
    #print(f"Canvas size: {canvas_width}x{canvas_height}")
    s, (width, height) = fig.canvas.print_to_buffer()
    #print(f"Buffer size: {len(s)} Expected size: {width, height, 4}")
    width, height = 2400,1200
    buf = np.frombuffer(s, np.uint8).reshape((height, width, 4))
    # we don't need the alpha channel
    buf = buf[:, :, 0:3]

    #plt.show()
    # if self.close_plot:
    plt.close(fig)

    
    return buf

def get_image_plot(ax, data,title):
    ax.imshow(data)
    ax.set_title(title)
    plt.axis('off')
    return ax

def plot_MSE_bar(ax, mse_err, agent_lost):
    #-- PRED/OB MSE ---#
    try:
        #print('mse data length', len(data['mse']), 'last mse', mse_err)
        if mse_err <0.5:
            color = 'blue'
        else:
            color = 'red'
        
        plt.bar('MSE error', mse_err , color = color, width = 0.1)
    except KeyError as e:
        ax.text(-0.3,0.9, e, fontsize=10, color='black')

    ax.set_ylim(0,1)
    if agent_lost:
        back_color = '#ff8970'
    else:
        back_color = '0.8'
    ax.set_facecolor(color=back_color)
    plt.grid(axis='y')
    plt.title('mse ob/expectation')

'''def plot_visited_rooms(ax, visited_rooms, env_definition):
    n_row = env_definition['n_row']
    n_col = env_definition['n_col']

    # Create a grid of the specified size with all white tiles
    grid = np.ones((n_row, n_col))  

    # Assign colors based on visited rooms
    for i in range(n_col):
        for j in range(n_row):
            if (i, j) in visited_rooms:
                position_in_list = visited_rooms.index((i, j))
                color_value = position_in_list / len(visited_rooms)
                grid[j][i] = color_value

    # Create the plot
    im = ax.imshow(grid, cmap='gist_heat', vmin=0, vmax=1)
    ax.set_xticks(np.arange(n_col))
    ax.set_yticks(np.arange(n_row))
    # Create a divider for existing axes instance
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.1)

    # Create a colorbar
    cbar = plt.colorbar(im, cax=cax, orientation='vertical', label='Visited Rooms')

    # Set custom tick labels
    cbar.set_ticks([0, 0.9, 1])
    cbar.set_ticklabels(['oldest', 'newest', 'unknown'])

    # Set title for the subplot
    ax.set_title('Rooms ordered by discovery')

    return ax'''
def plot_visited_rooms(ax, visited_rooms, env_definition):
    n_row = env_definition['n_row']
    n_col = env_definition['n_col']

    # Create a grid of the specified size with all white tiles
    grid = np.ones((n_row, n_col))  

    # Assign colors based on visited rooms
    for i in range(n_col):
        for j in range(n_row):
            if (i, j) in visited_rooms:
                position_in_list = visited_rooms.index((i, j))
                color_value = position_in_list / len(visited_rooms)
                grid[j][i] = color_value

    # Calculate total rooms and explored count
    total_rooms = n_row * n_col
    explored_rooms = len(visited_rooms)
    print(f"{explored_rooms} out of {total_rooms} total rooms explored")
    print("visited_rooms", visited_rooms)

    # Create the plot
    im = ax.imshow(grid, cmap='gist_heat', vmin=0, vmax=1)
    ax.set_xticks(np.arange(n_col))
    ax.set_yticks(np.arange(n_row))
    
    # Create a divider for existing axes instance
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.1)

    # Create a colorbar
    cbar = plt.colorbar(im, cax=cax, orientation='vertical', label='Visited Rooms')

    # Set custom tick labels
    cbar.set_ticks([0, 0.9, 1])
    cbar.set_ticklabels(['oldest', 'newest', 'unknown'])

    # Set title for the subplot
    ax.set_title('Rooms ordered by discovery')

    return ax



'''def plot_memory_map(ax, memory_map_data):
    plt.title('Exp Map')
    plt.grid()
    
    #if nothing to report in map, just pass
    if memory_map_data['current_exp_id'] < 0:
        return ax
    
    exps_positions = np.vstack(memory_map_data['exps_GP'])
    ax.scatter(exps_positions[:,0], exps_positions[:,1], c=memory_map_data['exps_decay'], cmap="viridis")
    
    if len(memory_map_data['ghost_exps_GP']) > 0:
        ghost_exps_positions = np.vstack(memory_map_data['ghost_exps_GP'])
        ax.scatter(ghost_exps_positions[:,0], ghost_exps_positions[:,1], c='m', linewidths=0.5, alpha=0.5)
                
        if len(memory_map_data['ghost_exps_link']) > 0:
            ghost_exps_links = np.vstack(memory_map_data['ghost_exps_link'])
            ax.plot(ghost_exps_links[:,0], ghost_exps_links[:,1], c='m', linestyle='dashed', alpha=0.2)
    
    if len(memory_map_data['exps_links']) > 0:
        exps_links = np.vstack(memory_map_data['exps_links'])
        ax.plot(exps_links[:,0], exps_links[:,1], 'grey')

    ax.plot(memory_map_data['current_exp_GP'][0],
            memory_map_data['current_exp_GP'][1], 'r', marker='x')

    s0 = str(memory_map_data['current_exp_id'])
    ax.text(memory_map_data['current_exp_GP'][0], memory_map_data['current_exp_GP'][1], s0, fontsize=10, color='red')

    return ax'''
def plot_memory_map(ax, memory_map_data, *, dbg=False):
    """
    Visualise the experience-map on a Matplotlib Axes.

    This version:
      - Guards against empty arrays
      - Drops incomplete link pairs
      - Deduplicates link segments
      - Clears the axes each call to avoid leakage
      - Batches debug output for readability
      - Increases link linewidth for clarity
      - Annotates each node with its ID
    """
    import numpy as np

    # Always clear old drawing
    ax.cla()
    ax.set_title("Experience Map")
    ax.grid(True)
    ax.set_aspect("equal", "datalim")

    curr_id = memory_map_data.get("current_exp_id", -1)
    if curr_id < 0:
        if dbg:
            print("[VIS] No current exp → skipping map.")
        return ax

    # --- Nodes ---
    exps_gp = memory_map_data.get("exps_GP", [])
    exps_decay = memory_map_data.get("exps_decay", [])
    if exps_gp:
        pts = np.array(exps_gp)
        ax.scatter(
            pts[:, 0], pts[:, 1],
            c=exps_decay,
            cmap="viridis",
            label="Places",
            s=60,              # slightly larger markers
            edgecolor="k",
            linewidth=0.5
        )
        # Annotate each node with its ID
        for idx, (x, y) in enumerate(pts):
            ax.text(
                x, y,
                str(idx),
                color="white",
                fontsize=8,
                weight="bold",
                ha="center",
                va="center"
            )
    elif dbg:
        print("[VIS] No place nodes to plot.")

    # --- Ghosts ---
    ghost_gp = memory_map_data.get("ghost_exps_GP", [])
    if ghost_gp:
        gpts = np.array(ghost_gp)
        ax.scatter(
            gpts[:, 0], gpts[:, 1],
            c="magenta",
            marker="^",
            alpha=0.5,
            label="Ghosts",
            s=40
        )
    ghost_links = memory_map_data.get("ghost_exps_link", [])
    # collect only complete pairs
    glinks = []
    for i in range(0, len(ghost_links) // 2 * 2, 2):
        p0 = tuple(ghost_links[i])
        p1 = tuple(ghost_links[i + 1])
        glinks.append((p0, p1))
    # dedupe
    glinks = list(dict.fromkeys(glinks))
    if glinks:
        if dbg:
            print(f"[VIS] Ghost links: showing {len(glinks)} pairs")
        for p0, p1 in glinks:
            ax.plot(
                [p0[0], p1[0]], [p0[1], p1[1]],
                ls="--",
                color="magenta",
                alpha=0.3,
                linewidth=2    # make ghost links more visible
            )

    # --- Real links ---
    exps_links = memory_map_data.get("exps_links", [])
    rlinks = []
    for i in range(0, len(exps_links) // 2 * 2, 2):
        p0 = tuple(exps_links[i])
        p1 = tuple(exps_links[i + 1])
        rlinks.append((p0, p1))
    rlinks = list(dict.fromkeys(rlinks))
    if dbg:
        print(f"[VIS] Real links: {len(rlinks)} pairs (deduped)")
    for p0, p1 in rlinks:
        ax.plot(
            [p0[0], p1[0]], [p0[1], p1[1]],
            color="gray",
            linewidth=2      # increase link thickness
        )

    # --- Current position ---
    cx, cy = memory_map_data["current_exp_GP"][:2]
    ax.plot(
        cx, cy,
        marker="X",
        color="red",
        markersize=10,
        label="Current"
    )
    ax.text(
        cx, cy,
        str(curr_id),
        color="white",
        fontsize=9,
        weight="bold",
        ha="center",
        va="center",
        bbox=dict(facecolor="red", alpha=0.5, pad=1)
    )

    # --- Legend & Return ---
    if dbg:
        ax.legend(loc="upper right", fontsize=8)
    return ax


'''def print_positions(ax, GP, pose, step_count):
    current_gp = [float(x) for x in np.around(np.array(GP), 2)]
    s1 = 'Global Position [x,y,th]: ' 
    ax.text(-0.3, 0.9, s1, fontsize=10, color='black')
    ax.text(0, 0.8, str(current_gp), fontsize=10, color='black')
    
    s3 = 'Local Position [x y th]: ' 
    ax.text(-0.3,0.6, s3, fontsize=10, color='black')
    ax.text(0, 0.5, str(pose), fontsize=10, color='black')
    ax.set_axis_off()
    s0 = 'Step: ' + str(step_count) 
    ax.text(-0.3,1.1, s0, fontsize=10, color='black')
    return ax'''
def print_positions(ax, GP, pose, step_count, mode_str=None):
    """Draw global/local pose and optional mode string."""
    current_gp = [float(x) for x in np.around(np.array(GP), 2)]

    s0 = f"Step: {step_count}"
    s1 = "Global Position [x,y,th]:"
    s2 = str(current_gp)
    s3 = "Local Position  [x y th]:"
    s4 = str(pose)

    ax.text(-0.3, 1.05, s0, fontsize=10, color="black")
    ax.text(-0.3, 0.90, s1, fontsize=10, color="black")
    ax.text( 0.0, 0.80, s2, fontsize=10, color="black")
    ax.text(-0.3, 0.60, s3, fontsize=10, color="black")
    ax.text( 0.0, 0.50, s4, fontsize=10, color="black")

    # -------------- NEW: mode / sub-mode line -----------------------
    if mode_str:
        ax.text(-0.3, 0.30, mode_str, fontsize=10, color="blue")

    ax.set_axis_off()
    return ax
#=============================== OTHER METHODS ===========================================
def plot_pose_cube(memory_graph:object):
    """ plot CAN pose cell gradual motion"""
    plt.figure(3)
    plt.title('Pose cube')
    
    ax = plt.axes(projection="3d")
    x, y, th = memory_graph.pc
    dim_xy = memory_graph.pose_cells.DIM_XY
    dim_th = memory_graph.pose_cells.DIM_TH
    ax.plot3D(x, y, th,'x')
    ax.plot3D([0, dim_xy], [y[-1], y[-1]], [th[-1], th[-1]], 'k')
    ax.plot3D([x[-1], x[-1]], [0, dim_xy], [th[-1], th[-1]], 'k')
    ax.plot3D([x[-1], x[-1]], [y[-1], y[-1]], [0, dim_th], 'k')
    ax.plot3D([x[-1]], [y[-1]], [th[-1]], 'mo')
    ax.grid()
    ax.axis([0, dim_xy, 0, dim_xy])
    ax.set_zlim(0, dim_th)
    plt.show()

def plot_room(allo_model:object, door_poses:list, step:int= None):
    '''
    plot the imagined room of the allocentric model
    '''
    # print(exps, len(exps))
    # print(type(exps[0]['observation']), type(exps[0]['observation'][0]))
    #place = place.unsqueeze(0).unsqueeze(0).repeat(5,1,1) #MultivariateNormal(torch.from_numpy(exps[rat_exps_id]['observation'])).unsqueeze(0).unsqueeze(0).repeat(5,1,1)
    #print('observation door poses 0', exps[rat_exps_id]['observation_door_poses'][0], type(exps[rat_exps_id]['observation_door_poses'][0]))
    dps = door_poses.copy()
        
    poses = []
    for door_p in dps:
        if   door_p[2] == 2:
            door_p[0] += 4
            door_p[1] += 1
        elif door_p[2] == 0:
            door_p[0] -= 2
        elif door_p[2] == 1:
            door_p[1] -= 2
        elif door_p[2] == 3:
            door_p[1] += 2

        for angle in range(4):
            door_p[2]= angle
            poses.append(door_p.copy())
    
    pred_pose = TensorDict({'pose_query': torch.Tensor(poses).unsqueeze(0).repeat(5,1,1)})
    #print('pred poses', pred_pose, pred_pose['pose_query'].shape)
    pred_step = allo_model.model.forward(pred_pose, place=None, reconstruct=True)

    #--- Show 1 mean image
    img_pred = torch.mean(pred_step['image_predicted'], dim=0)

    view_points = []
    for view in range(img_pred.shape[0]):
        image_predicted_data = vis_image(img_pred[view],show=False, fmt="torch").cpu().detach().numpy()
        image_predicted_data = np.transpose(image_predicted_data, (1, 2, 0)) * 255  # from normalized data 0 - 1 to 255 img
        image_predicted_data = image_predicted_data.astype(np.uint8)
        
        image_predicted_data = Image.fromarray(image_predicted_data, "RGB")
        view_points.append(image_predicted_data)
    
    background = Image.new("RGB", (180, 180), (255, 255, 255))
    middle_background = round(background.size[0]/2)

    for i in range(0,len(view_points), 4):
        if i == 0:
            shift = [0,0]
        else:
            #start = [middle_background,middle_background] #it's the middle of first image in background (so 3.5tiles*8px forward agent)
            shift = np.array([poses[0][0] - poses[i][0], poses[0][1] + poses[i][1]])*8
            print(poses[0],poses[i], shift)
            
        view_points[i].putalpha(180)
        img_look_forward = view_points[i]
        img_look_right = view_points[i+1].rotate(-90)
        img_look_behind = view_points[i+2].rotate(180)
        img_look_left = view_points[i+3].rotate(90)

        middle_img = round(img_look_forward.size[0]/2)
        H_agent_in_img = middle_img - (3*8)
        
        
        # starting at coordinates (x,y) upper/left agle is (0,0)
        background.paste(img_look_forward, (middle_background+shift[1]-middle_img, middle_background+shift[0]-middle_img), mask=img_look_forward)
        background.paste(img_look_right, (middle_background+shift[1]-H_agent_in_img, middle_background+shift[0]-H_agent_in_img), mask=img_look_forward)
        background.paste(img_look_left, (middle_background+shift[1]-img_look_left.size[0]+4, middle_background+shift[0]-H_agent_in_img), mask=img_look_forward)
        background.paste(img_look_behind, (middle_background+shift[1]-middle_img, middle_background+shift[0]+middle_img-8), mask=img_look_forward)
                
    background.show()
    background.save('imagination_results/imagined_room/imagined_room_step_'+str(step) +'.png')
    print('room saved as: imagination_results/imagined_room/imagined_room_step_'+str(step)+'.png')

def visualize_replay_buffer(replay_buffer):
    """
    Visualize the replay buffer in a grid of subplots.
    For each state stored in the replay buffer, display:
      - Left: the real image along with its associated real pose and action.
      - Right: the imagined (predicted) image along with its predicted pose.
      
    If the 'imagined_image' is None, a blank image is used instead.
    The layout of the figure allocates vertical space (row heights) proportionately
    based on the real image heights.
    """
    num_states = len(replay_buffer)
    if num_states == 0:
        print("Replay buffer is empty.")
        return

    # Close any previous figures to prevent multiple windows.
    plt.close('all')

    # Compute row heights (in pixels or any relative unit) using the real image's height.
    row_heights = []
    processed_states = []
    for state in replay_buffer:
        # Process real image.
        real_img = state.get('real_image')
        if real_img is not None:
            if torch.is_tensor(real_img):
                real_img = real_img.detach().cpu().numpy()
            else:
                real_img = np.array(real_img)
            # Ensure it has at least 2D (in case it's a scalar or very low-dim data).
            if real_img.ndim < 2:
                real_img = np.atleast_2d(real_img)
        else:
            # Default to blank image of size 10x10 if no real image.
            real_img = np.zeros((10,10), dtype=np.uint8)
            
        # Process imagined image.
        imagined_img = state.get('imagined_image')
        if imagined_img is not None: 
            # Squeeze an extra dimension if present.
            imagined_img = imagined_img.squeeze(1) 
            if torch.is_tensor(imagined_img):
                imagined_img = imagined_img.detach().cpu().numpy()
            # If 4D, take the first sample.
            if imagined_img.ndim == 4:
                imagined_img = imagined_img[0]
            # If 3D and in (channels, H, W) format (with channels 1 or 3), transpose to (H, W, C)
            if imagined_img.ndim == 3 and imagined_img.shape[0] in [1, 3]:
                imagined_img = imagined_img.transpose(1, 2, 0)
        else:
            # If no imagined image, create a blank image using the same H, W as real_img.
            blank_shape = real_img.shape[:2]
            imagined_img = np.zeros(blank_shape, dtype=np.uint8)

        # Save processed state info.
        processed_states.append({
            'real_img': real_img,
            'imagined_img': imagined_img,
            'real_pose': state.get('real_pose', 'unknown'),
            'imagined_pose': state.get('imagined_pose', 'unknown'),
            'action': state.get('action', 'none')
        })
        
        # For row height, use the height of the real image.
        row_heights.append(real_img.shape[0])
    
    # Set up the GridSpec with dynamic row heights.
    # You can adjust the scaling factor to control overall figure height.
    scaling_factor = 0.05  # Adjust this factor as needed (e.g., 0.05 inches per pixel)
    total_height = sum(row_heights) * scaling_factor
    fig = plt.figure(figsize=(18, 16))
    gs = gridspec.GridSpec(num_states, 2, height_ratios=row_heights)
    
    # Now, create subplots using the GridSpec.
    for idx, state in enumerate(processed_states):
        # Left subplot: Real image.
        ax_real = fig.add_subplot(gs[idx, 0])
        ax_real.imshow(state['real_img'], cmap='gray', vmin=0, vmax=255)
        ax_real.set_title(f"Real Image\nPose: {state['real_pose']} | Action: {state['action']}", fontsize=8)
        ax_real.axis('off')
        
        # Right subplot: Imagined image.
        ax_imagined = fig.add_subplot(gs[idx, 1])
        ax_imagined.imshow(state['imagined_img'], cmap='gray', vmin=0, vmax=255)
        ax_imagined.set_title(f"Imagined Image\nPredicted Pose:{state['imagined_pose']}" , fontsize=8)
        ax_imagined.axis('off')
    
    
    plt.show()

```

`hierarchical-nav/run.py`:

```py
#!/usr/bin/env python3
from control_eval.keyboard_control_navigation import MinigridInteraction as keyboard_interaction
from control_eval.automatic_env_run import run_test
from control_eval.arguments import parser
import sys, pathlib



if __name__ == "__main__":
# instantiating the decorator
    
    flags = parser.parse_args()
    print('in runs flags', flags)
    #rospy.init_node("nodo")

    if 'key' in flags.test :
        redraw_window = True
    else:
        redraw_window = False
    minigrid_interaction = keyboard_interaction(flags, redraw_window=redraw_window)
    if flags.test != 'key':
        # env_def= env_definition(flags)
        run_test(minigrid_interaction, flags)

   
        
  
```

`hierarchical-nav/run_docker.sh`:

```sh
docker run --rm -it --user root \
  --device=/dev/davinci1 \
  --device=/dev/davinci_manager \
  --device=/dev/devmm_svm \
  --device=/dev/hisi_hdc \
  -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
  -v /usr/local/Ascend/add-ons/:/usr/local/Ascend/add-ons/ \
  -v /var/log/npu/conf/slog/slog.conf:/var/log/npu/conf/slog/slog.conf \
  -v /var/log/npu/slog/:/var/log/npu/slog \
  -v /var/log/npu/profiling/:/var/log/npu/profiling \
  -v /var/log/npu/dump/:/var/log/npu/dump \
  -v /var/log/npu/:/usr/slog \
  -v /home/2023_06/docker_outputs/Summaries:/app/Summaries \
  --name hierar \
  hierar /bin/bash

```

`hierarchical-nav/run_tests.bash`:

```bash
#!/bin/bash

echo "Run model in 2x3 rooms environments with the desired variables"

declare -a ENVS=("4-tiles-ad-rooms" "5-tiles-ad-rooms")
declare -a LOOKAHEADS=("6" "7")


#TEST 0 : keyboard
#test 1: exploration
#test 2: 
export START_SEED=200
export END_SEED=213 
export TEST=$1
export ROW=$2 
export COL=$3 
export Saving_dir=check_refactoring

export GPULAB_dir=/project_ghent/ddtinguy/hierarchical_st_nav_aif/

echo Test $TEST
echo Saving dir $Saving_dir


for env_setting in $(seq 0 0)
	do
	ENV=${ENVS[$env_setting]}
	LOOKAHEAD=${LOOKAHEADS[$env_setting]}
	
	echo $ENV
    
    for seed in $(seq $START_SEED $END_SEED)
        do

        echo seed= $seed
        echo ROW= $ROW
        echo col= $COL
        #work here
        n_tile=${ENV:0:1}

        folder=${GPULAB_dir}${Saving_dir}/${n_tile}t_${ROW}x${COL}_s${seed} 
        mkdir -p $folder
        file=${TEST}_${ENV}_${ROW}x${COL}_s${seed}.txt
        echo saving logs in ${folder}/${file}
        python run.py --allo_config ${GPULAB_dir}runs/GQN_V2_AD/v2_GQN_AD_conv7x7_bi/GQN.yml --env $ENV --seed $seed --rooms_in_row $ROW --rooms_in_col $COL --test $TEST --video --save_dir $Saving_dir --lookahead $LOOKAHEAD >> ${folder}/${file}
        
    done #seed
            
done #envs
exit




```

`hierarchical-nav/runs/GQN_V2_AD/v2_GQN_AD_conv7x7_bi/GQN.yml`:

```yml
config: experiments/GQN_v2/GQN_v2.yml
dataset:
  device: cpu
  keys:
  - image
  - pose
  sequence_length: -1
  sequence_stride: -1
  train_set_location: https://cloud.ilabt.imec.be/index.php/s/ejSdTFgGk32CsXG/download
  type: FilePool
  val_set_location: https://cloud.ilabt.imec.be/index.php/s/dF8NBETygrCxRq6/download
  scenario: /home/idlab332/workspace/shortcuts_goal/data/tests/1r_exp_60s_ADwTiles/5t_1r_blue_seed4_wT
device: cpu
experiment_dir: /project_ghent/ddtinguy/shortcuts_goal/runs
id: v2_GQN_AD_conv7x7_bi
log_dir: runs/GQN_V2_AD/v2_GQN_AD_conv7x7_bi
loss:
  kl:
    key: place
    target: std_normal
    type: KL
    weight: 5
  pose_def:
    key: pose_query
    target: pose_predicted
    type: L1
  reconstruct:
    key: image_query
    target: image_predicted
    type: MSE
    weight: 5
model:
  PositionalDecoder:
    activation: LeakyReLU
    activation_args:
      negative_slope: 1
    channels:
    - 2048
    - 512
    - 128
    - 64
    pose_encoded_dim: 9
  RandomSelectQuery: true
  RandomSeqQuery: false
  SceneDecoder:
    ConvFilm: true
    MLP_channels: []
    activation: LeakyReLU
    channels:
    - 256
    - 128
    - 64
    - 32
    - 32
    interpolate_mode: bilinear
  SceneEncoder:
    ConvFilm: true
    MLP_channels: []
    activation: LeakyReLU
    aggregate_factor: 1
    channels:
    - 16
    - 32
    - 64
    - 128
    clip_variance: 0.25
    expand: true
  max_query: 15
  min_context: 15
  min_query: 5
  observations:
    image:
    - 3
    - 56
    - 56
    pose: 3
    z_size: 32
  type: GQN
optimizer:
  amsgrad: true
  lr: 0.0004
  type: Adam
remark: GQN 1room exploration with 4t aisle doors OPEN & 6w tiles in rand rooms. bilinear
  interpolation
start_epoch: 0
trainer:
  batch_size: 64
  log_epoch: 10
  num_epochs: 20000
  num_workers: 0
  save_epoch: 100
  vis_args:
    keys:
    - image_query
    - image_predicted
    - pose_query
    - pose_predicted
    vis_mapping:
      vector:
      - pose_query
      - pose_predicted
  vis_batch_size: 3
  vis_epoch:
    after_rate: 5
    before_rate: 10
    n: 100

```

`hierarchical-nav/runs/OZ/OZ_AD_Col16_8_id/OZ.yml`:

```yml
config: experiments/OZ/OZ.yml
params: '/Users/lab25/hierarchical-nav/runs/OZ/OZ_AD_Col16_8_id/models/model-1500.pt'
model_epoch: 1500
dataset:
  device: cpu
  keys:
  - image
  - action
  # - pose
  remark: data rooms exploration, 4t aisles, door in middle, 6white Tiles in rand
    rooms see aisle_door_6wFloor_3x3rooms_100_400s
  sequence_length: -1
  sequence_stride: -1
  train_set_location: https://cloud.ilabt.imec.be/index.php/s/To58jREwsPYpaHr/download
  type: FilePool
  val_set_location: https://cloud.ilabt.imec.be/index.php/s/Ln2ednMfxg3biGB/download
  scenario: /home/idlab332/workspace/shortcuts_goal/data/tests/1r_exp_60s_ADwTiles/5t_1r_blue_seed4_wT
device: cpu
experiment_dir: /project_ghent/ddtinguy/shortcuts_goal/runs
id: OZ_AD_Col16_8_id
log_dir: runs/OZ/OZ_AD_Col16_8_id
loss:
  OZ_collision_detection:
    key: collision_reconstructed
    target: collision
    type: BCE
  OZ_kl:
    key: posterior
    target: prior
    type: KL
    weight: 1
  OZ_reconstruct:
    key: image_reconstructed
    target: image
    type: MSE
model:
  hidden_layers:
  - 256
  lstm_cells:
  - 256
  lstm_posterior: true
  num_actions: 3
  num_states: 32
  observations:
    collision:
      activation: Identity
      hidden_layers:
      - 16
      - 8
      input_shape: 1
      mlp_activation: Hardsigmoid
      type: Bool
    image:
      channels:
      - 8
      - 16
      - 32
      input_shape:
      - 3
      - 56
      - 56
      type: Conv
  type: Conv
optimizer:
  amsgrad: true
  lr: 0.0004
  type: Adam
start_epoch: 0
trainer:
  batch_size: 32
  log_epoch: 10
  mixed_precision: true
  num_epochs: 10000
  num_workers: 0
  save_epoch: 100
  vis_batch_size: 2
  vis_epoch:
    after_rate: 100
    before_rate: 1
    n: 3
  warmup: 10

```

`hierarchical-nav/setup.py`:

```py
import setuptools

with open("README.md", "r") as readme:
    long_description = readme.read()

with open("requirements.txt", "r") as req:
    requirements = req.read().splitlines()

setuptools.setup(
    name="hierarchical_st_nav_aif",
    version="0.2.0",
    author="DML group",
    author_email="",
    description=" D Hierarchical active inference experiments",
    long_description=long_description,
    long_description_content_type="text/markdown",
    packages=setuptools.find_packages(),
    classifiers=[
        "Programming Language :: Python :: 3",
        "Operating System :: POSIX :: Linux",
        "Development Status :: 4 - Beta"
    ],
    python_requires=">3.9.7",
    install_requires=[
        'numpy>=1.17.4',
        'pynput>=1.6.8',
        'torch>=1.10.2',
        'matplotlib>=3.1.2',
        'utils>=1.0',
        'torchvision>0.11.3',
        'tqdm>=4.63.0',
        'gym==0.17.0',
        'dill>=0.3.6',
        'imageio>=2.16.1',
        'Pillow>=9.4.0',
        'pyquaternion>=0.9.9',
        'PyYAML>=5.1.2',
        'pandas>=1.4.1',
        'seaborn>=0.12.2',
        'setuptools>=45.2.0',
        'h5py>=2.6.0'
    ],

)
```