import torch
import numpy as np
from navigation_model.Services.model_modules import torch_observations
from navigation_model.Processes.motion_path_modules import (
    action_to_pose, define_policies_objectives, create_policies, dijkstra_weigthed, 
    dijkstra_shortest_node_path)
from navigation_model.Processes.AIF_modules import mse_elements


from navigation_model.visualisation_tools import visualise_image, transform_policy_from_hot_encoded_to_str
from env_specifics.env_calls import find_preferred_features_in_img, call_env_place_range


class ExploitativeBehaviour():
    def __init__(self, verbose:bool=True) :
        self.verbose = verbose
        self.preferred_objective = {}

#========================= SET GOAL ================================================#
    def get_preferred_objective(self)->dict:
        return self.preferred_objective
    
    def set_objective(self, pose:list=[], exp_id:int =-1, image=None, decay:float=100)->dict:
        ''' save pose, exp_id and ob as torched objective to reach'''
        goal_info = {'pose': pose, 'image': image}
        goal_data = torch_observations(goal_info, ['pose', 'image'])
        goal_data['exp'] = exp_id
        goal_data['decay'] = decay
        return goal_data
    
    def set_current_pose_as_goal(self, manager:object, ob:np.ndarray=[])->dict:
        '''
        Given a goal set during run , determine where it is IN ROOM
        '''
        pose = manager.get_best_place_hypothesis()['pose']
        if len(ob) == 0:
            ob = manager.get_best_place_hypothesis()['image_predicted'][0][0]
        
        current_exp_id = manager.get_current_exp_id()
        goal = self.set_objective(pose, current_exp_id, ob)
        return goal
    
    def define_preferred_objective(self,manager:object, preferred_features:np.ndarray):
        """
        given what the objective should look like and search for it in ego then allo
        The decay set the progression at which this information is lost, 
        meaning that we should refresh our memory regularly
        """
        #check conidtions to do so
        pref_objective = self.search_goal_around(manager, preferred_features)
        if pref_objective :
            self.preferred_objective = self.set_objective(**pref_objective, decay=0.75)
            return self.preferred_objective
        
        pref_objective = self.search_goal_in_memory(manager, preferred_features)
        if pref_objective:
            self.preferred_objective = self.set_objective(**pref_objective, decay=0.75)
            return self.preferred_objective
        
        return {}

    def goal_memory_decay(self, decay_value:float=0.25):
        self.preferred_objective['decay'] -= decay_value

#========================= SEARCH GOAL ================================================#
    
    def search_goal_around(self,manager:object, preferred_features:np.ndarray):
        """ 
        egocentric model search in vicinity a pose predicting the preferred_features.
        It returns the best pose and nothing else.
        """
        
        pref_goal = self.ego_match_preferred_features_wt_pose(manager, preferred_features)
       
        return pref_goal
    
    def search_goal_in_memory(self, manager:object, preferred_features:np.ndarray) -> dict:
        """ memory graph + allo search"""
        preferrences_in_memory = self.allo_match_preferred_features_wt_pose(manager, preferred_features)
        if preferrences_in_memory:
            best_preferrence = max(preferrences_in_memory.values(), key=lambda x: x['pref_likelihood'])
            print('best_preferrence', best_preferrence['image'].shape, best_preferrence['exp_id'], best_preferrence['pose'], best_preferrence['pref_likelihood'])
            del best_preferrence['pref_likelihood']
        else:
            best_preferrence = {}
        return best_preferrence

#============================ MOTION PUSHED BY OB ======================================#
    #NOTE: WHAT IF ERROR? (GOAL ACTUALLy NOT IN PLACE)
    def go_to_observation_in_place(self, manager:object, goal:dict)->list:
        '''
        output the best policy to reach the observation objective in place 
        (no obstacle)
        #NOTE: THERE IS A RISK OG GOING BACK AND FORTH BETWEEN TWO ORIENTATIONS LEADING TO GOAL, 
        depending on which policy holds the best goal prediction. NOT A BIG ISSUE BUT COULD BE 
        Annoying
        '''
      
        goal['image'] = manager.sample_visual_ob(goal['image'].unsqueeze(0))
        current_pose = manager.get_best_place_hypothesis()['pose'].copy()
        if np.all(current_pose == goal['pose'].tolist()):
            print("we are not entering go_to_observation_in_place",current_pose, goal['pose'].tolist())
            return [], False
        policy = self.reach_objective_wt_ego_short_term_pred(manager, goal, current_pose)
        print("policies in go_to_observation_in_place, wt ego short term", policy)
        #If a path to the goal is predicted by the ego model, reach there
        if len(policy)> 0:
            return policy, False
        
        policies = create_policies(current_pose, [goal['pose'].tolist()], exploration=False)
        print("policies", policies)
        policies_to_goal = self.evaluate_go_to_goal_pose_policies(manager,policies)
        print("policies_to_goal", policies_to_goal)
        if len(max(policies, key=len)) == 0:
            print('We have probably reached the objective')
            return [], False

        if len(policies_to_goal)==0:
            #If all policies leads to a colision. We will consider the full policies
            print("ERROR in go_to_observation_in_place: The goal pose is predicted unreachable")
            print('We will still continue the approach')
            policies_to_goal = policies
        
        if len(min(policies_to_goal, key=len)) < 3:
            #If the shortest path to the goal < 3 ... the ego model should have predicted the goal observation.
            #We shouldn't be here. there is been an error in goal prediction
            print('ERROR in go_to_observation_in_place: less than 3 steps to reach goal, \
                  yet ego model not seeing the goal. We consider the allo model allucinating the goal!')
            return min(policies_to_goal, key=len), True
        
        #------ We process all the policies through the allocentric model to select best approach ---# 
        best_policy = self.reach_objective_wt_allo_pred(manager, goal, policies_to_goal, current_pose)
        print(best_policy)
        return best_policy, False
        
    def evaluate_go_to_goal_pose_policies(self, manager:object,policies:list)-> list:
        """ 
        given policies to evaluate, check if they are dynamically plausible
        return those that are
        """
        policies_to_goal = []
        for policy in policies:
            #Check if there is any collision in the polcicies
            #print('policy for ego pred', transform_policy_from_hot_encoded_to_str(policy))
            returned_policy, _ = manager.policy_egocentric_prediction(policy)
            #if no collision, then this is a good path to take.
            if len(returned_policy) == len(policy):
                policies_to_goal.append(policy)
        return policies_to_goal
    
    def reach_objective_wt_allo_pred(self, manager:object, goal:dict, policies:list, current_pose:list) -> list:
        """
        we check if any of the policies hold an allocentric prediction matching it. 
        The policy having the best mse for a pred matching the goal is considered for the trajectory
        NOTE: can lead to loops turning between 2 re-try depending on pred quality
        """
        place = manager.get_best_place_hypothesis()['post']
        print("place inside reach_objective_wt_allo_pred", place)
        goal_matching_mse_pose = {}
        for policy in policies:
            print("policy entering estimate_policy_allo_pred",policy)
            goal_matching_mse_pose = self.estimate_policy_allo_pred_matching_goal(manager, place, current_pose, policy, goal, goal_matching_mse_pose)
            print("policy out", goal_matching_mse_pose)
        best_policy = self.get_policy_with_min_mse_pose(goal_matching_mse_pose)
        print("best_policy", best_policy)
        return best_policy
    
    def estimate_policy_allo_pred_matching_goal(self,manager:object, place:np.ndarray, current_pose:list,
                                                 policy:list, goal:dict, goal_matching_mse_pose:dict={}):
        ''' given the place, current pose and policy as well as the goal to compare to, 
        predict the observation for each pose of policy and compare each of them to the goal
        '''
        seq_pose = current_pose.copy()
        pose_queries = []
        pose_queries.extend([list(seq_pose := action_to_pose(action, seq_pose)) for action in policy])
        allo_prediction = manager.several_poses_allocentric_prediction(pose_queries, place) 
        goal_matching_mse_pose = self.compare_policy_prediction_to_preferred_ob(manager, goal, current_pose, allo_prediction, policy,
                                                key='image_predicted', goal_matching_mse_pose=goal_matching_mse_pose)
        return goal_matching_mse_pose
    
    def reach_objective_wt_ego_short_term_pred(self, manager:object, goal:dict, current_pose:list)-> tuple[dict,list]:
        '''
        we check in the immediate vicinity with the ego model 
        (it has a short term memory, it is not biased by long past data)
        if the observation is found, we update the objective to that pose. 
        return the modified goal and po
        #warning, if the agent needs to turn back on its path to see goal, this policy won't be formed with create_policy
        thus, the short term planning won't find anything
        #NOTE: we have to make sure that we consider the correct observation 
        and are not mistaken with aliases.
        '''
        policies = self.define_explo_policies(manager, current_pose)
        print("define_explo_policies", policies, "how many policies", len(policies))
        
        goal_matching_mse_pose = {}
        for policy in policies:
            print("policy entering", policy)
            policy, ego_prediction = manager.policy_egocentric_prediction(policy)
            print("policies inside the ego predict",policy, ego_prediction.keys(),ego_prediction["collision_reconstructed"])
            if len(policy) == 0:
                continue
            goal_matching_mse_pose = self.compare_policy_prediction_to_preferred_ob(manager, goal, current_pose, ego_prediction,
                                                    policy, 'image_reconstructed', 
                                                    goal_matching_mse_pose)
        
        print('goal_matching_mse_pose', goal_matching_mse_pose)
        best_policy = self.get_policy_with_min_mse_pose(goal_matching_mse_pose)

        return best_policy

    def compare_policy_prediction_to_preferred_ob(self, manager:object, goal:dict, current_pose:list, pred:dict, 
                                                    policy:list, key:str='image_reconstructed', 
                                                    goal_matching_mse_pose:dict={})-> dict:
        """
        given the manager
        the goal as a sampled dict as the predictions
        the prediction containing the image (the keys is an option to compare with both ego and allo preds)
        the whole policy and wether we want to compare new poses to an already existing dict goal_matching_mse_pose

        For each action in the policy, check if the prediction matches the preferred observation. If it does
        save it if it's new and the best approximation of the goal at this pose.
        Also, to avoid aliasing, we consider the goal orientation, must be the same between the pose and goal.

        """
        motion_pose = current_pose.copy()  
        for action_idx in range(len(policy)): 
            motion_pose = action_to_pose(policy[action_idx], motion_pose)
            pose = list(motion_pose)
            #CHECK IF IMAGE OF CORRECT SHAPE FOR COMPARISON
            mse_ob = mse_elements(pred[key][:,action_idx,...], goal['image'])
            print("this is the mse_ob, motion and pose", mse_ob,pose,motion_pose)
            if manager.mse_under_threshold(mse_ob, sensitivity=0.36):
                #only consider poses having the same orientation. To avoid aliasing
                #+ We take the best mse under threshold of a given pose
                key_pose  =tuple(pose)
                #We consider the length of the policy, the shortest it is, the higher the weightaccount
                #Thus the less steps there is in policy, the higher the mse.
                weight = 1/len(policy[:action_idx+1])
                #We consider when we have no goal pose and search one
                #or if we want to validate an approximated pose goal
                if 'pose' not in goal or goal['pose'][2] == pose[2] \
                    and (key_pose not in goal_matching_mse_pose\
                    or mse_ob*weight < goal_matching_mse_pose[key_pose]['weighted_mse']):
                    goal_matching_mse_pose[key_pose] = {'weighted_mse':mse_ob*weight, 'policy': policy[:action_idx+1]}

                    print(transform_policy_from_hot_encoded_to_str(policy[:action_idx]))
                    # visualise_image(ego_prediction[key][:,action_idx,...], title="pred pose" + str(pose), fig_id=len(goal_matching_mse_pose)+11)
                    # visualise_image(goal['image'], title="goal pose" + str(goal['pose']), fig_id=10)
        return goal_matching_mse_pose
    
    def get_policy_with_min_mse_pose(self, goal_matching_mse_pose:dict):
        """ given the dict of the poses having the lowest mse """
        if goal_matching_mse_pose:
            pose_tuple, pose_info = min(goal_matching_mse_pose.items(), key=lambda item: item[1]['weighted_mse'])
            # goal['pose'] = torch.tensor(pose_tuple)
            return pose_info['policy']
        return []

    def define_explo_policies(self, manager:object, current_pose:list)-> list:
        ''' 
        get the lookahead and divise it by 2 to stay near agent
        then look all around agent to set goals
        and create policies to reach all those goals
        '''
        search_lookahead = int(manager.get_current_lookahead()/2) #/2 is a random choice
        goals_explo_pose = define_policies_objectives(current_pose, lookahead=search_lookahead, full_exploration=True)
        policies = create_policies(current_pose, goals_explo_pose)
        return policies
#========================= FIND FEATURE IN PRED OB =====================================#

    def allo_match_preferred_features_wt_pose(self,manager:object, preferred_features:np.ndarray)-> dict:
        """
        For each location in memory, check if we find matching pref features. 
        If we do, we stop going through this exp and redo the process on the next exp
        from cloest to furthest to agent pose.
        NOTE: This is highly computationally intensive as it consider all possible poses in each exp
        Could be reduced, it is not to limit code quantity and allow easy reading.
        """
        poses_options_in_this_env = call_env_place_range(self.env_type, reduction = 2)
        exps = manager.get_exps_organised_by_distance_from_exp()

        preferrences_in_memory = {}
        for exp in exps:
            place = manager.torch_sample_place(exp['observation'])
            allo_prediction = manager.several_poses_allocentric_prediction(poses_options_in_this_env, place)
            for pose_idx in range(len(poses_options_in_this_env)):
                pref_pose, pref_likelihood = find_preferred_features_in_img(self.env_type, allo_prediction['image_predicted'][:,pose_idx,...], poses_options_in_this_env[pose_idx], preferred_features)
                if pref_pose is not None:
                    pref_pose_allo_pred = manager.single_pose_allocentric_prediction(pref_pose, place)
                    preferrences_in_memory[exp['id']]={'exp_id': exp['id'], 'pose':pref_pose, 'image':pref_pose_allo_pred['image_predicted'], 'pref_likelihood': pref_likelihood}
                    if self.verbose:
                        print(exp['id'], 'has found a preferred ob at pose', pref_pose, 'that is near the desired feature by:', pref_likelihood, '%') 
                    break #Only 1 pref ob by exp

        return preferrences_in_memory

#========================= FIND PATH TO DESIRED EXP ====================================#

    def apply_djikstra_over_memory(self, manager:object, objective:dict, weigth_on_unconnected_exps:float=100)->list:
        """ 
        Search shortest path using Djikstra between current exp and objective exp
        The djikstra: return a path containing the exps ID from current o objective.

        """
        exps_list = manager.get_all_exps_in_memory(wt_links = True)
        current_exp_id = manager.get_current_exp_id()
        #Get all the exps and their 'distance' between each other
        djikstra_exps_list = dijkstra_weigthed(exps_list, current_exp_id, weigth_on_unconnected_exps)
        #Find the target_exp in this list 
        target_exp_index_in_list = [exp['id'] for exp in djikstra_exps_list].index(objective['exp'])
        target_exp = djikstra_exps_list[target_exp_index_in_list]
        if objective['exp'] != target_exp['id']:
            raise ValueError("goal_id != target id in apply_goal_strategy, IMPLEMENTATION ERROR")
        
        #The first value of path is current exp, the last is the desired exp
        path = dijkstra_shortest_node_path(target_exp, [target_exp['id']])[::-1]
        print('The shortest node path from current to goal exp:  %s' %(path))

        if len(path) <= 1:
            raise ValueError("path " + str(path)+ "is impossible. ")
        
        return path

#========================= FIND NEXT PLACE IN PATH CONNECTED TO CURRENT ================#
    def determine_location_to_reach_in_path(self,manager:object,path:list)->dict:
        """ Search door connection poses between this place and linked exps,
        Then check if any of those exps lead to an ep in desired path
        IF no then we return {}
        Else we return the place to go to in path
        """
        #get all the connected places of current through doors
        linked_exps_info = manager.connect_places_to_current_location()
        if not linked_exps_info:
            print('ERROR in determine_location_to_reach_in_path:The current exp'+ str(manager.get_current_exp_id()) + 'does not link to any other exp' )
            return {}
        
        if self.verbose:
            print('This place is connected to:', linked_exps_info)
        
        #find which linked exp is closest to objective 
        place_index = -1
        for item in path[::-1]:
            for idx, idx_dict in linked_exps_info.items():
                if item == idx_dict['linked_exp_id']:
                    place_index = idx
                    break
            if place_index != -1:
                break

        #This is to push agent toward the exp having closest value to objective...But it's really situational
        # if place_index == -1 and len(linked_exps_info) > 0:
        #     place_index = min(linked_exps_info.keys(), key=lambda i: abs(linked_exps_info[i]['linked_exp_id'] - path[-1]))

        if place_index != -1:
            return linked_exps_info[place_index]
        
        return {}

class Goal_seeking_Minigrid(ExploitativeBehaviour):
    def __init__(self, env_type:str, verbose:bool=True,) :
        ExploitativeBehaviour.__init__(self, verbose)
        self.env_type = env_type

    def go_to_given_place(self, manager:object, door_to_place_to_go:list):
        '''
        given place to go, search policy to reach this pose, 
        then add a bit of a push to pass the corridor up to next place
        '''
        objective_to_reach = self.set_door_pose_as_goal_in_place(manager, door_to_place_to_go)
        print("this is the objective_to_reach", objective_to_reach)
        policy, error = self.go_to_observation_in_place(manager,objective_to_reach)
        if error:
            print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
            print('ERROR: The goal allocentric prediction is incorrect, please implement a solution')
            print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
        #if we are near the objective, we can assume there is been no long 
        #range mis-prediction and go straight ahead of this door pose to pass the
        #corridor to the next place
        if len(policy) < 3 and not error :
            policy = policy + [[1, 0, 0] for _ in range(5)]
            p_idx = len(policy)
        else:
            p_idx = 1
        
        return policy, p_idx

    def set_door_pose_as_goal_in_place(self, manager:object, pose_to_go:list)->dict:
        '''
        Given a goal pose corresponding to a relevant_ob (door), 
        determine where it is IN ROOM
        '''
        door_dict = manager.get_env_relevant_ob()
        door_ob = torch.mean(door_dict['image'], dim=0).squeeze(0)
        current_exp_id = manager.get_current_exp_id()
        sub_goal = self.set_objective(pose_to_go, current_exp_id, door_ob)
        return sub_goal

    #========================= FIND FEATURE IN PRED OB ====================================#
    def ego_match_preferred_features_wt_pose(self,manager:object, preferred_features:np.ndarray):
        """ 
        Search in policies around agent if a predicted observation contains a detail matching
        preferred_features. If so return that pose and corresponding observation, else return None
        """
        pref_goal = {}
        place_description = manager.get_best_place_hypothesis()
        current_pose = place_description['pose'].copy()
        
        policy = [[-1,0,0]]*3
        policy = self.evaluate_go_to_goal_pose_policies(manager, [policy])[0]
        next_exp_ahead = self.passing_from_exp_to_exp(manager, place_description, policy)
        policies = self.define_explo_policies(manager, current_pose)
        for policy in policies:
            policy, ego_prediction = manager.policy_egocentric_prediction(policy)
            if len(policy) == 0:
                continue
            agent_pose = current_pose
            for action_idx in range(len(policy)):
                agent_pose = action_to_pose(policy[action_idx], agent_pose)
                pref_pose, _ = find_preferred_features_in_img(self.env_type, ego_prediction['image_reconstructed'][:,action_idx,...], agent_pose, preferred_features)
                if pref_pose is not None :
                    if not next_exp_ahead:
                        policies_to_pref = create_policies(current_pose, [pref_pose], exploration=False)
                        policies_to_pref = self.evaluate_go_to_goal_pose_policies(manager, policies_to_pref)
                        #We are not checking their plausibility, just if we see a relevant ob meaning a place limit
                        for policy_to_pref in policies_to_pref:
                            next_exp_ahead = self.passing_from_exp_to_exp(manager, place_description, policy_to_pref)
                            if next_exp_ahead:
                                break
                    if next_exp_ahead and next_exp_ahead['id'] >= 0:
                        #If the goal is in another place, then reaching this place is the priority
                        pref_goal = {'exp_id': next_exp_ahead['id']}
                    else:
                        allo_pred_ob = manager.single_pose_allocentric_prediction(pref_pose,place_description['post'])['image_predicted']
                        allo_pred_ob = torch.mean(allo_pred_ob, dim=[0,1])
                        pref_goal = {'pose': pref_pose, 'exp_id': manager.get_current_exp_id(), 'image':allo_pred_ob}
                    #TODO: CONSIDER ALL ORIENTATION SINCE WE SHOULDN'T CARE, currently we consider one at "random"
                    if self.verbose:
                        print("pref_pose",pref_pose," seen at action", action_idx, 'of policy', transform_policy_from_hot_encoded_to_str(policy))
                        print(', '.join([f'{key}: {value}' for key, value in pref_goal.items() if key != 'image']))
                    return pref_goal
        return {}

    def passing_from_exp_to_exp(self, manager, place_description, policy):
        """ we check if this policy leads to another place acording to relevant ob match
        if so we retrieve the next place, next exp id and the connecting door
        """
        
        current_pose = place_description['pose']
        place = place_description['post']
        door_dict = manager.get_env_relevant_ob()

        goal_matching_mse_pose = self.estimate_policy_allo_pred_matching_goal(manager, place, current_pose, policy, door_dict, {})
        print('passing_from_exp_to_exp', goal_matching_mse_pose, 'policy', transform_policy_from_hot_encoded_to_str(policy))
        next_exp_ahead =  {}
        if goal_matching_mse_pose:
            pose_tuple, _ = min(goal_matching_mse_pose.items(), key=lambda item: item[1]['weighted_mse'])
            limit_pose = list(pose_tuple)
            next_exp_ahead['current_exp_limit_pose'] = limit_pose
            next_exp_ahead['place'], next_exp_ahead['id'], next_exp_ahead['limit_pose'] = manager.get_connected_place_info(limit_pose)

        return next_exp_ahead


        